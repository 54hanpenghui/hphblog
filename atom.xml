<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>菜鸟清风</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.hphblog.cn/"/>
  <updated>2020-01-21T01:41:27.788Z</updated>
  <id>https://www.hphblog.cn/</id>
  
  <author>
    <name>清风笑丶</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink运行架构</title>
    <link href="https://www.hphblog.cn/2020/01/20/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/"/>
    <id>https://www.hphblog.cn/2020/01/20/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/</id>
    <published>2020-01-20T03:11:39.000Z</published>
    <updated>2020-01-21T01:41:27.788Z</updated>
    
    <content type="html"><![CDATA[ Flink运行架构学习：<Excerpt in index | 首页摘要><a id="more"></a><p>我们已经知道了IDEA下如何快速搭建一个Flink的项目，让我们来了解一下Flink的架构。在了解架构之前，先熟悉几个名词。</p><h2 id="组件"><a href="#组件" class="headerlink" title="组件"></a>组件</h2><h3 id="JobManager"><a href="#JobManager" class="headerlink" title="JobManager"></a>JobManager</h3><ol><li><p>控制一个应用程序执行的主进程，每个应用程序都会被一个不同的JobManager所控制。</p></li><li><p>JobManager会先接收到应用程序，应用程序包括：作业图(JobGraph)、逻辑数据流图和打包的所有类库和其他资源的Jar包。</p></li><li><p>JobManager会把JobGraph转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph）,包含了所有可以并发执行的任务。</p></li><li><p>JobManager会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器上的slot。一旦获取到足够的资源，就会将执行图分发到真正运行的TaskManager上。</p></li></ol><h3 id="TaskManager"><a href="#TaskManager" class="headerlink" title="TaskManager"></a>TaskManager</h3><ol><li><p>每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。</p></li><li><p>启动后，TaskManager回向资源管理器注册它的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务(tasks)来执行了。</p></li><li><p>在执行过程中，一个TaskManager可以跟其他运行同一个应用程序的TaskManager交换数据。</p></li></ol><h3 id="ResourceManager"><a href="#ResourceManager" class="headerlink" title="ResourceManager"></a>ResourceManager</h3><ol><li><p>负责管理任务管理器(TaskManager)的插槽（slot）,TaskManager插槽是Flink中定义的处理资源单元。</p></li><li><p>Flink为不同的环境和资源管理工具提供了不同的资源管理器，比如yarn，mesos，k8s</p></li><li><p>当jobManager申请插槽资源时，resourceManager会将有空闲的插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足jobManager的请求，它还可以向资源提供平台发起会话，提供启动TaskManager进程的容器。</p></li></ol><h3 id="Dispatcher"><a href="#Dispatcher" class="headerlink" title="Dispatcher"></a>Dispatcher</h3><ol><li><p>可以跨作业运行，它为应用提交提供了rest接口。</p></li><li><p>当一个应用被提交执行时，分发器就会启动并将应用移交给一个jobManager。</p></li><li><p>Dispatcher也会启动一个web UI，用来方便展示和监控作业的执行信息。</p></li><li><p>Dispatcher在架构中可能并不是必须的，这取决于应用提交运行的方式。</p></li></ol><h2 id="YARN-提交"><a href="#YARN-提交" class="headerlink" title="YARN 提交"></a>YARN 提交</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200120120550.png" alt=""></p><p>Flink任务提交后，Client向HDFS上传Flink的Jar包和配置，之后向Yarn ResourceManager提交任务，ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster，ApplicationMaster启动后加载Flink的Jar包和配置构建环境，然后启动JobManager，之后ApplicationMaster向ResourceManager申请资源启动TaskManager，ResourceManager分配Container资源后，由ApplicationMaster通知资源所在节点的NodeManager启动TaskManager，NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager，TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200120112358.png" alt=""></p><p>当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。Client提交任务给JobManager，JobManager调度任务到各个TaskManager执行，TaskManager将心跳和统计信息汇报给JobManager。TaskManager之间以流的形式进行数据的传输，JobManger,TaskManager,Client均为独立的JVM进程。</p><ul><li><strong>Client</strong> 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可），负责接收用户的程序代码，为其创建数据流，将数据流提交给JobManager。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。</li><li><strong>JobManager</strong> 主要负责调度 Job 并协调 Task 做 checkpoint，从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。</li><li><strong>TaskManager</strong> 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。</li></ul>]]></content>
    
    <summary type="html">
    
      Flink运行架构学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="https://www.hphblog.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Flink初识</title>
    <link href="https://www.hphblog.cn/2020/01/05/Flink%E5%88%9D%E8%AF%86/"/>
    <id>https://www.hphblog.cn/2020/01/05/Flink%E5%88%9D%E8%AF%86/</id>
    <published>2020-01-05T14:06:30.000Z</published>
    <updated>2020-01-13T14:30:36.414Z</updated>
    
    <content type="html"><![CDATA[ Flink简介与安装：<Excerpt in index | 首页摘要><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Flink起源于2010~2014的柏林工业大学、柏林洪堡大学、哈索·普拉特纳研究所联名发起的Stratosphere项目，该项目于2014年捐赠给了Apache软件基金会。2014年12月成为Apache软件基金会的顶级项目。</p><p>在德语中Flink表示快速和灵巧。</p><p>Flink  Log：</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200105222031.png" alt=""></p><p>与Spark相比Flink是更加纯粹的流式计算，对于Spark来讲、Spark本质上还是基于批计算、即使是Spark Streaming 也是基于微批次计算。</p><h2 id="快速体验"><a href="#快速体验" class="headerlink" title="快速体验"></a>快速体验</h2><p>安装好Maven执行下面这条命令我们就可以快速开发Flink了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn archetype:generate -DarchetypeGroupId&#x3D;org.apache.flink -DarchetypeArtifactId&#x3D;flink-quickstart-scala -DarchetypeVersion&#x3D;1.7.0  -DarchetypeCatlog&#x3D;local</span><br></pre></td></tr></table></figure><p>这样会生成两个Scala类，流作业和批作业。</p><h3 id="批作业"><a href="#批作业" class="headerlink" title="批作业"></a>批作业</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">package</span> com.hph.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">BatchJob</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// set up the batch execution environment</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">ExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataset= env.readTextFile(<span class="string">"E:\\Words.txt"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">val</span> result =     dataset.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).groupBy(<span class="number">0</span>).sum(<span class="number">1</span>).print()</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果运行出现这种情况，</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106192645.png" alt=""></p><p>我们需要把IDEA中的<img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200112220745.png" alt="">勾选上去。</p><p>再次运行结果如图所示：</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106192820.png" alt=""></p><p>这样我们就轻松的完成了MapReduce中的WordCount。</p><p>文件文本如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Hadoop</span><br><span class="line">Spark</span><br><span class="line">Flink</span><br><span class="line">Flink</span><br><span class="line">Spark</span><br><span class="line">Hadoop</span><br><span class="line">Spark</span><br><span class="line">hphblog</span><br><span class="line">Clickhouse</span><br><span class="line">I love Flink</span><br></pre></td></tr></table></figure><h3 id="流作业"><a href="#流作业" class="headerlink" title="流作业"></a>流作业</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph.flink</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala._</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> <span class="type">StreamData</span>  = env.socketTextStream(<span class="string">"58.87.70.124"</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">var</span> result = <span class="type">StreamData</span>.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    env.execute(<span class="string">"Flink Streaming Scala API Skeleton"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这段代码则是监控hadoop102这台服务器端口为9999的数据信息。</p><p>运行一下</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/FlinkStreamingWordCount.gif" alt=""></p><p>我们轻松的实现了流的有状态统计而且和Spark Streaming 相比Flink 显得更加实时。什么是所谓的状态呢？所谓状态就是计算过程中产生的中间计算结果，每次计算新的数据进入到流式系统中都是基于中间状态结果的基础上进行运算，最终产生正确的统计结果。基于有状态计算的方式最大的优势是不需要将原始数据重新从外部存储中拿出来，从而进行全量计算，因为这种计算方式的代价可能是非常高的。从另一个角度讲，用户无须通过调度和协调各种批量计算工具，从数据仓库中获取数据统计结果，然后再落地存储，这些操作全部都可以基于流式计算完成，可以极大地减轻系统对其他框架的依赖，减少数据计算过程中的时间损耗以及硬件存储。</p><h2 id="集群安装"><a href="#集群安装" class="headerlink" title="集群安装"></a>集群安装</h2><p>由于虚拟机安装过Haoop Spark 所以我们选择安装的时候可以选择安装与我们Hadoop版本匹配的安装包。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf flink-1.7.2-bin-hadoop27-scala_2.11.tgz  -C /opt/module/</span><br></pre></td></tr></table></figure><p>切换到 <code>/opt/module/flink-1.7.2</code>下，执行</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106214003.png" alt=""></p><p>我们可以看到 Flink无需任何配置就可以完成安装，当然这个只是单机版的。访问 hadoop102:8081。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106214157.png" alt=""></p><p>这就是Flink的Web界面。</p><p>那么完全集群模式 集成YARN怎么安装呢。</p><p>对于Flink来说集群安装十分简单。只需要更改<strong>flink-conf.yaml</strong> 和<strong>slave</strong> 文件即可</p><p>修改f<strong>flink-conf.yaml</strong>文件</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106214803.png" alt=""></p><p>修改salav文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop102</span></span><br><span class="line"><span class="attr">hadoop103</span></span><br><span class="line"><span class="attr">hadoop104</span></span><br></pre></td></tr></table></figure><p>修改masters文件</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">hadoop102</span>:<span class="string">8081</span></span><br></pre></td></tr></table></figure><p>同步脚本如下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"><span class="meta">#</span><span class="bash">1 获取输入参数个数，如果没有参数，直接退出</span></span><br><span class="line">pcount=$#</span><br><span class="line">if((pcount==0)); then</span><br><span class="line">echo no args;</span><br><span class="line">exit;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">2 获取文件名称</span></span><br><span class="line">p1=$1</span><br><span class="line">fname=`basename $p1`</span><br><span class="line">echo fname=$fname</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">3 获取上级目录到绝对路径</span></span><br><span class="line">pdir=`cd -P $(dirname $p1); pwd`</span><br><span class="line">echo pdir=$pdir</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">4 获取当前用户名称</span></span><br><span class="line">user=`whoami`</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">5 循环</span></span><br><span class="line">for((host=102; host&lt;105; host++)); do</span><br><span class="line">        echo ------------------- hadoop$host --------------</span><br><span class="line">        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>同步发送flink配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/flink-1.7.2/</span><br></pre></td></tr></table></figure><p>重新启动Flink</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/start-cluster.sh</span><br></pre></td></tr></table></figure><p>打开web界面</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106215411.png" alt=""></p><p>集群安装完成。</p><h2 id="作业提交"><a href="#作业提交" class="headerlink" title="作业提交"></a>作业提交</h2><h3 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h3><p>执行<code>bin/start-scala-shell.sh local</code> 我们就可以进入类似于Spark-shell的界面，这里也出现了可爱的小松鼠</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200113220903.png" alt=""></p><p>在shell中执行以下命令</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//绑定端口数据</span></span><br><span class="line">  <span class="keyword">var</span> dataStream = senv.socketTextStream(<span class="string">"hadoop102"</span>,<span class="number">9999</span>)</span><br><span class="line">    </span><br><span class="line">  <span class="comment">//处理数据</span></span><br><span class="line">  <span class="keyword">import</span>  org.apache.flink.api.scala._</span><br><span class="line">  <span class="keyword">var</span> result =dataStream.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  result.print()</span><br><span class="line">  senv.execute(<span class="string">"Stream Job"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/flinkStreamingJob.gif" alt=""></p><p>Web界面如下</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200113222800.png" alt=""></p><p>看到了Flink的单机版的Job作业调试如此方便。和Spark-shell一样如此友好，下面我们可以尝试一些常规的生产中的经常使用到的Jar包提交的方式 。</p><h3 id="Mavn依赖"><a href="#Mavn依赖" class="headerlink" title="Mavn依赖"></a>Mavn依赖</h3><p>创建Maven项目pom包如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph.flink.<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>FlinkJob<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-streaming-scala_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.4.6<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="comment">&lt;!-- 声明绑定到maven的compile阶段 --&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="流作业-1"><a href="#流作业-1" class="headerlink" title="流作业"></a>流作业</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph.job</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.<span class="type">ParameterTool</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.scala.<span class="type">ExecutionEnvironment</span></span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.scala.<span class="type">StreamExecutionEnvironment</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamJob</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 从外部命令中获取参数</span></span><br><span class="line">    <span class="keyword">val</span> params: <span class="type">ParameterTool</span> =  <span class="type">ParameterTool</span>.fromArgs(args)</span><br><span class="line">    <span class="keyword">val</span> host: <span class="type">String</span> = params.get(<span class="string">"host"</span>)</span><br><span class="line">    <span class="keyword">val</span> port: <span class="type">Int</span> = params.getInt(<span class="string">"port"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建流处理环境</span></span><br><span class="line">    <span class="keyword">val</span> env = <span class="type">StreamExecutionEnvironment</span>.getExecutionEnvironment</span><br><span class="line"></span><br><span class="line">    <span class="comment">//绑定端口数据</span></span><br><span class="line">    <span class="keyword">var</span> dataStream = env.socketTextStream(<span class="string">"hadoop102"</span>,<span class="number">9999</span>)</span><br><span class="line">      </span><br><span class="line">    <span class="comment">//处理数据</span></span><br><span class="line">    <span class="keyword">import</span>  org.apache.flink.api.scala._</span><br><span class="line">    <span class="keyword">var</span> result =dataStream.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).keyBy(<span class="number">0</span>).sum(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line">    env.execute(<span class="string">"Stream Job"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="WebUI-提交"><a href="#WebUI-提交" class="headerlink" title="WebUI 提交"></a>WebUI 提交</h3><p>我们把打好的jar包提交到WebUI上看一下</p><p>提交jar包</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106224233.png" alt=""></p><p>指定一下类名和参数</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106225643.png" alt=""></p><p>提交作业后则会</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200113220036.png" alt=""></p><p>(因为最近电脑出现了问题，这张图今天给大家补上的实在不好意思)</p><p>我们在hadoop102服务器上输入几个字符</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106225805.png" alt=""></p><p>你一定会很好奇结果出现在了哪里，我想你已经猜到了就在Task Manage中这里的TM就相当于干活的人，也就相当于Spark中的Executor。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106230151.png" alt=""></p><p>就这样flink 的流式wordcount就部署起来了。</p><h3 id="命令行提交"><a href="#命令行提交" class="headerlink" title="命令行提交"></a>命令行提交</h3><p>当然我们也可以使用命令行的方式提交作业这样做起来会更酷。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink run -c com.hph.job.StreamJob  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar --host hadoop102 --port 9999</span><br></pre></td></tr></table></figure><p>我们刚才取消掉了那个流式任务现在看一下这个任务</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106231156.png" alt=""></p><p>输入几个数据测试一下</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200106231334.png" alt=""></p><p>TaskManager下我们发现了刚才的输入的数据计算的结果。</p><h3 id="Yarn提交"><a href="#Yarn提交" class="headerlink" title="Yarn提交"></a>Yarn提交</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink run  -m yarn-cluster -c  com.hph.job.StreamJob  /ext/flink0503-1.0-SNAPSHOT.jar  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar --host hadoop102 --port 9999</span><br></pre></td></tr></table></figure><p>然而一直再报出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2020-01-06 23:27:44,167 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster</span><br></pre></td></tr></table></figure><p>这是需要我们调整分配的资源因为虚拟机的资源不够所以导致无法申请到相应的资源</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./flink run -m yarn-cluster -nm FinkStreamWordCount  -c  com.hph.job.StreamJob  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar    -n 1 -s 1 -jm 768 -tm 768  --host hadoop102 --port 9999</span><br></pre></td></tr></table></figure><p>我们把资源调小，在YARN界面上就可以看到</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200107001803.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200107002024.png" alt=""></p><p>点击ApplicationMaster即可进入Flink Web UI</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//%E5%A4%A7%E6%95%B0%E6%8D%AE/Flink/20200107002328.png" alt=""></p><p>输出结果如上所述。</p><p>这样Flink的搭建以及提交作业到Yarn就基本完成了。</p>]]></content>
    
    <summary type="html">
    
      Flink简介与安装：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Flink" scheme="https://www.hphblog.cn/tags/Flink/"/>
    
  </entry>
  
  <entry>
    <title>Spark内核解析3</title>
    <link href="https://www.hphblog.cn/2019/06/10/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%903/"/>
    <id>https://www.hphblog.cn/2019/06/10/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%903/</id>
    <published>2019-06-10T04:37:00.000Z</published>
    <updated>2020-01-12T13:08:27.611Z</updated>
    
    <content type="html"><![CDATA[ Spark的交互流程 - 应用提交：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190610123821.png" alt=""></p><p><code>橙色：提交用户Spark程序</code></p><p>1) 用户提交一个Spark程序，主要的流程如下所示：<br>2) 用户spark-submit脚本提交一个Spark程序，会创建一个ClientEndpoint对象，该对象负责与Master通信交互<br>3) ClientEndpoint向Master发送一个RequestSubmitDriver消息，表示提交用户程序<br>4) Master收到RequestSubmitDriver消息，向ClientEndpoint回复SubmitDriverResponse，表示用户程序已经完成注册<br>ClientEndpoint向Master发送RequestDriverStatus消息，请求Driver状态<br>5) 如果当前用户程序对应的Driver已经启动，则ClientEndpoint直接退出，完成提交用户程序<br>紫色：启动Driver进程<br>当用户提交用户Spark程序后，需要启动Driver来处理用户程序的计算逻辑，完成计算任务，这时Master协调需要启动一个Driver，具体流程如下所示：</p><p>1) Maser内存中维护着用户提交计算的任务Application，每次内存结构变更都会触发调度，向Worker发送LaunchDriver请求<br>2) Worker收到LaunchDriver消息，会启动一个DriverRunner线程去执行LaunchDriver的任务<br>3) DriverRunner线程在Worker上启动一个新的JVM实例，该JVM实例内运行一个Driver进程，该Driver会创建SparkContext对象<br><code>红色：注册Application</code><br>Dirver启动以后，它会创建SparkContext对象，初始化计算过程中必需的基本组件，并向Master注册Application，流程描述如下：</p><p>1) 创建SparkEnv对象，创建并管理一些基本组件<br>2) 创建TaskScheduler，负责Task调度<br>3) 创建StandaloneSchedulerBackend，负责与ClusterManager进行资源协商<br>4) 创建DriverEndpoint，其它组件可以与Driver进行通信<br>5) 在StandaloneSchedulerBackend内部创建一个StandaloneAppClient，负责处理与Master的通信交互<br>6) StandaloneAppClient创建一个ClientEndpoint，实际负责与Master通信<br>7) ClientEndpoint向Master发送RegisterApplication消息，注册Application<br>8) Master收到RegisterApplication请求后，回复ClientEndpoint一个RegisteredApplication消息，表示已经注册成功<br><code>蓝色：启动Executor进程</code><br>1)Master向Worker发送LaunchExecutor消息，请求启动Executor；同时Master会向Driver发送ExecutorAdded消息，表示Master已经新增了一个Executor（此时还未启动）<br>2) Worker收到LaunchExecutor消息，会启动一个ExecutorRunner线程去执行LaunchExecutor的任务<br>3) Worker向Master发送ExecutorStageChanged消息，通知Executor状态已发生变化<br>4) Master向Driver发送ExecutorUpdated消息，此时Executor已经启动<br><code>粉色：启动Task执行</code></p><p>1) StandaloneSchedulerBackend启动一个DriverEndpoint<br>2) DriverEndpoint启动后，会周期性地检查Driver维护的Executor的状态，如果有空闲的Executor便会调度任务执行<br>3) DriverEndpoint向TaskScheduler发送Resource Offer请求<br>4) 如果有可用资源启动Task，则DriverEndpoint向Executor发送LaunchTask请求<br>5) Executor进程内部的CoarseGrainedExecutorBackend调用内部的Executor线程的launchTask方法启动Task<br>6) Executor线程内部维护一个线程池，创建一个TaskRunner线程并提交到线程池执行<br><code>绿色：Task运行完成</code></p><p>1) Executor进程内部的Executor线程通知CoarseGrainedExecutorBackend，Task运行完成<br>2) CoarseGrainedExecutorBackend向DriverEndpoint发送StatusUpdated消息，通知Driver运行的Task状态发生变更<br>3) StandaloneSchedulerBackend调用TaskScheduler的updateStatus方法更新Task状态<br>4) StandaloneSchedulerBackend继续调用TaskScheduler的resourceOffers方法，调度其他任务运行</p><h2 id="应用提交"><a href="#应用提交" class="headerlink" title="应用提交"></a>应用提交</h2><h3 id="SparkSubumit"><a href="#SparkSubumit" class="headerlink" title="SparkSubumit"></a>SparkSubumit</h3><p>提交任务需要用到SparkSubumit这个类我们先看一下<code>main</code>方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args) <span class="comment">//解析参数</span></span><br><span class="line">  <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">    <span class="comment">// scalastyle:off println</span></span><br><span class="line">    printStream.println(appArgs)</span><br><span class="line">    <span class="comment">// scalastyle:on println</span></span><br><span class="line">  &#125;</span><br><span class="line">  appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs) <span class="comment">//提交</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs) <span class="comment">//杀死</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs) <span class="comment">//查看状态</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看一下SparkSubmitArguments里面的参数是什么</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> master: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> deployMode: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> executorMemory: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> executorCores: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> totalExecutorCores: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> propertiesFile: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> driverMemory: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> driverExtraClassPath: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> driverExtraLibraryPath: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> driverExtraJavaOptions: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> queue: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> numExecutors: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> files: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> archives: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> mainClass: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> primaryResource: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> name: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> childArgs: <span class="type">ArrayBuffer</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line"><span class="keyword">var</span> jars: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> packages: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> repositories: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> ivyRepoPath: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> packagesExclusions: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> verbose: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">var</span> isPython: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">var</span> pyFiles: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> isR: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">var</span> action: <span class="type">SparkSubmitAction</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">val</span> sparkProperties: <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line"><span class="keyword">var</span> proxyUser: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> principal: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> keytab: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Standalone cluster mode only</span></span><br><span class="line"><span class="keyword">var</span> supervise: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line"><span class="keyword">var</span> driverCores: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> submissionToKill: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> submissionToRequestStatusFor: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line"><span class="keyword">var</span> useRest: <span class="type">Boolean</span> = <span class="literal">true</span> <span class="comment">// used internally</span></span><br></pre></td></tr></table></figure><p>这些参数在运行的时候我们都可以指定比如</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master yarn \</span><br><span class="line">--deploy-mode cluster \</span><br><span class="line">--driver-memory 1g \</span><br><span class="line">--executor-memory 1g \</span><br><span class="line">--executor-cores 1 \</span><br><span class="line">--queue thequeue \</span><br><span class="line">examples/target/scala-2.11/jars/spark-examples*.jar 10</span><br></pre></td></tr></table></figure><table><thead><tr><th>参数名</th><th>参数说明</th></tr></thead><tbody><tr><td>–master</td><td>master 的地址，提交任务到哪里执行，例如 spark://host:port,  yarn,  local</td></tr><tr><td>–deploy-mode</td><td>在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client</td></tr><tr><td>–class</td><td>应用程序的主类，仅针对 java 或 scala 应用</td></tr><tr><td>–name</td><td>应用程序的名称</td></tr><tr><td>–jars</td><td>用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下</td></tr><tr><td>–packages</td><td>包含在driver 和executor 的 classpath 中的 jar 的 maven 坐标</td></tr><tr><td>–exclude-packages</td><td>为了避免冲突 而指定不包含的 package</td></tr><tr><td>–repositories</td><td>远程 repository</td></tr><tr><td>–conf PROP=VALUE</td><td>指定 spark 配置属性的值， 例如 -conf spark.executor.extraJavaOptions=”-XX:MaxPermSize=256m”</td></tr><tr><td>–properties-file</td><td>加载的配置文件，默认为 conf/spark-defaults.conf</td></tr><tr><td>–driver-memory</td><td>Driver内存，默认 1G</td></tr><tr><td>–driver-java-options</td><td>传给 driver 的额外的 Java 选项</td></tr><tr><td>–driver-library-path</td><td>传给 driver 的额外的库路径</td></tr><tr><td>–driver-class-path</td><td>传给 driver 的额外的类路径</td></tr><tr><td>–driver-cores</td><td>Driver 的核数，默认是1。在 yarn 或者 standalone 下使用</td></tr><tr><td>–executor-memory</td><td>每个 executor 的内存，默认是1G</td></tr><tr><td>–total-executor-cores</td><td>所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用</td></tr><tr><td>–num-executors</td><td>启动的 executor 数量。默认为2。在 yarn 下使用</td></tr><tr><td>–executor-core</td><td>每个 executor 的核数。在yarn或者standalone下使用</td></tr></tbody></table><h3 id="submit"><a href="#submit" class="headerlink" title="submit"></a>submit</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@tailrec</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submit</span></span>(args: <span class="type">SparkSubmitArguments</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//根据参数准备提交应用时所需的环境</span></span><br><span class="line">  <span class="keyword">val</span> (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">doRunMain</span></span>(): <span class="type">Unit</span> = &#123; </span><br><span class="line">    <span class="keyword">if</span> (args.proxyUser != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> proxyUser = <span class="type">UserGroupInformation</span>.createProxyUser(args.proxyUser,</span><br><span class="line">        <span class="type">UserGroupInformation</span>.getCurrentUser())</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        proxyUser.doAs(<span class="keyword">new</span> <span class="type">PrivilegedExceptionAction</span>[<span class="type">Unit</span>]() &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">            runMain(childArgsm, childClasspath, sysProps, childMainClass, args.verbose)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          <span class="comment">// Hadoop's AuthorizationException suppresses the exception's stack trace, which</span></span><br><span class="line">          <span class="comment">// makes the message printed to the output by the JVM not very helpful. Instead,</span></span><br><span class="line">          <span class="comment">// detect exceptions with empty stack traces here, and treat them differently.</span></span><br><span class="line">          <span class="keyword">if</span> (e.getStackTrace().length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="comment">// scalastyle:off println</span></span><br><span class="line">            printStream.println(<span class="string">s"ERROR: <span class="subst">$&#123;e.getClass().getName()&#125;</span>: <span class="subst">$&#123;e.getMessage()&#125;</span>"</span>)</span><br><span class="line">            <span class="comment">// scalastyle:on println</span></span><br><span class="line">            exitFn(<span class="number">1</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> e</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// In standalone cluster mode, there are two submission gateways:</span></span><br><span class="line">   <span class="comment">//   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper</span></span><br><span class="line">   <span class="comment">//   (2) The new REST-based gateway introduced in Spark 1.3</span></span><br><span class="line">   <span class="comment">// The latter is the default behavior as of Spark 1.3, but Spark submit will fail over</span></span><br><span class="line">   <span class="comment">// to use the legacy gateway if the master endpoint turns out to be not a REST server.</span></span><br><span class="line">  <span class="keyword">if</span> (args.isStandaloneCluster &amp;&amp; args.useRest) &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      printStream.println(<span class="string">"Running Spark using the REST application submission protocol."</span>)</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">      doRunMain()  <span class="comment">//调用doRunMain</span></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="comment">// Fail over to use the legacy submission gateway</span></span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">SubmitRestConnectionException</span> =&gt;</span><br><span class="line">        printWarning(<span class="string">s"Master endpoint <span class="subst">$&#123;args.master&#125;</span> was not a REST server. "</span> +</span><br><span class="line">          <span class="string">"Falling back to legacy submission gateway instead."</span>)</span><br><span class="line">        args.useRest = <span class="literal">false</span></span><br><span class="line">        submit(args)</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// In all other modes, just run the main class as prepared</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    doRunMain() <span class="comment">//doRunMain</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="runMain"><a href="#runMain" class="headerlink" title="runMain"></a>runMain</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runMain</span></span>(</span><br><span class="line">     childArgs: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">     childClasspath: <span class="type">Seq</span>[<span class="type">String</span>],</span><br><span class="line">     sysProps: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>],</span><br><span class="line">     childMainClass: <span class="type">String</span>,</span><br><span class="line">     verbose: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// scalastyle:off println</span></span><br><span class="line">   <span class="keyword">if</span> (verbose) &#123; <span class="comment">//如果打开调试 则会输出</span></span><br><span class="line">     printStream.println(<span class="string">s"Main class:\n<span class="subst">$childMainClass</span>"</span>)</span><br><span class="line">     printStream.println(<span class="string">s"Arguments:\n<span class="subst">$&#123;childArgs.mkString("\n")&#125;</span>"</span>)</span><br><span class="line">     printStream.println(<span class="string">s"System properties:\n<span class="subst">$&#123;sysProps.mkString("\n")&#125;</span>"</span>)</span><br><span class="line">     printStream.println(<span class="string">s"Classpath elements:\n<span class="subst">$&#123;childClasspath.mkString("\n")&#125;</span>"</span>)</span><br><span class="line">     printStream.println(<span class="string">"\n"</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// scalastyle:on println</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">val</span> loader =</span><br><span class="line">     <span class="keyword">if</span> (sysProps.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</span><br><span class="line">         <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="keyword">new</span> <span class="type">Array</span>[<span class="type">URL</span>](<span class="number">0</span>),</span><br><span class="line">         <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">     &#125;</span><br><span class="line">   <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> (jar &lt;- childClasspath) &#123;</span><br><span class="line">     addJarToClasspath(jar, loader) <span class="comment">//添加jar包到类路径下</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">for</span> ((key, value) &lt;- sysProps) &#123;</span><br><span class="line">     <span class="type">System</span>.setProperty(key, value) <span class="comment">//sysProps 添加到 System</span></span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">var</span> mainClass: <span class="type">Class</span>[_] = <span class="literal">null</span> </span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">     mainClass = <span class="type">Utils</span>.classForName(childMainClass)  <span class="comment">//利用了反射childMainClass得到mainClass</span></span><br><span class="line">   &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">ClassNotFoundException</span> =&gt; <span class="comment">//异常ClassNotFoundException</span></span><br><span class="line">       e.printStackTrace(printStream)</span><br><span class="line">       <span class="keyword">if</span> (childMainClass.contains(<span class="string">"thriftserver"</span>)) &#123;</span><br><span class="line">         <span class="comment">// scalastyle:off println</span></span><br><span class="line">         printStream.println(<span class="string">s"Failed to load main class <span class="subst">$childMainClass</span>."</span>)</span><br><span class="line">         printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</span><br><span class="line">         <span class="comment">// scalastyle:on println</span></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">NoClassDefFoundError</span> =&gt;</span><br><span class="line">       e.printStackTrace(printStream)</span><br><span class="line">       <span class="keyword">if</span> (e.getMessage.contains(<span class="string">"org/apache/hadoop/hive"</span>)) &#123;</span><br><span class="line">         <span class="comment">// scalastyle:off println</span></span><br><span class="line">         printStream.println(<span class="string">s"Failed to load hive class."</span>)</span><br><span class="line">         printStream.println(<span class="string">"You need to build Spark with -Phive and -Phive-thriftserver."</span>)</span><br><span class="line">         <span class="comment">// scalastyle:on println</span></span><br><span class="line">       &#125;</span><br><span class="line">       <span class="type">System</span>.exit(<span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// SPARK-4170   BUG 尚未解决的  缺陷跟踪</span></span><br><span class="line">   <span class="keyword">if</span> (classOf[scala.<span class="type">App</span>].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">     printWarning(<span class="string">"Subclasses of scala.App may not work correctly. Use a main() method instead."</span>)</span><br><span class="line">   &#125; </span><br><span class="line"><span class="comment">//get 一个  main 方法</span></span><br><span class="line">   <span class="keyword">val</span> mainMethod = mainClass.getMethod(<span class="string">"main"</span>, <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">String</span>](<span class="number">0</span>).getClass)</span><br><span class="line">   <span class="keyword">if</span> (!<span class="type">Modifier</span>.isStatic(mainMethod.getModifiers)) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"The main method in the given main class must be static"</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="meta">@tailrec</span></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">findCause</span></span>(t: <span class="type">Throwable</span>): <span class="type">Throwable</span> = t <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">UndeclaredThrowableException</span> =&gt;</span><br><span class="line">       <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">InvocationTargetException</span> =&gt;</span><br><span class="line">       <span class="keyword">if</span> (e.getCause() != <span class="literal">null</span>) findCause(e.getCause()) <span class="keyword">else</span> e</span><br><span class="line">     <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt;</span><br><span class="line">       e</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">try</span> &#123;</span><br><span class="line">       <span class="comment">//反射执行通过childArgs</span></span><br><span class="line">     mainMethod.invoke(<span class="literal">null</span>, childArgs.toArray)</span><br><span class="line">   &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">       findCause(t) <span class="keyword">match</span> &#123;</span><br><span class="line">         <span class="keyword">case</span> <span class="type">SparkUserAppException</span>(exitCode) =&gt;</span><br><span class="line">           <span class="type">System</span>.exit(exitCode)</span><br><span class="line"></span><br><span class="line">         <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">           <span class="keyword">throw</span> t</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>由于<code>mainClass</code>利用了反射我们需要寻找一下<code>childMainClass</code> 注意观察 submit方法中<code>val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)</code>我们查看一下<code>prepareSubmitEnvironment</code></p><h3 id="prepareSubmitEnvironment"><a href="#prepareSubmitEnvironment" class="headerlink" title="prepareSubmitEnvironment"></a>prepareSubmitEnvironment</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[deploy] <span class="function"><span class="keyword">def</span> <span class="title">prepareSubmitEnvironment</span></span>(args: <span class="type">SparkSubmitArguments</span>)</span><br><span class="line">    : (<span class="type">Seq</span>[<span class="type">String</span>], <span class="type">Seq</span>[<span class="type">String</span>], <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>], <span class="type">String</span>) = &#123;</span><br><span class="line">  <span class="comment">// Return values</span></span><br><span class="line">  <span class="keyword">val</span> childArgs = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">val</span> childClasspath = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">val</span> sysProps = <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">String</span>, <span class="type">String</span>]()</span><br><span class="line">  <span class="keyword">var</span> childMainClass = <span class="string">""</span>  <span class="comment">//目标出现</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Set the cluster manager  </span></span><br><span class="line">  <span class="keyword">val</span> clusterManager: <span class="type">Int</span> = args.master <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"yarn"</span> =&gt; <span class="type">YARN</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">"yarn-client"</span> | <span class="string">"yarn-cluster"</span> =&gt;</span><br><span class="line">      printWarning(<span class="string">s"Master <span class="subst">$&#123;args.master&#125;</span> is deprecated since 2.0."</span> +</span><br><span class="line">        <span class="string">" Please use master \"yarn\" with specified deploy mode instead."</span>)</span><br><span class="line">      <span class="type">YARN</span></span><br><span class="line">    <span class="keyword">case</span> m <span class="keyword">if</span> m.startsWith(<span class="string">"spark"</span>) =&gt; <span class="type">STANDALONE</span></span><br><span class="line">    <span class="keyword">case</span> m <span class="keyword">if</span> m.startsWith(<span class="string">"mesos"</span>) =&gt; <span class="type">MESOS</span></span><br><span class="line">    <span class="keyword">case</span> m <span class="keyword">if</span> m.startsWith(<span class="string">"local"</span>) =&gt; <span class="type">LOCAL</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      printErrorAndExit(<span class="string">"Master must either be yarn or start with spark, mesos, local"</span>)</span><br><span class="line">      <span class="number">-1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Set the deploy mode; default is client mode  设置 deploy 模式</span></span><br><span class="line">  <span class="keyword">var</span> deployMode: <span class="type">Int</span> = args.deployMode <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"client"</span> | <span class="literal">null</span> =&gt; <span class="type">CLIENT</span> <span class="comment">// 如果是NULL 或者 "client" 默认为CLIENT</span></span><br><span class="line">    <span class="keyword">case</span> <span class="string">"cluster"</span> =&gt; <span class="type">CLUSTER</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt; printErrorAndExit(<span class="string">"Deploy mode must be either client or cluster"</span>); <span class="number">-1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Because the deprecated way of specifying "yarn-cluster" and "yarn-client" encapsulate both</span></span><br><span class="line">  <span class="comment">// the master and deploy mode, we have some logic to infer the master and deploy mode</span></span><br><span class="line">  <span class="comment">// from each other if only one is specified, or exit early if they are at odds.</span></span><br><span class="line">  <span class="keyword">if</span> (clusterManager == <span class="type">YARN</span>) &#123;   <span class="comment">//如果是YARN模式</span></span><br><span class="line">    (args.master, args.deployMode) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> (<span class="string">"yarn-cluster"</span>, <span class="literal">null</span>) =&gt;</span><br><span class="line">        deployMode = <span class="type">CLUSTER</span></span><br><span class="line">        args.master = <span class="string">"yarn"</span></span><br><span class="line">      <span class="keyword">case</span> (<span class="string">"yarn-cluster"</span>, <span class="string">"client"</span>) =&gt;</span><br><span class="line">        printErrorAndExit(<span class="string">"Client deploy mode is not compatible with master \"yarn-cluster\""</span>)</span><br><span class="line">      <span class="keyword">case</span> (<span class="string">"yarn-client"</span>, <span class="string">"cluster"</span>) =&gt;</span><br><span class="line">        printErrorAndExit(<span class="string">"Cluster deploy mode is not compatible with master \"yarn-client\""</span>)</span><br><span class="line">      <span class="keyword">case</span> (_, mode) =&gt;</span><br><span class="line">        args.master = <span class="string">"yarn"</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Make sure YARN is included in our build if we're trying to use it</span></span><br><span class="line">    <span class="keyword">if</span> (!<span class="type">Utils</span>.classIsLoadable(<span class="string">"org.apache.spark.deploy.yarn.Client"</span>) &amp;&amp; !<span class="type">Utils</span>.isTesting) &#123;</span><br><span class="line">      printErrorAndExit(</span><br><span class="line">        <span class="string">"Could not load YARN classes. "</span> +</span><br><span class="line">        <span class="string">"This copy of Spark may not have been compiled with YARN support."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Update args.deployMode if it is null. It will be passed down as a Spark property later.</span></span><br><span class="line">  (args.deployMode, deployMode) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> (<span class="literal">null</span>, <span class="type">CLIENT</span>) =&gt; args.deployMode = <span class="string">"client"</span></span><br><span class="line">    <span class="keyword">case</span> (<span class="literal">null</span>, <span class="type">CLUSTER</span>) =&gt; args.deployMode = <span class="string">"cluster"</span></span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> isYarnCluster = clusterManager == <span class="type">YARN</span> &amp;&amp; deployMode == <span class="type">CLUSTER</span></span><br><span class="line">  <span class="keyword">val</span> isMesosCluster = clusterManager == <span class="type">MESOS</span> &amp;&amp; deployMode == <span class="type">CLUSTER</span></span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">   .....</span><br><span class="line">  <span class="comment">// A list of rules to map each argument to system properties or command-line options in</span></span><br><span class="line">  <span class="comment">// each deploy mode; we iterate through these below </span></span><br><span class="line">  <span class="keyword">val</span> options = <span class="type">List</span>[<span class="type">OptionAssigner</span>]( <span class="comment">// 整理一下 options 把参数聚集到了一块</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// All cluster managers</span></span><br><span class="line">    <span class="type">OptionAssigner</span>(args.master, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.master"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.deployMode, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.submit.deployMode"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.name, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.app.name"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.ivyRepoPath, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">CLIENT</span>, sysProp = <span class="string">"spark.jars.ivy"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverMemory, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">CLIENT</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.memory"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverExtraClassPath, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.extraClassPath"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverExtraJavaOptions, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.extraJavaOptions"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverExtraLibraryPath, <span class="type">ALL_CLUSTER_MGRS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.extraLibraryPath"</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Yarn only</span></span><br><span class="line">    <span class="type">OptionAssigner</span>(args.queue, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.queue"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.numExecutors, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.executor.instances"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.jars, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.dist.jars"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.files, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.dist.files"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.archives, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.dist.archives"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.principal, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.principal"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.keytab, <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.yarn.keytab"</span>),</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Other options</span></span><br><span class="line">    <span class="type">OptionAssigner</span>(args.executorCores, <span class="type">STANDALONE</span> | <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.executor.cores"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.executorMemory, <span class="type">STANDALONE</span> | <span class="type">MESOS</span> | <span class="type">YARN</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.executor.memory"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.totalExecutorCores, <span class="type">STANDALONE</span> | <span class="type">MESOS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.cores.max"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.files, <span class="type">LOCAL</span> | <span class="type">STANDALONE</span> | <span class="type">MESOS</span>, <span class="type">ALL_DEPLOY_MODES</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.files"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.jars, <span class="type">LOCAL</span>, <span class="type">CLIENT</span>, sysProp = <span class="string">"spark.jars"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.jars, <span class="type">STANDALONE</span> | <span class="type">MESOS</span>, <span class="type">ALL_DEPLOY_MODES</span>, sysProp = <span class="string">"spark.jars"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverMemory, <span class="type">STANDALONE</span> | <span class="type">MESOS</span> | <span class="type">YARN</span>, <span class="type">CLUSTER</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.memory"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.driverCores, <span class="type">STANDALONE</span> | <span class="type">MESOS</span> | <span class="type">YARN</span>, <span class="type">CLUSTER</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.cores"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.supervise.toString, <span class="type">STANDALONE</span> | <span class="type">MESOS</span>, <span class="type">CLUSTER</span>,</span><br><span class="line">      sysProp = <span class="string">"spark.driver.supervise"</span>),</span><br><span class="line">    <span class="type">OptionAssigner</span>(args.ivyRepoPath, <span class="type">STANDALONE</span>, <span class="type">CLUSTER</span>, sysProp = <span class="string">"spark.jars.ivy"</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In client mode, launch the application main class directly</span></span><br><span class="line">  <span class="comment">// In addition, add the main application jar and any added jars (if any) to the classpath</span></span><br><span class="line">  <span class="comment">// Also add the main application jar and any added jars to classpath in case YARN client</span></span><br><span class="line">  <span class="comment">// requires these jars.</span></span><br><span class="line">  <span class="keyword">if</span> (deployMode == <span class="type">CLIENT</span> || isYarnCluster) &#123;</span><br><span class="line">    childMainClass = args.mainClass</span><br><span class="line">    <span class="keyword">if</span> (isUserJar(args.primaryResource)) &#123;</span><br><span class="line">      childClasspath += args.primaryResource</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (args.jars != <span class="literal">null</span>) &#123; childClasspath ++= args.jars.split(<span class="string">","</span>) &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (deployMode == <span class="type">CLIENT</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.childArgs != <span class="literal">null</span>) &#123; childArgs ++= args.childArgs &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Map all arguments to command-line options or system properties for our chosen mode</span></span><br><span class="line">  <span class="keyword">for</span> (opt &lt;- options) &#123;</span><br><span class="line">    <span class="keyword">if</span> (opt.value != <span class="literal">null</span> &amp;&amp;</span><br><span class="line">        (deployMode &amp; opt.deployMode) != <span class="number">0</span> &amp;&amp;</span><br><span class="line">        (clusterManager &amp; opt.clusterManager) != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (opt.clOption != <span class="literal">null</span>) &#123; childArgs += (opt.clOption, opt.value) &#125;</span><br><span class="line">      <span class="keyword">if</span> (opt.sysProp != <span class="literal">null</span>) &#123; sysProps.put(opt.sysProp, opt.value) &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Add the application jar automatically so the user doesn't have to call sc.addJar</span></span><br><span class="line">  <span class="comment">// For YARN cluster mode, the jar is already distributed on each node as "app.jar"</span></span><br><span class="line">  <span class="comment">// For python and R files, the primary resource is already distributed as a regular file</span></span><br><span class="line">  <span class="keyword">if</span> (!isYarnCluster &amp;&amp; !args.isPython &amp;&amp; !args.isR) &#123;</span><br><span class="line">    <span class="keyword">var</span> jars = sysProps.get(<span class="string">"spark.jars"</span>).map(x =&gt; x.split(<span class="string">","</span>).toSeq).getOrElse(<span class="type">Seq</span>.empty)</span><br><span class="line">    <span class="keyword">if</span> (isUserJar(args.primaryResource)) &#123;</span><br><span class="line">      jars = jars ++ <span class="type">Seq</span>(args.primaryResource)</span><br><span class="line">    &#125;</span><br><span class="line">    sysProps.put(<span class="string">"spark.jars"</span>, jars.mkString(<span class="string">","</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In standalone cluster mode, use the REST client to submit the application (Spark 1.3+).</span></span><br><span class="line">  <span class="comment">// All Spark parameters are expected to be passed to the client through system properties.</span></span><br><span class="line">  <span class="keyword">if</span> (args.isStandaloneCluster) &#123; <span class="comment">//如果使用的是Standalone集群模式</span></span><br><span class="line">    <span class="keyword">if</span> (args.useRest) &#123;  <span class="comment">//如果用了useRest</span></span><br><span class="line">      childMainClass = <span class="string">"org.apache.spark.deploy.rest.RestSubmissionClient"</span> </span><br><span class="line">      childArgs += (args.primaryResource, args.mainClass)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123; <span class="comment">//如果没有用如useRest</span></span><br><span class="line">      <span class="comment">// In legacy standalone cluster mode, use Client as a wrapper around the user class</span></span><br><span class="line">      childMainClass = <span class="string">"org.apache.spark.deploy.Client"</span>  <span class="comment">//这个类使用到了Client 类</span></span><br><span class="line">      <span class="keyword">if</span> (args.supervise) &#123; childArgs += <span class="string">"--supervise"</span> &#125;</span><br><span class="line">      <span class="type">Option</span>(args.driverMemory).foreach &#123; m =&gt; childArgs += (<span class="string">"--memory"</span>, m) &#125;</span><br><span class="line">      <span class="type">Option</span>(args.driverCores).foreach &#123; c =&gt; childArgs += (<span class="string">"--cores"</span>, c) &#125;</span><br><span class="line">      childArgs += <span class="string">"launch"</span></span><br><span class="line">      childArgs += (args.master, args.primaryResource, args.mainClass)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (args.childArgs != <span class="literal">null</span>) &#123;</span><br><span class="line">      childArgs ++= args.childArgs</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Let YARN know it's a pyspark app, so it distributes needed libraries.</span></span><br><span class="line">  <span class="keyword">if</span> (clusterManager == <span class="type">YARN</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.isPython) &#123;</span><br><span class="line">      sysProps.put(<span class="string">"spark.yarn.isPython"</span>, <span class="string">"true"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (args.pyFiles != <span class="literal">null</span>) &#123;</span><br><span class="line">      sysProps(<span class="string">"spark.submit.pyFiles"</span>) = args.pyFiles</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// assure a keytab is available from any place in a JVM</span></span><br><span class="line">  <span class="keyword">if</span> (clusterManager == <span class="type">YARN</span> || clusterManager == <span class="type">LOCAL</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (args.principal != <span class="literal">null</span>) &#123;</span><br><span class="line">      require(args.keytab != <span class="literal">null</span>, <span class="string">"Keytab must be specified when principal is specified"</span>)</span><br><span class="line">      <span class="keyword">if</span> (!<span class="keyword">new</span> <span class="type">File</span>(args.keytab).exists()) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Keytab file: <span class="subst">$&#123;args.keytab&#125;</span> does not exist"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Add keytab and principal configurations in sysProps to make them available</span></span><br><span class="line">        <span class="comment">// for later use; e.g. in spark sql, the isolated class loader used to talk</span></span><br><span class="line">        <span class="comment">// to HiveMetastore will use these settings. They will be set as Java system</span></span><br><span class="line">        <span class="comment">// properties and then loaded by SparkConf</span></span><br><span class="line">        sysProps.put(<span class="string">"spark.yarn.keytab"</span>, args.keytab)</span><br><span class="line">        sysProps.put(<span class="string">"spark.yarn.principal"</span>, args.principal)</span><br><span class="line"></span><br><span class="line">        <span class="type">UserGroupInformation</span>.loginUserFromKeytab(args.principal, args.keytab)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// In yarn-cluster mode, use yarn.Client as a wrapper around the user class</span></span><br><span class="line">  <span class="keyword">if</span> (isYarnCluster) &#123;</span><br><span class="line">    childMainClass = <span class="string">"org.apache.spark.deploy.yarn.Client"</span>  <span class="comment">//我们重点关注一下</span></span><br><span class="line">    <span class="keyword">if</span> (args.isPython) &#123;</span><br><span class="line">      childArgs += (<span class="string">"--primary-py-file"</span>, args.primaryResource)</span><br><span class="line">      childArgs += (<span class="string">"--class"</span>, <span class="string">"org.apache.spark.deploy.PythonRunner"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (args.isR) &#123;</span><br><span class="line">      <span class="keyword">val</span> mainFile = <span class="keyword">new</span> <span class="type">Path</span>(args.primaryResource).getName</span><br><span class="line">      childArgs += (<span class="string">"--primary-r-file"</span>, mainFile)</span><br><span class="line">      childArgs += (<span class="string">"--class"</span>, <span class="string">"org.apache.spark.deploy.RRunner"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">if</span> (args.primaryResource != <span class="type">SparkLauncher</span>.<span class="type">NO_RESOURCE</span>) &#123;</span><br><span class="line">        childArgs += (<span class="string">"--jar"</span>, args.primaryResource)</span><br><span class="line">      &#125;</span><br><span class="line">      childArgs += (<span class="string">"--class"</span>, args.mainClass)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (args.childArgs != <span class="literal">null</span>) &#123;</span><br><span class="line">      args.childArgs.foreach &#123; arg =&gt; childArgs += (<span class="string">"--arg"</span>, arg) &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isMesosCluster) &#123;</span><br><span class="line">   .....</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Load any properties specified through --conf and the default properties file</span></span><br><span class="line">  <span class="keyword">for</span> ((k, v) &lt;- args.sparkProperties) &#123;</span><br><span class="line">    sysProps.getOrElseUpdate(k, v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Ignore invalid spark.driver.host in cluster modes.</span></span><br><span class="line">  <span class="keyword">if</span> (deployMode == <span class="type">CLUSTER</span>) &#123;</span><br><span class="line">    sysProps -= <span class="string">"spark.driver.host"</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Resolve paths in certain spark properties</span></span><br><span class="line">  <span class="keyword">val</span> pathConfigs = <span class="type">Seq</span>(</span><br><span class="line">    <span class="string">"spark.jars"</span>,</span><br><span class="line">    <span class="string">"spark.files"</span>,</span><br><span class="line">    <span class="string">"spark.yarn.dist.files"</span>,</span><br><span class="line">    <span class="string">"spark.yarn.dist.archives"</span>,</span><br><span class="line">    <span class="string">"spark.yarn.dist.jars"</span>)</span><br><span class="line">  pathConfigs.foreach &#123; config =&gt;</span><br><span class="line">    <span class="comment">// Replace old URIs with resolved URIs, if they exist</span></span><br><span class="line">    sysProps.get(config).foreach &#123; oldValue =&gt;</span><br><span class="line">      sysProps(config) = <span class="type">Utils</span>.resolveURIs(oldValue)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Resolve and format python file paths properly before adding them to the PYTHONPATH.</span></span><br><span class="line">  <span class="comment">// The resolving part is redundant in the case of --py-files, but necessary if the user</span></span><br><span class="line">  <span class="comment">// explicitly sets `spark.submit.pyFiles` in his/her default properties file.</span></span><br><span class="line">  sysProps.get(<span class="string">"spark.submit.pyFiles"</span>).foreach &#123; pyFiles =&gt;</span><br><span class="line">    <span class="keyword">val</span> resolvedPyFiles = <span class="type">Utils</span>.resolveURIs(pyFiles)</span><br><span class="line">    <span class="keyword">val</span> formattedPyFiles = <span class="keyword">if</span> (!isYarnCluster &amp;&amp; !isMesosCluster) &#123;</span><br><span class="line">      <span class="type">PythonRunner</span>.formatPaths(resolvedPyFiles).mkString(<span class="string">","</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Ignoring formatting python path in yarn and mesos cluster mode, these two modes</span></span><br><span class="line">      <span class="comment">// support dealing with remote python files, they could distribute and add python files</span></span><br><span class="line">      <span class="comment">// locally.</span></span><br><span class="line">      resolvedPyFiles</span><br><span class="line">    &#125;</span><br><span class="line">    sysProps(<span class="string">"spark.submit.pyFiles"</span>) = formattedPyFiles</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  (childArgs, childClasspath, sysProps, childMainClass)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><p>反射执行的main方法应该是Client的main方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Client</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// scalastyle:off println</span></span><br><span class="line">    <span class="keyword">if</span> (!sys.props.contains(<span class="string">"SPARK_SUBMIT"</span>)) &#123;</span><br><span class="line">      println(<span class="string">"WARNING: This client is deprecated and will be removed in a future version of Spark"</span>) <span class="comment">//未来client 可能会被弃用</span></span><br><span class="line">      println(<span class="string">"Use ./bin/spark-submit with \"--master spark://host:port\""</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// scalastyle:on println</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">    <span class="keyword">val</span> driverArgs = <span class="keyword">new</span> <span class="type">ClientArguments</span>(args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!conf.contains(<span class="string">"spark.rpc.askTimeout"</span>)) &#123;</span><br><span class="line">      conf.set(<span class="string">"spark.rpc.askTimeout"</span>, <span class="string">"10s"</span>) <span class="comment">//设置rpc的超时时间</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Logger</span>.getRootLogger.setLevel(driverArgs.logLevel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rpcEnv =  <span class="comment">//创建 rpcEnv   需要主动和Master 发消息</span></span><br><span class="line">      <span class="type">RpcEnv</span>.create(<span class="string">"driverClient"</span>, <span class="type">Utils</span>.localHostName(), <span class="number">0</span>, conf, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf)) </span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> masterEndpoints = driverArgs.masters.map(<span class="type">RpcAddress</span>.fromSparkURL). <span class="comment">//创建masterEndpoints </span></span><br><span class="line">      map(rpcEnv.setupEndpointRef(_, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)) <span class="comment">//获取master的Ref</span></span><br><span class="line">      <span class="comment">//给自己启动一个ClientEndpoint</span></span><br><span class="line">    rpcEnv.setupEndpoint(<span class="string">"client"</span>, <span class="keyword">new</span> <span class="type">ClientEndpoint</span>(rpcEnv, driverArgs, masterEndpoints, conf))</span><br><span class="line"></span><br><span class="line">    rpcEnv.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ClientEndpoint"><a href="#ClientEndpoint" class="headerlink" title="ClientEndpoint"></a>ClientEndpoint</h3><p>我们需要查看一下ClientEndpoint</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ClientEndpoint</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    driverArgs: <span class="type">ClientArguments</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masterEndpoints: <span class="type">Seq</span>[<span class="type">RpcEndpointRef</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ThreadSafeRpcEndpoint</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;  <span class="comment">//继承了ThreadSafeRpcEndpoint 我们关注一下具体方法</span></span><br><span class="line">      ....</span><br><span class="line">      </span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    driverArgs.cmd <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"launch"</span> =&gt; <span class="comment">//是launch</span></span><br><span class="line">        <span class="comment">// <span class="doctag">TODO:</span> We could add an env variable here and intercept it in `sc.addJar` that would</span></span><br><span class="line">        <span class="comment">//       truncate filesystem paths similar to what YARN does. For now, we just require</span></span><br><span class="line">        <span class="comment">//       people call `addJar` assuming the jar is in the same directory.</span></span><br><span class="line">        <span class="keyword">val</span> mainClass = <span class="string">"org.apache.spark.deploy.worker.DriverWrapper"</span>  <span class="comment">//图中紫色的DriverWorker</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> classPathConf = <span class="string">"spark.driver.extraClassPath"</span></span><br><span class="line">        <span class="keyword">val</span> classPathEntries = sys.props.get(classPathConf).toSeq.flatMap &#123; cp =&gt;</span><br><span class="line">          cp.split(java.io.<span class="type">File</span>.pathSeparator)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> libraryPathConf = <span class="string">"spark.driver.extraLibraryPath"</span></span><br><span class="line">        <span class="keyword">val</span> libraryPathEntries = sys.props.get(libraryPathConf).toSeq.flatMap &#123; cp =&gt;</span><br><span class="line">          cp.split(java.io.<span class="type">File</span>.pathSeparator)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> extraJavaOptsConf = <span class="string">"spark.driver.extraJavaOptions"</span></span><br><span class="line">        <span class="keyword">val</span> extraJavaOpts = sys.props.get(extraJavaOptsConf)</span><br><span class="line">          .map(<span class="type">Utils</span>.splitCommandString).getOrElse(<span class="type">Seq</span>.empty)</span><br><span class="line">        <span class="keyword">val</span> sparkJavaOpts = <span class="type">Utils</span>.sparkJavaOpts(conf)</span><br><span class="line">        <span class="keyword">val</span> javaOpts = sparkJavaOpts ++ extraJavaOpts</span><br><span class="line">        <span class="keyword">val</span> command = <span class="keyword">new</span> <span class="type">Command</span>(mainClass,  <span class="comment">//command是一个case Class 这段代码相当于组装进去了配置</span></span><br><span class="line">          <span class="type">Seq</span>(<span class="string">"&#123;&#123;WORKER_URL&#125;&#125;"</span>, <span class="string">"&#123;&#123;USER_JAR&#125;&#125;"</span>, driverArgs.mainClass) ++ driverArgs.driverOptions,</span><br><span class="line">          sys.env, classPathEntries, libraryPathEntries, javaOpts)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> driverDescription = <span class="keyword">new</span> <span class="type">DriverDescription</span>( <span class="comment">//创建了DriverDescription的描述</span></span><br><span class="line">          driverArgs.jarUrl,</span><br><span class="line">          driverArgs.memory, <span class="comment">//Driver内存小慎用Collect会把Executor所有的数据load进Driver的JVM中</span></span><br><span class="line">          driverArgs.cores,</span><br><span class="line">          driverArgs.supervise,</span><br><span class="line">          command)</span><br><span class="line">        ayncSendToMasterAndForwardReply[<span class="type">SubmitDriverResponse</span>]( <span class="comment">//异步地发送给Master Reply</span></span><br><span class="line">          <span class="type">RequestSubmitDriver</span>(driverDescription)) <span class="comment">//发送driverDescription信息</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"kill"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> driverId = driverArgs.driverId</span><br><span class="line">        ayncSendToMasterAndForwardReply[<span class="type">KillDriverResponse</span>](<span class="type">RequestKillDriver</span>(driverId))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"> ....</span><br><span class="line">   <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SubmitDriverResponse</span>(master, success, driverId, message) =&gt;</span><br><span class="line">      logInfo(message)</span><br><span class="line">      <span class="keyword">if</span> (success) &#123;</span><br><span class="line">        activeMasterEndpoint = master</span><br><span class="line">        pollAndReportStatus(driverId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="type">Utils</span>.responseFromBackup(message)) &#123;</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">KillDriverResponse</span>(master, driverId, success, message) =&gt;</span><br><span class="line">      logInfo(message)</span><br><span class="line">      <span class="keyword">if</span> (success) &#123;</span><br><span class="line">        activeMasterEndpoint = master</span><br><span class="line">        pollAndReportStatus(driverId)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="type">Utils</span>.responseFromBackup(message)) &#123;</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="回到Master"><a href="#回到Master" class="headerlink" title="回到Master"></a>回到Master</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;     </span><br><span class="line">...</span><br><span class="line">   <span class="keyword">case</span> <span class="type">RequestSubmitDriver</span>(description) =&gt;  </span><br><span class="line">     <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123; <span class="comment">//如果没有存活</span></span><br><span class="line">       <span class="keyword">val</span> msg = <span class="string">s"<span class="subst">$&#123;Utils.BACKUP_STANDALONE_MASTER_PREFIX&#125;</span>: <span class="subst">$state</span>. "</span> +</span><br><span class="line">         <span class="string">"Can only accept driver submissions in ALIVE state."</span></span><br><span class="line">       context.reply(<span class="type">SubmitDriverResponse</span>(self, <span class="literal">false</span>, <span class="type">None</span>, msg)) <span class="comment">//设置为fasle</span></span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       logInfo(<span class="string">"Driver submitted "</span> + description.command.mainClass)</span><br><span class="line">       <span class="keyword">val</span> driver = createDriver(description) <span class="comment">//创建一个driver</span></span><br><span class="line">       persistenceEngine.addDriver(driver)</span><br><span class="line">       waitingDrivers += driver  <span class="comment">//将driver添加到waitingDrivers:集群不止一个jar在提交还有其它地,</span></span><br><span class="line">       drivers.add(driver)</span><br><span class="line">       schedule()</span><br><span class="line"></span><br><span class="line">       <span class="comment">// <span class="doctag">TODO:</span> It might be good to instead have the submission client poll the master to determine</span></span><br><span class="line">       <span class="comment">//       the current status of the driver. For now it's simply "fire and forget".</span></span><br><span class="line"></span><br><span class="line">       context.reply(<span class="type">SubmitDriverResponse</span>(self, <span class="literal">true</span>, <span class="type">Some</span>(driver.id), <span class="comment">//返回给Client</span></span><br><span class="line">         <span class="string">s"Driver successfully submitted as <span class="subst">$&#123;driver.id&#125;</span>"</span>))</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createDriver</span></span>(desc: <span class="type">DriverDescription</span>): <span class="type">DriverInfo</span> = &#123;</span><br><span class="line">   <span class="keyword">val</span> now = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">   <span class="keyword">val</span> date = <span class="keyword">new</span> <span class="type">Date</span>(now)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">DriverInfo</span>(now, newDriverId(date), desc, date)  <span class="comment">//创建了一个DriverInfo</span></span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="回到Clinet"><a href="#回到Clinet" class="headerlink" title="回到Clinet"></a>回到Clinet</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">SubmitDriverResponse</span>(master, success, driverId, message) =&gt;</span><br><span class="line">    logInfo(message) </span><br><span class="line">    <span class="keyword">if</span> (success) &#123; <span class="comment">//如果成功了</span></span><br><span class="line">      activeMasterEndpoint = master</span><br><span class="line">      pollAndReportStatus(driverId.get) <span class="comment">//调用此方法</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (!<span class="type">Utils</span>.responseFromBackup(message)) &#123;</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="pollAndReportStatus"><a href="#pollAndReportStatus" class="headerlink" title="pollAndReportStatus"></a>pollAndReportStatus</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pollAndReportStatus</span></span>(driverId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// Since ClientEndpoint is the only RpcEndpoint in the process, blocking the event loop thread</span></span><br><span class="line">  <span class="comment">// is fine.</span></span><br><span class="line">  logInfo(<span class="string">"... waiting before polling master for driver state"</span>)</span><br><span class="line">  <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line">  logInfo(<span class="string">"... polling master for driver state"</span>)</span><br><span class="line">  <span class="keyword">val</span> statusResponse = <span class="comment">//如果没有发送成功   重试</span></span><br><span class="line">    activeMasterEndpoint.askWithRetry[<span class="type">DriverStatusResponse</span>](<span class="type">RequestDriverStatus</span>(driverId))</span><br><span class="line">  <span class="keyword">if</span> (statusResponse.found) &#123; <span class="comment">//如果 没有问题</span></span><br><span class="line">    logInfo(<span class="string">s"State of <span class="subst">$driverId</span> is <span class="subst">$&#123;statusResponse.state.get&#125;</span>"</span>)</span><br><span class="line">    <span class="comment">// Worker node, if present</span></span><br><span class="line">    (statusResponse.workerId, statusResponse.workerHostPort, statusResponse.state) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> (<span class="type">Some</span>(id), <span class="type">Some</span>(hostPort), <span class="type">Some</span>(<span class="type">DriverState</span>.<span class="type">RUNNING</span>)) =&gt;</span><br><span class="line">        logInfo(<span class="string">s"Driver running on <span class="subst">$hostPort</span> (<span class="subst">$id</span>)"</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Exception, if present</span></span><br><span class="line">    statusResponse.exception <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(e) =&gt;</span><br><span class="line">        logError(<span class="string">s"Exception from cluster was: <span class="subst">$e</span>"</span>)</span><br><span class="line">        e.printStackTrace()</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">0</span>)  <span class="comment">//退出</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123; <span class="comment">//如果错误</span></span><br><span class="line">    logError(<span class="string">s"ERROR: Cluster master did not recognize <span class="subst">$driverId</span>"</span>)</span><br><span class="line">    <span class="type">System</span>.exit(<span class="number">-1</span>)  <span class="comment">//错误退出</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到Master-1"><a href="#回到Master-1" class="headerlink" title="回到Master"></a>回到Master</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterWorker</span>(</span><br><span class="line">        id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Registering worker %s:%d with %d cores, %s RAM"</span>.format(</span><br><span class="line">        workerHost, workerPort, cores, <span class="type">Utils</span>.megabytesToString(memory)))</span><br><span class="line">      <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">STANDBY</span>) &#123;</span><br><span class="line">        context.reply(<span class="type">MasterInStandby</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (idToWorker.contains(id)) &#123;</span><br><span class="line">        context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Duplicate worker ID"</span>))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> worker = <span class="keyword">new</span> <span class="type">WorkerInfo</span>(id, workerHost, workerPort, cores, memory,</span><br><span class="line">          workerRef, workerWebUiUrl)</span><br><span class="line">        <span class="keyword">if</span> (registerWorker(worker)) &#123;</span><br><span class="line">          persistenceEngine.addWorker(worker)</span><br><span class="line">          context.reply(<span class="type">RegisteredWorker</span>(self, masterWebUiUrl))</span><br><span class="line">          schedule()  <span class="comment">// 调用了schedule()方法</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> workerAddress = worker.endpoint.address</span><br><span class="line">          logWarning(<span class="string">"Worker registration failed. Attempted to re-register worker at same "</span> +</span><br><span class="line">            <span class="string">"address: "</span> + workerAddress)</span><br><span class="line">          context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Attempted to re-register worker at same address: "</span></span><br><span class="line">            + workerAddress))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><h3 id="schedule"><a href="#schedule" class="headerlink" title="schedule"></a>schedule</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Schedule the currently available resources among waiting apps. This method will be called</span></span><br><span class="line"><span class="comment">   * every time a new app joins or resource availability changes.</span></span><br><span class="line"><span class="comment">   */</span>  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">schedule</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;  <span class="comment">//如果当前地状态不是ALIVE</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Drivers take strict precedence over executors</span></span><br><span class="line">    <span class="keyword">val</span> shuffledAliveWorkers = <span class="type">Random</span>.shuffle(workers.toSeq.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>))</span><br><span class="line">    <span class="keyword">val</span> numWorkersAlive = shuffledAliveWorkers.size</span><br><span class="line">    <span class="keyword">var</span> curPos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> (driver &lt;- waitingDrivers.toList) &#123; <span class="comment">// iterate over a copy of waitingDrivers</span></span><br><span class="line">      <span class="comment">// We assign workers to each waiting driver in a round-robin fashion. For each driver, we</span></span><br><span class="line">      <span class="comment">// start from the last worker that was assigned a driver, and continue onwards until we have</span></span><br><span class="line">      <span class="comment">// explored all alive workers.</span></span><br><span class="line">      <span class="keyword">var</span> launched = <span class="literal">false</span>   <span class="comment">//launched为false</span></span><br><span class="line">      <span class="keyword">var</span> numWorkersVisited = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123;</span><br><span class="line">        <span class="keyword">val</span> worker = shuffledAliveWorkers(curPos)</span><br><span class="line">        numWorkersVisited += <span class="number">1</span>   <span class="comment">//如果worker中的内存.CPU,</span></span><br><span class="line">        <span class="keyword">if</span> (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123;</span><br><span class="line">          launchDriver(worker, driver)  <span class="comment">//调用launchDriver</span></span><br><span class="line">          waitingDrivers -= driver    <span class="comment">//将这个driver从 waitingDrivers 中去掉</span></span><br><span class="line">          launched = <span class="literal">true</span>    <span class="comment">//设置 launched 为true  </span></span><br><span class="line">        &#125;                       <span class="comment">//现在控制权应该走到了launchDriver</span></span><br><span class="line">        curPos = (curPos + <span class="number">1</span>) % numWorkersAlive</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    startExecutorsOnWorkers()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="launchDriver"><a href="#launchDriver" class="headerlink" title="launchDriver"></a>launchDriver</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchDriver</span></span>(worker: <span class="type">WorkerInfo</span>, driver: <span class="type">DriverInfo</span>) &#123;</span><br><span class="line">  logInfo(<span class="string">"Launching driver "</span> + driver.id + <span class="string">" on worker "</span> + worker.id) </span><br><span class="line">  worker.addDriver(driver) <span class="comment">//这个worker在WorkerInfo中把当前的driver加上 Master可以知道那个Application运行在你的节点上</span></span><br><span class="line">  driver.worker = <span class="type">Some</span>(worker)</span><br><span class="line"> worker.endpoint.send(<span class="type">LaunchDriver</span>(driver.id, driver.desc))  <span class="comment">// 这个时候控制权应该在在Worker</span></span><br><span class="line">  driver.state = <span class="type">DriverState</span>.<span class="type">RUNNING</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到Worker"><a href="#回到Worker" class="headerlink" title="回到Worker"></a>回到Worker</h3><h3 id="LaunchDriver"><a href="#LaunchDriver" class="headerlink" title="LaunchDriver"></a>LaunchDriver</h3><p>DriverRunner包装了一下信息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">LaunchDriver</span>(driverId, driverDesc) =&gt; <span class="comment">// Client中的</span></span><br><span class="line">    logInfo(<span class="string">s"Asked to launch driver <span class="subst">$driverId</span>"</span>) <span class="type">DriverRunner</span></span><br><span class="line">    <span class="keyword">val</span> driver = <span class="keyword">new</span> <span class="type">DriverRunner</span>( <span class="comment">// new 了一个 </span></span><br><span class="line">      conf,</span><br><span class="line">      driverId,</span><br><span class="line">      workDir,</span><br><span class="line">      sparkHome, </span><br><span class="line">      driverDesc.copy(command = <span class="type">Worker</span>.maybeUpdateSSLSettings(driverDesc.command, conf)), <span class="comment">//copy一下</span></span><br><span class="line">      self,</span><br><span class="line">      workerUri,</span><br><span class="line">      securityMgr)</span><br><span class="line">    drivers(driverId) = driver <span class="comment">//这里的drivers是一个HashMap:drivers保存每一个application的ID和DricerRunneer</span></span><br><span class="line">    driver.start()  <span class="comment">//调用一下方法</span></span><br><span class="line"></span><br><span class="line">    coresUsed += driverDesc.cores  <span class="comment">//粗粒度 统计 core 的使用情况</span></span><br><span class="line">    memoryUsed += driverDesc.me</span><br></pre></td></tr></table></figure><h3 id="调转DriverRunner"><a href="#调转DriverRunner" class="headerlink" title="调转DriverRunner"></a>调转DriverRunner</h3><h3 id="driver-star"><a href="#driver-star" class="headerlink" title="driver.star"></a>driver.star</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Starts a thread to run and manage the driver. */</span></span><br><span class="line"><span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"DriverRunner for "</span> + driverId) &#123;    <span class="comment">//创建一个线程</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; </span><br><span class="line">      <span class="keyword">var</span> shutdownHook: <span class="type">AnyRef</span> = <span class="literal">null</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        shutdownHook = <span class="type">ShutdownHookManager</span>.addShutdownHook &#123; () =&gt;</span><br><span class="line">          logInfo(<span class="string">s"Worker shutting down, killing driver <span class="subst">$driverId</span>"</span>)</span><br><span class="line">          kill()</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// prepare driver jars and run driver</span></span><br><span class="line">        <span class="keyword">val</span> exitCode = prepareAndRunDriver() </span><br><span class="line"></span><br><span class="line">        <span class="comment">// set final state depending on if forcibly killed and process exit code</span></span><br><span class="line">        finalState = <span class="keyword">if</span> (exitCode == <span class="number">0</span>) &#123;</span><br><span class="line">          <span class="type">Some</span>(<span class="type">DriverState</span>.<span class="type">FINISHED</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (killed) &#123;</span><br><span class="line">          <span class="type">Some</span>(<span class="type">DriverState</span>.<span class="type">KILLED</span>)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="type">Some</span>(<span class="type">DriverState</span>.<span class="type">FAILED</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">          kill()</span><br><span class="line">          finalState = <span class="type">Some</span>(<span class="type">DriverState</span>.<span class="type">ERROR</span>)</span><br><span class="line">          finalException = <span class="type">Some</span>(e)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (shutdownHook != <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="type">ShutdownHookManager</span>.removeShutdownHook(shutdownHook)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"><span class="comment">//没有出异常就Worker发送信息</span></span><br><span class="line">      <span class="comment">// notify worker of final driver state, possible exception  </span></span><br><span class="line">      worker.send(<span class="type">DriverStateChanged</span>(driverId, finalState.get, finalException))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.start()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="prepareAndRunDriver"><a href="#prepareAndRunDriver" class="headerlink" title="prepareAndRunDriver"></a>prepareAndRunDriver</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"> <span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">prepareAndRunDriver</span></span>(): <span class="type">Int</span> = &#123;   </span><br><span class="line">   <span class="keyword">val</span> driverDir = createWorkingDirectory()  <span class="comment">//创建一个工作目录</span></span><br><span class="line">   <span class="keyword">val</span> localJarFilename = downloadUserJar(driverDir) <span class="comment">//在DriverClinet有个小的http服务器,就是为了提供jar包的下载 这时候把jar包下载到我们的Worker里面了</span></span><br><span class="line"></span><br><span class="line">   <span class="function"><span class="keyword">def</span> <span class="title">substituteVariables</span></span>(argument: <span class="type">String</span>): <span class="type">String</span> = argument <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> <span class="string">"&#123;&#123;WORKER_URL&#125;&#125;"</span> =&gt; workerUrl</span><br><span class="line">     <span class="keyword">case</span> <span class="string">"&#123;&#123;USER_JAR&#125;&#125;"</span> =&gt; localJarFilename</span><br><span class="line">     <span class="keyword">case</span> other =&gt; other</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// <span class="doctag">TODO:</span> If we add ability to submit multiple jars they should also be added here</span></span><br><span class="line">     <span class="comment">//启动一个进程</span></span><br><span class="line">   <span class="keyword">val</span> builder = <span class="type">CommandUtils</span>.buildProcessBuilder(driverDesc.command, securityManager,</span><br><span class="line">     driverDesc.mem, sparkHome.getAbsolutePath, substituteVariables)</span><br><span class="line"><span class="comment">//这是</span></span><br><span class="line">   runDriver(builder, driverDir, driverDesc.supervise)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>我们查看一下<code>buildProcessBuilder</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildProcessBuilder</span></span>(</span><br><span class="line">    command: <span class="type">Command</span>,</span><br><span class="line">    securityMgr: <span class="type">SecurityManager</span>,</span><br><span class="line">    memory: <span class="type">Int</span>,</span><br><span class="line">    sparkHome: <span class="type">String</span>,</span><br><span class="line">    substituteArguments: <span class="type">String</span> =&gt; <span class="type">String</span>,</span><br><span class="line">    classPaths: <span class="type">Seq</span>[<span class="type">String</span>] = <span class="type">Seq</span>[<span class="type">String</span>](),</span><br><span class="line">    env: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = sys.env): <span class="type">ProcessBuilder</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> localCommand = buildLocalCommand(</span><br><span class="line">    command, securityMgr, substituteArguments, classPaths, env)</span><br><span class="line">  <span class="keyword">val</span> commandSeq = buildCommandSeq(localCommand, memory, sparkHome)</span><br><span class="line">  <span class="keyword">val</span> builder = <span class="keyword">new</span> <span class="type">ProcessBuilder</span>(commandSeq: _*)    <span class="comment">//Java中与运行本地命令的</span></span><br><span class="line">  <span class="keyword">val</span> environment = builder.environment()</span><br><span class="line">  <span class="keyword">for</span> ((key, value) &lt;- localCommand.environment) &#123;</span><br><span class="line">    environment.put(key, value)  </span><br><span class="line">  &#125;</span><br><span class="line">  builder</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="runDriver"><a href="#runDriver" class="headerlink" title="runDriver"></a>runDriver</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">runDriver</span></span>(builder: <span class="type">ProcessBuilder</span>, baseDir: <span class="type">File</span>, supervise: <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  builder.directory(baseDir)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(process: <span class="type">Process</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Redirect stdout and stderr to files</span></span><br><span class="line">    <span class="keyword">val</span> stdout = <span class="keyword">new</span> <span class="type">File</span>(baseDir, <span class="string">"stdout"</span>)  <span class="comment">//获取正确输出</span></span><br><span class="line">    <span class="type">CommandUtils</span>.redirectStream(process.getInputStream, stdout)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//获取异常</span></span><br><span class="line">    <span class="keyword">val</span> stderr = <span class="keyword">new</span> <span class="type">File</span>(baseDir, <span class="string">"stderr"</span>)</span><br><span class="line">    <span class="keyword">val</span> formattedCommand = builder.command.asScala.mkString(<span class="string">"\""</span>, <span class="string">"\" \""</span>, <span class="string">"\""</span>)</span><br><span class="line">    <span class="keyword">val</span> header = <span class="string">"Launch Command: %s\n%s\n\n"</span>.format(formattedCommand, <span class="string">"="</span> * <span class="number">40</span>)</span><br><span class="line">    <span class="type">Files</span>.append(header, stderr, <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>)   <span class="comment">//把日志输出到文件</span></span><br><span class="line">    <span class="type">CommandUtils</span>.redirectStream(process.getErrorStream, stderr)</span><br><span class="line">  &#125;</span><br><span class="line">    <span class="comment">//重试</span></span><br><span class="line">  runCommandWithRetry(<span class="type">ProcessBuilderLike</span>(builder), initialize, supervise)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="runCommandWithRetry"><a href="#runCommandWithRetry" class="headerlink" title="runCommandWithRetry"></a>runCommandWithRetry</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">runCommandWithRetry</span></span>(</span><br><span class="line">     command: <span class="type">ProcessBuilderLike</span>, initialize: <span class="type">Process</span> =&gt; <span class="type">Unit</span>, supervise: <span class="type">Boolean</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">   <span class="keyword">var</span> exitCode = <span class="number">-1</span>  <span class="comment">//退出代码</span></span><br><span class="line">   <span class="comment">// Time to wait between submission retries.</span></span><br><span class="line">   <span class="keyword">var</span> waitSeconds = <span class="number">1</span></span><br><span class="line">   <span class="comment">// A run of this many seconds resets the exponential back-off.</span></span><br><span class="line">   <span class="keyword">val</span> successfulRunDuration = <span class="number">5</span></span><br><span class="line">   <span class="keyword">var</span> keepTrying = !killed</span><br><span class="line"><span class="comment">//重试</span></span><br><span class="line">   <span class="keyword">while</span> (keepTrying) &#123;</span><br><span class="line">     logInfo(<span class="string">"Launch Command: "</span> + command.command.mkString(<span class="string">"\""</span>, <span class="string">"\" \""</span>, <span class="string">"\""</span>))</span><br><span class="line"></span><br><span class="line">     synchronized &#123;</span><br><span class="line">       <span class="keyword">if</span> (killed) &#123; <span class="keyword">return</span> exitCode &#125;  </span><br><span class="line">       process = <span class="type">Some</span>(command.start()) <span class="comment">//这个process可能就是你的DriverWrapper</span></span><br><span class="line">       initialize(process.get)</span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">     <span class="keyword">val</span> processStart = clock.getTimeMillis()</span><br><span class="line">     exitCode = process.get.waitFor()</span><br><span class="line"></span><br><span class="line">     <span class="comment">// check if attempting another run</span></span><br><span class="line">     keepTrying = supervise &amp;&amp; exitCode != <span class="number">0</span> &amp;&amp; !killed</span><br><span class="line">     <span class="keyword">if</span> (keepTrying) &#123;</span><br><span class="line">       <span class="keyword">if</span> (clock.getTimeMillis() - processStart &gt; successfulRunDuration * <span class="number">1000</span>) &#123;</span><br><span class="line">         waitSeconds = <span class="number">1</span></span><br><span class="line">       &#125;</span><br><span class="line">       logInfo(<span class="string">s"Command exited with status <span class="subst">$exitCode</span>, re-launching after <span class="subst">$waitSeconds</span> s."</span>)</span><br><span class="line">       sleeper.sleep(waitSeconds)</span><br><span class="line">       waitSeconds = waitSeconds * <span class="number">2</span> <span class="comment">// exponential back-off</span></span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   exitCode</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="进入DriverWrapper"><a href="#进入DriverWrapper" class="headerlink" title="进入DriverWrapper"></a>进入DriverWrapper</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DriverWrapper</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    args.toList <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="comment">/*</span></span><br><span class="line"><span class="comment">       * IMPORTANT: Spark 1.3 provides a stable application submission gateway that is both</span></span><br><span class="line"><span class="comment">       * backward and forward compatible across future Spark versions. Because this gateway</span></span><br><span class="line"><span class="comment">       * uses this class to launch the driver, the ordering and semantics of the arguments</span></span><br><span class="line"><span class="comment">       * here must also remain consistent across versions.</span></span><br><span class="line"><span class="comment">       */</span></span><br><span class="line">      <span class="keyword">case</span> workerUrl :: userJar :: mainClass :: extraArgs =&gt;</span><br><span class="line">        <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="string">"Driver"</span>,    <span class="comment">//创建rpcEnv</span></span><br><span class="line">          <span class="type">Utils</span>.localHostName(), <span class="number">0</span>, conf, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf))</span><br><span class="line">        rpcEnv.setupEndpoint(<span class="string">"workerWatcher"</span>, <span class="keyword">new</span> <span class="type">WorkerWatcher</span>(rpcEnv, workerUrl))  </span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> currentLoader = <span class="type">Thread</span>.currentThread.getContextClassLoader</span><br><span class="line">        <span class="keyword">val</span> userJarUrl = <span class="keyword">new</span> <span class="type">File</span>(userJar).toURI().toURL() <span class="comment">//userJarUrl 是我们的Jar包</span></span><br><span class="line">        <span class="keyword">val</span> loader = <span class="comment">//获得线程</span></span><br><span class="line">          <span class="keyword">if</span> (sys.props.getOrElse(<span class="string">"spark.driver.userClassPathFirst"</span>, <span class="string">"false"</span>).toBoolean) &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChildFirstURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">new</span> <span class="type">MutableURLClassLoader</span>(<span class="type">Array</span>(userJarUrl), currentLoader)</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="type">Thread</span>.currentThread.setContextClassLoader(loader)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Delegate to supplied main class</span></span><br><span class="line">        <span class="comment">//把main线程加载进来</span></span><br><span class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(mainClass)</span><br><span class="line">        <span class="comment">//反射jar包中的main方法</span></span><br><span class="line">        <span class="keyword">val</span> mainMethod = clazz.getMethod(<span class="string">"main"</span>, classOf[<span class="type">Array</span>[<span class="type">String</span>]])</span><br><span class="line">        <span class="comment">//执行jar包中的main方法</span></span><br><span class="line">        mainMethod.invoke(<span class="literal">null</span>, extraArgs.toArray[<span class="type">String</span>])</span><br><span class="line"></span><br><span class="line">        rpcEnv.shutdown()  <span class="comment">//最后Drive shutdown  因此我们可以有理由断定 Driver程序就是Jar包</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        <span class="comment">// scalastyle:off println</span></span><br><span class="line">        <span class="type">System</span>.err.println(<span class="string">"Usage: DriverWrapper &lt;workerUrl&gt; &lt;userJar&gt; &lt;driverMainClass&gt; [options]"</span>)</span><br><span class="line">        <span class="comment">// scalastyle:on println</span></span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">-1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们回到driver.start看<code>worker.send(DriverStateChanged(driverId, finalState.get, finalException))</code> 给worker发送</p><p>DriverStateChanged</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Starts a thread to run and manage the driver. */</span></span><br><span class="line"><span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"DriverRunner for "</span> + driverId) &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">      <span class="keyword">var</span> shutdownHook: <span class="type">AnyRef</span> = <span class="literal">null</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        shutdownHook = <span class="type">ShutdownHookManager</span>.addShutdownHook &#123; () =&gt;</span><br><span class="line">          logInfo(<span class="string">s"Worker shutting down, killing driver <span class="subst">$driverId</span>"</span>)</span><br><span class="line">          kill()</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure><h3 id="回到Driver"><a href="#回到Driver" class="headerlink" title="回到Driver"></a>回到Driver</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> driverStateChanged @ <span class="type">DriverStateChanged</span>(driverId, state, exception) =&gt;</span><br><span class="line">  handleDriverStateChanged(driverStateChanged)</span><br></pre></td></tr></table></figure><p>我们进入handleDriverStateChanged</p><h3 id="handleDriverStateChanged"><a href="#handleDriverStateChanged" class="headerlink" title="handleDriverStateChanged"></a>handleDriverStateChanged</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">handleDriverStateChanged</span></span>(driverStateChanged: <span class="type">DriverStateChanged</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> driverId = driverStateChanged.driverId</span><br><span class="line">  <span class="keyword">val</span> exception = driverStateChanged.exception</span><br><span class="line">  <span class="keyword">val</span> state = driverStateChanged.state</span><br><span class="line">  state <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">ERROR</span> =&gt; <span class="comment">//打印一些消息</span></span><br><span class="line">      logWarning(<span class="string">s"Driver <span class="subst">$driverId</span> failed with unrecoverable exception: <span class="subst">$&#123;exception.get&#125;</span>"</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">FAILED</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">s"Driver <span class="subst">$driverId</span> exited with failure"</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">FINISHED</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">s"Driver <span class="subst">$driverId</span> exited successfully"</span>)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">KILLED</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">s"Driver <span class="subst">$driverId</span> was killed by user"</span>)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      logDebug(<span class="string">s"Driver <span class="subst">$driverId</span> changed state to <span class="subst">$state</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  sendToMaster(driverStateChanged) <span class="comment">//发送给Master driverStateChanged</span></span><br><span class="line">  <span class="keyword">val</span> driver = drivers.remove(driverId).get</span><br><span class="line">  finishedDrivers(driverId) = driver</span><br><span class="line">  trimFinishedDriversIfNecessary()</span><br><span class="line">  memoryUsed -= driver.driverDesc.mem</span><br><span class="line">  coresUsed -= driver.driverDesc.cores</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sendToMaster"><a href="#sendToMaster" class="headerlink" title="sendToMaster"></a>sendToMaster</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendToMaster</span></span>(message: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  master <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(masterRef) =&gt; masterRef.send(message)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      logWarning(</span><br><span class="line">        <span class="string">s"Dropping <span class="subst">$message</span> because the connection to master has not yet been established"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到Master-Driver启动完毕"><a href="#回到Master-Driver启动完毕" class="headerlink" title="回到Master(Driver启动完毕)"></a>回到Master(Driver启动完毕)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">DriverStateChanged</span>(driverId, state, exception) =&gt;</span><br><span class="line">   state <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">ERROR</span> | <span class="type">DriverState</span>.<span class="type">FINISHED</span> | <span class="type">DriverState</span>.<span class="type">KILLED</span> | <span class="type">DriverState</span>.<span class="type">FAILED</span> =&gt;</span><br><span class="line">       removeDriver(driverId, state, exception)</span><br><span class="line">     <span class="keyword">case</span> _ =&gt;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"Received unexpected state update for driver <span class="subst">$driverId</span>: <span class="subst">$state</span>"</span>)</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p>到这一步Driver已经基本启动</p><h2 id="注册Application"><a href="#注册Application" class="headerlink" title="注册Application"></a>注册Application</h2><p>我们需要看SparkContext的代码代码有3000多行我们跳着看吧</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkContext</span>(<span class="params">config: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;  <span class="comment">//SparkConf 一个配置类</span></span><br><span class="line">   ....</span><br><span class="line">    <span class="comment">//这里是Spark内部的一些变量</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _conf: <span class="type">SparkConf</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogDir: <span class="type">Option</span>[<span class="type">URI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogCodec: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _env: <span class="type">SparkEnv</span> = _ </span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _jobProgressListener: <span class="type">JobProgressListener</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _statusTracker: <span class="type">SparkStatusTracker</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _progressBar: <span class="type">Option</span>[<span class="type">ConsoleProgressBar</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _ui: <span class="type">Option</span>[<span class="type">SparkUI</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _hadoopConfiguration: <span class="type">Configuration</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorMemory: <span class="type">Int</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _schedulerBackend: <span class="type">SchedulerBackend</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _taskScheduler: <span class="type">TaskScheduler</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _heartbeatReceiver: <span class="type">RpcEndpointRef</span> = _</span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> _dagScheduler: <span class="type">DAGScheduler</span> = _ <span class="comment">//DAG任务分解其</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationId: <span class="type">String</span> = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _applicationAttemptId: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _eventLogger: <span class="type">Option</span>[<span class="type">EventLoggingListener</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _executorAllocationManager: <span class="type">Option</span>[<span class="type">ExecutorAllocationManager</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _cleaner: <span class="type">Option</span>[<span class="type">ContextCleaner</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _listenerBusStarted: <span class="type">Boolean</span> = <span class="literal">false</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _jars: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _files: <span class="type">Seq</span>[<span class="type">String</span>] = _</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> _shutdownHookRef: <span class="type">AnyRef</span> = _</span><br></pre></td></tr></table></figure><p>我们首先看一下SparkEnv吧</p><h3 id="SparkEnv"><a href="#SparkEnv" class="headerlink" title="SparkEnv"></a>SparkEnv</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@DeveloperApi</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SparkEnv</span> (<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val executorId: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    private[spark] val rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val closureSerializer: <span class="type">Serializer</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializerManager: <span class="type">SerializerManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val mapOutputTracker: <span class="type">MapOutputTracker</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val shuffleManager: <span class="type">ShuffleManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val broadcastManager: <span class="type">BroadcastManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val blockManager: <span class="type">BlockManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val securityManager: <span class="type">SecurityManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val metricsSystem: <span class="type">MetricsSystem</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val memoryManager: <span class="type">MemoryManager</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val outputCommitCoordinator: <span class="type">OutputCommitCoordinator</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SparkConf</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190610213205.png" alt=""></p><h3 id="主要初始化方法"><a href="#主要初始化方法" class="headerlink" title="主要初始化方法"></a>主要初始化方法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">//主要初始化方法</span></span><br><span class="line"> <span class="keyword">try</span> &#123;</span><br><span class="line">   _conf = config.clone()</span><br><span class="line">   _conf.validateSettings()</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (!_conf.contains(<span class="string">"spark.master"</span>)) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"A master URL must be set in your configuration"</span>)</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="keyword">if</span> (!_conf.contains(<span class="string">"spark.app.name"</span>)) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"An application name must be set in your configuration"</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster</span></span><br><span class="line">   <span class="keyword">if</span> (master == <span class="string">"yarn"</span> &amp;&amp; deployMode == <span class="string">"cluster"</span> &amp;&amp; !_conf.contains(<span class="string">"spark.yarn.app.id"</span>)) &#123;</span><br><span class="line">     <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Detected yarn cluster mode, but isn't running on a cluster. "</span> +</span><br><span class="line">       <span class="string">"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit."</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.logConf"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">     logInfo(<span class="string">"Spark configuration:\n"</span> + _conf.toDebugString)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Set Spark driver host and port system properties. This explicitly sets the configuration</span></span><br><span class="line">   <span class="comment">// instead of relying on the default value of the config constant.</span></span><br><span class="line">   _conf.set(<span class="type">DRIVER_HOST_ADDRESS</span>, _conf.get(<span class="type">DRIVER_HOST_ADDRESS</span>))</span><br><span class="line">   _conf.setIfMissing(<span class="string">"spark.driver.port"</span>, <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">   _conf.set(<span class="string">"spark.executor.id"</span>, <span class="type">SparkContext</span>.<span class="type">DRIVER_IDENTIFIER</span>)</span><br><span class="line"></span><br><span class="line">   _jars = <span class="type">Utils</span>.getUserJars(_conf)</span><br><span class="line">   _files = _conf.getOption(<span class="string">"spark.files"</span>).map(_.split(<span class="string">","</span>)).map(_.filter(_.nonEmpty))</span><br><span class="line">     .toSeq.flatten</span><br><span class="line"></span><br><span class="line">   _eventLogDir =</span><br><span class="line">     <span class="keyword">if</span> (isEventLogEnabled) &#123;</span><br><span class="line">       <span class="keyword">val</span> unresolvedDir = conf.get(<span class="string">"spark.eventLog.dir"</span>, <span class="type">EventLoggingListener</span>.<span class="type">DEFAULT_LOG_DIR</span>)</span><br><span class="line">         .stripSuffix(<span class="string">"/"</span>)</span><br><span class="line">       <span class="type">Some</span>(<span class="type">Utils</span>.resolveURI(unresolvedDir))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   _eventLogCodec = &#123;</span><br><span class="line">     <span class="keyword">val</span> compress = _conf.getBoolean(<span class="string">"spark.eventLog.compress"</span>, <span class="literal">false</span>)</span><br><span class="line">     <span class="keyword">if</span> (compress &amp;&amp; isEventLogEnabled) &#123;</span><br><span class="line">       <span class="type">Some</span>(<span class="type">CompressionCodec</span>.getCodecName(_conf)).map(<span class="type">CompressionCodec</span>.getShortName)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (master == <span class="string">"yarn"</span> &amp;&amp; deployMode == <span class="string">"client"</span>) <span class="type">System</span>.setProperty(<span class="string">"SPARK_YARN_MODE"</span>, <span class="string">"true"</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// "_jobProgressListener" should be set up before creating SparkEnv because when creating</span></span><br><span class="line">   <span class="comment">// "SparkEnv", some messages will be posted to "listenerBus" and we should not miss them.</span></span><br><span class="line">   _jobProgressListener = <span class="keyword">new</span> <span class="type">JobProgressListener</span>(_conf)</span><br><span class="line">   listenerBus.addListener(jobProgressListener)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Create the Spark execution environment (cache, map output tracker, etc)</span></span><br><span class="line">   _env = createSparkEnv(_conf, isLocal, listenerBus)</span><br><span class="line">   <span class="type">SparkEnv</span>.set(_env)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// If running the REPL, register the repl's output dir with the file server.</span></span><br><span class="line">   _conf.getOption(<span class="string">"spark.repl.class.outputDir"</span>).foreach &#123; path =&gt;</span><br><span class="line">     <span class="keyword">val</span> replUri = _env.rpcEnv.fileServer.addDirectory(<span class="string">"/classes"</span>, <span class="keyword">new</span> <span class="type">File</span>(path))</span><br><span class="line">     _conf.set(<span class="string">"spark.repl.class.uri"</span>, replUri)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   _statusTracker = <span class="keyword">new</span> <span class="type">SparkStatusTracker</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">   _progressBar =</span><br><span class="line">     <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.ui.showConsoleProgress"</span>, <span class="literal">true</span>) &amp;&amp; !log.isInfoEnabled) &#123;</span><br><span class="line">       <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ConsoleProgressBar</span>(<span class="keyword">this</span>))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   _ui =</span><br><span class="line">     <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.ui.enabled"</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">       <span class="type">Some</span>(<span class="type">SparkUI</span>.createLiveUI(<span class="keyword">this</span>, _conf, listenerBus, _jobProgressListener,</span><br><span class="line">         _env.securityManager, appName, startTime = startTime))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="comment">// For tests, do not enable the UI</span></span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line">   <span class="comment">// Bind the UI before starting the task scheduler to communicate</span></span><br><span class="line">   <span class="comment">// the bound port to the cluster manager properly</span></span><br><span class="line">   _ui.foreach(_.bind())</span><br><span class="line"></span><br><span class="line">   _hadoopConfiguration = <span class="type">SparkHadoopUtil</span>.get.newConfiguration(_conf)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Add each JAR given through the constructor</span></span><br><span class="line">   <span class="keyword">if</span> (jars != <span class="literal">null</span>) &#123;</span><br><span class="line">     jars.foreach(addJar)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="keyword">if</span> (files != <span class="literal">null</span>) &#123;</span><br><span class="line">     files.foreach(addFile)</span><br><span class="line">   &#125;</span><br><span class="line"><span class="comment">//获取配置</span></span><br><span class="line">   _executorMemory = _conf.getOption(<span class="string">"spark.executor.memory"</span>)</span><br><span class="line">     .orElse(<span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_EXECUTOR_MEMORY"</span>)))</span><br><span class="line">     .orElse(<span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_MEM"</span>))</span><br><span class="line">     .map(warnSparkMem))</span><br><span class="line">     .map(<span class="type">Utils</span>.memoryStringToMb)</span><br><span class="line">     .getOrElse(<span class="number">1024</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Convert java options to env vars as a work around</span></span><br><span class="line">   <span class="comment">// since we can't set env vars directly in sbt.</span></span><br><span class="line">   <span class="keyword">for</span> &#123; (envKey, propKey) &lt;- <span class="type">Seq</span>((<span class="string">"SPARK_TESTING"</span>, <span class="string">"spark.testing"</span>))</span><br><span class="line">     value &lt;- <span class="type">Option</span>(<span class="type">System</span>.getenv(envKey)).orElse(<span class="type">Option</span>(<span class="type">System</span>.getProperty(propKey)))&#125; &#123;</span><br><span class="line">     executorEnvs(envKey) = value</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="type">Option</span>(<span class="type">System</span>.getenv(<span class="string">"SPARK_PREPEND_CLASSES"</span>)).foreach &#123; v =&gt;</span><br><span class="line">     executorEnvs(<span class="string">"SPARK_PREPEND_CLASSES"</span>) = v</span><br><span class="line">   &#125;</span><br><span class="line">   <span class="comment">// The Mesos scheduler backend relies on this environment variable to set executor memory.</span></span><br><span class="line">   <span class="comment">// <span class="doctag">TODO:</span> Set this only in the Mesos scheduler.</span></span><br><span class="line">   executorEnvs(<span class="string">"SPARK_EXECUTOR_MEMORY"</span>) = executorMemory + <span class="string">"m"</span></span><br><span class="line">   executorEnvs ++= _conf.getExecutorEnv</span><br><span class="line">   executorEnvs(<span class="string">"SPARK_USER"</span>) = sparkUser</span><br><span class="line"></span><br><span class="line">   <span class="comment">// We need to register "HeartbeatReceiver" before "createTaskScheduler" because Executor will</span></span><br><span class="line">   <span class="comment">// retrieve "HeartbeatReceiver" in the constructor. (SPARK-6640)</span></span><br><span class="line">   _heartbeatReceiver = env.rpcEnv.setupEndpoint( <span class="comment">//心跳</span></span><br><span class="line">     <span class="type">HeartbeatReceiver</span>.<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">HeartbeatReceiver</span>(<span class="keyword">this</span>))</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Create and start the scheduler  </span></span><br><span class="line">   <span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">   _schedulerBackend = sched   <span class="comment">//创建 _schedulerBackend</span></span><br><span class="line">   _taskScheduler = ts  <span class="comment">//创建 _taskScheduler</span></span><br><span class="line">   _dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>) <span class="comment">//创建 DAGScheduler</span></span><br><span class="line">   _heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's</span></span><br><span class="line">   <span class="comment">// constructor</span></span><br><span class="line">   _taskScheduler.start()</span><br><span class="line"></span><br><span class="line">   _applicationId = _taskScheduler.applicationId()</span><br><span class="line">   _applicationAttemptId = taskScheduler.applicationAttemptId()</span><br><span class="line">   _conf.set(<span class="string">"spark.app.id"</span>, _applicationId)</span><br><span class="line">   <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.ui.reverseProxy"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">     <span class="type">System</span>.setProperty(<span class="string">"spark.ui.proxyBase"</span>, <span class="string">"/proxy/"</span> + _applicationId)</span><br><span class="line">   &#125;</span><br><span class="line">   _ui.foreach(_.setAppId(_applicationId))</span><br><span class="line">   _env.blockManager.initialize(_applicationId)</span><br><span class="line"></span><br><span class="line">   <span class="comment">// The metrics system for Driver need to be set spark.app.id to app ID.</span></span><br><span class="line">   <span class="comment">// So it should start after we get app ID from the task scheduler and set spark.app.id.</span></span><br><span class="line">   _env.metricsSystem.start()</span><br><span class="line">   <span class="comment">// Attach the driver metrics servlet handler to the web ui after the metrics system is started.</span></span><br><span class="line">   _env.metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler)))</span><br><span class="line"></span><br><span class="line">   _eventLogger =</span><br><span class="line">     <span class="keyword">if</span> (isEventLogEnabled) &#123;</span><br><span class="line">       <span class="keyword">val</span> logger =</span><br><span class="line">         <span class="keyword">new</span> <span class="type">EventLoggingListener</span>(_applicationId, _applicationAttemptId, _eventLogDir.get,</span><br><span class="line">           _conf, _hadoopConfiguration)</span><br><span class="line">       logger.start()</span><br><span class="line">       listenerBus.addListener(logger)</span><br><span class="line">       <span class="type">Some</span>(logger)</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Optionally scale number of executors dynamically based on workload. Exposed for testing.</span></span><br><span class="line">   <span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)</span><br><span class="line">   _executorAllocationManager =</span><br><span class="line">     <span class="keyword">if</span> (dynamicAllocationEnabled) &#123;</span><br><span class="line">       schedulerBackend <span class="keyword">match</span> &#123;</span><br><span class="line">         <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;</span><br><span class="line">           <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(</span><br><span class="line">             schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf))</span><br><span class="line">         <span class="keyword">case</span> _ =&gt;</span><br><span class="line">           <span class="type">None</span></span><br><span class="line">       &#125;</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line">   _executorAllocationManager.foreach(_.start())</span><br><span class="line"></span><br><span class="line">   _cleaner =</span><br><span class="line">     <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.cleaner.referenceTracking"</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">       <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ContextCleaner</span>(<span class="keyword">this</span>))</span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="type">None</span></span><br><span class="line">     &#125;</span><br><span class="line">   _cleaner.foreach(_.start())</span><br><span class="line"></span><br><span class="line">   setupAndStartListenerBus()</span><br><span class="line">   postEnvironmentUpdate()</span><br><span class="line">   postApplicationStart()</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Post init</span></span><br><span class="line">   _taskScheduler.postStartHook()</span><br><span class="line">   _env.metricsSystem.registerSource(_dagScheduler.metricsSource)</span><br><span class="line">   _env.metricsSystem.registerSource(<span class="keyword">new</span> <span class="type">BlockManagerSource</span>(_env.blockManager))</span><br><span class="line">   _executorAllocationManager.foreach &#123; e =&gt;</span><br><span class="line">     _env.metricsSystem.registerSource(e.executorAllocationManagerSource)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// Make sure the context is stopped if the user forgets about it. This avoids leaving</span></span><br><span class="line">   <span class="comment">// unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM</span></span><br><span class="line">   <span class="comment">// is killed, though.</span></span><br><span class="line">   logDebug(<span class="string">"Adding shutdown hook"</span>) <span class="comment">// force eager creation of logger</span></span><br><span class="line">   _shutdownHookRef = <span class="type">ShutdownHookManager</span>.addShutdownHook(</span><br><span class="line">     <span class="type">ShutdownHookManager</span>.<span class="type">SPARK_CONTEXT_SHUTDOWN_PRIORITY</span>) &#123; () =&gt;</span><br><span class="line">     logInfo(<span class="string">"Invoking stop() from shutdown hook"</span>)</span><br><span class="line">     stop()</span><br><span class="line">   &#125;</span><br><span class="line"> &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">   <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">     logError(<span class="string">"Error initializing SparkContext."</span>, e)</span><br><span class="line">     <span class="keyword">try</span> &#123;</span><br><span class="line">       stop()</span><br><span class="line">     &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">       <span class="keyword">case</span> <span class="type">NonFatal</span>(inner) =&gt;</span><br><span class="line">         logError(<span class="string">"Error stopping SparkContext after init error."</span>, inner)</span><br><span class="line">     &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">       <span class="keyword">throw</span> e</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="createTaskScheduler"><a href="#createTaskScheduler" class="headerlink" title="createTaskScheduler"></a>createTaskScheduler</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createTaskScheduler</span></span>(</span><br><span class="line">    sc: <span class="type">SparkContext</span>,</span><br><span class="line">    master: <span class="type">String</span>,</span><br><span class="line">    deployMode: <span class="type">String</span>): (<span class="type">SchedulerBackend</span>, <span class="type">TaskScheduler</span>) = &#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">SparkMasterRegex</span>._</span><br><span class="line"></span><br><span class="line">  <span class="comment">// When running locally, don't try to re-execute tasks on failure.</span></span><br><span class="line">  <span class="keyword">val</span> <span class="type">MAX_LOCAL_TASK_FAILURES</span> = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">  master <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="string">"local"</span> =&gt;  <span class="comment">//如果是local模式</span></span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, <span class="number">1</span>)</span><br><span class="line">      scheduler.initialize(backend)</span><br><span class="line">      (backend, scheduler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_N_REGEX</span>(threads) =&gt; <span class="comment">//或者是多线程的local模式</span></span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</span><br><span class="line">      <span class="comment">// local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.</span></span><br><span class="line">      <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</span><br><span class="line">      <span class="keyword">if</span> (threadCount &lt;= <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Asked to run locally with <span class="subst">$threadCount</span> threads"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, <span class="type">MAX_LOCAL_TASK_FAILURES</span>, isLocal = <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)</span><br><span class="line">      scheduler.initialize(backend)</span><br><span class="line">      (backend, scheduler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_N_FAILURES_REGEX</span>(threads, maxFailures) =&gt;</span><br><span class="line">      <span class="function"><span class="keyword">def</span> <span class="title">localCpuCount</span></span>: <span class="type">Int</span> = <span class="type">Runtime</span>.getRuntime.availableProcessors()</span><br><span class="line">      <span class="comment">// local[*, M] means the number of cores on the computer with M failures</span></span><br><span class="line">      <span class="comment">// local[N, M] means exactly N threads with M failures</span></span><br><span class="line">      <span class="keyword">val</span> threadCount = <span class="keyword">if</span> (threads == <span class="string">"*"</span>) localCpuCount <span class="keyword">else</span> threads.toInt</span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc, maxFailures.toInt, isLocal = <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">LocalSchedulerBackend</span>(sc.getConf, scheduler, threadCount)</span><br><span class="line">      scheduler.initialize(backend)</span><br><span class="line">      (backend, scheduler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">SPARK_REGEX</span>(sparkUrl) =&gt; <span class="comment">//StandaloneSchedulerBackend 模式</span></span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)</span><br><span class="line">      <span class="keyword">val</span> masterUrls = sparkUrl.split(<span class="string">","</span>).map(<span class="string">"spark://"</span> + _)</span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)</span><br><span class="line">      scheduler.initialize(backend)</span><br><span class="line">      (backend, scheduler)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">LOCAL_CLUSTER_REGEX</span>(numSlaves, coresPerSlave, memoryPerSlave) =&gt;</span><br><span class="line">      <span class="comment">// Check to make sure memory requested &lt;= memoryPerSlave. Otherwise Spark will just hang.</span></span><br><span class="line">      <span class="keyword">val</span> memoryPerSlaveInt = memoryPerSlave.toInt</span><br><span class="line">      <span class="keyword">if</span> (sc.executorMemory &gt; memoryPerSlaveInt) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">          <span class="string">"Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker"</span>.format(</span><br><span class="line">            memoryPerSlaveInt, sc.executorMemory))</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> scheduler = <span class="keyword">new</span> <span class="type">TaskSchedulerImpl</span>(sc)</span><br><span class="line">      <span class="keyword">val</span> localCluster = <span class="keyword">new</span> <span class="type">LocalSparkCluster</span>(</span><br><span class="line">        numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)</span><br><span class="line">      <span class="keyword">val</span> masterUrls = localCluster.start()</span><br><span class="line">      <span class="keyword">val</span> backend = <span class="keyword">new</span> <span class="type">StandaloneSchedulerBackend</span>(scheduler, sc, masterUrls)</span><br><span class="line">      scheduler.initialize(backend)</span><br><span class="line">      backend.shutdownCallback = (backend: <span class="type">StandaloneSchedulerBackend</span>) =&gt; &#123;</span><br><span class="line">        localCluster.stop()</span><br><span class="line">      &#125;</span><br><span class="line">      (backend, scheduler)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">case</span> masterUrl =&gt; <span class="comment">//根据你的提交模式 会创建不同的 scheduler backend </span></span><br><span class="line">      <span class="keyword">val</span> cm = getClusterManager(masterUrl) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(clusterMgr) =&gt; clusterMgr</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Could not parse Master URL: '"</span> + master + <span class="string">"'"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> scheduler = cm.createTaskScheduler(sc, masterUrl)</span><br><span class="line">        <span class="keyword">val</span> backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) </span><br><span class="line">        cm.initialize(scheduler, backend)</span><br><span class="line">        (backend, scheduler)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> se: <span class="type">SparkException</span> =&gt; <span class="keyword">throw</span> se</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"External scheduler cannot be instantiated"</span>, e)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// Create and start the scheduler</span></span><br><span class="line">  <span class="keyword">val</span> (sched, ts) = <span class="type">SparkContext</span>.createTaskScheduler(<span class="keyword">this</span>, master, deployMode)</span><br><span class="line">  _schedulerBackend = sched</span><br><span class="line">  _taskScheduler = ts</span><br><span class="line">  _dagScheduler = <span class="keyword">new</span> <span class="type">DAGScheduler</span>(<span class="keyword">this</span>)</span><br><span class="line">  _heartbeatReceiver.ask[<span class="type">Boolean</span>](<span class="type">TaskSchedulerIsSet</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's</span></span><br><span class="line">  <span class="comment">// constructor</span></span><br><span class="line">  _taskScheduler.start()  <span class="comment">//启动任务</span></span><br><span class="line"></span><br><span class="line">  _applicationId = _taskScheduler.applicationId()</span><br><span class="line">  _applicationAttemptId = taskScheduler.applicationAttemptId()</span><br><span class="line">  _conf.set(<span class="string">"spark.app.id"</span>, _applicationId)</span><br><span class="line"> <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.ui.reverseProxy"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">    <span class="type">System</span>.setProperty(<span class="string">"spark.ui.proxyBase"</span>, <span class="string">"/proxy/"</span> + _applicationId)</span><br><span class="line">  &#125;</span><br><span class="line">  _ui.foreach(_.setAppId(_applicationId))</span><br><span class="line">  _env.blockManager.initialize(_applicationId)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// The metrics system for Driver need to be set spark.app.id to app ID.</span></span><br><span class="line">  <span class="comment">// So it should start after we get app ID from the task scheduler and set spark.app.id.</span></span><br><span class="line">  _env.metricsSystem.start()</span><br><span class="line">  <span class="comment">// Attach the driver metrics servlet handler to the web ui after the metrics system is started.</span></span><br><span class="line">  _env.metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler)))</span><br><span class="line"></span><br><span class="line">  _eventLogger =</span><br><span class="line">    <span class="keyword">if</span> (isEventLogEnabled) &#123;</span><br><span class="line">      <span class="keyword">val</span> logger =</span><br><span class="line">        <span class="keyword">new</span> <span class="type">EventLoggingListener</span>(_applicationId, _applicationAttemptId, _eventLogDir.get,</span><br><span class="line">          _conf, _hadoopConfiguration)</span><br><span class="line">      logger.start()</span><br><span class="line">      listenerBus.addListener(logger)</span><br><span class="line">      <span class="type">Some</span>(logger)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Optionally scale number of executors dynamically based on workload. Exposed for testing.</span></span><br><span class="line">  <span class="keyword">val</span> dynamicAllocationEnabled = <span class="type">Utils</span>.isDynamicAllocationEnabled(_conf)</span><br><span class="line">  _executorAllocationManager =</span><br><span class="line">    <span class="keyword">if</span> (dynamicAllocationEnabled) &#123;</span><br><span class="line">      schedulerBackend <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> b: <span class="type">ExecutorAllocationClient</span> =&gt;</span><br><span class="line">          <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ExecutorAllocationManager</span>(</span><br><span class="line">            schedulerBackend.asInstanceOf[<span class="type">ExecutorAllocationClient</span>], listenerBus, _conf))</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">          <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  _executorAllocationManager.foreach(_.start())</span><br><span class="line"></span><br><span class="line">  _cleaner =</span><br><span class="line">    <span class="keyword">if</span> (_conf.getBoolean(<span class="string">"spark.cleaner.referenceTracking"</span>, <span class="literal">true</span>)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ContextCleaner</span>(<span class="keyword">this</span>))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  _cleaner.foreach(_.start())</span><br><span class="line"></span><br><span class="line">  setupAndStartListenerBus()</span><br><span class="line">  postEnvironmentUpdate()  </span><br><span class="line">  postApplicationStart()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Post init</span></span><br><span class="line">  _taskScheduler.postStartHook()</span><br><span class="line">  _env.metricsSystem.registerSource(_dagScheduler.metricsSource)</span><br><span class="line">  _env.metricsSystem.registerSource(<span class="keyword">new</span> <span class="type">BlockManagerSource</span>(_env.blockManager))</span><br><span class="line">  _executorAllocationManager.foreach &#123; e =&gt;</span><br><span class="line">    _env.metricsSystem.registerSource(e.executorAllocationManagerSource)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Make sure the context is stopped if the user forgets about it. This avoids leaving</span></span><br><span class="line">  <span class="comment">// unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM</span></span><br><span class="line">  <span class="comment">// is killed, though.</span></span><br><span class="line">  logDebug(<span class="string">"Adding shutdown hook"</span>) <span class="comment">// force eager creation of logger</span></span><br><span class="line">  _shutdownHookRef = <span class="type">ShutdownHookManager</span>.addShutdownHook( <span class="comment">//注册一个ShutdownHook (钩子函数)</span></span><br><span class="line">    <span class="type">ShutdownHookManager</span>.<span class="type">SPARK_CONTEXT_SHUTDOWN_PRIORITY</span>) &#123; () =&gt;</span><br><span class="line">    logInfo(<span class="string">"Invoking stop() from shutdown hook"</span>)</span><br><span class="line">    stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">    logError(<span class="string">"Error initializing SparkContext."</span>, e)</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      stop()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">NonFatal</span>(inner) =&gt;</span><br><span class="line">        logError(<span class="string">"Error stopping SparkContext after init error."</span>, inner)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">throw</span> e</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Excutor分配"><a href="#Excutor分配" class="headerlink" title="Excutor分配"></a>Excutor分配</h2><h3 id="StandaloneSchedulerBackend"><a href="#StandaloneSchedulerBackend" class="headerlink" title="StandaloneSchedulerBackend"></a>StandaloneSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A [[SchedulerBackend]] implementation for Spark's standalone cluster manager.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">StandaloneSchedulerBackend</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    scheduler: <span class="type">TaskSchedulerImpl</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masters: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">CoarseGrainedSchedulerBackend</span>(<span class="params">scheduler, sc.env.rpcEnv</span>)<span class="title">//继承了一个粗粒度schedulerBackend</span>  <span class="title">在应用运行之间就把资源计算好</span>,<span class="title">在应用运行期间资源是不动的固定的</span>. <span class="title">Messon有粗细粒度之分再YARN</span> <span class="title">集群和Stanlone中</span> <span class="title">都为粗粒度的</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">StandaloneAppClientListener</span></span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> client: <span class="type">StandaloneAppClient</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> stopping = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> launcherBackend = <span class="keyword">new</span> <span class="type">LauncherBackend</span>() &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">onStopRequest</span></span>(): <span class="type">Unit</span> = stop(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">KILLED</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">var</span> shutdownCallback: <span class="type">StandaloneSchedulerBackend</span> =&gt; <span class="type">Unit</span> = _</span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> appId: <span class="type">String</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> registrationBarrier = <span class="keyword">new</span> <span class="type">Semaphore</span>(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> maxCores = conf.getOption(<span class="string">"spark.cores.max"</span>).map(_.toInt)</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> totalExpectedCores = maxCores.getOrElse(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123; <span class="comment">//我们关注一下start的方法</span></span><br><span class="line">    <span class="keyword">super</span>.start()</span><br><span class="line">    launcherBackend.connect()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The endpoint for executors to talk to us</span></span><br><span class="line">    <span class="keyword">val</span> driverUrl = <span class="type">RpcEndpointAddress</span>(</span><br><span class="line">      sc.conf.get(<span class="string">"spark.driver.host"</span>),</span><br><span class="line">      sc.conf.get(<span class="string">"spark.driver.port"</span>).toInt,</span><br><span class="line">      <span class="type">CoarseGrainedSchedulerBackend</span>.<span class="type">ENDPOINT_NAME</span>).toString</span><br><span class="line">    <span class="keyword">val</span> args = <span class="type">Seq</span>(</span><br><span class="line">      <span class="string">"--driver-url"</span>, driverUrl,</span><br><span class="line">      <span class="string">"--executor-id"</span>, <span class="string">"&#123;&#123;EXECUTOR_ID&#125;&#125;"</span>,</span><br><span class="line">      <span class="string">"--hostname"</span>, <span class="string">"&#123;&#123;HOSTNAME&#125;&#125;"</span>,</span><br><span class="line">      <span class="string">"--cores"</span>, <span class="string">"&#123;&#123;CORES&#125;&#125;"</span>,</span><br><span class="line">      <span class="string">"--app-id"</span>, <span class="string">"&#123;&#123;APP_ID&#125;&#125;"</span>,</span><br><span class="line">      <span class="string">"--worker-url"</span>, <span class="string">"&#123;&#123;WORKER_URL&#125;&#125;"</span>)</span><br><span class="line">    <span class="keyword">val</span> extraJavaOpts = sc.conf.getOption(<span class="string">"spark.executor.extraJavaOptions"</span>)</span><br><span class="line">      .map(<span class="type">Utils</span>.splitCommandString).getOrElse(<span class="type">Seq</span>.empty)</span><br><span class="line">    <span class="keyword">val</span> classPathEntries = sc.conf.getOption(<span class="string">"spark.executor.extraClassPath"</span>)</span><br><span class="line">      .map(_.split(java.io.<span class="type">File</span>.pathSeparator).toSeq).getOrElse(<span class="type">Nil</span>)</span><br><span class="line">    <span class="keyword">val</span> libraryPathEntries = sc.conf.getOption(<span class="string">"spark.executor.extraLibraryPath"</span>)</span><br><span class="line">      .map(_.split(java.io.<span class="type">File</span>.pathSeparator).toSeq).getOrElse(<span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// When testing, expose the parent class path to the child. This is processed by</span></span><br><span class="line">    <span class="comment">// compute-classpath.&#123;cmd,sh&#125; and makes all needed jars available to child processes</span></span><br><span class="line">    <span class="comment">// when the assembly is built with the "*-provided" profiles enabled.</span></span><br><span class="line">    <span class="keyword">val</span> testingClassPath =</span><br><span class="line">      <span class="keyword">if</span> (sys.props.contains(<span class="string">"spark.testing"</span>)) &#123;</span><br><span class="line">        sys.props(<span class="string">"java.class.path"</span>).split(java.io.<span class="type">File</span>.pathSeparator).toSeq</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">Nil</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Start executors with a few necessary configs for registering with the scheduler</span></span><br><span class="line">    <span class="keyword">val</span> sparkJavaOpts = <span class="type">Utils</span>.sparkJavaOpts(conf, <span class="type">SparkConf</span>.isExecutorStartupConf)</span><br><span class="line">    <span class="keyword">val</span> javaOpts = sparkJavaOpts ++ extraJavaOpts</span><br><span class="line">    <span class="keyword">val</span> command = <span class="type">Command</span>(<span class="string">"org.apache.spark.executor.CoarseGrainedExecutorBackend"</span>,</span><br><span class="line">      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)</span><br><span class="line">    <span class="keyword">val</span> appUIAddress = sc.ui.map(_.appUIAddress).getOrElse(<span class="string">""</span>)</span><br><span class="line">    <span class="keyword">val</span> coresPerExecutor = conf.getOption(<span class="string">"spark.executor.cores"</span>).map(_.toInt)</span><br><span class="line">    <span class="comment">// If we're using dynamic allocation, set our initial executor limit to 0 for now.</span></span><br><span class="line">    <span class="comment">// ExecutorAllocationManager will send the real initial limit to the Master later.</span></span><br><span class="line">    <span class="keyword">val</span> initialExecutorLimit =</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">Utils</span>.isDynamicAllocationEnabled(conf)) &#123;</span><br><span class="line">        <span class="type">Some</span>(<span class="number">0</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line">    <span class="keyword">val</span> appDesc = <span class="keyword">new</span> <span class="type">ApplicationDescription</span>(sc.appName, maxCores, sc.executorMemory, command,</span><br><span class="line">      appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)</span><br><span class="line">    client = <span class="keyword">new</span> <span class="type">StandaloneAppClient</span>(sc.env.rpcEnv, masters, appDesc, <span class="keyword">this</span>, conf)</span><br><span class="line">    client.start()</span><br><span class="line">    launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">SUBMITTED</span>)</span><br><span class="line">    waitForRegistration()</span><br><span class="line">    launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">RUNNING</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">stop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    stop(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">FINISHED</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">connected</span></span>(appId: <span class="type">String</span>) &#123;</span><br><span class="line">    logInfo(<span class="string">"Connected to Spark cluster with app ID "</span> + appId)</span><br><span class="line">    <span class="keyword">this</span>.appId = appId</span><br><span class="line">    notifyContext()</span><br><span class="line">    launcherBackend.setAppId(appId)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">disconnected</span></span>() &#123;</span><br><span class="line">    notifyContext()</span><br><span class="line">    <span class="keyword">if</span> (!stopping.get) &#123;</span><br><span class="line">      logWarning(<span class="string">"Disconnected from Spark cluster! Waiting for reconnection..."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dead</span></span>(reason: <span class="type">String</span>) &#123;</span><br><span class="line">    notifyContext()</span><br><span class="line">    <span class="keyword">if</span> (!stopping.get) &#123;</span><br><span class="line">      launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">KILLED</span>)</span><br><span class="line">      logError(<span class="string">"Application has been killed. Reason: "</span> + reason)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        scheduler.error(reason)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// Ensure the application terminates, as we can no longer run jobs.</span></span><br><span class="line">        sc.stopInNewThread()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="CoarseGrainedSchedulerBackend"><a href="#CoarseGrainedSchedulerBackend" class="headerlink" title="CoarseGrainedSchedulerBackend"></a>CoarseGrainedSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">  <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[(<span class="type">String</span>, <span class="type">String</span>)]</span><br><span class="line">  <span class="keyword">for</span> ((key, value) &lt;- scheduler.sc.conf.getAll) &#123;</span><br><span class="line">    <span class="keyword">if</span> (key.startsWith(<span class="string">"spark."</span>)) &#123;</span><br><span class="line">      properties += ((key, value))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// TODO (prashant) send conf instead of properties</span></span><br><span class="line">  driverEndpoint = createDriverEndpointRef(properties)  <span class="comment">//创建了一个driverEndpoint</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到StandaloneSchedulerBackend"><a href="#回到StandaloneSchedulerBackend" class="headerlink" title="回到StandaloneSchedulerBackend"></a>回到StandaloneSchedulerBackend</h3><p>我们看一下再start方法中的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建了一个ApplicationDescription</span></span><br><span class="line"><span class="keyword">val</span> appDesc = <span class="keyword">new</span> <span class="type">ApplicationDescription</span>(sc.appName, maxCores, sc.executorMemory, command,</span><br><span class="line">     appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)</span><br><span class="line"><span class="comment">//创建了一个StandaloneAppClient</span></span><br><span class="line">   client = <span class="keyword">new</span> <span class="type">StandaloneAppClient</span>(sc.env.rpcEnv, masters, appDesc, <span class="keyword">this</span>, conf)  <span class="comment">//控制权限转到StandaloneAppClient</span></span><br><span class="line">   client.start() </span><br><span class="line">   launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">SUBMITTED</span>)</span><br><span class="line">   waitForRegistration()</span><br><span class="line">   launcherBackend.setState(<span class="type">SparkAppHandle</span>.<span class="type">State</span>.<span class="type">RUNNING</span>)</span><br></pre></td></tr></table></figure><h3 id="跳转StandaloneAppClient"><a href="#跳转StandaloneAppClient" class="headerlink" title="跳转StandaloneAppClient"></a>跳转StandaloneAppClient</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">StandaloneAppClient</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masterUrls: <span class="type">Array</span>[<span class="type">String</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    appDescription: <span class="type">ApplicationDescription</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    listener: <span class="type">StandaloneAppClientListener</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123; <span class="comment">//Client </span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> masterRpcAddresses = masterUrls.map(<span class="type">RpcAddress</span>.fromSparkURL(_))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">REGISTRATION_TIMEOUT_SECONDS</span> = <span class="number">20</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">REGISTRATION_RETRIES</span> = <span class="number">3</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> endpoint = <span class="keyword">new</span> <span class="type">AtomicReference</span>[<span class="type">RpcEndpointRef</span>]</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> appId = <span class="keyword">new</span> <span class="type">AtomicReference</span>[<span class="type">String</span>]</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> registered = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"><span class="comment">//也是个端点,</span></span><br><span class="line">  <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">ClientEndpoint</span>(<span class="params">override val rpcEnv: <span class="type">RpcEnv</span></span>) <span class="keyword">extends</span> <span class="title">ThreadSafeRpcEndpoint</span></span></span><br><span class="line"><span class="class">    <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> master: <span class="type">Option</span>[<span class="type">RpcEndpointRef</span>] = <span class="type">None</span></span><br><span class="line">    <span class="comment">// To avoid calling listener.disconnected() multiple times</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> alreadyDisconnected = <span class="literal">false</span></span><br><span class="line">    <span class="comment">// To avoid calling listener.dead() multiple times</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> alreadyDead = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> registerMasterFutures = <span class="keyword">new</span> <span class="type">AtomicReference</span>[<span class="type">Array</span>[<span class="type">JFuture</span>[_]]]</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> registrationRetryTimer = <span class="keyword">new</span> <span class="type">AtomicReference</span>[<span class="type">JScheduledFuture</span>[_]]</span><br><span class="line">....</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">    <span class="comment">// Just launch an rpcEndpoint; it will call back into the listener.</span></span><br><span class="line"><span class="comment">//创建了一个ClientEndpoint</span></span><br><span class="line">    endpoint.set(rpcEnv.setupEndpoint(<span class="string">"AppClient"</span>, <span class="keyword">new</span> <span class="type">ClientEndpoint</span>(rpcEnv))) </span><br><span class="line">  &#125;</span><br><span class="line">   ....</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">RegisteredApplication</span>(appId_, masterRef) =&gt;</span><br><span class="line">        <span class="comment">// FIXME How to handle the following cases?</span></span><br><span class="line">        <span class="comment">// 1. A master receives multiple registrations and sends back multiple</span></span><br><span class="line">        <span class="comment">// RegisteredApplications due to an unstable network.</span></span><br><span class="line">        <span class="comment">// 2. Receive multiple RegisteredApplication from different masters because the master is</span></span><br><span class="line">        <span class="comment">// changing.</span></span><br><span class="line">        appId.set(appId_)</span><br><span class="line">        registered.set(<span class="literal">true</span>)</span><br><span class="line">        master = <span class="type">Some</span>(masterRef)</span><br><span class="line">        listener.connected(appId.get)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">ApplicationRemoved</span>(message) =&gt;</span><br><span class="line">        markDead(<span class="string">"Master removed our application: %s"</span>.format(message))</span><br><span class="line">        stop()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">ExecutorAdded</span>(id: <span class="type">Int</span>, workerId: <span class="type">String</span>, hostPort: <span class="type">String</span>, cores: <span class="type">Int</span>, memory: <span class="type">Int</span>) =&gt;</span><br><span class="line">        <span class="keyword">val</span> fullId = appId + <span class="string">"/"</span> + id</span><br><span class="line">        logInfo((<span class="string">"Ex"</span> +</span><br><span class="line">          <span class="string">"ecutor added: %s on %s (%s) with %d cores"</span>).format(fullId, workerId, hostPort,</span><br><span class="line">          cores))</span><br><span class="line">        listener.executorAdded(fullId, workerId, hostPort, cores, memory)</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">ExecutorUpdated</span>(id, state, message, exitStatus, workerLost) =&gt;</span><br><span class="line">        <span class="keyword">val</span> fullId = appId + <span class="string">"/"</span> + id</span><br><span class="line">        <span class="keyword">val</span> messageText = message.map(s =&gt; <span class="string">" ("</span> + s + <span class="string">")"</span>).getOrElse(<span class="string">""</span>)</span><br><span class="line">        logInfo(<span class="string">"Executor updated: %s is now %s%s"</span>.format(fullId, state, messageText))</span><br><span class="line">        <span class="keyword">if</span> (<span class="type">ExecutorState</span>.isFinished(state)) &#123;</span><br><span class="line">          listener.executorRemoved(fullId, message.getOrElse(<span class="string">""</span>), exitStatus, workerLost)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">MasterChanged</span>(masterRef, masterWebUiUrl) =&gt;</span><br><span class="line">        logInfo(<span class="string">"Master has changed, new master is at "</span> + masterRef.address.toSparkURL)</span><br><span class="line">        master = <span class="type">Some</span>(masterRef)</span><br><span class="line">        alreadyDisconnected = <span class="literal">false</span></span><br><span class="line">        masterRef.send(<span class="type">MasterChangeAcknowledged</span>(appId.get))</span><br><span class="line">    &#125; </span><br><span class="line">        </span><br><span class="line">   .... </span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">StopAppClient</span> =&gt;</span><br><span class="line">        markDead(<span class="string">"Application has been stopped."</span>)</span><br><span class="line">        sendToMaster(<span class="type">UnregisterApplication</span>(appId.get))</span><br><span class="line">        context.reply(<span class="literal">true</span>)</span><br><span class="line">        stop()      </span><br><span class="line">  &#125;</span><br><span class="line"> <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWithMaster</span></span>(nthRetry: <span class="type">Int</span>) &#123;  <span class="comment">//向Master 注册</span></span><br><span class="line">      registerMasterFutures.set(tryRegisterAllMasters())  <span class="comment">//查看这个方法</span></span><br><span class="line">      registrationRetryTimer.set(registrationRetryThread.schedule(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (registered.get) &#123;</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(<span class="literal">true</span>))</span><br><span class="line">            registerMasterThreadPool.shutdownNow()</span><br><span class="line">          &#125; <span class="keyword">else</span> <span class="keyword">if</span> (nthRetry &gt;= <span class="type">REGISTRATION_RETRIES</span>) &#123;</span><br><span class="line">            markDead(<span class="string">"All masters are unresponsive! Giving up."</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            registerMasterFutures.get.foreach(_.cancel(<span class="literal">true</span>))</span><br><span class="line">            registerWithMaster(nthRetry + <span class="number">1</span>)  <span class="comment">//重试</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, <span class="type">REGISTRATION_TIMEOUT_SECONDS</span>, <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="tryRegisterAllMasters"><a href="#tryRegisterAllMasters" class="headerlink" title="tryRegisterAllMasters"></a>tryRegisterAllMasters</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">tryRegisterAllMasters</span></span>(): <span class="type">Array</span>[<span class="type">JFuture</span>[_]] = &#123;</span><br><span class="line">   <span class="keyword">for</span> (masterAddress &lt;- masterRpcAddresses) <span class="keyword">yield</span> &#123;</span><br><span class="line">     registerMasterThreadPool.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">       <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="keyword">try</span> &#123;</span><br><span class="line">         <span class="keyword">if</span> (registered.get) &#123;</span><br><span class="line">           <span class="keyword">return</span></span><br><span class="line">         &#125;</span><br><span class="line">         logInfo(<span class="string">"Connecting to master "</span> + masterAddress.toSparkURL + <span class="string">"..."</span>)</span><br><span class="line">         <span class="keyword">val</span> masterRef = rpcEnv.setupEndpointRef(masterAddress, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">            <span class="comment">//给master发送RegisterApplication</span></span><br><span class="line">         masterRef.send(<span class="type">RegisterApplication</span>(appDescription, self)) <span class="comment">//控制权走到Master</span></span><br><span class="line">       &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">         <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// Cancelled</span></span><br><span class="line">         <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">s"Failed to connect to master <span class="subst">$masterAddress</span>"</span>, e)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="回到Master-2"><a href="#回到Master-2" class="headerlink" title="回到Master"></a>回到Master</h3><h3 id="RegisterApplication"><a href="#RegisterApplication" class="headerlink" title="RegisterApplication"></a>RegisterApplication</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">RegisterApplication</span>(description, driver) =&gt;</span><br><span class="line">  <span class="comment">// TODO Prevent repeated registrations from some driver</span></span><br><span class="line">  <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">STANDBY</span>) &#123;</span><br><span class="line">    <span class="comment">// ignore, don't send response</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    logInfo(<span class="string">"Registering app "</span> + description.name)</span><br><span class="line">    <span class="keyword">val</span> app = createApplication(description, driver)</span><br><span class="line">    registerApplication(app)</span><br><span class="line">    logInfo(<span class="string">"Registered app "</span> + description.name + <span class="string">" with ID "</span> + app.id)</span><br><span class="line">    persistenceEngine.addApplication(app)</span><br><span class="line">    driver.send(<span class="type">RegisteredApplication</span>(app.id, self)) <span class="comment">//给driver 送法消息 </span></span><br><span class="line">    <span class="comment">// 启动分配Executor</span></span><br><span class="line">    schedule()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="schedule-1"><a href="#schedule-1" class="headerlink" title="schedule"></a>schedule</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">schedule</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Drivers take strict precedence over executors</span></span><br><span class="line">    <span class="keyword">val</span> shuffledAliveWorkers = <span class="type">Random</span>.shuffle(workers.toSeq.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>))</span><br><span class="line">    <span class="keyword">val</span> numWorkersAlive = shuffledAliveWorkers.size</span><br><span class="line">    <span class="keyword">var</span> curPos = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> (driver &lt;- waitingDrivers.toList) &#123; <span class="comment">// iterate over a copy of waitingDrivers</span></span><br><span class="line">      <span class="comment">// We assign workers to each waiting driver in a round-robin fashion. For each driver, we</span></span><br><span class="line">      <span class="comment">// start from the last worker that was assigned a driver, and continue onwards until we have</span></span><br><span class="line">      <span class="comment">// explored all alive workers.</span></span><br><span class="line">      <span class="keyword">var</span> launched = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">var</span> numWorkersVisited = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span> (numWorkersVisited &lt; numWorkersAlive &amp;&amp; !launched) &#123;</span><br><span class="line">        <span class="keyword">val</span> worker = shuffledAliveWorkers(curPos)</span><br><span class="line">        numWorkersVisited += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123;</span><br><span class="line">          launchDriver(worker, driver)</span><br><span class="line">          waitingDrivers -= driver</span><br><span class="line">          launched = <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">        curPos = (curPos + <span class="number">1</span>) % numWorkersAlive</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    startExecutorsOnWorkers()  <span class="comment">//Worker已经部署 我们需要看这个</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="startExecutorsOnWorkers"><a href="#startExecutorsOnWorkers" class="headerlink" title="startExecutorsOnWorkers"></a>startExecutorsOnWorkers</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startExecutorsOnWorkers</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app</span></span><br><span class="line">  <span class="comment">// in the queue, then the second app, etc.</span></span><br><span class="line">    <span class="comment">//这里的应用并不能并行执行,因为 再for循环里面 的任务智能一个一个分配</span></span><br><span class="line">  <span class="keyword">for</span> (app &lt;- waitingApps <span class="keyword">if</span> app.coresLeft &gt; <span class="number">0</span>) &#123; <span class="comment">//这里waitingApps保存所有需要分配资源的Driver</span></span><br><span class="line">    <span class="keyword">val</span> coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>] = app.desc.coresPerExecutor</span><br><span class="line">    <span class="comment">// Filter out workers that don't have enough resources to launch an executor</span></span><br><span class="line">    <span class="keyword">val</span> usableWorkers = workers.toArray.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>) <span class="comment">//调出满足应用的Exector需要的Worker </span></span><br><span class="line">      .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp; </span><br><span class="line">        worker.coresFree &gt;= coresPerExecutor.getOrElse(<span class="number">1</span>))</span><br><span class="line">      .sortBy(_.coresFree).reverse</span><br><span class="line"></span><br><span class="line">      <span class="comment">//scheduleExecutorsOnWorkers是资源分配的核心算法我们来看一下</span></span><br><span class="line">    <span class="keyword">val</span> assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Now that we've decided how many cores to allocate on each worker, let's allocate them</span></span><br><span class="line">    <span class="keyword">for</span> (pos &lt;- <span class="number">0</span> until usableWorkers.length <span class="keyword">if</span> assignedCores(pos) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      allocateWorkerResourceToExecutors( </span><br><span class="line">        app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="scheduleExecutorsOnWorkers"><a href="#scheduleExecutorsOnWorkers" class="headerlink" title="scheduleExecutorsOnWorkers"></a>scheduleExecutorsOnWorkers</h3><p>资源分配的核心代码</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">scheduleExecutorsOnWorkers</span></span>(</span><br><span class="line">    app: <span class="type">ApplicationInfo</span>,</span><br><span class="line">    usableWorkers: <span class="type">Array</span>[<span class="type">WorkerInfo</span>],</span><br><span class="line">    spreadOutApps: <span class="type">Boolean</span>): <span class="type">Array</span>[<span class="type">Int</span>] = &#123; <span class="comment">//如果参数为True 仅量吧你需要的Executor分配到不同的Worker上</span></span><br><span class="line">  <span class="comment">// 每一个Executor有多少Core</span></span><br><span class="line">  <span class="keyword">val</span> coresPerExecutor = app.desc.coresPerExecutor</span><br><span class="line">  <span class="comment">// minCoresPerExecutor 如果你没定义那么就是1，如果定义了就是coresPerExecutor</span></span><br><span class="line">  <span class="keyword">val</span> minCoresPerExecutor = coresPerExecutor.getOrElse(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 是否允许一个worker上有多个Executor</span></span><br><span class="line">  <span class="keyword">val</span> oneExecutorPerWorker = coresPerExecutor.isEmpty</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 指定Memory 1G</span></span><br><span class="line">  <span class="keyword">val</span> memoryPerExecutor = app.desc.memoryPerExecutorMB</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获得usableWorkers 长度</span></span><br><span class="line">  <span class="keyword">val</span> numUsable = usableWorkers.length</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> assignedCores = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](numUsable) <span class="comment">// Number of cores to give to each worker</span></span><br><span class="line">  <span class="keyword">val</span> assignedExecutors = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](numUsable) <span class="comment">// Number of new executors on each worker</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// coresToAssign 保险</span></span><br><span class="line">  <span class="keyword">var</span> coresToAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Return whether the specified worker can launch an executor for this app. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">canLaunchExecutor</span></span>(pos: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> keepScheduling = coresToAssign &gt;= minCoresPerExecutor</span><br><span class="line">    <span class="keyword">val</span> enoughCores = usableWorkers(pos).coresFree - assignedCores(pos) &gt;= minCoresPerExecutor</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If we allow multiple executors per worker, then we can always launch new executors.</span></span><br><span class="line">    <span class="comment">// Otherwise, if there is already an executor on this worker, just give it more cores.</span></span><br><span class="line">    <span class="keyword">val</span> launchingNewExecutor = !oneExecutorPerWorker || assignedExecutors(pos) == <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> (launchingNewExecutor) &#123;</span><br><span class="line">      <span class="keyword">val</span> assignedMemory = assignedExecutors(pos) * memoryPerExecutor</span><br><span class="line">      <span class="keyword">val</span> enoughMemory = usableWorkers(pos).memoryFree - assignedMemory &gt;= memoryPerExecutor</span><br><span class="line">      <span class="keyword">val</span> underLimit = assignedExecutors.sum + app.executors.size &lt; app.executorLimit</span><br><span class="line">      keepScheduling &amp;&amp; enoughCores &amp;&amp; enoughMemory &amp;&amp; underLimit</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// We're adding cores to an existing executor, so no need</span></span><br><span class="line">      <span class="comment">// to check memory and executor limits</span></span><br><span class="line">      keepScheduling &amp;&amp; enoughCores</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Keep launching executors until no more workers can accommodate any</span></span><br><span class="line">  <span class="comment">// more executors, or if we have reached this application's limits</span></span><br><span class="line">  <span class="keyword">var</span> freeWorkers = (<span class="number">0</span> until numUsable).filter(canLaunchExecutor)</span><br><span class="line">  <span class="keyword">while</span> (freeWorkers.nonEmpty) &#123;</span><br><span class="line">    freeWorkers.foreach &#123; pos =&gt;</span><br><span class="line">      <span class="keyword">var</span> keepScheduling = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">while</span> (keepScheduling &amp;&amp; canLaunchExecutor(pos)) &#123;</span><br><span class="line">        coresToAssign -= minCoresPerExecutor</span><br><span class="line">        assignedCores(pos) += minCoresPerExecutor</span><br><span class="line"></span><br><span class="line">        <span class="comment">// If we are launching one executor per worker, then every iteration assigns 1 core</span></span><br><span class="line">        <span class="comment">// to the executor. Otherwise, every iteration assigns cores to a new executor.</span></span><br><span class="line">        <span class="keyword">if</span> (oneExecutorPerWorker) &#123;</span><br><span class="line">          assignedExecutors(pos) = <span class="number">1</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          assignedExecutors(pos) += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Spreading out an application means spreading out its executors across as</span></span><br><span class="line">        <span class="comment">// many workers as possible. If we are not spreading out, then we should keep</span></span><br><span class="line">        <span class="comment">// scheduling executors on this worker until we use all of its resources.</span></span><br><span class="line">        <span class="comment">// Otherwise, just move on to the next worker.</span></span><br><span class="line">        <span class="keyword">if</span> (spreadOutApps) &#123;</span><br><span class="line">          keepScheduling = <span class="literal">false</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    freeWorkers = freeWorkers.filter(canLaunchExecutor)</span><br><span class="line">  &#125;</span><br><span class="line">  assignedCores</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="startExecutorsOnWorkers-1"><a href="#startExecutorsOnWorkers-1" class="headerlink" title="startExecutorsOnWorkers"></a>startExecutorsOnWorkers</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Schedule and launch executors on workers</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">startExecutorsOnWorkers</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app</span></span><br><span class="line">  <span class="comment">// in the queue, then the second app, etc.</span></span><br><span class="line">  <span class="keyword">for</span> (app &lt;- waitingApps <span class="keyword">if</span> app.coresLeft &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">val</span> coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>] = app.desc.coresPerExecutor</span><br><span class="line">    <span class="comment">// Filter out workers that don't have enough resources to launch an executor</span></span><br><span class="line">    <span class="keyword">val</span> usableWorkers = workers.toArray.filter(_.state == <span class="type">WorkerState</span>.<span class="type">ALIVE</span>)</span><br><span class="line">      .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp;</span><br><span class="line">        worker.coresFree &gt;= coresPerExecutor.getOrElse(<span class="number">1</span>))</span><br><span class="line">      .sortBy(_.coresFree).reverse</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Now that we've decided how many cores to allocate on each worker, let's allocate them</span></span><br><span class="line">    <span class="keyword">for</span> (pos &lt;- <span class="number">0</span> until usableWorkers.length <span class="keyword">if</span> assignedCores(pos) &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      allocateWorkerResourceToExecutors(  <span class="comment">//控制权 回到allocateWorkerResourceToExecutors</span></span><br><span class="line">        app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="allocateWorkerResourceToExecutors"><a href="#allocateWorkerResourceToExecutors" class="headerlink" title="allocateWorkerResourceToExecutors"></a>allocateWorkerResourceToExecutors</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">allocateWorkerResourceToExecutors</span></span>(</span><br><span class="line">    app: <span class="type">ApplicationInfo</span>,</span><br><span class="line">    assignedCores: <span class="type">Int</span>,</span><br><span class="line">    coresPerExecutor: <span class="type">Option</span>[<span class="type">Int</span>],</span><br><span class="line">    worker: <span class="type">WorkerInfo</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// If the number of cores per executor is specified, we divide the cores assigned</span></span><br><span class="line">  <span class="comment">// to this worker evenly among the executors with no remainder.</span></span><br><span class="line">  <span class="comment">// Otherwise, we launch a single executor that grabs all the assignedCores on this worker.</span></span><br><span class="line">  <span class="keyword">val</span> numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(<span class="number">1</span>)</span><br><span class="line">  <span class="keyword">val</span> coresToAssign = coresPerExecutor.getOrElse(assignedCores)</span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to numExecutors) &#123;</span><br><span class="line">    <span class="keyword">val</span> exec = app.addExecutor(worker, coresToAssign)</span><br><span class="line">    launchExecutor(worker, exec) <span class="comment">//启动Executor  给 Worker</span></span><br><span class="line">    app.state = <span class="type">ApplicationState</span>.<span class="type">RUNNING</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="launchExecutor"><a href="#launchExecutor" class="headerlink" title="launchExecutor"></a>launchExecutor</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchExecutor</span></span>(worker: <span class="type">WorkerInfo</span>, exec: <span class="type">ExecutorDesc</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">   logInfo(<span class="string">"Launching executor "</span> + exec.fullId + <span class="string">" on worker "</span> + worker.id)</span><br><span class="line">   worker.addExecutor(exec)</span><br><span class="line">   worker.endpoint.send(<span class="type">LaunchExecutor</span>(masterUrl, <span class="comment">//给worker发送 LaunchExecutor</span></span><br><span class="line">     exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))</span><br><span class="line">   exec.application.driver.send(</span><br><span class="line">     <span class="type">ExecutorAdded</span>(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="回到Worker-1"><a href="#回到Worker-1" class="headerlink" title="回到Worker"></a>回到Worker</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">LaunchExecutor</span>(masterUrl, appId, execId, appDesc, cores_, memory_) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (masterUrl != activeMasterUrl) &#123;</span><br><span class="line">        logWarning(<span class="string">"Invalid Master ("</span> + masterUrl + <span class="string">") attempted to launch executor."</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          logInfo(<span class="string">"Asked to launch executor %s/%d for %s"</span>.format(appId, execId, appDesc.name))</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Create the executor's working directory</span></span><br><span class="line">          <span class="keyword">val</span> executorDir = <span class="keyword">new</span> <span class="type">File</span>(workDir, appId + <span class="string">"/"</span> + execId) <span class="comment">//创建工作目录</span></span><br><span class="line">          <span class="keyword">if</span> (!executorDir.mkdirs()) &#123; <span class="comment">//如果没有创建</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IOException</span>(<span class="string">"Failed to create directory "</span> + executorDir)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="comment">// Create local dirs for the executor. These are passed to the executor via the</span></span><br><span class="line">          <span class="comment">// SPARK_EXECUTOR_DIRS environment variable, and deleted by the Worker when the</span></span><br><span class="line">          <span class="comment">// application finishes.</span></span><br><span class="line">          <span class="keyword">val</span> appLocalDirs = appDirectories.getOrElse(appId,</span><br><span class="line">            <span class="type">Utils</span>.getOrCreateLocalRootDirs(conf).map &#123; dir =&gt;</span><br><span class="line">              <span class="keyword">val</span> appDir = <span class="type">Utils</span>.createDirectory(dir, namePrefix = <span class="string">"executor"</span>)</span><br><span class="line">              <span class="type">Utils</span>.chmod700(appDir)</span><br><span class="line">              appDir.getAbsolutePath()</span><br><span class="line">            &#125;.toSeq)</span><br><span class="line">          appDirectories(appId) = appLocalDirs </span><br><span class="line">          <span class="keyword">val</span> manager = <span class="keyword">new</span> <span class="type">ExecutorRunner</span>( <span class="comment">//如果可以</span></span><br><span class="line">            appId,</span><br><span class="line">            execId,</span><br><span class="line">            appDesc.copy(command = <span class="type">Worker</span>.maybeUpdateSSLSettings(appDesc.command, conf)),</span><br><span class="line">            cores_,</span><br><span class="line">            memory_,</span><br><span class="line">            self,</span><br><span class="line">            workerId,</span><br><span class="line">            host,</span><br><span class="line">            webUi.boundPort,</span><br><span class="line">            publicAddress,</span><br><span class="line">            sparkHome,</span><br><span class="line">            executorDir,</span><br><span class="line">            workerUri,</span><br><span class="line">            conf,</span><br><span class="line">            appLocalDirs, <span class="type">ExecutorState</span>.<span class="type">RUNNING</span>)</span><br><span class="line">          executors(appId + <span class="string">"/"</span> + execId) = manager</span><br><span class="line">          manager.start() <span class="comment">//控制权到了这里</span></span><br><span class="line">          coresUsed += cores_</span><br><span class="line">          memoryUsed += memory_</span><br><span class="line">          sendToMaster(<span class="type">ExecutorStateChanged</span>(appId, execId, manager.state, <span class="type">None</span>, <span class="type">None</span>)) <span class="comment">//给Master发送Executor的资源分配已经好了 </span></span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">            logError(<span class="string">s"Failed to launch executor <span class="subst">$appId</span>/<span class="subst">$execId</span> for <span class="subst">$&#123;appDesc.name&#125;</span>."</span>, e)</span><br><span class="line">            <span class="keyword">if</span> (executors.contains(appId + <span class="string">"/"</span> + execId)) &#123;</span><br><span class="line">              executors(appId + <span class="string">"/"</span> + execId).kill()</span><br><span class="line">              executors -= appId + <span class="string">"/"</span> + execId</span><br><span class="line">            &#125;</span><br><span class="line">            sendToMaster(<span class="type">ExecutorStateChanged</span>(appId, execId, <span class="type">ExecutorState</span>.<span class="type">FAILED</span>,</span><br><span class="line">              <span class="type">Some</span>(e.toString), <span class="type">None</span>))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><h3 id="跳转到ExecutorRunner"><a href="#跳转到ExecutorRunner" class="headerlink" title="跳转到ExecutorRunner"></a>跳转到ExecutorRunner</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[worker] <span class="function"><span class="keyword">def</span> <span class="title">start</span></span>() &#123;</span><br><span class="line">  workerThread = <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"ExecutorRunner for "</span> + fullId) &#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123; fetchAndRunExecutor() &#125; <span class="comment">//启动Executro我们点进来看一下</span></span><br><span class="line">  &#125;</span><br><span class="line">  workerThread.start()</span><br><span class="line">  <span class="comment">// Shutdown hook that kills actors on shutdown.</span></span><br><span class="line">  shutdownHook = <span class="type">ShutdownHookManager</span>.addShutdownHook &#123; () =&gt;</span><br><span class="line">    <span class="comment">// It's possible that we arrive here before calling `fetchAndRunExecutor`, then `state` will</span></span><br><span class="line">    <span class="comment">// be `ExecutorState.RUNNING`. In this case, we should set `state` to `FAILED`.</span></span><br><span class="line">    <span class="keyword">if</span> (state == <span class="type">ExecutorState</span>.<span class="type">RUNNING</span>) &#123;</span><br><span class="line">      state = <span class="type">ExecutorState</span>.<span class="type">FAILED</span></span><br><span class="line">    &#125;</span><br><span class="line">    killProcess(<span class="type">Some</span>(<span class="string">"Worker shutting down"</span>)) &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="fetchAndRunExecutor"><a href="#fetchAndRunExecutor" class="headerlink" title="fetchAndRunExecutor"></a>fetchAndRunExecutor</h3><p>没有特别指出</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">fetchAndRunExecutor</span></span>() &#123;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// Launch the process</span></span><br><span class="line">    <span class="keyword">val</span> builder = <span class="type">CommandUtils</span>.buildProcessBuilder(appDesc.command, <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf),</span><br><span class="line">      memory, sparkHome.getAbsolutePath, substituteVariables)</span><br><span class="line">    <span class="keyword">val</span> command = builder.command() <span class="comment">//这里的builder应该是processbuilder 运行一个本地命令</span></span><br><span class="line">    <span class="keyword">val</span> formattedCommand = command.asScala.mkString(<span class="string">"\""</span>, <span class="string">"\" \""</span>, <span class="string">"\""</span>)</span><br><span class="line">    logInfo(<span class="string">s"Launch command: <span class="subst">$formattedCommand</span>"</span>)</span><br><span class="line"></span><br><span class="line">    builder.directory(executorDir)</span><br><span class="line">    builder.environment.put(<span class="string">"SPARK_EXECUTOR_DIRS"</span>, appLocalDirs.mkString(<span class="type">File</span>.pathSeparator))</span><br><span class="line">    <span class="comment">// In case we are running this from within the Spark Shell, avoid creating a "scala"</span></span><br><span class="line">    <span class="comment">// parent process for the executor command</span></span><br><span class="line">    builder.environment.put(<span class="string">"SPARK_LAUNCH_WITH_SCALA"</span>, <span class="string">"0"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Add webUI log urls</span></span><br><span class="line">    <span class="keyword">val</span> baseUrl =</span><br><span class="line">      <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.ui.reverseProxy"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">        <span class="string">s"/proxy/<span class="subst">$workerId</span>/logPage/?appId=<span class="subst">$appId</span>&amp;executorId=<span class="subst">$execId</span>&amp;logType="</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="string">s"http://<span class="subst">$publicAddress</span>:<span class="subst">$webUiPort</span>/logPage/?appId=<span class="subst">$appId</span>&amp;executorId=<span class="subst">$execId</span>&amp;logType="</span></span><br><span class="line">      &#125;</span><br><span class="line">    builder.environment.put(<span class="string">"SPARK_LOG_URL_STDERR"</span>, <span class="string">s"<span class="subst">$&#123;baseUrl&#125;</span>stderr"</span>)</span><br><span class="line">    builder.environment.put(<span class="string">"SPARK_LOG_URL_STDOUT"</span>, <span class="string">s"<span class="subst">$&#123;baseUrl&#125;</span>stdout"</span>)</span><br><span class="line"></span><br><span class="line">    process = builder.start() <span class="comment">// 根据流程图 会创建一个 CoarseGrainedSchedulerBackend 我们去哪里看下</span></span><br><span class="line">    <span class="keyword">val</span> header = <span class="string">"Spark Executor Command: %s\n%s\n\n"</span>.format(</span><br><span class="line">      formattedCommand, <span class="string">"="</span> * <span class="number">40</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Redirect its stdout and stderr to files</span></span><br><span class="line">    <span class="keyword">val</span> stdout = <span class="keyword">new</span> <span class="type">File</span>(executorDir, <span class="string">"stdout"</span>)</span><br><span class="line">    stdoutAppender = <span class="type">FileAppender</span>(process.getInputStream, stdout, conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> stderr = <span class="keyword">new</span> <span class="type">File</span>(executorDir, <span class="string">"stderr"</span>)</span><br><span class="line">    <span class="type">Files</span>.write(header, stderr, <span class="type">StandardCharsets</span>.<span class="type">UTF_8</span>)</span><br><span class="line">    stderrAppender = <span class="type">FileAppender</span>(process.getErrorStream, stderr, conf)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Wait for it to exit; executor may exit with code 0 (when driver instructs it to shutdown)</span></span><br><span class="line">    <span class="comment">// or with nonzero exit code</span></span><br><span class="line">    <span class="keyword">val</span> exitCode = process.waitFor()</span><br><span class="line">    state = <span class="type">ExecutorState</span>.<span class="type">EXITED</span></span><br><span class="line">    <span class="keyword">val</span> message = <span class="string">"Command exited with code "</span> + exitCode</span><br><span class="line">    worker.send(<span class="type">ExecutorStateChanged</span>(appId, execId, state, <span class="type">Some</span>(message), <span class="type">Some</span>(exitCode)))</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> interrupted: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line">      logInfo(<span class="string">"Runner thread for executor "</span> + fullId + <span class="string">" interrupted"</span>)</span><br><span class="line">      state = <span class="type">ExecutorState</span>.<span class="type">KILLED</span></span><br><span class="line">      killProcess(<span class="type">None</span>)</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logError(<span class="string">"Error running executor"</span>, e)</span><br><span class="line">      state = <span class="type">ExecutorState</span>.<span class="type">FAILED</span></span><br><span class="line">      killProcess(<span class="type">Some</span>(e.toString))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到Master-3"><a href="#回到Master-3" class="headerlink" title="回到Master"></a>回到Master</h3><p>因为<code>sendToMaster(ExecutorStateChanged(appId, execId, manager.state, None, None))</code>给Master发送了一条消息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">case</span> <span class="type">ExecutorStateChanged</span>(appId, execId, state, message, exitStatus) =&gt;</span><br><span class="line">   <span class="keyword">val</span> execOption = idToApp.get(appId).flatMap(app =&gt; app.executors.get(execId))</span><br><span class="line">   execOption <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Some</span>(exec) =&gt;</span><br><span class="line">       <span class="keyword">val</span> appInfo = idToApp(appId)</span><br><span class="line">       <span class="keyword">val</span> oldState = exec.state</span><br><span class="line">       exec.state = state</span><br><span class="line"></span><br><span class="line">       <span class="keyword">if</span> (state == <span class="type">ExecutorState</span>.<span class="type">RUNNING</span>) &#123;</span><br><span class="line">         assert(oldState == <span class="type">ExecutorState</span>.<span class="type">LAUNCHING</span>,</span><br><span class="line">           <span class="string">s"executor <span class="subst">$execId</span> state transfer from <span class="subst">$oldState</span> to RUNNING is illegal"</span>)</span><br><span class="line">         appInfo.resetRetryCount()</span><br><span class="line">       &#125;</span><br><span class="line"><span class="comment">//查看一下ExecutorUpdated 再StandaloneAppClient中</span></span><br><span class="line">       exec.application.driver.send(<span class="type">ExecutorUpdated</span>(execId, state, message, exitStatus, <span class="literal">false</span>))</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="跳转到StandaloneAppClient"><a href="#跳转到StandaloneAppClient" class="headerlink" title="跳转到StandaloneAppClient"></a>跳转到StandaloneAppClient</h3><h3 id="ExecutorUpdated"><a href="#ExecutorUpdated" class="headerlink" title="ExecutorUpdated"></a>ExecutorUpdated</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">ExecutorAdded</span>(id: <span class="type">Int</span>, workerId: <span class="type">String</span>, hostPort: <span class="type">String</span>, cores: <span class="type">Int</span>, memory: <span class="type">Int</span>) =&gt;</span><br><span class="line">     <span class="keyword">val</span> fullId = appId + <span class="string">"/"</span> + id</span><br><span class="line">     logInfo((<span class="string">"Ex"</span> +</span><br><span class="line">       <span class="string">"ecutor added: %s on %s (%s) with %d cores"</span>).format(fullId, workerId, hostPort,</span><br><span class="line">       cores))</span><br><span class="line">     listener.executorAdded(fullId, workerId, hostPort, cores, memory) <span class="comment">//这里的listener</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">case</span> <span class="type">ExecutorUpdated</span>(id, state, message, exitStatus, workerLost) =&gt;</span><br><span class="line">     <span class="keyword">val</span> fullId = appId + <span class="string">"/"</span> + id</span><br><span class="line">     <span class="keyword">val</span> messageText = message.map(s =&gt; <span class="string">" ("</span> + s + <span class="string">")"</span>).getOrElse(<span class="string">""</span>)</span><br><span class="line">     logInfo(<span class="string">"Executor updated: %s is now %s%s"</span>.format(fullId, state, messageText))</span><br><span class="line">     <span class="keyword">if</span> (<span class="type">ExecutorState</span>.isFinished(state)) &#123;</span><br><span class="line">       listener.executorRemoved(fullId, message.getOrElse(<span class="string">""</span>), exitStatus, workerLost)</span><br><span class="line">     &#125;  <span class="comment">//Executor资源基本完成</span></span><br><span class="line"></span><br><span class="line">   <span class="keyword">case</span> <span class="type">MasterChanged</span>(masterRef, masterWebUiUrl) =&gt;</span><br><span class="line">     logInfo(<span class="string">"Master has changed, new master is at "</span> + masterRef.address.toSparkURL)</span><br><span class="line">     master = <span class="type">Some</span>(masterRef)</span><br><span class="line">     alreadyDisconnected = <span class="literal">false</span></span><br><span class="line">     masterRef.send(<span class="type">MasterChangeAcknowledged</span>(appId.get)) </span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h3 id="listener"><a href="#listener" class="headerlink" title="listener"></a>listener</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">StandaloneAppClient</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masterUrls: <span class="type">Array</span>[<span class="type">String</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    appDescription: <span class="type">ApplicationDescription</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    listener: <span class="type">StandaloneAppClientListener</span>, //这就是<span class="type">Executor</span>中的listener</span></span></span><br><span class="line"><span class="class"><span class="params">    conf: <span class="type">SparkConf</span></span>)</span></span><br></pre></td></tr></table></figure><h3 id="回到StandaloneSchedulerBackend-1"><a href="#回到StandaloneSchedulerBackend-1" class="headerlink" title="回到StandaloneSchedulerBackend"></a>回到StandaloneSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">StandaloneSchedulerBackend</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    scheduler: <span class="type">TaskSchedulerImpl</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    sc: <span class="type">SparkContext</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masters: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">CoarseGrainedSchedulerBackend</span>(<span class="params">scheduler, sc.env.rpcEnv</span>) </span></span><br><span class="line"><span class="class">  <span class="keyword">with</span> <span class="title">StandaloneAppClientListener</span>  <span class="title">//继承了那个listner</span></span>;</span><br><span class="line">  <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line">      ....</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="StandaloneAppClientListener"><a href="#StandaloneAppClientListener" class="headerlink" title="StandaloneAppClientListener"></a>StandaloneAppClientListener</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">StandaloneAppClientListener</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">connected</span></span>(appId: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Disconnection may be a temporary state, as we fail over to a new Master. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">disconnected</span></span>(): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/** An application death is an unrecoverable failure condition. */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">dead</span></span>(reason: <span class="type">String</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executorAdded</span></span>(</span><br><span class="line">      fullId: <span class="type">String</span>, workerId: <span class="type">String</span>, hostPort: <span class="type">String</span>, cores: <span class="type">Int</span>, memory: <span class="type">Int</span>): <span class="type">Unit</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">executorRemoved</span></span>(</span><br><span class="line">      fullId: <span class="type">String</span>, message: <span class="type">String</span>, exitStatus: <span class="type">Option</span>[<span class="type">Int</span>], workerLost: <span class="type">Boolean</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="查看executorAdded"><a href="#查看executorAdded" class="headerlink" title="查看executorAdded"></a>查看executorAdded</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">executorAdded</span></span>(fullId: <span class="type">String</span>, workerId: <span class="type">String</span>, hostPort: <span class="type">String</span>, cores: <span class="type">Int</span>,</span><br><span class="line">    memory: <span class="type">Int</span>) &#123;</span><br><span class="line">    logInfo(<span class="string">"Granted executor ID %s on hostPort %s with %d cores, %s RAM"</span>.format(</span><br><span class="line">      fullId, hostPort, cores, <span class="type">Utils</span>.megabytesToString(memory)))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>到此为止步,Executor进程启动完成.</p><h2 id="任务提交和反馈"><a href="#任务提交和反馈" class="headerlink" title="任务提交和反馈"></a>任务提交和反馈</h2><p>通过对RDD的观察行动算子最终调用的方法runJob</p><h3 id="SparkContext"><a href="#SparkContext" class="headerlink" title="SparkContext"></a>SparkContext</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Run a function on a given set of partitions in an RDD and pass the results to the given</span></span><br><span class="line"><span class="comment"> * handler function. This is the main entry point for all actions in Spark.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"SparkContext has been shutdown"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  logInfo(<span class="string">"Starting job: "</span> + callSite.shortForm)</span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.logLineage"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">    logInfo(<span class="string">"RDD's recursive dependencies:\n"</span> + rdd.toDebugString)</span><br><span class="line">  &#125;  <span class="comment">//我们查看一下dagScheduler</span></span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br><span class="line">  rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Run a function on a given set of partitions in an RDD and return the results as an array.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> results = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">U</span>](partitions.size)</span><br><span class="line">  runJob[<span class="type">T</span>, <span class="type">U</span>](rdd, func, partitions, (index, res) =&gt; results(index) = res)</span><br><span class="line">  results</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="DAGScheduler"><a href="#DAGScheduler" class="headerlink" title="DAGScheduler"></a>DAGScheduler</h3><h3 id="runrunJob"><a href="#runrunJob" class="headerlink" title="runrunJob"></a>runrunJob</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Run an action job on the given RDD and pass all the results to the resultHandler function as</span></span><br><span class="line"><span class="comment"> * they arrive.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param rdd target RDD to run tasks on</span></span><br><span class="line"><span class="comment"> * @param func a function to run on each partition of the RDD</span></span><br><span class="line"><span class="comment"> * @param partitions set of partitions to run on; some jobs may not want to compute on all</span></span><br><span class="line"><span class="comment"> *   partitions of the target RDD, e.g. for operations like first()</span></span><br><span class="line"><span class="comment"> * @param callSite where in the user program this job was called</span></span><br><span class="line"><span class="comment"> * @param resultHandler callback to pass each result to</span></span><br><span class="line"><span class="comment"> * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @throws Exception when the job fails</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime    <span class="comment">/**提交Job**/</span></span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) </span><br><span class="line">  <span class="comment">// Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,</span></span><br><span class="line">  <span class="comment">// which causes concurrent SQL executions to fail if a fork-join pool is used. Note that</span></span><br><span class="line">  <span class="comment">// due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it's</span></span><br><span class="line">  <span class="comment">// safe to pass in null here. For more detail, see SPARK-13747.</span></span><br><span class="line">  <span class="keyword">val</span> awaitPermission = <span class="literal">null</span>.asInstanceOf[scala.concurrent.<span class="type">CanAwait</span>]</span><br><span class="line">  waiter.completionFuture.ready(<span class="type">Duration</span>.<span class="type">Inf</span>)(awaitPermission)</span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d finished: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">      logInfo(<span class="string">"Job %d failed: %s, took %f s"</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">      <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">      exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">      <span class="keyword">throw</span> exception</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="submitJob"><a href="#submitJob" class="headerlink" title="submitJob"></a>submitJob</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="comment">// Check to make sure we are not launching a task on a partition that does not exist.</span></span><br><span class="line">  <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">  partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">      <span class="string">"Attempting to access a non-existent partition: "</span> + p + <span class="string">". "</span> +</span><br><span class="line">        <span class="string">"Total number of partitions: "</span> + maxPartitions)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">  <span class="keyword">if</span> (partitions.size == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// Return immediately if the job is running 0 tasks</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  assert(partitions.size &gt; <span class="number">0</span>)</span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">  <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>(<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>( <span class="comment">//post一个JobSubmitted事件</span></span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">SerializationUtils</span>.clone(properties)))</span><br><span class="line">  waiter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="eventProcessLoop"><a href="#eventProcessLoop" class="headerlink" title="eventProcessLoop"></a>eventProcessLoop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="keyword">val</span> eventProcessLoop = <span class="keyword">new</span> <span class="type">DAGSchedulerEventProcessLoop</span>(<span class="keyword">this</span>) </span><br><span class="line">taskScheduler.setDAGScheduler(<span class="keyword">this</span>)</span><br></pre></td></tr></table></figure><h3 id="DAGSchedulerEventProcessLoop"><a href="#DAGSchedulerEventProcessLoop" class="headerlink" title="DAGSchedulerEventProcessLoop"></a>DAGSchedulerEventProcessLoop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="params">"dag-scheduler-event-loop"</span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123; <span class="comment">//点击EventLoop</span></span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="EventLoop"><a href="#EventLoop" class="headerlink" title="EventLoop"></a>EventLoop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">EventLoop</span>[<span class="type">E</span>](<span class="params">name: <span class="type">String</span></span>) <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventQueue: <span class="type">BlockingQueue</span>[<span class="type">E</span>] = <span class="keyword">new</span> <span class="type">LinkedBlockingDeque</span>[<span class="type">E</span>]() <span class="comment">//阻塞队列</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> stopped = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> eventThread = <span class="keyword">new</span> <span class="type">Thread</span>(name) &#123;</span><br><span class="line">    setDaemon(<span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123; <span class="comment">//run方法</span></span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">while</span> (!stopped.get) &#123;</span><br><span class="line">          <span class="keyword">val</span> event = eventQueue.take() <span class="comment">//循环读取队列</span></span><br><span class="line">          <span class="keyword">try</span> &#123; <span class="comment">//读到序列</span></span><br><span class="line">            onReceive(event)  <span class="comment">//开启</span></span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">              <span class="keyword">try</span> &#123;</span><br><span class="line">                onError(e)</span><br><span class="line">              &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">"Unexpected error in "</span> + name, e)</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit even if eventQueue is not empty</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(<span class="string">"Unexpected error in "</span> + name, e)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">....</span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">E</span>): <span class="type">Unit</span>  <span class="comment">//抽象方法</span></span><br></pre></td></tr></table></figure><h3 id="回到DAGSchedulerEventProcessLoop"><a href="#回到DAGSchedulerEventProcessLoop" class="headerlink" title="回到DAGSchedulerEventProcessLoop"></a>回到DAGSchedulerEventProcessLoop</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;  </span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)  <span class="comment">//我们关住一下handleJobSubmitted</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">StageCancelled</span>(stageId) =&gt;</span><br><span class="line">      dagScheduler.handleStageCancellation(stageId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobCancelled</span>(jobId) =&gt;</span><br><span class="line">      dagScheduler.handleJobCancellation(jobId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobGroupCancelled</span>(groupId) =&gt;</span><br><span class="line">      dagScheduler.handleJobGroupCancelled(groupId)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">AllJobsCancelled</span> =&gt;</span><br><span class="line">      dagScheduler.doCancelAllJobs()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorAdded</span>(execId, host) =&gt;</span><br><span class="line">      dagScheduler.handleExecutorAdded(execId, host)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorLost</span>(execId, reason) =&gt;</span><br><span class="line">      <span class="keyword">val</span> filesLost = reason <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">SlaveLost</span>(_, <span class="literal">true</span>) =&gt; <span class="literal">true</span></span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="literal">false</span></span><br><span class="line">      &#125;</span><br><span class="line">      dagScheduler.handleExecutorLost(execId, filesLost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">BeginEvent</span>(task, taskInfo) =&gt;</span><br><span class="line">      dagScheduler.handleBeginEvent(task, taskInfo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">GettingResultEvent</span>(taskInfo) =&gt;</span><br><span class="line">      dagScheduler.handleGetTaskResult(taskInfo)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> completion: <span class="type">CompletionEvent</span> =&gt;</span><br><span class="line">      dagScheduler.handleTaskCompletion(completion)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">TaskSetFailed</span>(taskSet, reason, exception) =&gt;</span><br><span class="line">      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ResubmitFailedStages</span> =&gt;</span><br><span class="line">      dagScheduler.resubmitFailedStages()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="handleJobSubmitted"><a href="#handleJobSubmitted" class="headerlink" title="handleJobSubmitted"></a>handleJobSubmitted</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>) &#123;</span><br><span class="line">    <span class="comment">//一下代码是如何将代码拆分成一个个stage 再把</span></span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// New stage creation may throw an exception if, for example, jobs are run on a</span></span><br><span class="line">    <span class="comment">// HadoopRDD whose underlying HDFS files have been deleted.</span></span><br><span class="line"></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">"Creating new stage failed due to exception - job: "</span> + jobId, e)</span><br><span class="line">      listener.jobFailed(e)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">  clearCacheLocs()</span><br><span class="line">  logInfo(<span class="string">"Got job %s (%s) with %d output partitions"</span>.format(</span><br><span class="line">    job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">  logInfo(<span class="string">"Final stage: "</span> + finalStage + <span class="string">" ("</span> + finalStage.name + <span class="string">")"</span>)</span><br><span class="line">  logInfo(<span class="string">"Parents of final stage: "</span> + finalStage.parents)</span><br><span class="line">  logInfo(<span class="string">"Missing parents: "</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">  jobIdToActiveJob(jobId) = job</span><br><span class="line">  activeJobs += job</span><br><span class="line">  finalStage.setActiveJob(job)</span><br><span class="line">  <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">  <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">  listenerBus.post(</span><br><span class="line">    <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos, properties))</span><br><span class="line">    <span class="comment">//最后提交了submitStage</span></span><br><span class="line">  submitStage(finalStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="submitStage"><a href="#submitStage" class="headerlink" title="submitStage"></a>submitStage</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">        <span class="comment">//stage是从后往前划分</span></span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">"missing: "</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123; </span><br><span class="line">        logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//如果获得了missing 我们卡产一下是怎么处理的</span></span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="getMissingParentStages"><a href="#getMissingParentStages" class="headerlink" title="getMissingParentStages"></a>getMissingParentStages</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMissingParentStages</span></span>(stage: <span class="type">Stage</span>): <span class="type">List</span>[<span class="type">Stage</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> missing = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">Stage</span>]</span><br><span class="line">  <span class="keyword">val</span> visited = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">RDD</span>[_]]</span><br><span class="line">  <span class="comment">// We are manually maintaining a stack here to prevent StackOverflowError</span></span><br><span class="line">  <span class="comment">// caused by recursively visiting</span></span><br><span class="line">  <span class="keyword">val</span> waitingForVisit = <span class="keyword">new</span> <span class="type">Stack</span>[<span class="type">RDD</span>[_]] <span class="comment">//栈结构的 这是为什么Stage划分从后往前 压栈执行</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">visit</span></span>(rdd: <span class="type">RDD</span>[_]) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!visited(rdd)) &#123;</span><br><span class="line">      visited += rdd</span><br><span class="line">      <span class="keyword">val</span> rddHasUncachedPartitions = getCacheLocs(rdd).contains(<span class="type">Nil</span>)</span><br><span class="line">      <span class="keyword">if</span> (rddHasUncachedPartitions) &#123;</span><br><span class="line">        <span class="keyword">for</span> (dep &lt;- rdd.dependencies) &#123;</span><br><span class="line">          dep <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> shufDep: <span class="type">ShuffleDependency</span>[_, _, _] =&gt; <span class="comment">//如果是 宽依赖</span></span><br><span class="line">              <span class="keyword">val</span> mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)</span><br><span class="line">              <span class="keyword">if</span> (!mapStage.isAvailable) &#123; </span><br><span class="line">                missing += mapStage</span><br><span class="line">              &#125;</span><br><span class="line">            <span class="keyword">case</span> narrowDep: <span class="type">NarrowDependency</span>[_] =&gt; <span class="comment">//如果是窄依赖</span></span><br><span class="line">              waitingForVisit.push(narrowDep.rdd)  <span class="comment">//主要是填充stage</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  waitingForVisit.push(stage.rdd)</span><br><span class="line">  <span class="keyword">while</span> (waitingForVisit.nonEmpty) &#123;</span><br><span class="line">    visit(waitingForVisit.pop())</span><br><span class="line">  &#125;</span><br><span class="line">  missing.toList</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="submitStage-1"><a href="#submitStage-1" class="headerlink" title="submitStage"></a>submitStage</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Submits stage, but first recursively submits any missing parents. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">"submitStage("</span> + stage + <span class="string">")"</span>) </span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">"missing: "</span> + missing) </span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123; </span><br><span class="line">        logInfo(<span class="string">"Submitting "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">"), which has no missing parents"</span>)</span><br><span class="line">        submitMissingTasks(stage, jobId.get)<span class="comment">//递归调用</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent) <span class="comment">//递归</span></span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">"No active job for stage "</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="submitMissingTasks"><a href="#submitMissingTasks" class="headerlink" title="submitMissingTasks"></a>submitMissingTasks</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Called when stage's parents are available and we can now do its task. */</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>) &#123;</span><br><span class="line">  logDebug(<span class="string">"submitMissingTasks("</span> + stage + <span class="string">")"</span>)</span><br><span class="line">  <span class="comment">// Get our pending tasks and remember them in our pendingTasks entry</span></span><br><span class="line">  stage.pendingPartitions.clear()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// First figure out the indexes of partition ids to compute.</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Use the scheduling pool, job group, description, etc. from an ActiveJob associated</span></span><br><span class="line">  <span class="comment">// with this Stage</span></span><br><span class="line">  <span class="keyword">val</span> properties = jobIdToActiveJob(jobId).properties</span><br><span class="line"></span><br><span class="line">  runningStages += stage</span><br><span class="line">  <span class="comment">// SparkListenerStageSubmitted should be posted before testing whether tasks are</span></span><br><span class="line">  <span class="comment">// serializable. If tasks are not serializable, a SparkListenerStageCompleted event</span></span><br><span class="line">  <span class="comment">// will be posted, which should always come after a corresponding SparkListenerStageSubmitted</span></span><br><span class="line">  <span class="comment">// event.</span></span><br><span class="line">  stage <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">      outputCommitCoordinator.stageStart(</span><br><span class="line">        stage = s.id, maxPartitionId = s.rdd.partitions.length - <span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">          (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">        &#125;.toMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">      listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line">      abortStage(stage, <span class="string">s"Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>"</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo, properties))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.</span></span><br><span class="line">  <span class="comment">// Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast</span></span><br><span class="line">  <span class="comment">// the serialized copy of the RDD and for each task we will deserialize it, which means each</span></span><br><span class="line">  <span class="comment">// task gets a different copy of the RDD. This provides stronger isolation between tasks that</span></span><br><span class="line">  <span class="comment">// might modify state of objects referenced in their closures. This is necessary in Hadoop</span></span><br><span class="line">  <span class="comment">// where the JobConf/Configuration object is not thread-safe.</span></span><br><span class="line">  <span class="keyword">var</span> taskBinary: <span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Byte</span>]] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).</span></span><br><span class="line">    <span class="comment">// For ResultTask, serialize and broadcast (rdd, func).</span></span><br><span class="line">    <span class="keyword">val</span> taskBinaryBytes: <span class="type">Array</span>[<span class="type">Byte</span>] = stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        <span class="type">JavaUtils</span>.bufferToArray(</span><br><span class="line">          closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class="type">AnyRef</span>))</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class="type">AnyRef</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="comment">// In the case of a failure during serialization, abort the stage.</span></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">"Task not serializable: "</span> + e.toString, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Abort execution</span></span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s"Task serialization failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>"</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;  <span class="comment">//转换成一系列Task任务</span></span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = stage.rdd.partitions(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">            taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = stage.rdd.partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptId,</span><br><span class="line">            taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s"Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>"</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    logInfo(<span class="string">"Submitting "</span> + tasks.size + <span class="string">" missing tasks from "</span> + stage + <span class="string">" ("</span> + stage.rdd + <span class="string">")"</span>)</span><br><span class="line">    stage.pendingPartitions ++= tasks.map(_.partitionId)</span><br><span class="line">    logDebug(<span class="string">"New pending partitions: "</span> + stage.pendingPartitions)</span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>( <span class="comment">//提交我们的任务,把我们所有的的任务封装成了一个对象</span></span><br><span class="line">      tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))</span><br><span class="line">    stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// Because we posted SparkListenerStageSubmitted earlier, we should mark</span></span><br><span class="line">    <span class="comment">// the stage as completed here in case there are no tasks to run</span></span><br><span class="line">    markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> debugString = stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        <span class="string">s"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; "</span> +</span><br><span class="line">          <span class="string">s"(available: <span class="subst">$&#123;stage.isAvailable&#125;</span>,"</span> +</span><br><span class="line">          <span class="string">s"available outputs: <span class="subst">$&#123;stage.numAvailableOutputs&#125;</span>,"</span> +</span><br><span class="line">          <span class="string">s"partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span></span><br><span class="line">      <span class="keyword">case</span> stage : <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        <span class="string">s"Stage <span class="subst">$&#123;stage&#125;</span> is actually done; (partitions: <span class="subst">$&#123;stage.numPartitions&#125;</span>)"</span></span><br><span class="line">    &#125;</span><br><span class="line">    logDebug(debugString)</span><br><span class="line"></span><br><span class="line">    submitWaitingChildStages(stage)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="submitTasks"><a href="#submitTasks" class="headerlink" title="submitTasks"></a>submitTasks</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>): <span class="type">Unit</span></span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190611230725.png" alt=""></p><p>找到实现类</p><h3 id="跳转TaskSchedulerImpl"><a href="#跳转TaskSchedulerImpl" class="headerlink" title="跳转TaskSchedulerImpl"></a>跳转TaskSchedulerImpl</h3><h3 id="submitTasks-1"><a href="#submitTasks-1" class="headerlink" title="submitTasks"></a>submitTasks</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> tasks = taskSet.tasks</span><br><span class="line">  logInfo(<span class="string">"Adding task set "</span> + taskSet.id + <span class="string">" with "</span> + tasks.length + <span class="string">" tasks"</span>)</span><br><span class="line">  <span class="keyword">this</span>.synchronized &#123;  <span class="comment">//同步</span></span><br><span class="line">    <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">    <span class="keyword">val</span> stage = taskSet.stageId</span><br><span class="line">    <span class="keyword">val</span> stageTaskSets =</span><br><span class="line">      taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">TaskSetManager</span>])</span><br><span class="line">    stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">    <span class="keyword">val</span> conflictingTaskSet = stageTaskSets.exists &#123; <span class="keyword">case</span> (_, ts) =&gt;</span><br><span class="line">      ts.taskSet != taskSet &amp;&amp; !ts.isZombie</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (conflictingTaskSet) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">s"more than one active taskSet for stage <span class="subst">$stage</span>:"</span> +</span><br><span class="line">        <span class="string">s" <span class="subst">$&#123;stageTaskSets.toSeq.map&#123;_._2.taskSet.id&#125;</span>.mkString("</span>,<span class="string">")&#125;"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">      starvationTimer.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">TimerTask</span>() &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>() &#123;</span><br><span class="line">          <span class="keyword">if</span> (!hasLaunchedTask) &#123;</span><br><span class="line">            logWarning(<span class="string">"Initial job has not accepted any resources; "</span> +</span><br><span class="line">              <span class="string">"check your cluster UI to ensure that workers are registered "</span> +</span><br><span class="line">              <span class="string">"and have sufficient resources"</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.cancel()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, <span class="type">STARVATION_TIMEOUT_MS</span>, <span class="type">STARVATION_TIMEOUT_MS</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    hasReceivedTask = <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">  backend.reviveOffers() <span class="comment">//这个backend再StandaloneSchedulerBackend出现过</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在tandaloneSchedulerBackend没有找到.我们去<code>CoarseGrainedSchedulerBackend</code>中寻找一下</p><h3 id="回到CoarseGrainedSchedulerBackend"><a href="#回到CoarseGrainedSchedulerBackend" class="headerlink" title="回到CoarseGrainedSchedulerBackend"></a>回到CoarseGrainedSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">ReviveOffers</span> =&gt;</span><br><span class="line">        makeOffers() <span class="comment">//跳转到</span></span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>() &#123;</span><br><span class="line">    driverEndpoint.send(<span class="type">ReviveOffers</span>) <span class="comment">//</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="makeOffers"><a href="#makeOffers" class="headerlink" title="makeOffers"></a>makeOffers</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Make fake resource offers on all executors</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>() &#123;</span><br><span class="line">    <span class="comment">// Filter out executors under killing</span></span><br><span class="line">    <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(executorIsAlive)</span><br><span class="line">    <span class="keyword">val</span> workOffers = activeExecutors.map &#123; <span class="keyword">case</span> (id, executorData) =&gt;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores)</span><br><span class="line">    &#125;.toIndexedSeq</span><br><span class="line">    launchTasks(scheduler.resourceOffers(workOffers <span class="comment">//运行任务 申请资源  resourceOffers 具体做关键资源</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="跳转TaskSchedulerImpl-1"><a href="#跳转TaskSchedulerImpl-1" class="headerlink" title="跳转TaskSchedulerImpl"></a>跳转TaskSchedulerImpl</h3><h3 id="resourceOffers"><a href="#resourceOffers" class="headerlink" title="resourceOffers"></a>resourceOffers</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Called by cluster manager to offer resources on slaves. We respond by asking our active task</span></span><br><span class="line"><span class="comment">   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so</span></span><br><span class="line"><span class="comment">   * that tasks are balanced across the cluster.</span></span><br><span class="line"><span class="comment">   */</span>  </span><br><span class="line"><span class="comment">//  该类具体做具体的资源分配</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">resourceOffers</span></span>(offers: <span class="type">IndexedSeq</span>[<span class="type">WorkerOffer</span>]): <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]] = synchronized &#123;</span><br><span class="line">    <span class="comment">// Mark each slave as alive and remember its hostname</span></span><br><span class="line">    <span class="comment">// Also track if new executor is added</span></span><br><span class="line">    <span class="keyword">var</span> newExecAvail = <span class="literal">false</span></span><br><span class="line">    <span class="keyword">for</span> (o &lt;- offers) &#123;</span><br><span class="line">      <span class="keyword">if</span> (!hostToExecutors.contains(o.host)) &#123;</span><br><span class="line">        hostToExecutors(o.host) = <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!executorIdToRunningTaskIds.contains(o.executorId)) &#123;</span><br><span class="line">        hostToExecutors(o.host) += o.executorId</span><br><span class="line">        executorAdded(o.executorId, o.host)</span><br><span class="line">        executorIdToHost(o.executorId) = o.host</span><br><span class="line">        executorIdToRunningTaskIds(o.executorId) = <span class="type">HashSet</span>[<span class="type">Long</span>]()</span><br><span class="line">        newExecAvail = <span class="literal">true</span></span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">for</span> (rack &lt;- getRackForHost(o.host)) &#123;</span><br><span class="line">        hostsByRack.getOrElseUpdate(rack, <span class="keyword">new</span> <span class="type">HashSet</span>[<span class="type">String</span>]()) += o.host</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Randomly shuffle offers to avoid always placing tasks on the same set of workers.</span></span><br><span class="line">    <span class="keyword">val</span> shuffledOffers = <span class="type">Random</span>.shuffle(offers)</span><br><span class="line">    <span class="comment">// Build a list of tasks to assign to each worker.</span></span><br><span class="line">    <span class="keyword">val</span> tasks = shuffledOffers.map(o =&gt; <span class="keyword">new</span> <span class="type">ArrayBuffer</span>[<span class="type">TaskDescription</span>](o.cores))</span><br><span class="line">    <span class="keyword">val</span> availableCpus = shuffledOffers.map(o =&gt; o.cores).toArray</span><br><span class="line">    <span class="keyword">val</span> sortedTaskSets = rootPool.getSortedTaskSetQueue</span><br><span class="line">    <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</span><br><span class="line">      logDebug(<span class="string">"parentName: %s, name: %s, runningTasks: %s"</span>.format(</span><br><span class="line">        taskSet.parent.name, taskSet.name, taskSet.runningTasks))</span><br><span class="line">      <span class="keyword">if</span> (newExecAvail) &#123;</span><br><span class="line">        taskSet.executorAdded()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Take each TaskSet in our scheduling order, and then offer it each node in increasing order</span></span><br><span class="line">    <span class="comment">// of locality levels so that it gets a chance to launch local tasks on all of them.</span></span><br><span class="line">    <span class="comment">// <span class="doctag">NOTE:</span> the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY</span></span><br><span class="line">    <span class="keyword">for</span> (taskSet &lt;- sortedTaskSets) &#123;</span><br><span class="line">      <span class="keyword">var</span> launchedAnyTask = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">var</span> launchedTaskAtCurrentMaxLocality = <span class="literal">false</span></span><br><span class="line">      <span class="keyword">for</span> (currentMaxLocality &lt;- taskSet.myLocalityLevels) &#123;</span><br><span class="line">        do &#123;</span><br><span class="line">          launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet(</span><br><span class="line">            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)</span><br><span class="line">          launchedAnyTask |= launchedTaskAtCurrentMaxLocality</span><br><span class="line">        &#125; <span class="keyword">while</span> (launchedTaskAtCurrentMaxLocality)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (!launchedAnyTask) &#123;</span><br><span class="line">        taskSet.abortIfCompletelyBlacklisted(hostToExecutors)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (tasks.size &gt; <span class="number">0</span>) &#123;</span><br><span class="line">      hasLaunchedTask = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> tasks</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="回到CoarseGrainedSchedulerBackend-1"><a href="#回到CoarseGrainedSchedulerBackend-1" class="headerlink" title="回到CoarseGrainedSchedulerBackend"></a>回到CoarseGrainedSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">  <span class="comment">// Launch tasks returned by a set of resource offers</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]) &#123;</span><br><span class="line">    <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">      <span class="keyword">val</span> serializedTask = ser.serialize(task)</span><br><span class="line">      <span class="keyword">if</span> (serializedTask.limit &gt;= maxRpcMessageSize) &#123;</span><br><span class="line">        scheduler.taskIdToTaskSetManager.get(task.taskId).foreach &#123; taskSetMgr =&gt;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">var</span> msg = <span class="string">"Serialized task %s:%d was %d bytes, which exceeds max allowed: "</span> +</span><br><span class="line">              <span class="string">"spark.rpc.message.maxSize (%d bytes). Consider increasing "</span> +</span><br><span class="line">              <span class="string">"spark.rpc.message.maxSize or using broadcast variables for large values."</span></span><br><span class="line">            msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize)</span><br><span class="line">            taskSetMgr.abort(msg)</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">"Exception in error callback"</span>, e)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> executorData = executorDataMap(task.executorId)</span><br><span class="line">        executorData.freeCores -= scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line"></span><br><span class="line">        logDebug(<span class="string">s"Launching task <span class="subst">$&#123;task.taskId&#125;</span> on executor id: <span class="subst">$&#123;task.executorId&#125;</span> hostname: "</span> +</span><br><span class="line">          <span class="string">s"<span class="subst">$&#123;executorData.executorHost&#125;</span>."</span>)</span><br><span class="line"><span class="comment">//控制权转到CoarseGrainedExecutorBackend的LaunchTask</span></span><br><span class="line">        executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="跳转CoarseGrainedExecutorBackend"><a href="#跳转CoarseGrainedExecutorBackend" class="headerlink" title="跳转CoarseGrainedExecutorBackend"></a>跳转CoarseGrainedExecutorBackend</h3><h3 id="LaunchTask"><a href="#LaunchTask" class="headerlink" title="LaunchTask"></a>LaunchTask</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">   <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">     exitExecutor(<span class="number">1</span>, <span class="string">"Received LaunchTask command but executor was null"</span>)</span><br><span class="line">   &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">     <span class="keyword">val</span> taskDesc = ser.deserialize[<span class="type">TaskDescription</span>](data.value)</span><br><span class="line">     logInfo(<span class="string">"Got assigned task "</span> + taskDesc.taskId) <span class="comment">//调用executor的launchTask</span></span><br><span class="line">     executor.launchTask(<span class="keyword">this</span>, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,</span><br><span class="line">       taskDesc.name, taskDesc.serializedTask)</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="跳转到Executor"><a href="#跳转到Executor" class="headerlink" title="跳转到Executor"></a>跳转到Executor</h3><h3 id="launchTask"><a href="#launchTask" class="headerlink" title="launchTask"></a>launchTask</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(</span><br><span class="line">    context: <span class="type">ExecutorBackend</span>,</span><br><span class="line">    taskId: <span class="type">Long</span>,</span><br><span class="line">    attemptNumber: <span class="type">Int</span>,</span><br><span class="line">    taskName: <span class="type">String</span>,</span><br><span class="line">    serializedTask: <span class="type">ByteBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建一个TaskRunner</span></span><br><span class="line">  <span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskId = taskId, attemptNumber = attemptNumber, taskName,</span><br><span class="line">    serializedTask)</span><br><span class="line">  runningTasks.put(taskId, tr)</span><br><span class="line">  threadPool.execute(tr)  <span class="comment">//线程池 </span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TaskRunner"><a href="#TaskRunner" class="headerlink" title="TaskRunner"></a>TaskRunner</h3><p>我们关注一下run方法 上面的西线程池运行的是此处的润方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line">   <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    threadId = <span class="type">Thread</span>.currentThread.getId</span><br><span class="line">    <span class="type">Thread</span>.currentThread.setName(threadName)</span><br><span class="line">    <span class="keyword">val</span> threadMXBean = <span class="type">ManagementFactory</span>.getThreadMXBean</span><br><span class="line">    <span class="keyword">val</span> taskMemoryManager = <span class="keyword">new</span> <span class="type">TaskMemoryManager</span>(env.memoryManager, taskId)</span><br><span class="line">    <span class="keyword">val</span> deserializeStartTime = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">    <span class="keyword">val</span> deserializeStartCpuTime = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">      threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">    <span class="type">Thread</span>.currentThread.setContextClassLoader(replClassLoader)</span><br><span class="line">    <span class="keyword">val</span> ser = env.closureSerializer.newInstance()</span><br><span class="line">    logInfo(<span class="string">s"Running <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>)</span><br><span class="line">    execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">RUNNING</span>, <span class="type">EMPTY_BYTE_BUFFER</span>)</span><br><span class="line">    <span class="keyword">var</span> taskStart: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">var</span> taskStartCpu: <span class="type">Long</span> = <span class="number">0</span></span><br><span class="line">    startGCTime = computeTotalGcTime()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> (taskFiles, taskJars, taskProps, taskBytes) =</span><br><span class="line">        <span class="type">Task</span>.deserializeWithDependencies(serializedTask)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Must be set before updateDependencies() is called, in case fetching dependencies</span></span><br><span class="line">      <span class="comment">// requires access to properties contained within (e.g. for access control).</span></span><br><span class="line">      <span class="type">Executor</span>.taskDeserializationProps.set(taskProps)</span><br><span class="line"></span><br><span class="line">      updateDependencies(taskFiles, taskJars)</span><br><span class="line">      task = ser.deserialize[<span class="type">Task</span>[<span class="type">Any</span>]](taskBytes, <span class="type">Thread</span>.currentThread.getContextClassLoader)</span><br><span class="line">      task.localProperties = taskProps</span><br><span class="line">      task.setTaskMemoryManager(taskMemoryManager)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If this task has been killed before we deserialized it, let's quit now. Otherwise,</span></span><br><span class="line">      <span class="comment">// continue executing the task.</span></span><br><span class="line">      <span class="keyword">if</span> (killed) &#123;</span><br><span class="line">        <span class="comment">// Throw an exception rather than returning, because returning within a try&#123;&#125; block</span></span><br><span class="line">        <span class="comment">// causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl</span></span><br><span class="line">        <span class="comment">// exception will be caught by the catch block, leading to an incorrect ExceptionFailure</span></span><br><span class="line">        <span class="comment">// for the task.</span></span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TaskKilledException</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      logDebug(<span class="string">"Task "</span> + taskId + <span class="string">"'s epoch is "</span> + task.epoch)</span><br><span class="line">      env.mapOutputTracker.updateEpoch(task.epoch)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Run the actual task and measure its runtime.</span></span><br><span class="line">      taskStart = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      taskStartCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line">      <span class="keyword">var</span> threwException = <span class="literal">true</span></span><br><span class="line">      <span class="keyword">val</span> value = <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> res = task.run(</span><br><span class="line">          taskAttemptId = taskId,</span><br><span class="line">          attemptNumber = attemptNumber,</span><br><span class="line">          metricsSystem = env.metricsSystem)</span><br><span class="line">        threwException = <span class="literal">false</span></span><br><span class="line">        res</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)</span><br><span class="line">        <span class="keyword">val</span> freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (freedMemory &gt; <span class="number">0</span> &amp;&amp; !threwException) &#123;</span><br><span class="line">          <span class="keyword">val</span> errMsg = <span class="string">s"Managed memory leak detected; size = <span class="subst">$freedMemory</span> bytes, TID = <span class="subst">$taskId</span>"</span></span><br><span class="line">          <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.unsafe.exceptionOnMemoryLeak"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logWarning(errMsg)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (releasedLocks.nonEmpty &amp;&amp; !threwException) &#123;</span><br><span class="line">          <span class="keyword">val</span> errMsg =</span><br><span class="line">            <span class="string">s"<span class="subst">$&#123;releasedLocks.size&#125;</span> block locks were not released by TID = <span class="subst">$taskId</span>:\n"</span> +</span><br><span class="line">              releasedLocks.mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</span><br><span class="line">          <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.storage.exceptionOnPinLeak"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(errMsg)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logWarning(errMsg)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> taskFinish = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> taskFinishCpu = <span class="keyword">if</span> (threadMXBean.isCurrentThreadCpuTimeSupported) &#123;</span><br><span class="line">        threadMXBean.getCurrentThreadCpuTime</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="number">0</span>L</span><br><span class="line"></span><br><span class="line">      <span class="comment">// If the task has been killed, let's fail it.</span></span><br><span class="line">      <span class="keyword">if</span> (task.killed) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">TaskKilledException</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> resultSer = env.serializer.newInstance()</span><br><span class="line">      <span class="keyword">val</span> beforeSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">      <span class="keyword">val</span> valueBytes = resultSer.serialize(value)</span><br><span class="line">      <span class="keyword">val</span> afterSerialization = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Deserialization happens in two parts: first, we deserialize a Task object, which</span></span><br><span class="line">      <span class="comment">// includes the Partition. Second, Task.run() deserializes the RDD and function to be run.</span></span><br><span class="line">      task.metrics.setExecutorDeserializeTime(</span><br><span class="line">        (taskStart - deserializeStartTime) + task.executorDeserializeTime)</span><br><span class="line">      task.metrics.setExecutorDeserializeCpuTime(</span><br><span class="line">        (taskStartCpu - deserializeStartCpuTime) + task.executorDeserializeCpuTime)</span><br><span class="line">      <span class="comment">// We need to subtract Task.run()'s deserialization time to avoid double-counting</span></span><br><span class="line">      task.metrics.setExecutorRunTime((taskFinish - taskStart) - task.executorDeserializeTime)</span><br><span class="line">      task.metrics.setExecutorCpuTime(</span><br><span class="line">        (taskFinishCpu - taskStartCpu) - task.executorDeserializeCpuTime)</span><br><span class="line">      task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)</span><br><span class="line">      task.metrics.setResultSerializationTime(afterSerialization - beforeSerialization)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Note: accumulator updates must be collected after TaskMetrics is updated</span></span><br><span class="line">      <span class="keyword">val</span> accumUpdates = task.collectAccumulatorUpdates()</span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> do not serialize value twice</span></span><br><span class="line">      <span class="keyword">val</span> directResult = <span class="keyword">new</span> <span class="type">DirectTaskResult</span>(valueBytes, accumUpdates)</span><br><span class="line">      <span class="keyword">val</span> serializedDirectResult = ser.serialize(directResult)</span><br><span class="line">      <span class="keyword">val</span> resultSize = serializedDirectResult.limit</span><br><span class="line"></span><br><span class="line">      <span class="comment">// directSend = sending directly back to the driver</span></span><br><span class="line">      <span class="keyword">val</span> serializedResult: <span class="type">ByteBuffer</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (maxResultSize &gt; <span class="number">0</span> &amp;&amp; resultSize &gt; maxResultSize) &#123;</span><br><span class="line">          logWarning(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). Result is larger than maxResultSize "</span> +</span><br><span class="line">            <span class="string">s"(<span class="subst">$&#123;Utils.bytesToString(resultSize)&#125;</span> &gt; <span class="subst">$&#123;Utils.bytesToString(maxResultSize)&#125;</span>), "</span> +</span><br><span class="line">            <span class="string">s"dropping it."</span>)</span><br><span class="line">          ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](<span class="type">TaskResultBlockId</span>(taskId), resultSize))</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span> (resultSize &gt; maxDirectResultSize) &#123;</span><br><span class="line">          <span class="keyword">val</span> blockId = <span class="type">TaskResultBlockId</span>(taskId)</span><br><span class="line">          env.blockManager.putBytes(</span><br><span class="line">            blockId,</span><br><span class="line">            <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(serializedDirectResult.duplicate()),</span><br><span class="line">            <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>)</span><br><span class="line">          logInfo(</span><br><span class="line">            <span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent via BlockManager)"</span>)</span><br><span class="line">          ser.serialize(<span class="keyword">new</span> <span class="type">IndirectTaskResult</span>[<span class="type">Any</span>](blockId, resultSize))</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          logInfo(<span class="string">s"Finished <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>). <span class="subst">$resultSize</span> bytes result sent to driver"</span>)</span><br><span class="line">          serializedDirectResult</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"><span class="comment">//无论失败与否发送一个状态statusUpdate  点击statusUpdate 我们查看一下</span></span><br><span class="line">      execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FINISHED</span>, serializedResult)</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ffe: <span class="type">FetchFailedException</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> reason = ffe.toTaskFailedReason</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">TaskKilledException</span> =&gt;</span><br><span class="line">        logInfo(<span class="string">s"Executor killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>)</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(<span class="type">TaskKilled</span>))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> _: <span class="type">InterruptedException</span> <span class="keyword">if</span> task.killed =&gt;</span><br><span class="line">        logInfo(<span class="string">s"Executor interrupted and killed <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>)</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">KILLED</span>, ser.serialize(<span class="type">TaskKilled</span>))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">CausedBy</span>(cDE: <span class="type">CommitDeniedException</span>) =&gt;</span><br><span class="line">        <span class="keyword">val</span> reason = cDE.toTaskFailedReason</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, ser.serialize(reason))</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        <span class="comment">// Attempt to exit cleanly by informing the driver of our failure.</span></span><br><span class="line">        <span class="comment">// If anything goes wrong (or this was a fatal exception), we will delegate to</span></span><br><span class="line">        <span class="comment">// the default uncaught exception handler, which will terminate the Executor.</span></span><br><span class="line">        logError(<span class="string">s"Exception in <span class="subst">$taskName</span> (TID <span class="subst">$taskId</span>)"</span>, t)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Collect latest accumulator values to report back to the driver</span></span><br><span class="line">        <span class="keyword">val</span> accums: <span class="type">Seq</span>[<span class="type">AccumulatorV2</span>[_, _]] =</span><br><span class="line">          <span class="keyword">if</span> (task != <span class="literal">null</span>) &#123;</span><br><span class="line">            task.metrics.setExecutorRunTime(<span class="type">System</span>.currentTimeMillis() - taskStart)</span><br><span class="line">            task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)</span><br><span class="line">            task.collectAccumulatorUpdates(taskFailed = <span class="literal">true</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">Seq</span>.empty</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> accUpdates = accums.map(acc =&gt; acc.toInfo(<span class="type">Some</span>(acc.value), <span class="type">None</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> serializedTaskEndReason = &#123;</span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates).withAccums(accums))</span><br><span class="line">          &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> _: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">              <span class="comment">// t is not serializable so just send the stacktrace</span></span><br><span class="line">              ser.serialize(<span class="keyword">new</span> <span class="type">ExceptionFailure</span>(t, accUpdates, <span class="literal">false</span>).withAccums(accums))</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        setTaskFinishedAndClearInterruptStatus()</span><br><span class="line">        execBackend.statusUpdate(taskId, <span class="type">TaskState</span>.<span class="type">FAILED</span>, serializedTaskEndReason)</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Don't forcibly exit unless the exception was inherently fatal, to avoid</span></span><br><span class="line">        <span class="comment">// stopping other tasks unnecessarily.</span></span><br><span class="line">        <span class="keyword">if</span> (<span class="type">Utils</span>.isFatalError(t)) &#123;</span><br><span class="line">          <span class="type">SparkUncaughtExceptionHandler</span>.uncaughtException(t)</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      runningTasks.remove(taskId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="statusUpdate"><a href="#statusUpdate" class="headerlink" title="statusUpdate"></a>statusUpdate</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">ExecutorBackend</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">statusUpdate</span></span>(taskId: <span class="type">Long</span>, state: <span class="type">TaskState</span>, data: <span class="type">ByteBuffer</span>): <span class="type">Unit</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190611233512.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">statusUpdate</span></span>(taskId: <span class="type">Long</span>, state: <span class="type">TaskState</span>, data: <span class="type">ByteBuffer</span>) &#123;</span><br><span class="line">  <span class="keyword">val</span> msg = <span class="type">StatusUpdate</span>(executorId, taskId, state, data)</span><br><span class="line">  driver <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(driverRef) =&gt; driverRef.send(msg)  <span class="comment">//给driver发送了一条消息 控制权回到Driver</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; logWarning(<span class="string">s"Drop <span class="subst">$msg</span> because has not yet connected to driver"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回到CoarseGrainedSchedulerBackend-2"><a href="#回到CoarseGrainedSchedulerBackend-2" class="headerlink" title="回到CoarseGrainedSchedulerBackend"></a>回到CoarseGrainedSchedulerBackend</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">   <span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data) =&gt;</span><br><span class="line">     scheduler.statusUpdate(taskId, state, data.value) <span class="comment">//调用了scheduler(TaskSchedulerImpl)的statusUpdate方法</span></span><br><span class="line">     <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123; <span class="comment">//如果任务结束了</span></span><br><span class="line">       executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">         <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt; <span class="comment">//如果还有executor的资源</span></span><br><span class="line">           executorInfo.freeCores += scheduler.<span class="type">CPUS_PER_TASK</span></span><br><span class="line">           makeOffers(executorId) <span class="comment">//继续调度</span></span><br><span class="line">         <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">           <span class="comment">// Ignoring the update since we don't know about the executor.</span></span><br><span class="line">           logWarning(<span class="string">s"Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) "</span> +</span><br><span class="line">             <span class="string">s"from unknown executor with ID <span class="subst">$executorId</span>"</span>)</span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br></pre></td></tr></table></figure><h3 id="回到TaskSchedulerImpl"><a href="#回到TaskSchedulerImpl" class="headerlink" title="回到TaskSchedulerImpl"></a>回到TaskSchedulerImpl</h3><h3 id="statusUpdate-1"><a href="#statusUpdate-1" class="headerlink" title="statusUpdate"></a>statusUpdate</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">statusUpdate</span></span>(tid: <span class="type">Long</span>, state: <span class="type">TaskState</span>, serializedData: <span class="type">ByteBuffer</span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> failedExecutor: <span class="type">Option</span>[<span class="type">String</span>] = <span class="type">None</span></span><br><span class="line">  <span class="keyword">var</span> reason: <span class="type">Option</span>[<span class="type">ExecutorLossReason</span>] = <span class="type">None</span></span><br><span class="line">  synchronized &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      taskIdToTaskSetManager.get(tid) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(taskSet) =&gt;</span><br><span class="line">          <span class="keyword">if</span> (state == <span class="type">TaskState</span>.<span class="type">LOST</span>) &#123; <span class="comment">//如果所有的任务状态都完成了</span></span><br><span class="line">            <span class="comment">// TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode,</span></span><br><span class="line">            <span class="comment">// where each executor corresponds to a single task, so mark the executor as failed.</span></span><br><span class="line">            <span class="keyword">val</span> execId = taskIdToExecutorId.getOrElse(tid, <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(</span><br><span class="line">              <span class="string">"taskIdToTaskSetManager.contains(tid) &lt;=&gt; taskIdToExecutorId.contains(tid)"</span>))</span><br><span class="line">            <span class="keyword">if</span> (executorIdToRunningTaskIds.contains(execId)) &#123;</span><br><span class="line">              reason = <span class="type">Some</span>(</span><br><span class="line">                <span class="type">SlaveLost</span>(<span class="string">s"Task <span class="subst">$tid</span> was lost, so marking the executor as lost as well."</span>))</span><br><span class="line">              removeExecutor(execId, reason.get)  <span class="comment">//删除Executor</span></span><br><span class="line">              failedExecutor = <span class="type">Some</span>(execId)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">            cleanupTaskState(tid)</span><br><span class="line">            taskSet.removeRunningTask(tid)</span><br><span class="line">            <span class="keyword">if</span> (state == <span class="type">TaskState</span>.<span class="type">FINISHED</span>) &#123;</span><br><span class="line">              taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (<span class="type">Set</span>(<span class="type">TaskState</span>.<span class="type">FAILED</span>, <span class="type">TaskState</span>.<span class="type">KILLED</span>, <span class="type">TaskState</span>.<span class="type">LOST</span>).contains(state)) &#123;</span><br><span class="line">              taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          logError(</span><br><span class="line">            (<span class="string">"Ignoring update with state %s for TID %s because its task set is gone (this is "</span> +</span><br><span class="line">              <span class="string">"likely the result of receiving duplicate task finished status updates) or its "</span> +</span><br><span class="line">              <span class="string">"executor has been marked as failed."</span>)</span><br><span class="line">              .format(state, tid))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt; logError(<span class="string">"Exception in statusUpdate"</span>, e)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Update the DAGScheduler without holding a lock on this, since that can deadlock</span></span><br><span class="line">  <span class="keyword">if</span> (failedExecutor.isDefined) &#123;</span><br><span class="line">    assert(reason.isDefined)</span><br><span class="line">    dagScheduler.executorLost(failedExecutor.get, reason.get)</span><br><span class="line">    backend.reviveOffers()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>至此任务提交已经基本完成。</p>]]></content>
    
    <summary type="html">
    
      Spark的交互流程 - 应用提交：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark内核解析2</title>
    <link href="https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/"/>
    <id>https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/</id>
    <published>2019-06-09T07:41:31.000Z</published>
    <updated>2020-01-12T13:08:27.148Z</updated>
    
    <content type="html"><![CDATA[ Spark的交互流程- - -节点启动：<Excerpt in index | 首页摘要><a id="more"></a> <p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609154253.png" alt=""></p><p>Master启动时首先创一个RpcEnv对象，负责管理所有通信逻辑<br>Master通过RpcEnv对象创建一个Endpoint，Master就是一个Endpoint，Worker可以与其进行通信<br>Worker启动时也是创一个RpcEnv对象<br>Worker通过RpcEnv对象创建一个Endpoint<br>Worker通过RpcEnv对，建立到Master的连接，获取到一个RpcEndpointRef对象，通过该对象可以与Master通信<br>Worker向Master注册，注册内容包括主机名、端口、CPU Core数量、内存数量<br>Master接收到Worker的注册，将注册信息维护在内存中的Table中，其中还包含了一个到Worker的RpcEndpointRef对象引用<br>Master回复Worker已经接收到注册，告知Worker已经注册成功<br>此时如果有用户提交Spark程序，Master需要协调启动Driver；而Worker端收到成功注册响应后，开始周期性向Master发送心跳</p><h2 id="Mster"><a href="#Mster" class="headerlink" title="Mster"></a>Mster</h2><p>Master的main方法启动创建了一个startRpcEnvAndEndpoint</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 1、初始化log对象</span></span><br><span class="line">    <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">    <span class="comment">// 2、加载SparkConf</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></span><br><span class="line">    <span class="comment">// 3、解析Master启动参数</span></span><br><span class="line">    <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">MasterArguments</span>(argStrings, conf)  </span><br><span class="line">    <span class="comment">// 4、启动RPC框架端点</span></span><br><span class="line">    <span class="keyword">val</span> (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)</span><br><span class="line">    rpcEnv.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们点击<code>startRpcEnvAndEndpoint</code>查看中创建了一个Master</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startRpcEnvAndEndpoint</span></span>(</span><br><span class="line">      host: <span class="type">String</span>,  </span><br><span class="line">      port: <span class="type">Int</span>,</span><br><span class="line">      webUiPort: <span class="type">Int</span>,</span><br><span class="line">      conf: <span class="type">SparkConf</span>): (<span class="type">RpcEnv</span>, <span class="type">Int</span>, <span class="type">Option</span>[<span class="type">Int</span>]) = &#123;</span><br><span class="line">    <span class="keyword">val</span> securityMgr = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(<span class="type">SYSTEM_NAME</span>, host, port, conf, securityMgr)</span><br><span class="line">    <span class="keyword">val</span> masterEndpoint = rpcEnv.setupEndpoint(<span class="type">ENDPOINT_NAME</span>,</span><br><span class="line">      <span class="keyword">new</span> <span class="type">Master</span>(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf))  <span class="comment">//创建一个Master</span></span><br><span class="line">    <span class="keyword">val</span> portsResponse = masterEndpoint.askWithRetry[<span class="type">BoundPortsResponse</span>](<span class="type">BoundPortsRequest</span>)</span><br><span class="line">    (rpcEnv, portsResponse.webUIPort, portsResponse.restPort)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Master中我们重点关注一下onStart()方法,绑定WebUI,和定期心跳检测</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    logInfo(<span class="string">"Starting Spark master at "</span> + masterUrl)</span><br><span class="line">    logInfo(<span class="string">s"Running Spark version <span class="subst">$&#123;org.apache.spark.SPARK_VERSION&#125;</span>"</span>)</span><br><span class="line">    webUi = <span class="keyword">new</span> <span class="type">MasterWebUI</span>(<span class="keyword">this</span>, webUiPort)</span><br><span class="line">    webUi.bind()</span><br><span class="line">    masterWebUiUrl = <span class="string">"http://"</span> + masterPublicAddress + <span class="string">":"</span> + webUi.boundPort</span><br><span class="line">    <span class="keyword">if</span> (reverseProxy) &#123;</span><br><span class="line">      masterWebUiUrl = conf.get(<span class="string">"spark.ui.reverseProxyUrl"</span>, masterWebUiUrl)</span><br><span class="line">      logInfo(<span class="string">s"Spark Master is acting as a reverse proxy. Master, Workers and "</span> +</span><br><span class="line">       <span class="string">s"Applications UIs are available at <span class="subst">$masterWebUiUrl</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">        self.send(<span class="type">CheckForWorkerTimeOut</span>)  <span class="comment">//周期性的自己给自己发送一条消息</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, <span class="number">0</span>, <span class="type">WORKER_TIMEOUT_MS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)  <span class="comment">//WORKER_TIMEOUT_MS为60*1000 也就是一分钟超时时间为1分钟</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (restServerEnabled) &#123;</span><br><span class="line">      <span class="keyword">val</span> port = conf.getInt(<span class="string">"spark.master.rest.port"</span>, <span class="number">6066</span>)</span><br><span class="line">      restServer = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">StandaloneRestServer</span>(address.host, port, conf, self, masterUrl))</span><br><span class="line">    &#125;</span><br><span class="line">    restServerBoundPort = restServer.map(_.start())</span><br><span class="line"></span><br><span class="line">    masterMetricsSystem.registerSource(masterSource)</span><br><span class="line">    masterMetricsSystem.start()</span><br><span class="line">    applicationMetricsSystem.start()</span><br><span class="line">    <span class="comment">// Attach the master and app metrics servlet handler to the web ui after the metrics systems are</span></span><br><span class="line">    <span class="comment">// started.</span></span><br><span class="line">    masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)</span><br><span class="line">    applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> serializer = <span class="keyword">new</span> <span class="type">JavaSerializer</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> (persistenceEngine_, leaderElectionAgent_) = <span class="type">RECOVERY_MODE</span> <span class="keyword">match</span> &#123; <span class="comment">//RECOVERY_MODE是否设置</span></span><br><span class="line">      <span class="keyword">case</span> <span class="string">"ZOOKEEPER"</span> =&gt;</span><br><span class="line">        logInfo(<span class="string">"Persisting recovery state to ZooKeeper"</span>)</span><br><span class="line">        <span class="keyword">val</span> zkFactory =</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ZooKeeperRecoveryModeFactory</span>(conf, serializer)</span><br><span class="line">        (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(<span class="keyword">this</span>))</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"FILESYSTEM"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> fsFactory =</span><br><span class="line">          <span class="keyword">new</span> <span class="type">FileSystemRecoveryModeFactory</span>(conf, serializer)</span><br><span class="line">        (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(<span class="keyword">this</span>))</span><br><span class="line">      <span class="keyword">case</span> <span class="string">"CUSTOM"</span> =&gt;</span><br><span class="line">        <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(conf.get(<span class="string">"spark.deploy.recoveryMode.factory"</span>))</span><br><span class="line">        <span class="keyword">val</span> factory = clazz.getConstructor(classOf[<span class="type">SparkConf</span>], classOf[<span class="type">Serializer</span>])</span><br><span class="line">          .newInstance(conf, serializer)</span><br><span class="line">          .asInstanceOf[<span class="type">StandaloneRecoveryModeFactory</span>]</span><br><span class="line">        (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(<span class="keyword">this</span>))</span><br><span class="line">      <span class="keyword">case</span> _ =&gt;</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">BlackHolePersistenceEngine</span>(), <span class="keyword">new</span> <span class="type">MonarchyLeaderAgent</span>(<span class="keyword">this</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    persistenceEngine = persistenceEngine_</span><br><span class="line">    leaderElectionAgent = leaderElectionAgent_</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们在查看一下receive 这些只是受到消息,然而并没有恢复</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">ElectedLeader</span> =&gt; <span class="comment">// Leader选举,用在SparkHA中</span></span><br><span class="line">      <span class="keyword">val</span> (storedApps, storedDrivers, storedWorkers) = persistenceEngine.readPersistedData(rpcEnv)</span><br><span class="line">      state = <span class="keyword">if</span> (storedApps.isEmpty &amp;&amp; storedDrivers.isEmpty &amp;&amp; storedWorkers.isEmpty) &#123;</span><br><span class="line">        <span class="type">RecoveryState</span>.<span class="type">ALIVE</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">RecoveryState</span>.<span class="type">RECOVERING</span></span><br><span class="line">      &#125;</span><br><span class="line">      logInfo(<span class="string">"I have been elected leader! New state: "</span> + state)</span><br><span class="line">      <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">RECOVERING</span>) &#123;</span><br><span class="line">        beginRecovery(storedApps, storedDrivers, storedWorkers)</span><br><span class="line">        recoveryCompletionTask = forwardMessageThread.schedule(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">            self.send(<span class="type">CompleteRecovery</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, <span class="type">WORKER_TIMEOUT_MS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">CompleteRecovery</span> =&gt; completeRecovery() <span class="comment">//成功地恢复</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RevokedLeadership</span> =&gt; <span class="comment">//</span></span><br><span class="line">      logError(<span class="string">"Leadership has been revoked -- master shutting down."</span>)</span><br><span class="line">      <span class="type">System</span>.exit(<span class="number">0</span>)</span><br><span class="line"><span class="comment">//注册应用</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterApplication</span>(description, driver) =&gt;</span><br><span class="line">      <span class="comment">// TODO Prevent repeated registrations from some driver</span></span><br><span class="line">      <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">STANDBY</span>) &#123;</span><br><span class="line">        <span class="comment">// ignore, don't send response</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logInfo(<span class="string">"Registering app "</span> + description.name)</span><br><span class="line">        <span class="keyword">val</span> app = createApplication(description, driver)</span><br><span class="line">        registerApplication(app)</span><br><span class="line">        logInfo(<span class="string">"Registered app "</span> + description.name + <span class="string">" with ID "</span> + app.id)</span><br><span class="line">        persistenceEngine.addApplication(app)</span><br><span class="line">        driver.send(<span class="type">RegisteredApplication</span>(app.id, self))</span><br><span class="line">        <span class="comment">// 启动分配Executor</span></span><br><span class="line">        schedule()</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// 检测 Executor</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">ExecutorStateChanged</span>(appId, execId, state, message, exitStatus) =&gt;</span><br><span class="line">      <span class="keyword">val</span> execOption = idToApp.get(appId).flatMap(app =&gt; app.executors.get(execId))</span><br><span class="line">      execOption <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(exec) =&gt;</span><br><span class="line">          <span class="keyword">val</span> appInfo = idToApp(appId)</span><br><span class="line">          <span class="keyword">val</span> oldState = exec.state</span><br><span class="line">          exec.state = state</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (state == <span class="type">ExecutorState</span>.<span class="type">RUNNING</span>) &#123;</span><br><span class="line">            assert(oldState == <span class="type">ExecutorState</span>.<span class="type">LAUNCHING</span>,</span><br><span class="line">              <span class="string">s"executor <span class="subst">$execId</span> state transfer from <span class="subst">$oldState</span> to RUNNING is illegal"</span>)</span><br><span class="line">            appInfo.resetRetryCount()</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          exec.application.driver.send(<span class="type">ExecutorUpdated</span>(execId, state, message, exitStatus, <span class="literal">false</span>))</span><br><span class="line"></span><br><span class="line">          <span class="keyword">if</span> (<span class="type">ExecutorState</span>.isFinished(state)) &#123;</span><br><span class="line">            <span class="comment">// Remove this executor from the worker and app</span></span><br><span class="line">            logInfo(<span class="string">s"Removing executor <span class="subst">$&#123;exec.fullId&#125;</span> because it is <span class="subst">$state</span>"</span>)</span><br><span class="line">            <span class="comment">// If an application has already finished, preserve its</span></span><br><span class="line">            <span class="comment">// state to display its information properly on the UI</span></span><br><span class="line">            <span class="keyword">if</span> (!appInfo.isFinished) &#123;</span><br><span class="line">              appInfo.removeExecutor(exec)</span><br><span class="line">            &#125;</span><br><span class="line">            exec.worker.removeExecutor(exec)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">val</span> normalExit = exitStatus == <span class="type">Some</span>(<span class="number">0</span>)</span><br><span class="line">            <span class="comment">// Only retry certain number of times so we don't go into an infinite loop.</span></span><br><span class="line">            <span class="comment">// Important note: this code path is not exercised by tests, so be very careful when</span></span><br><span class="line">            <span class="comment">// changing this `if` condition.</span></span><br><span class="line">            <span class="keyword">if</span> (!normalExit</span><br><span class="line">                &amp;&amp; appInfo.incrementRetryCount() &gt;= <span class="type">MAX_EXECUTOR_RETRIES</span></span><br><span class="line">                &amp;&amp; <span class="type">MAX_EXECUTOR_RETRIES</span> &gt;= <span class="number">0</span>) &#123; <span class="comment">// &lt; 0 disables this application-killing path</span></span><br><span class="line">              <span class="keyword">val</span> execs = appInfo.executors.values</span><br><span class="line">              <span class="keyword">if</span> (!execs.exists(_.state == <span class="type">ExecutorState</span>.<span class="type">RUNNING</span>)) &#123;</span><br><span class="line">                logError(<span class="string">s"Application <span class="subst">$&#123;appInfo.desc.name&#125;</span> with ID <span class="subst">$&#123;appInfo.id&#125;</span> failed "</span> +</span><br><span class="line">                  <span class="string">s"<span class="subst">$&#123;appInfo.retryCount&#125;</span> times; removing it"</span>)</span><br><span class="line">                removeApplication(appInfo, <span class="type">ApplicationState</span>.<span class="type">FAILED</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          schedule()</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          logWarning(<span class="string">s"Got status update for unknown executor <span class="subst">$appId</span>/<span class="subst">$execId</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">DriverStateChanged</span>(driverId, state, exception) =&gt;</span><br><span class="line">      state <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">DriverState</span>.<span class="type">ERROR</span> | <span class="type">DriverState</span>.<span class="type">FINISHED</span> | <span class="type">DriverState</span>.<span class="type">KILLED</span> | <span class="type">DriverState</span>.<span class="type">FAILED</span> =&gt;</span><br><span class="line">          removeDriver(driverId, state, exception)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">Exception</span>(<span class="string">s"Received unexpected state update for driver <span class="subst">$driverId</span>: <span class="subst">$state</span>"</span>)</span><br><span class="line">      &#125;</span><br><span class="line"><span class="comment">//心跳机制</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Heartbeat</span>(workerId, worker) =&gt;</span><br><span class="line">      idToWorker.get(workerId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(workerInfo) =&gt;</span><br><span class="line">          workerInfo.lastHeartbeat = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="keyword">if</span> (workers.map(_.id).contains(workerId)) &#123;</span><br><span class="line">            logWarning(<span class="string">s"Got heartbeat from unregistered worker <span class="subst">$workerId</span>."</span> +</span><br><span class="line">              <span class="string">" Asking it to re-register."</span>)</span><br><span class="line">            worker.send(<span class="type">ReconnectWorker</span>(masterUrl))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            logWarning(<span class="string">s"Got heartbeat from unregistered worker <span class="subst">$workerId</span>."</span> +</span><br><span class="line">              <span class="string">" This worker was never registered, so ignoring the heartbeat."</span>)</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterChangeAcknowledged</span>(appId) =&gt;</span><br><span class="line">      idToApp.get(appId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(app) =&gt;</span><br><span class="line">          logInfo(<span class="string">"Application has been re-registered: "</span> + appId)</span><br><span class="line">          app.state = <span class="type">ApplicationState</span>.<span class="type">WAITING</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          logWarning(<span class="string">"Master change ack from unknown app: "</span> + appId)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (canCompleteRecovery) &#123; completeRecovery() &#125;</span><br><span class="line"><span class="comment">//反馈Worker的运行状态</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">WorkerSchedulerStateResponse</span>(workerId, executors, driverIds) =&gt;</span><br><span class="line">      idToWorker.get(workerId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(worker) =&gt;</span><br><span class="line">          logInfo(<span class="string">"Worker has been re-registered: "</span> + workerId)</span><br><span class="line">          worker.state = <span class="type">WorkerState</span>.<span class="type">ALIVE</span></span><br><span class="line"></span><br><span class="line">          <span class="keyword">val</span> validExecutors = executors.filter(exec =&gt; idToApp.get(exec.appId).isDefined)</span><br><span class="line">          <span class="keyword">for</span> (exec &lt;- validExecutors) &#123;</span><br><span class="line">            <span class="keyword">val</span> app = idToApp.get(exec.appId).get</span><br><span class="line">            <span class="keyword">val</span> execInfo = app.addExecutor(worker, exec.cores, <span class="type">Some</span>(exec.execId))</span><br><span class="line">            worker.addExecutor(execInfo)</span><br><span class="line">            execInfo.copyState(exec)</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">for</span> (driverId &lt;- driverIds) &#123;</span><br><span class="line">            drivers.find(_.id == driverId).foreach &#123; driver =&gt;</span><br><span class="line">              driver.worker = <span class="type">Some</span>(worker)</span><br><span class="line">              driver.state = <span class="type">DriverState</span>.<span class="type">RUNNING</span></span><br><span class="line">              worker.drivers(driverId) = driver</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          logWarning(<span class="string">"Scheduler state from unknown worker: "</span> + workerId)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (canCompleteRecovery) &#123; completeRecovery() &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">WorkerLatestState</span>(workerId, executors, driverIds) =&gt;</span><br><span class="line">      idToWorker.get(workerId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(worker) =&gt;</span><br><span class="line">          <span class="keyword">for</span> (exec &lt;- executors) &#123;</span><br><span class="line">            <span class="keyword">val</span> executorMatches = worker.executors.exists &#123;</span><br><span class="line">              <span class="keyword">case</span> (_, e) =&gt; e.application.id == exec.appId &amp;&amp; e.id == exec.execId</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span> (!executorMatches) &#123;</span><br><span class="line">              <span class="comment">// master doesn't recognize this executor. So just tell worker to kill it.</span></span><br><span class="line">              worker.endpoint.send(<span class="type">KillExecutor</span>(masterUrl, exec.appId, exec.execId))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line"></span><br><span class="line">          <span class="keyword">for</span> (driverId &lt;- driverIds) &#123;</span><br><span class="line">            <span class="keyword">val</span> driverMatches = worker.drivers.exists &#123; <span class="keyword">case</span> (id, _) =&gt; id == driverId &#125;</span><br><span class="line">            <span class="keyword">if</span> (!driverMatches) &#123;</span><br><span class="line">              <span class="comment">// master doesn't recognize this driver. So just tell worker to kill it.</span></span><br><span class="line">              worker.endpoint.send(<span class="type">KillDriver</span>(driverId))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          logWarning(<span class="string">"Worker state from unknown worker: "</span> + workerId)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">UnregisterApplication</span>(applicationId) =&gt;</span><br><span class="line">      logInfo(<span class="string">s"Received unregister request from application <span class="subst">$applicationId</span>"</span>)</span><br><span class="line">      idToApp.get(applicationId).foreach(finishApplication)</span><br><span class="line"><span class="comment">//给积极扫那个一个消息,检测自己是否挂掉</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">CheckForWorkerTimeOut</span> =&gt;</span><br><span class="line">      timeOutDeadWorkers()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>注意:上述都是没有回复的,我们需要查看一下<code>receiveAndReply</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">  <span class="keyword">case</span> <span class="type">RequestSubmitDriver</span>(description) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> msg = <span class="string">s"<span class="subst">$&#123;Utils.BACKUP_STANDALONE_MASTER_PREFIX&#125;</span>: <span class="subst">$state</span>. "</span> +</span><br><span class="line">        <span class="string">"Can only accept driver submissions in ALIVE state."</span></span><br><span class="line">      context.reply(<span class="type">SubmitDriverResponse</span>(self, <span class="literal">false</span>, <span class="type">None</span>, msg))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logInfo(<span class="string">"Driver submitted "</span> + description.command.mainClass)</span><br><span class="line">      <span class="keyword">val</span> driver = createDriver(description)</span><br><span class="line">      persistenceEngine.addDriver(driver)</span><br><span class="line">      waitingDrivers += driver</span><br><span class="line">      drivers.add(driver)</span><br><span class="line">      schedule()</span><br><span class="line"></span><br><span class="line">      <span class="comment">// <span class="doctag">TODO:</span> It might be good to instead have the submission client poll the master to determine</span></span><br><span class="line">      <span class="comment">//       the current status of the driver. For now it's simply "fire and forget".</span></span><br><span class="line"></span><br><span class="line">      context.reply(<span class="type">SubmitDriverResponse</span>(self, <span class="literal">true</span>, <span class="type">Some</span>(driver.id),</span><br><span class="line">        <span class="string">s"Driver successfully submitted as <span class="subst">$&#123;driver.id&#125;</span>"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">RequestKillDriver</span>(driverId) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> msg = <span class="string">s"<span class="subst">$&#123;Utils.BACKUP_STANDALONE_MASTER_PREFIX&#125;</span>: <span class="subst">$state</span>. "</span> +</span><br><span class="line">        <span class="string">s"Can only kill drivers in ALIVE state."</span></span><br><span class="line">      context.reply(<span class="type">KillDriverResponse</span>(self, driverId, success = <span class="literal">false</span>, msg))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logInfo(<span class="string">"Asked to kill driver "</span> + driverId)</span><br><span class="line">      <span class="keyword">val</span> driver = drivers.find(_.id == driverId)</span><br><span class="line">      driver <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(d) =&gt;</span><br><span class="line">          <span class="keyword">if</span> (waitingDrivers.contains(d)) &#123;</span><br><span class="line">            waitingDrivers -= d</span><br><span class="line">            self.send(<span class="type">DriverStateChanged</span>(driverId, <span class="type">DriverState</span>.<span class="type">KILLED</span>, <span class="type">None</span>))</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// We just notify the worker to kill the driver here. The final bookkeeping occurs</span></span><br><span class="line">            <span class="comment">// on the return path when the worker submits a state change back to the master</span></span><br><span class="line">            <span class="comment">// to notify it that the driver was successfully killed.</span></span><br><span class="line">            d.worker.foreach &#123; w =&gt;</span><br><span class="line">              w.endpoint.send(<span class="type">KillDriver</span>(driverId))</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          <span class="comment">// <span class="doctag">TODO:</span> It would be nice for this to be a synchronous response</span></span><br><span class="line">          <span class="keyword">val</span> msg = <span class="string">s"Kill request for <span class="subst">$driverId</span> submitted"</span></span><br><span class="line">          logInfo(msg)</span><br><span class="line">          <span class="comment">//回复KillDriverResponse</span></span><br><span class="line">          context.reply(<span class="type">KillDriverResponse</span>(self, driverId, success = <span class="literal">true</span>, msg))</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="keyword">val</span> msg = <span class="string">s"Driver <span class="subst">$driverId</span> has already finished or does not exist"</span></span><br><span class="line">          logWarning(msg)</span><br><span class="line">          context.reply(<span class="type">KillDriverResponse</span>(self, driverId, success = <span class="literal">false</span>, msg))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">RequestDriverStatus</span>(driverId) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (state != <span class="type">RecoveryState</span>.<span class="type">ALIVE</span>) &#123;</span><br><span class="line">      <span class="keyword">val</span> msg = <span class="string">s"<span class="subst">$&#123;Utils.BACKUP_STANDALONE_MASTER_PREFIX&#125;</span>: <span class="subst">$state</span>. "</span> +</span><br><span class="line">        <span class="string">"Can only request driver status in ALIVE state."</span></span><br><span class="line">      context.reply(</span><br><span class="line">        <span class="type">DriverStatusResponse</span>(found = <span class="literal">false</span>, <span class="type">None</span>, <span class="type">None</span>, <span class="type">None</span>, <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">Exception</span>(msg))))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      (drivers ++ completedDrivers).find(_.id == driverId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(driver) =&gt;</span><br><span class="line">          context.reply(<span class="type">DriverStatusResponse</span>(found = <span class="literal">true</span>, <span class="type">Some</span>(driver.state),</span><br><span class="line">            driver.worker.map(_.id), driver.worker.map(_.hostPort), driver.exception))</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          context.reply(<span class="type">DriverStatusResponse</span>(found = <span class="literal">false</span>, <span class="type">None</span>, <span class="type">None</span>, <span class="type">None</span>, <span class="type">None</span>))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">RequestMasterState</span> =&gt;</span><br><span class="line">    context.reply(<span class="type">MasterStateResponse</span>(</span><br><span class="line">      address.host, address.port, restServerBoundPort,</span><br><span class="line">      workers.toArray, apps.toArray, completedApps.toArray,</span><br><span class="line">      drivers.toArray, completedDrivers.toArray, state))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">BoundPortsRequest</span> =&gt;</span><br><span class="line">    context.reply(<span class="type">BoundPortsResponse</span>(address.port, webUi.boundPort, restServerBoundPort))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">RequestExecutors</span>(appId, requestedTotal) =&gt;</span><br><span class="line">    context.reply(handleRequestExecutors(appId, requestedTotal))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="type">KillExecutors</span>(appId, executorIds) =&gt;</span><br><span class="line">    <span class="keyword">val</span> formattedExecutorIds = formatExecutorIds(executorIds)</span><br><span class="line">    context.reply(handleKillExecutors(appId, formattedExecutorIds))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[deploy] <span class="class"><span class="keyword">class</span> <span class="title">Worker</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    override val rpcEnv: <span class="type">RpcEnv</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    webUiPort: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    cores: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    memory: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    masterRpcAddresses: <span class="type">Array</span>[<span class="type">RpcAddress</span>],</span></span></span><br><span class="line"><span class="class"><span class="params">    endpointName: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    workDirPath: <span class="type">String</span> = null,</span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val securityMgr: <span class="type">SecurityManager</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">ThreadSafeRpcEndpoint</span> <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123; <span class="comment">//Worker继承了ThreadSafeRpcEndpoint,它也是一个通信端点</span></span><br></pre></td></tr></table></figure><p>我们查看一下Worker的main方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>  <span class="comment">//创建SparkConf</span></span><br><span class="line">  <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">WorkerArguments</span>(argStrings, conf) <span class="comment">//解析参数</span></span><br><span class="line">  <span class="keyword">val</span> rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores, <span class="comment">//创建rpcEnv</span></span><br><span class="line">    args.memory, args.masters, args.workDir, conf = conf)</span><br><span class="line">  rpcEnv.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们在查看一下调用的<code>startRpcEnvAndEndpoint</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">startRpcEnvAndEndpoint</span></span>(</span><br><span class="line">     host: <span class="type">String</span>,</span><br><span class="line">     port: <span class="type">Int</span>,</span><br><span class="line">     webUiPort: <span class="type">Int</span>,</span><br><span class="line">     cores: <span class="type">Int</span>,</span><br><span class="line">     memory: <span class="type">Int</span>,</span><br><span class="line">     masterUrls: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">     workDir: <span class="type">String</span>,</span><br><span class="line">     workerNumber: <span class="type">Option</span>[<span class="type">Int</span>] = <span class="type">None</span>,</span><br><span class="line">     conf: <span class="type">SparkConf</span> = <span class="keyword">new</span> <span class="type">SparkConf</span>): <span class="type">RpcEnv</span> = &#123;</span><br><span class="line"></span><br><span class="line">   <span class="comment">// The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments</span></span><br><span class="line">   <span class="keyword">val</span> systemName = <span class="type">SYSTEM_NAME</span> + workerNumber.map(_.toString).getOrElse(<span class="string">""</span>)</span><br><span class="line">   <span class="keyword">val</span> securityMgr = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf)</span><br><span class="line">   <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, host, port, conf, securityMgr)</span><br><span class="line">   <span class="keyword">val</span> masterAddresses = masterUrls.map(<span class="type">RpcAddress</span>.fromSparkURL(_))</span><br><span class="line">   rpcEnv.setupEndpoint(<span class="type">ENDPOINT_NAME</span>, <span class="keyword">new</span> <span class="type">Worker</span>(rpcEnv, webUiPort, cores, memory, <span class="comment">//生成Worker会有很多属性.</span></span><br><span class="line">     masterAddresses, <span class="type">ENDPOINT_NAME</span>, workDir, conf, securityMgr))</span><br><span class="line">   rpcEnv</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>我们在查看一下<code>onStar</code>t方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>() &#123;</span><br><span class="line">    assert(!registered)</span><br><span class="line">    logInfo(<span class="string">"Starting Spark worker %s:%d with %d cores, %s RAM"</span>.format(</span><br><span class="line">      host, port, cores, <span class="type">Utils</span>.megabytesToString(memory)))</span><br><span class="line">    logInfo(<span class="string">s"Running Spark version <span class="subst">$&#123;org.apache.spark.SPARK_VERSION&#125;</span>"</span>)</span><br><span class="line">    logInfo(<span class="string">"Spark home: "</span> + sparkHome)  <span class="comment">//打印日志</span></span><br><span class="line">    createWorkDir()</span><br><span class="line">    shuffleService.startIfEnabled()</span><br><span class="line">    webUi = <span class="keyword">new</span> <span class="type">WorkerWebUI</span>(<span class="keyword">this</span>, workDir, webUiPort) <span class="comment">//创建WorkerWebUI</span></span><br><span class="line">    webUi.bind()</span><br><span class="line"></span><br><span class="line">    workerWebUiUrl = <span class="string">s"http://<span class="subst">$publicAddress</span>:<span class="subst">$&#123;webUi.boundPort&#125;</span>"</span> <span class="comment">//8081端</span></span><br><span class="line">    registerWithMaster() <span class="comment">//注册点击查看</span></span><br><span class="line"><span class="comment">//测量系统启动</span></span><br><span class="line">    metricsSystem.registerSource(workerSource)</span><br><span class="line">    metricsSystem.start()</span><br><span class="line">    <span class="comment">// Attach the worker metrics servlet handler to the web ui after the metrics system is started.</span></span><br><span class="line">    metricsSystem.getServletHandlers.foreach(webUi.attachHandler)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们查看一下<code>registerWithMaster</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWithMaster</span></span>() &#123;</span><br><span class="line">   <span class="comment">// onDisconnected may be triggered multiple times, so don't attempt registration</span></span><br><span class="line">   <span class="comment">// if there are outstanding registration attempts scheduled.</span></span><br><span class="line">   registrationRetryTimer <span class="keyword">match</span> &#123;  <span class="comment">//重试机制 </span></span><br><span class="line">     <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">       registered = <span class="literal">false</span> <span class="comment">//未注册的</span></span><br><span class="line">       registerMasterFutures = tryRegisterAllMasters()</span><br><span class="line">       connectionAttemptCount = <span class="number">0</span></span><br><span class="line">       registrationRetryTimer = <span class="type">Some</span>(forwordMessageScheduler.scheduleAtFixedRate( <span class="comment">//</span></span><br><span class="line">         <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">           <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">             <span class="type">Option</span>(self).foreach(_.send(<span class="type">ReregisterWithMaster</span>)) <span class="comment">//发送</span></span><br><span class="line">           &#125;</span><br><span class="line">         &#125;,</span><br><span class="line">         <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</span><br><span class="line">         <span class="type">INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</span><br><span class="line">         <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</span><br><span class="line">     <span class="keyword">case</span> <span class="type">Some</span>(_) =&gt;</span><br><span class="line">       logInfo(<span class="string">"Not spawning another attempt to register with the master, since there is an"</span> +</span><br><span class="line">         <span class="string">" attempt scheduled already."</span>)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>点击<code>ReregisterWithMaster</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">ReregisterWithMaster</span> =&gt;</span><br><span class="line">  reregisterWithMaster()  <span class="comment">// 调用方法</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">reregisterWithMaster</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Utils</span>.tryOrExit &#123;</span><br><span class="line">      connectionAttemptCount += <span class="number">1</span></span><br><span class="line">      <span class="keyword">if</span> (registered) &#123;</span><br><span class="line">        cancelLastRegistrationRetry()</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (connectionAttemptCount &lt;= <span class="type">TOTAL_REGISTRATION_RETRIES</span>) &#123;</span><br><span class="line">        logInfo(<span class="string">s"Retrying connection to master (attempt # <span class="subst">$connectionAttemptCount</span>)"</span>)</span><br><span class="line">      <span class="comment">//</span></span><br><span class="line">        master <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(masterRef) =&gt; <span class="comment">//获得masterRef</span></span><br><span class="line">            <span class="comment">// registered == false &amp;&amp; master != None means we lost the connection to master, so</span></span><br><span class="line">            <span class="comment">// masterRef cannot be used and we need to recreate it again. Note: we must not set</span></span><br><span class="line">            <span class="comment">// master to None due to the above comments.</span></span><br><span class="line">            <span class="keyword">if</span> (registerMasterFutures != <span class="literal">null</span>) &#123;</span><br><span class="line">              registerMasterFutures.foreach(_.cancel(<span class="literal">true</span>))</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">val</span> masterAddress = masterRef.address</span><br><span class="line">            registerMasterFutures = <span class="type">Array</span>(registerMasterThreadPool.submit(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                  logInfo(<span class="string">"Connecting to master "</span> + masterAddress + <span class="string">"..."</span>)</span><br><span class="line">                  <span class="keyword">val</span> masterEndpoint = rpcEnv.setupEndpointRef(masterAddress, <span class="type">Master</span>.<span class="type">ENDPOINT_NAME</span>)</span><br><span class="line">                  registerWithMaster(masterEndpoint) <span class="comment">//重试机制</span></span><br><span class="line">                &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">                  <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// Cancelled</span></span><br><span class="line">                  <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logWarning(<span class="string">s"Failed to connect to master <span class="subst">$masterAddress</span>"</span>, e)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;))</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="keyword">if</span> (registerMasterFutures != <span class="literal">null</span>) &#123;</span><br><span class="line">              registerMasterFutures.foreach(_.cancel(<span class="literal">true</span>))</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// We are retrying the initial registration</span></span><br><span class="line">            registerMasterFutures = tryRegisterAllMasters()</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// We have exceeded the initial registration retry threshold</span></span><br><span class="line">        <span class="comment">// All retries from now on should use a higher interval</span></span><br><span class="line">        <span class="keyword">if</span> (connectionAttemptCount == <span class="type">INITIAL_REGISTRATION_RETRIES</span>) &#123;</span><br><span class="line">          registrationRetryTimer.foreach(_.cancel(<span class="literal">true</span>))</span><br><span class="line">          registrationRetryTimer = <span class="type">Some</span>(</span><br><span class="line">            forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">              <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">                self.send(<span class="type">ReregisterWithMaster</span>) <span class="comment">//重试机制</span></span><br><span class="line">              &#125;</span><br><span class="line">            &#125;, <span class="type">PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</span><br><span class="line">              <span class="type">PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS</span>,</span><br><span class="line">              <span class="type">TimeUnit</span>.<span class="type">SECONDS</span>))</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        logError(<span class="string">"All masters are unresponsive! Giving up."</span>)</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="Master"><a href="#Master" class="headerlink" title="Master"></a>Master</h2><p>在Master中我们查看一下<code>receiveAndReply</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">RegisterWorker</span>(</span><br><span class="line">      id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =&gt;</span><br><span class="line">    logInfo(<span class="string">"Registering worker %s:%d with %d cores, %s RAM"</span>.format( <span class="comment">//正在注册</span></span><br><span class="line">      workerHost, workerPort, cores, <span class="type">Utils</span>.megabytesToString(memory)))</span><br><span class="line">    <span class="keyword">if</span> (state == <span class="type">RecoveryState</span>.<span class="type">STANDBY</span>) &#123; <span class="comment">//如果state不是主节点</span></span><br><span class="line">      context.reply(<span class="type">MasterInStandby</span>) <span class="comment">//回复MasterInStandby</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (idToWorker.contains(id)) &#123;</span><br><span class="line">      context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Duplicate worker ID"</span>))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> worker = <span class="keyword">new</span> <span class="type">WorkerInfo</span>(id, workerHost, workerPort, cores, memory, <span class="comment">//否则new 一个WorkerInfo 将接收到的消息传入WorkerInfo 注册信息包含有主机名端口 CPU数量 内存数量</span></span><br><span class="line">        workerRef, workerWebUiUrl)</span><br><span class="line">      <span class="keyword">if</span> (registerWorker(worker)) &#123; <span class="comment">//点击查看registerWorker</span></span><br><span class="line">        persistenceEngine.addWorker(worker)</span><br><span class="line">        context.reply(<span class="type">RegisteredWorker</span>(self, masterWebUiUrl)) <span class="comment">//成功放入</span></span><br><span class="line">        schedule()</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">//放入失败</span></span><br><span class="line">        <span class="keyword">val</span> workerAddress = worker.endpoint.address</span><br><span class="line">        logWarning(<span class="string">"Worker registration failed. Attempted to re-register worker at same "</span> +</span><br><span class="line">          <span class="string">"address: "</span> + workerAddress)</span><br><span class="line">        context.reply(<span class="type">RegisterWorkerFailed</span>(<span class="string">"Attempted to re-register worker at same address: "</span></span><br><span class="line">          + workerAddress))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><p>点击查看<code>registerWorker</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">registerWorker</span></span>(worker: <span class="type">WorkerInfo</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  <span class="comment">// There may be one or more refs to dead workers on this same node (w/ different ID's),</span></span><br><span class="line">  <span class="comment">// remove them.</span></span><br><span class="line">  workers.filter &#123; w =&gt;</span><br><span class="line">    (w.host == worker.host &amp;&amp; w.port == worker.port) &amp;&amp; (w.state == <span class="type">WorkerState</span>.<span class="type">DEAD</span>)</span><br><span class="line">  &#125;.foreach &#123; w =&gt;</span><br><span class="line">    workers -= w</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> workerAddress = worker.endpoint.address</span><br><span class="line">  <span class="keyword">if</span> (addressToWorker.contains(workerAddress)) &#123;</span><br><span class="line">    <span class="keyword">val</span> oldWorker = addressToWorker(workerAddress)</span><br><span class="line">    <span class="keyword">if</span> (oldWorker.state == <span class="type">WorkerState</span>.<span class="type">UNKNOWN</span>) &#123;</span><br><span class="line">      <span class="comment">// A worker registering from UNKNOWN implies that the worker was restarted during recovery.</span></span><br><span class="line">      <span class="comment">// The old worker must thus be dead, so we will remove it and accept the new worker.</span></span><br><span class="line">      removeWorker(oldWorker)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logInfo(<span class="string">"Attempted to re-register worker at same address: "</span> + workerAddress)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  workers += worker  <span class="comment">//这是一个HashSet[WorkerInfo]   维护的是Worker的注册信息</span></span><br><span class="line">  idToWorker(worker.id) = worker</span><br><span class="line">  addressToWorker(workerAddress) = worker</span><br><span class="line">  <span class="keyword">if</span> (reverseProxy) &#123;</span><br><span class="line">     webUi.addProxyTargets(worker.id, worker.webUiAddress)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在Master上如果我们成功了则调用<code>context.reply(RegisteredWorker(self, masterWebUiUrl))</code> 告知Worker注册成功</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">handleRegisterResponse</span></span>(msg: <span class="type">RegisterWorkerResponse</span>): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">  msg <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisteredWorker</span>(masterRef, masterWebUiUrl) =&gt;  <span class="comment">//注册成功打印日志</span></span><br><span class="line">      logInfo(<span class="string">"Successfully registered with master "</span> +   masterRef.address.toSparkURL)</span><br><span class="line">      registered = <span class="literal">true</span>  <span class="comment">//改变registered的状态</span></span><br><span class="line">      changeMaster(masterRef, masterWebUiUrl)</span><br><span class="line">      forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">          self.send(<span class="type">SendHeartbeat</span>) <span class="comment">//发送心跳</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, <span class="number">0</span>, <span class="type">HEARTBEAT_MILLIS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">CLEANUP_ENABLED</span>) &#123;</span><br><span class="line">        logInfo(</span><br><span class="line">          <span class="string">s"Worker cleanup enabled; old application directories will be deleted in: <span class="subst">$workDir</span>"</span>)</span><br><span class="line">        forwordMessageScheduler.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">          <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">            self.send(<span class="type">WorkDirCleanup</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;, <span class="type">CLEANUP_INTERVAL_MILLIS</span>, <span class="type">CLEANUP_INTERVAL_MILLIS</span>, <span class="type">TimeUnit</span>.<span class="type">MILLISECONDS</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> execs = executors.values.map &#123; e =&gt;</span><br><span class="line">        <span class="keyword">new</span> <span class="type">ExecutorDescription</span>(e.appId, e.execId, e.cores, e.state)</span><br><span class="line">      &#125;</span><br><span class="line">      masterRef.send(<span class="type">WorkerLatestState</span>(workerId, execs.toList, drivers.keys.toSeq))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">RegisterWorkerFailed</span>(message) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (!registered) &#123;</span><br><span class="line">        logError(<span class="string">"Worker registration failed: "</span> + message)</span><br><span class="line">        <span class="type">System</span>.exit(<span class="number">1</span>)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MasterInStandby</span> =&gt;</span><br><span class="line">      <span class="comment">// Ignore. Master not yet ready.</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = synchronized &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">SendHeartbeat</span> =&gt; <span class="comment">//发送的消息为workerId 和 它自身</span></span><br><span class="line">    <span class="keyword">if</span> (connected) &#123; sendToMaster(<span class="type">Heartbeat</span>(workerId, self)) &#125; <span class="comment">//sendToMaster给Master发送心跳</span></span><br><span class="line"></span><br><span class="line"> ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>点击进入<code>sendToMaster</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">sendToMaster</span></span>(message: <span class="type">Any</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  master <span class="keyword">match</span> &#123; </span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(masterRef) =&gt; masterRef.send(message) <span class="comment">//把消息发送给了Master</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      logWarning(</span><br><span class="line">        <span class="string">s"Dropping <span class="subst">$message</span> because the connection to master has not yet been established"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Master中收到发送的消息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">Heartbeat</span>(workerId, worker) =&gt;</span><br><span class="line">  idToWorker.get(workerId) <span class="keyword">match</span> &#123; <span class="comment">//Worker中是否有</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(workerInfo) =&gt;  <span class="comment">//如果有</span></span><br><span class="line">      workerInfo.lastHeartbeat = <span class="type">System</span>.currentTimeMillis() <span class="comment">//更新lastHeartbeat的时间</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="comment">//如果没有发现</span></span><br><span class="line">      <span class="keyword">if</span> (workers.map(_.id).contains(workerId)) &#123;</span><br><span class="line">        logWarning(<span class="string">s"Got heartbeat from unregistered worker <span class="subst">$workerId</span>."</span> +</span><br><span class="line">          <span class="string">" Asking it to re-register."</span>)</span><br><span class="line">        worker.send(<span class="type">ReconnectWorker</span>(masterUrl))<span class="comment">//需要重新链接masterUrl</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; </span><br><span class="line">        logWarning(<span class="string">s"Got heartbeat from unregistered worker <span class="subst">$workerId</span>."</span> +</span><br><span class="line">          <span class="string">" This worker was never registered, so ignoring the heartbeat."</span>)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>我们回过头看一下Master中的<code>Onstart</code>调用的<code>self.send(CheckForWorkerTimeOut)</code> </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="type">CheckForWorkerTimeOut</span> =&gt;</span><br><span class="line">  timeOutDeadWorkers() <span class="comment">//调用</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">timeOutDeadWorkers</span></span>() &#123;</span><br><span class="line">   <span class="comment">// Copy the workers into an array so we don't modify the hashset while iterating through it</span></span><br><span class="line">   <span class="keyword">val</span> currentTime = <span class="type">System</span>.currentTimeMillis()  <span class="comment">//获取当前时间</span></span><br><span class="line">   <span class="keyword">val</span> toRemove = workers.filter(_.lastHeartbeat &lt; currentTime - <span class="type">WORKER_TIMEOUT_MS</span>).toArray <span class="comment">//</span></span><br><span class="line">   <span class="keyword">for</span> (worker &lt;- toRemove) &#123; <span class="comment">//如果被filter出来</span></span><br><span class="line">     <span class="keyword">if</span> (worker.state != <span class="type">WorkerState</span>.<span class="type">DEAD</span>) &#123;</span><br><span class="line">       logWarning(<span class="string">"Removing %s because we got no heartbeat in %d seconds"</span>.format(</span><br><span class="line">         worker.id, <span class="type">WORKER_TIMEOUT_MS</span> / <span class="number">1000</span>))</span><br><span class="line">       removeWorker(worker) <span class="comment">//worker 被移除</span></span><br><span class="line">     &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">       <span class="keyword">if</span> (worker.lastHeartbeat &lt; currentTime - ((<span class="type">REAPER_ITERATIONS</span> + <span class="number">1</span>) * <span class="type">WORKER_TIMEOUT_MS</span>)) &#123;</span><br><span class="line">         workers -= worker <span class="comment">// we've seen this DEAD worker in the UI, etc. for long enough; cull it</span></span><br><span class="line">       &#125;</span><br><span class="line">     &#125;</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Spark的交互流程- - -节点启动：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark内核解析1</title>
    <link href="https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/"/>
    <id>https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/</id>
    <published>2019-06-09T02:32:57.000Z</published>
    <updated>2020-01-12T13:08:27.790Z</updated>
    
    <content type="html"><![CDATA[ Spark通讯架构 脚本探究：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 内核泛指 Spark 的核心运行机制，包括 Spark 核心组件的运行机制、Spark 任务调度机制、Spark 内存管理机制、Spark 核心功能的运行原理。</p><h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p><ol><li><p>将用户程序转化为任务（job）；</p></li><li><p>在 Executor 之间调度任务（task）；</p></li><li><p>跟踪 Executor 的执行情况；</p></li><li><p>通过 UI 展示查询运行情况；</p></li></ol><h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Spark Executor 节点是一个 JVM 进程，负责在 Spark 作业中运行具体任务，任务彼此之间相互独立。Spark 应用启动时， Executor  节点被同时启动， 并且始终伴随着整个 Spark  应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他 Executor 节点上继续运行。</p><p>Executor 有两个核心功能：</p><ol><li><p>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程； </p></li><li><p>它们通过自身的块管理器（ Block Manager）为用户程序中要求缓存的 RDD提供内存式存储。RDD  是直接缓存在 Executor  进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p></li></ol><h3 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190609103825.png" alt=""></p><p>Spark 通用运行流程， 不论 Spark 以何种模式进行部署， 任务提交后， 都会先启动 Driver 进程，随后 Driver 进程向集群管理器注册应用程序，之后集群管理器根据此任务的配置文件分配 Executor 并启动，当 Driver 所需的资源全部满足后， </p><p>Driver 开始执行 main 函数， Spark 查询为懒执行， 当执行到 action 算子时开始<code>反向推算</code>，根据宽依赖进行 stage 的划分，随后每一个 stage 对应一个 taskset，taskset 中有多个 task，根据本地化原则， task 会被分发到指定的 Executor 去执行，在任务执行的过程中， Executor 也会不断与 Driver 进行通信，报告任务运行情况。</p><h2 id="部署模式"><a href="#部署模式" class="headerlink" title="部署模式"></a>部署模式</h2><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>Standalone 集群有四个重要组成部分， 分别是：</p><p>Driver： 是一个进程，我们编写的 Spark  应用程序就运行在 Driver  上， 由Driver 进程执行； </p><p>Master：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责； </p><p>Worker：是一个进程，一个 Worker 运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储 RDD 的某个或某些 partition；另一个是启动其他进程和线程（Executor） ，对 RDD 上的 partition 进行并行的处理和计算。</p><p>Executor：是一个进程， 一个 Worker 上可以运行多个 Executor， Executor 通过启动多个线程（ task）来执行对 RDD 的 partition 进行并行计算，也就是执行我们对 RDD 定义的例如 map、flatMap、reduce 等算子操作。</p><h4 id="Standalone-Client"><a href="#Standalone-Client" class="headerlink" title="Standalone Client"></a>Standalone Client</h4><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609114709.png" alt=""></p><p>Standalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p><h4 id="Standalone-Cluster"><a href="#Standalone-Cluster" class="headerlink" title="Standalone Cluster"></a>Standalone Cluster</h4><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115516.png" alt=""></p><p> Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker 启动 Driver进程， Driver 启动后向 Master 注册应用程序， Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p><p>注意， Standalone  的两种模式下（ client/Cluster） ， Master  在接到 Driver  注册</p><p>Spark 应用程序的请求后，会获取其所管理的剩余资源能够启动一个 Executor 的所有 Worker， 然后在这些 Worker 之间分发 Executor， 此时的分发只考虑 Worker 上的资源是否足够使用，直到当前应用程序所需的所有 Executor 都分配完毕， Executor 反向注册完毕后，Driver 开始执行 main 程序。</p><h3 id="YARN-Client"><a href="#YARN-Client" class="headerlink" title="YARN Client"></a>YARN Client</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115616.png" alt=""></p><p>在 YARN Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster， 随后 ResourceManager 分配 container ， 在 合 适 的 NodeManager   上启动 ApplicationMaster ，此时的 </p><p>ApplicationMaster  的功能相当于一个 ExecutorLaucher， 只负责向 ResourceManager 申请 Executor 内存。</p><p>ResourceManager  接到 ApplicationMaster  的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p><h3 id="YARN-Cluster"><a href="#YARN-Cluster" class="headerlink" title="YARN Cluster"></a>YARN Cluster</h3><p>在 YARN  Cluster  模式下， 任务提交后会和 ResourceManager  通讯申请启动ApplicationMaster， 随后 ResourceManager  分配 container，在合适的 NodeManager上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。</p><p>Driver 启动后向 ResourceManager 申请 Executor 内存， ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动 Executor 进程，Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage  生成对应的 taskSet，之后将 task  分发到各个Executor 上执行。</p><h2 id="通讯架构"><a href="#通讯架构" class="headerlink" title="通讯架构"></a>通讯架构</h2><p>Spark2.x 版本使用 Netty 通讯框架作为内部通讯组件。spark  基于 netty 新的 rpc框架借鉴了 Akka 的中的设计， 它是基于Actor 模型。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120140.png" alt=""></p><p>Scala里面处理通信采用Actor架构，Actor架构其实就是一个邮局模型， AKKA为给予Actor模型的工程实现。Akka不同版本之间无法通信，存在兼容性问题。用户使用Akka与Spark中的Akka存在冲突。Spark对Akka没有自身维护，需要新功能时只能等待新版本，比较牵制Spark发展。因此在Spark2中已经抛弃了Akka。</p><p>Spark早期版本中采用Akka作为内部通信部件。<br>Spark1.3中引入Netty通信框架，为了解决Shuffle的大数据传输问题使用<br>Spark1.6中Akka和Netty可以配置使用。Netty完全实现了Akka在Spark中的功能。<br>Spark2系列中，Spark抛弃Akka，使用Netty。</p><p>Spark 通讯框架中各个组件（ Client/Master/Worker）可以认为是一个个独立的实体，各个实体之间通过消息来进行通信。具体各个组件之间的关系图如下： </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120949.png" alt=""></p><p>Endpoint（ Client/Master/Worker）有 1 个 InBox 和 N 个 OutBox（ N&gt;=1，N 取决于当前 Endpoint 与多少其他的 Endpoint 进行通信， 一个与其通讯的其他 Endpoint 对应一个 OutBox）， Endpoint  接收到的消息被写入 InBox， 发送出去的消息写入OutBox 并被发送到其他 Endpoint 的 InBox 中。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121127.png" alt=""></p><p>1) RpcEndpoint：RPC 端点，Spark 针对每个节点（ Client/Master/Worker）都称之为一个 Rpc 端点，且都实现 RpcEndpoint 接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用 Dispatcher；</p><p>2) RpcEnv： RPC  上下文环境， 每个 RPC  端点运行时依赖的上下文环境称为RpcEnv；</p><p>3) Dispatcher：消息分发器，针对于 RPC 端点需要发送消息或者从远程 RPC 接收到的消息，分发至对应的指令收件箱/发件箱。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱； </p><p>4)  Inbox：指令消息收件箱，一个本地 RpcEndpoint 对应一个收件箱，Dispatcher 在每次向 Inbox 存入消息时， 都将对应 EndpointData 加入内部 ReceiverQueue 中， 另外 Dispatcher 创建时会启动一个单独线程进行轮询 ReceiverQueue，进行收件箱消息消费； </p><p>5)  RpcEndpointRef：RpcEndpointRef 是对远程 RpcEndpoint 的一个引用。当我们需要向一个具体的 RpcEndpoint 发送消息时，一般我们需要获取到该 RpcEndpoint 的引用，然后通过该应用发送消息。</p><p>6)  OutBox ： 指令消息发件箱 ， 对于当前 RpcEndpoint 来说 ， 一个目标RpcEndpoint  对应一个发件箱， 如果向多个目标 RpcEndpoint 发送信息， 则有多个OutBox。当消息放入 Outbox 后，紧接着通过 TransportClient 将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行； </p><p>7)  RpcAddress： 表示远程的 RpcEndpointRef 的地址， Host + Port。</p><p>8)  TransportClient：Netty 通信客户端，一个 OutBox 对应一个 TransportClient，TransportClient 不断轮询 OutBox，根据 OutBox 消息的 receiver 信息，请求对应的远程 TransportServer；</p><p>9)  TransportServer ： Netty通信服务端 ， 一 个 RpcEndpoint 对应一个TransportServer，接受远程消息后调用Dispatcher 分发消息至对应收发件箱； </p><p>RpcEndPoint就代表一个通信端点， 一个端点就有一个inbox，  一个 transportServer  一个 Dispatcher，  根据你通信的其他端点的数目，就有多个Outbox， 一个outbox有一个 transportClient，   transportClient主要负责和 transportServer来通信。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609122558.png" alt=""></p><p>在我们的传统认知中，多个端点要通信，中间要有一个节点类似于总的路由，节点之间的通信靠中间的“路由”，而 Spark没有中间的这个“路由”，如果中间的“路由”存在一定会存在瓶颈问题。Spark很巧妙的把中间的“路由”拆分到各个节点上。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123350.png" alt=""></p><h3 id="高层视图"><a href="#高层视图" class="headerlink" title="高层视图"></a>高层视图</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121509.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123756.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">RpcEndpoint</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The [[RpcEnv]] that this [[RpcEndpoint]] is registered to.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">val</span> rpcEnv: <span class="type">RpcEnv</span></span><br><span class="line">   ....</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.send]] or [[RpcCallContext.reply)]]. If receiving a</span></span><br><span class="line"><span class="comment">   * unmatched message, [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">" does not implement 'receive'"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.ask]]. If receiving a unmatched message,</span></span><br><span class="line"><span class="comment">   * [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; context.sendFailure(<span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">" won't reply anything"</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked before [[RpcEndpoint]] starts to handle any message.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>   RpcEndpoint  注意三个方法，</p><p>1、receive   改方法被子类实现，用于接收其他节点发送的消息。<br>2、receiveAndReply    该方法被子类实现，用于接收并回复其他节点发送的消息。<br>3、onStart    该方法被子类实现，该方法在端口启动的时候自动调用。</p><p>我们查看以下RpcEnv的实现发现实现是NettyRpcEnv</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[netty] <span class="class"><span class="keyword">class</span> <span class="title">NettyRpcEnv</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    javaSerializerInstance: <span class="type">JavaSerializerInstance</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    host: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    securityManager: <span class="type">SecurityManager</span></span>) <span class="keyword">extends</span> <span class="title">RpcEnv</span>(<span class="params">conf</span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="keyword">val</span> transportConf = <span class="type">SparkTransportConf</span>.fromSparkConf(</span><br><span class="line">    conf.clone.set(<span class="string">"spark.rpc.io.numConnectionsPerPeer"</span>, <span class="string">"1"</span>),</span><br><span class="line">    <span class="string">"rpc"</span>,</span><br><span class="line">    conf.getInt(<span class="string">"spark.rpc.io.threads"</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置一个消息分发器</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dispatcher: <span class="type">Dispatcher</span> = <span class="keyword">new</span> <span class="type">Dispatcher</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> streamManager = <span class="keyword">new</span> <span class="type">NettyStreamManager</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> transportContext = <span class="keyword">new</span> <span class="type">TransportContext</span>(transportConf,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">NettyRpcHandler</span>(dispatcher, <span class="keyword">this</span>, streamManager))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createClientBootstraps</span></span>(): java.util.<span class="type">List</span>[<span class="type">TransportClientBootstrap</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (securityManager.isAuthenticationEnabled()) &#123;</span><br><span class="line">      java.util.<span class="type">Arrays</span>.asList(<span class="keyword">new</span> <span class="type">SaslClientBootstrap</span>(transportConf, <span class="string">""</span>, securityManager,</span><br><span class="line">        securityManager.isSaslEncryptionEnabled()))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      java.util.<span class="type">Collections</span>.emptyList[<span class="type">TransportClientBootstrap</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> clientFactory = transportContext.createClientFactory(createClientBootstraps())</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A separate client factory for file downloads. This avoids using the same RPC handler as</span></span><br><span class="line"><span class="comment">   * the main RPC context, so that events caused by these clients are kept isolated from the</span></span><br><span class="line"><span class="comment">   * main RPC traffic.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * It also allows for different configuration of certain properties, such as the number of</span></span><br><span class="line"><span class="comment">   * connections per peer.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> fileDownloadFactory: <span class="type">TransportClientFactory</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> timeoutScheduler = <span class="type">ThreadUtils</span>.newDaemonSingleThreadScheduledExecutor(<span class="string">"netty-rpc-env-timeout"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Because TransportClientFactory.createClient is blocking, we need to run it in this thread pool</span></span><br><span class="line">  <span class="comment">// to implement non-blocking send/ask.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> a non-blocking TransportClientFactory.createClient in future</span></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="keyword">val</span> clientConnectionExecutor = <span class="type">ThreadUtils</span>.newDaemonCachedThreadPool(</span><br><span class="line">    <span class="string">"netty-rpc-connection"</span>,</span><br><span class="line">    conf.getInt(<span class="string">"spark.rpc.connect.threads"</span>, <span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> server: <span class="type">TransportServer</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> stopped = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],</span></span><br><span class="line"><span class="comment">   * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="comment">// 多个地址对应的发件箱</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> outboxes = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">RpcAddress</span>, <span class="type">Outbox</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove the address's Outbox and stop it.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="function"><span class="keyword">def</span> <span class="title">removeOutbox</span></span>(address: <span class="type">RpcAddress</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> outbox = outboxes.remove(address)</span><br><span class="line">    <span class="keyword">if</span> (outbox != <span class="literal">null</span>) &#123;</span><br><span class="line">      outbox.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 启动TransportServer来接收远程消息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startServer</span></span>(bindAddress: <span class="type">String</span>, port: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bootstraps: java.util.<span class="type">List</span>[<span class="type">TransportServerBootstrap</span>] =</span><br><span class="line">      <span class="keyword">if</span> (securityManager.isAuthenticationEnabled()) &#123;</span><br><span class="line">        java.util.<span class="type">Arrays</span>.asList(<span class="keyword">new</span> <span class="type">SaslServerBootstrap</span>(transportConf, securityManager))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        java.util.<span class="type">Collections</span>.emptyList()</span><br><span class="line">      &#125;</span><br><span class="line">    server = transportContext.createServer(bindAddress, port, bootstraps)</span><br><span class="line">    dispatcher.registerRpcEndpoint(</span><br><span class="line">      <span class="type">RpcEndpointVerifier</span>.<span class="type">NAME</span>, <span class="keyword">new</span> <span class="type">RpcEndpointVerifier</span>(<span class="keyword">this</span>, dispatcher))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Nullable</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> address: <span class="type">RpcAddress</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (server != <span class="literal">null</span>) <span class="type">RpcAddress</span>(host, server.getPort()) <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 注册当前端点</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupEndpoint</span></span>(name: <span class="type">String</span>, endpoint: <span class="type">RpcEndpoint</span>): <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">    dispatcher.registerRpcEndpoint(name, endpoint)</span><br><span class="line">  &#125;</span><br><span class="line">....</span><br></pre></td></tr></table></figure><p>我们似乎没有看到Inbox在哪里点击Dispatcher</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">EndpointData</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val endpoint: <span class="type">RpcEndpoint</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val ref: <span class="type">NettyRpcEndpointRef</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> inbox = <span class="keyword">new</span> <span class="type">Inbox</span>(ref, endpoint)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h2><h3 id="start-all-sh"><a href="#start-all-sh" class="headerlink" title="start-all.sh"></a>start-all.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Start all spark daemons.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts the master on this node.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts a worker on each node specified <span class="keyword">in</span> conf/slaves</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then  #如果没有发现Spark环境变量</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)" # 获得当前的目录把当前目录设置为SPARK_HOME</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Load the Spark configuration</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh" #加载 spark-config.sh配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start Master</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start Workers</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-slaves.sh</span><br></pre></td></tr></table></figure><h3 id="spark-config-sh"><a href="#spark-config-sh" class="headerlink" title="spark-config.sh"></a>spark-config.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> included <span class="keyword">in</span> all the spark scripts with <span class="built_in">source</span> <span class="built_in">command</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> should not be executable directly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> also should not be passed any arguments, since we need original $*</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> symlink and absolute path should rely on SPARK_HOME to resolve</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;/conf"&#125;" #设置 SPARK_CONF_DIR 目录</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the PySpark classes to the PYTHONPATH:</span></span><br><span class="line">if [ -z "$&#123;PYSPARK_PYTHONPATH_SET&#125;" ]; then</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.4-src.zip:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYSPARK_PYTHONPATH_SET=1</span><br><span class="line">fi</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_162</span><br></pre></td></tr></table></figure><h3 id="start-master-sh"><a href="#start-master-sh" class="headerlink" title="start-master.sh"></a>start-master.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts the master on the machine this script is executed on.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line">CLASS="org.apache.spark.deploy.master.Master"  #调用Master</span><br><span class="line"></span><br><span class="line">if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-master.sh [options]"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">ORIGINAL_ARGS="$@"</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"  </span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh" #加载环境变量</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then # 如果没有端口 默认7077</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)           # 如果没有设置HOST 则把/usr/sbin/check-hostname作为主机名</span><br><span class="line">          SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">          ;;</span><br><span class="line">      (*)</span><br><span class="line">          SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">          ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS 1 \</span><br><span class="line">  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \</span><br><span class="line"><span class="meta">  $</span><span class="bash">ORIGINAL_ARGS</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="comment">// 1、初始化log对象</span></span><br><span class="line">  <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">  <span class="comment">// 2、加载SparkConf</span></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></span><br><span class="line">  <span class="comment">// 3、解析Master启动参数</span></span><br><span class="line">  <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">MasterArguments</span>(argStrings, conf)</span><br><span class="line">  <span class="comment">// 4、启动RPC框架端点</span></span><br><span class="line">  <span class="keyword">val</span> (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)</span><br><span class="line">  rpcEnv.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="start-slaves-sh"><a href="#start-slaves-sh" class="headerlink" title="start-slaves.sh"></a>start-slaves.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Starts a slave instance on each machine specified <span class="keyword">in</span> the conf/slaves file.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)" #获取当前的目录</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh" #加载配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the port number <span class="keyword">for</span> the master</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">          SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">          ;;</span><br><span class="line">      (*)</span><br><span class="line">          SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">          ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch the slaves 调用了start-slave.sh</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin/slaves.sh" cd "$&#123;SPARK_HOME&#125;" \; "$&#123;SPARK_HOME&#125;/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"</span><br></pre></td></tr></table></figure><h3 id="start-slave-sh"><a href="#start-slave-sh" class="headerlink" title="start-slave.sh"></a>start-slave.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Starts a slave on the machine this script is executed on.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Environment Variables</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_INSTANCES  The number of worker instances to run on this</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           slave.  Default is 1.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_PORT       The base port number <span class="keyword">for</span> the first worker. If <span class="built_in">set</span>,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           subsequent workers will increment this number.  If</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           <span class="built_in">unset</span>, Spark will find a valid port number, but</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           with no guarantee of a predictable pattern.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_WEBUI_PORT The base port <span class="keyword">for</span> the web interface of the first</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           worker.  Subsequent workers will increment this</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           number.  Default is 8081.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line">CLASS="org.apache.spark.deploy.worker.Worker"</span><br><span class="line"></span><br><span class="line">if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-slave.sh [options] &lt;master&gt;"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> First argument should be the master; we need to store it aside because we may</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> need to insert arguments between it and the other arguments</span></span><br><span class="line">MASTER=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Determine desired worker port</span></span><br><span class="line">if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start up the appropriate number of workers on this machine.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> quick <span class="built_in">local</span> <span class="keyword">function</span> to start a worker</span></span><br><span class="line">function start_instance &#123;</span><br><span class="line">  WORKER_NUM=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_WORKER_PORT" = "" ]; then</span><br><span class="line">    PORT_FLAG=</span><br><span class="line">    PORT_NUM=</span><br><span class="line">  else</span><br><span class="line">    PORT_FLAG="--port"</span><br><span class="line">    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))</span><br><span class="line">  fi</span><br><span class="line">  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))</span><br><span class="line"><span class="meta">   #</span><span class="bash">调用org.apache.spark.deploy.worker.Worker</span></span><br><span class="line">  "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \</span><br><span class="line">     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@" </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_WORKER_INSTANCES" = "" ]; then</span><br><span class="line">  start_instance 1 "$@"</span><br><span class="line">else</span><br><span class="line">  for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do</span><br><span class="line">    start_instance $(( 1 + $i )) "$@"</span><br><span class="line">  done</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>workerMain方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></span><br><span class="line">    <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">WorkerArguments</span>(argStrings, conf)</span><br><span class="line">    <span class="keyword">val</span> rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,</span><br><span class="line">      args.memory, args.masters, args.workDir, conf = conf)</span><br><span class="line">    rpcEnv.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h2 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h2><h3 id="spark-submit"><a href="#spark-submit" class="headerlink" title="spark-submit"></a>spark-submit</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> randomized <span class="built_in">hash</span> <span class="keyword">for</span> string <span class="keyword">in</span> Python 3.3+</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure><h3 id="spark-class"><a href="#spark-class" class="headerlink" title="spark-class"></a>spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The launcher library will <span class="built_in">print</span> arguments separated by a NULL character, to allow arguments with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> characters that would be otherwise interpreted by the shell. Read that <span class="keyword">in</span> a <span class="keyword">while</span> loop, populating</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> an array that will be used to <span class="built_in">exec</span> the final <span class="built_in">command</span>.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The <span class="built_in">exit</span> code of the launcher is appended to the output, so the parent shell removes it from the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> array and checks the value to see <span class="keyword">if</span> the launcher succeeded.</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure><p>查看SparkSubmit</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSubmit</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Cluster managers</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">YARN</span> = <span class="number">1</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">STANDALONE</span> = <span class="number">2</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">MESOS</span> = <span class="number">4</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">LOCAL</span> = <span class="number">8</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">ALL_CLUSTER_MGRS</span> = <span class="type">YARN</span> | <span class="type">STANDALONE</span> | <span class="type">MESOS</span> | <span class="type">LOCAL</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Deploy modes</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLIENT</span> = <span class="number">1</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLUSTER</span> = <span class="number">2</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">ALL_DEPLOY_MODES</span> = <span class="type">CLIENT</span> | <span class="type">CLUSTER</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Special primary resource names that represent shells rather than application jars.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARK_SHELL</span> = <span class="string">"spark-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">PYSPARK_SHELL</span> = <span class="string">"pyspark-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARKR_SHELL</span> = <span class="string">"sparkr-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARKR_PACKAGE_ARCHIVE</span> = <span class="string">"sparkr.zip"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">R_PACKAGE_ARCHIVE</span> = <span class="string">"rpkg.zip"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span> = <span class="number">101</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// scalastyle:off println</span></span><br><span class="line">  <span class="comment">// Exposed for testing</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">var</span> exitFn: <span class="type">Int</span> =&gt; <span class="type">Unit</span> = (exitCode: <span class="type">Int</span>) =&gt; <span class="type">System</span>.exit(exitCode)</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">var</span> printStream: <span class="type">PrintStream</span> = <span class="type">System</span>.err</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printWarning</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = printStream.println(<span class="string">"Warning: "</span> + str)</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printErrorAndExit</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printStream.println(<span class="string">"Error: "</span> + str)</span><br><span class="line">    printStream.println(<span class="string">"Run with --help for usage help or --verbose for debug output"</span>)</span><br><span class="line">    exitFn(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printVersionAndExit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printStream.println(<span class="string">""</span><span class="string">"Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version %s</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string">                        "</span><span class="string">""</span>.format(<span class="type">SPARK_VERSION</span>))  </span><br><span class="line">    printStream.println(<span class="string">"Using Scala %s, %s, %s"</span>.format(</span><br><span class="line">      <span class="type">Properties</span>.versionString, <span class="type">Properties</span>.javaVmName, <span class="type">Properties</span>.javaVersion))</span><br><span class="line">    printStream.println(<span class="string">"Branch %s"</span>.format(<span class="type">SPARK_BRANCH</span>))</span><br><span class="line">    printStream.println(<span class="string">"Compiled by user %s on %s"</span>.format(<span class="type">SPARK_BUILD_USER</span>, <span class="type">SPARK_BUILD_DATE</span>))</span><br><span class="line">    printStream.println(<span class="string">"Revision %s"</span>.format(<span class="type">SPARK_REVISION</span>))</span><br><span class="line">    printStream.println(<span class="string">"Url %s"</span>.format(<span class="type">SPARK_REPO_URL</span>))</span><br><span class="line">    printStream.println(<span class="string">"Type --help for more information."</span>)</span><br><span class="line">    exitFn(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// scalastyle:on println</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</span><br><span class="line">    <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      printStream.println(appArgs)</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h3 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Shell script <span class="keyword">for</span> starting the Spark Shell REPL</span></span><br><span class="line"></span><br><span class="line">cygwin=false</span><br><span class="line">case "$(uname)" in</span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Enter posix mode <span class="keyword">for</span> bash</span></span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE="Usage: ./bin/spark-shell [options]"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK-4161: scala does not assume use of the java classpath,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> so we need to add the <span class="string">"-Dscala.usejavacp=true"</span> flag manually. We</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">do</span> this specifically <span class="keyword">for</span> the Spark shell because the scala REPL</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> has its own class loader, and any additional classpath specified</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> through spark.driver.extraClassPath is not automatically propagated.</span></span><br><span class="line">SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # Workaround for issue involving JLine and Cygwin</span><br><span class="line">    # (see http://sourceforge.net/p/jline/bugs/40/).</span><br><span class="line">    # If you're using the Mintty terminal emulator in Cygwin, may need to set the</span><br><span class="line">    # "Backspace sends ^H" setting in "Keys" section of the Mintty options</span><br><span class="line">    # (see https://github.com/sbt/sbt/issues/562).</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">  fi  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Copy restore-TTY-on-exit <span class="built_in">functions</span> from Scala script so spark-shell exits properly even <span class="keyword">in</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> binary distribution of Spark <span class="built_in">where</span> Scala is not installed</span></span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> restore stty settings (<span class="built_in">echo</span> <span class="keyword">in</span> particular)</span></span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=""</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ "$saved_stty" != "" ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> to reenable <span class="built_in">echo</span> <span class="keyword">if</span> we are interrupted before completing.</span></span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> save terminal settings</span></span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"><span class="meta">#</span><span class="bash"> clear on error so we don<span class="string">'t later try to restore them</span></span></span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=""</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">main "$@" #调用的main函数 最终执行的依旧是spark-submi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> record the <span class="built_in">exit</span> status lest it be overwritten:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">then</span> reenable <span class="built_in">echo</span> and propagate the code.</span></span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.repl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Main</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  initializeLogIfNecessary(<span class="literal">true</span>)</span><br><span class="line">  <span class="type">Signaling</span>.cancelOnInterrupt()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  <span class="keyword">val</span> rootDir = conf.getOption(<span class="string">"spark.repl.classdir"</span>).getOrElse(<span class="type">Utils</span>.getLocalDir(conf))</span><br><span class="line">  <span class="keyword">val</span> outputDir = <span class="type">Utils</span>.createTempDir(root = rootDir, namePrefix = <span class="string">"repl"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> sparkContext: <span class="type">SparkContext</span> = _</span><br><span class="line">  <span class="keyword">var</span> sparkSession: <span class="type">SparkSession</span> = _</span><br><span class="line">  <span class="comment">// this is a public var because tests reset it.</span></span><br><span class="line">  <span class="keyword">var</span> interp: <span class="type">SparkILoop</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> hasErrors = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">scalaOptionError</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    hasErrors = <span class="literal">true</span></span><br><span class="line">    <span class="type">Console</span>.err.println(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    doMain(args, <span class="keyword">new</span> <span class="type">SparkILoop</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Visible for testing</span></span><br><span class="line">  <span class="keyword">private</span>[repl] <span class="function"><span class="keyword">def</span> <span class="title">doMain</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], _interp: <span class="type">SparkILoop</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    interp = _interp</span><br><span class="line">    <span class="keyword">val</span> jars = <span class="type">Utils</span>.getUserJars(conf, isShell = <span class="literal">true</span>).mkString(<span class="type">File</span>.pathSeparator)</span><br><span class="line">    <span class="keyword">val</span> interpArguments = <span class="type">List</span>(</span><br><span class="line">      <span class="string">"-Yrepl-class-based"</span>,</span><br><span class="line">      <span class="string">"-Yrepl-outdir"</span>, <span class="string">s"<span class="subst">$&#123;outputDir.getAbsolutePath&#125;</span>"</span>,</span><br><span class="line">      <span class="string">"-classpath"</span>, jars</span><br><span class="line">    ) ++ args.toList</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> settings = <span class="keyword">new</span> <span class="type">GenericRunnerSettings</span>(scalaOptionError)</span><br><span class="line">    settings.processArguments(interpArguments, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!hasErrors) &#123;</span><br><span class="line">      interp.process(settings) <span class="comment">// Repl starts and goes in loop of R.E.P.L</span></span><br><span class="line">      <span class="type">Option</span>(sparkContext).map(_.stop)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSparkSession</span></span>(): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> execUri = <span class="type">System</span>.getenv(<span class="string">"SPARK_EXECUTOR_URI"</span>)</span><br><span class="line">    conf.setIfMissing(<span class="string">"spark.app.name"</span>, <span class="string">"Spark shell"</span>)</span><br><span class="line">    <span class="comment">// SparkContext will detect this configuration and register it with the RpcEnv's</span></span><br><span class="line">    <span class="comment">// file server, setting spark.repl.class.uri to the actual URI for executors to</span></span><br><span class="line">    <span class="comment">// use. This is sort of ugly but since executors are started as part of SparkContext</span></span><br><span class="line">    <span class="comment">// initialization in certain cases, there's an initialization order issue that prevents</span></span><br><span class="line">    <span class="comment">// this from being set after SparkContext is instantiated.</span></span><br><span class="line">    conf.set(<span class="string">"spark.repl.class.outputDir"</span>, outputDir.getAbsolutePath())</span><br><span class="line">    <span class="keyword">if</span> (execUri != <span class="literal">null</span>) &#123;</span><br><span class="line">      conf.set(<span class="string">"spark.executor.uri"</span>, execUri)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">System</span>.getenv(<span class="string">"SPARK_HOME"</span>) != <span class="literal">null</span>) &#123;</span><br><span class="line">      conf.setSparkHome(<span class="type">System</span>.getenv(<span class="string">"SPARK_HOME"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> builder = <span class="type">SparkSession</span>.builder.config(conf)</span><br><span class="line">    <span class="keyword">if</span> (conf.get(<span class="type">CATALOG_IMPLEMENTATION</span>.key, <span class="string">"hive"</span>).toLowerCase == <span class="string">"hive"</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">SparkSession</span>.hiveClassesArePresent) &#123;</span><br><span class="line">        <span class="comment">// In the case that the property is not set at all, builder's config</span></span><br><span class="line">        <span class="comment">// does not have this value set to 'hive' yet. The original default</span></span><br><span class="line">        <span class="comment">// behavior is that when there are hive classes, we use hive catalog.</span></span><br><span class="line">        sparkSession = builder.enableHiveSupport().getOrCreate()</span><br><span class="line">        logInfo(<span class="string">"Created Spark session with Hive support"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Need to change it back to 'in-memory' if no hive classes are found</span></span><br><span class="line">        <span class="comment">// in the case that the property is set to hive in spark-defaults.conf</span></span><br><span class="line">        builder.config(<span class="type">CATALOG_IMPLEMENTATION</span>.key, <span class="string">"in-memory"</span>)</span><br><span class="line">        sparkSession = builder.getOrCreate()</span><br><span class="line">        logInfo(<span class="string">"Created Spark session"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// In the case that the property is set but not to 'hive', the internal</span></span><br><span class="line">      <span class="comment">// default is 'in-memory'. So the sparkSession will use in-memory catalog.</span></span><br><span class="line">      sparkSession = builder.getOrCreate()</span><br><span class="line">      logInfo(<span class="string">"Created Spark session"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    sparkContext = sparkSession.sparkContext</span><br><span class="line">    sparkSession</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[repl] <span class="class"><span class="keyword">trait</span> <span class="title">SparkILoopInit</span> </span>&#123;</span><br><span class="line">  self: <span class="type">SparkILoop</span> =&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Print a welcome message */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printWelcome</span></span>() &#123;</span><br><span class="line">    echo(<span class="string">""</span><span class="string">"Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version %s</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.format(<span class="type">SPARK_VERSION</span>))</span><br><span class="line">    <span class="keyword">import</span> <span class="type">Properties</span>._</span><br><span class="line">    <span class="keyword">val</span> welcomeMsg = <span class="string">"Using Scala %s (%s, Java %s)"</span>.format(</span><br><span class="line">      versionString, javaVmName, javaVersion)</span><br><span class="line">    echo(welcomeMsg)</span><br><span class="line">    echo(<span class="string">"Type in expressions to have them evaluated."</span>)</span><br><span class="line">    echo(<span class="string">"Type :help for more information."</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncMessage</span></span>(msg: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isReplInfo || isReplPower)</span><br><span class="line">      echoAndRefresh(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initLock = <span class="keyword">new</span> java.util.concurrent.locks.<span class="type">ReentrantLock</span>()</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initCompilerCondition = initLock.newCondition() <span class="comment">// signal the compiler is initialized</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initLoopCondition = initLock.newCondition()     <span class="comment">// signal the whole repl is initialized</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initStart = <span class="type">System</span>.nanoTime</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withLock</span></span>[<span class="type">T</span>](body: =&gt; <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    initLock.lock()</span><br><span class="line">    <span class="keyword">try</span> body</span><br><span class="line">    <span class="keyword">finally</span> initLock.unlock()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// a condition used to ensure serial access to the compiler.</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initIsComplete = <span class="literal">false</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initError: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">elapsed</span></span>() = <span class="string">"%.3f"</span>.format((<span class="type">System</span>.nanoTime - initStart).toDouble / <span class="number">1000000000</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the method to be called when the interpreter is initialized.</span></span><br><span class="line">  <span class="comment">// Very important this method does nothing synchronous (i.e. do</span></span><br><span class="line">  <span class="comment">// not try to use the interpreter) because until it returns, the</span></span><br><span class="line">  <span class="comment">// repl's lazy val `global` is still locked.</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">initializedCallback</span></span>() = withLock(initCompilerCondition.signal())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Spins off a thread which awaits a single message once the interpreter</span></span><br><span class="line">  <span class="comment">// has been initialized.</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createAsyncListener</span></span>() = &#123;</span><br><span class="line">    io.spawn &#123;</span><br><span class="line">      withLock(initCompilerCondition.await())</span><br><span class="line">      asyncMessage(<span class="string">"[info] compiler init time: "</span> + elapsed() + <span class="string">" s."</span>)</span><br><span class="line">      postInitialization()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// called from main repl loop</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">awaitInitialized</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!initIsComplete)</span><br><span class="line">      withLock &#123; <span class="keyword">while</span> (!initIsComplete) initLoopCondition.await() &#125;</span><br><span class="line">    <span class="keyword">if</span> (initError != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      println(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |Failed to initialize the REPL due to an unexpected error.</span></span><br><span class="line"><span class="string">        |This is a bug, please, report it along with the error diagnostics printed below.</span></span><br><span class="line"><span class="string">        |%s."</span><span class="string">""</span>.stripMargin.format(initError)</span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// private def warningsThunks = List(</span></span><br><span class="line">  <span class="comment">//   () =&gt; intp.bind("lastWarnings", "" + typeTag[List[(Position, String)]], intp.lastWarnings _),</span></span><br><span class="line">  <span class="comment">// )</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">postInitThunks</span> </span>= <span class="type">List</span>[<span class="type">Option</span>[() =&gt; <span class="type">Unit</span>]](</span><br><span class="line">    <span class="type">Some</span>(intp.setContextClassLoader _),</span><br><span class="line">    <span class="keyword">if</span> (isReplPower) <span class="type">Some</span>(() =&gt; enablePowerMode(<span class="literal">true</span>)) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  ).flatten</span><br><span class="line">  <span class="comment">// ++ (</span></span><br><span class="line">  <span class="comment">//   warningsThunks</span></span><br><span class="line">  <span class="comment">// )</span></span><br><span class="line">  <span class="comment">// called once after init condition is signalled</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">postInitialization</span></span>() &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      postInitThunks foreach (f =&gt; addThunk(f()))</span><br><span class="line">      runThunks()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ex: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        initError = stackTraceString(ex)</span><br><span class="line">        <span class="keyword">throw</span> ex</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      initIsComplete = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isAsync) &#123;</span><br><span class="line">        asyncMessage(<span class="string">"[info] total init time: "</span> + elapsed() + <span class="string">" s."</span>)</span><br><span class="line">        withLock(initLoopCondition.signal())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initializeSpark</span></span>() &#123;</span><br><span class="line">    intp.beQuietDuring &#123;</span><br><span class="line">      command(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        @transient val spark = org.apache.spark.repl.Main.interp.createSparkSession()</span></span><br><span class="line"><span class="string">        @transient val sc = &#123;</span></span><br><span class="line"><span class="string">          val _sc = spark.sparkContext</span></span><br><span class="line"><span class="string">          if (_sc.getConf.getBoolean("</span>spark.ui.reverseP<span class="string">roxy", false)) &#123;</span></span><br><span class="line"><span class="string">            val proxyUrl = _sc.getConf.get("</span>spark.ui.reverseProxyU<span class="string">rl", null)</span></span><br><span class="line"><span class="string">            if (proxyUrl != null) &#123;</span></span><br><span class="line"><span class="string">              println(s"</span><span class="type">Spark</span> <span class="type">Context</span> <span class="type">Web</span> <span class="type">UI</span> is available at $&#123;proxyUrl&#125;/proxy/$&#123;_sc.applicationId&#125;<span class="string">")</span></span><br><span class="line"><span class="string">            &#125; else &#123;</span></span><br><span class="line"><span class="string">              println(s"</span><span class="type">Spark</span> <span class="type">Context</span> <span class="type">Web</span> <span class="type">UI</span> is available at <span class="type">Spark</span> <span class="type">Master</span> <span class="type">Public</span> <span class="type">URL</span><span class="string">")</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          &#125; else &#123;</span></span><br><span class="line"><span class="string">            _sc.uiWebUrl.foreach &#123;</span></span><br><span class="line"><span class="string">              webUrl =&gt; println(s"</span><span class="type">Spark</span> context <span class="type">Web</span> <span class="type">UI</span> available at $&#123;webUrl&#125;<span class="string">")</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">          println("</span><span class="type">Spark</span> context available as <span class="symbol">'s</span>c' <span class="string">" +</span></span><br><span class="line"><span class="string">            s"</span>(master = $&#123;_sc.master&#125;, app id = $&#123;_sc.applicationId&#125;).<span class="string">")</span></span><br><span class="line"><span class="string">          println("</span><span class="type">Spark</span> session available as <span class="symbol">'spar</span>k'.<span class="string">")</span></span><br><span class="line"><span class="string">          _sc</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span>)</span><br><span class="line">      command(<span class="string">"import org.apache.spark.SparkContext._"</span>)</span><br><span class="line">      command(<span class="string">"import spark.implicits._"</span>)</span><br><span class="line">      command(<span class="string">"import spark.sql"</span>)</span><br><span class="line">      command(<span class="string">"import org.apache.spark.sql.functions._"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// code to be executed only after the interpreter is initialized</span></span><br><span class="line">  <span class="comment">// and the lazy val `global` can be accessed without risk of deadlock.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> pendingThunks: <span class="type">List</span>[() =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">addThunk</span></span>(body: =&gt; <span class="type">Unit</span>) = synchronized &#123;</span><br><span class="line">    pendingThunks :+= (() =&gt; body)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">runThunks</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (pendingThunks.nonEmpty)</span><br><span class="line">      logDebug(<span class="string">"Clearing "</span> + pendingThunks.size + <span class="string">" thunks."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (pendingThunks.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> thunk = pendingThunks.head</span><br><span class="line">      pendingThunks = pendingThunks.tail</span><br><span class="line">      thunk()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      Spark通讯架构 脚本探究：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>Spark之GraphX</title>
    <link href="https://www.hphblog.cn/2019/06/08/Spark%E4%B9%8BGraphX/"/>
    <id>https://www.hphblog.cn/2019/06/08/Spark%E4%B9%8BGraphX/</id>
    <published>2019-06-08T06:33:32.000Z</published>
    <updated>2020-01-12T13:08:26.445Z</updated>
    
    <content type="html"><![CDATA[ GraphX相关学习：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>GraphX 是 Spark 图表和图形并行计算的新组件。GraphX 延伸 Spark <a href="http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD" target="_blank" rel="noopener">RDD</a> 通过引入新的<a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#property_graph" target="_blank" rel="noopener">图形</a>的抽象：计算与连接到每个顶点和边缘性的向量。以支持图形计算，GraphX 公开了一组基本的操作符（例如  <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#structural_operators" target="_blank" rel="noopener">subgraph</a>, <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#join_operators" target="_blank" rel="noopener">joinVertices</a>和 <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#aggregateMessages" target="_blank" rel="noopener">aggregateMessages</a>）以及一个优化高阶API。此外，GraphX 包括的图形越来越多的收集 <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_algorithms" target="_blank" rel="noopener">algorithms</a> 和 <a href="http://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_builders" target="_blank" rel="noopener">builders</a> ，以简化图形分析任务。</p><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><h3 id="顶点"><a href="#顶点" class="headerlink" title="顶点"></a>顶点</h3><p><code>RDD[(VertexId, VD)]</code>表示顶点。  VertexId 就是Long类型，表示顶点的ID【主键】。 VD表示类型参数，可以是任意类型, 表示的是该顶点的属性。<br>VertexRDD[VD] 继承了RDD[(VertexId, VD)]， 他是顶点的另外一种表示方式， 在内部的计算上提供了很多的优化还有一些更高级的API。</p><h3 id="边"><a href="#边" class="headerlink" title="边"></a>边</h3><p>RDD[Edge[VD]]  表示边，  Edge中有三个东西： srcId表示 源顶点的ID， dstId表示的是目标顶点的ID， attr表示表的属性，属性的类型是VD类型，VD是一个类型参数，可以是任意类型。</p><p>EdgeRDD[ED] 继承了 RDD[Edge[ED]] ,他是边的另外一种表示方式，在内部的计算上提供您改了很多的优化还有一些更高级的API。</p><h3 id="三元组"><a href="#三元组" class="headerlink" title="三元组"></a>三元组</h3><p> EdgeTriplet[VD, ED] extends Edge[ED]   他表示一个三元组， 比边多了两个顶点的属性</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608150952.png" alt=""></p><h3 id="图"><a href="#图" class="headerlink" title="图"></a>图</h3><p> Graph[VD: ClassTag, ED: ClassTag]  VD 是顶点的属性、  ED是边的属性</p><h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>1、直接创建 sparkConf  -》  sparkContext<br>2、创建顶点的RDD  RDD[(VertexId, VD)]<br>3、创建边的RDD  RDD[Edge[ED]]<br>4、根据边和顶点创建 Graph<br>5、对图进行计算<br>6、关闭 SparkContext</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.graphx.&#123;<span class="type">Edge</span>, <span class="type">Graph</span>, <span class="type">VertexId</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">GraphxHelloWorld</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">//创建sparkConf</span></span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"graphx"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建SparkContext</span></span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//业务逻辑</span></span><br><span class="line">  <span class="keyword">val</span> users: <span class="type">RDD</span>[(<span class="type">VertexId</span>, (<span class="type">String</span>, <span class="type">String</span>))] =</span><br><span class="line">    sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">      (<span class="number">3</span>L, (<span class="string">"rxin"</span>, <span class="string">"student"</span>)),</span><br><span class="line">      (<span class="number">7</span>L, (<span class="string">"jgonzal"</span>, <span class="string">"postdoc"</span>)),</span><br><span class="line">      (<span class="number">5</span>L, (<span class="string">"franklin"</span>, <span class="string">"prof"</span>)),</span><br><span class="line">      (<span class="number">2</span>L, (<span class="string">"istoica"</span>, <span class="string">"prof"</span>)))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> relationships: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">String</span>]] =</span><br><span class="line">    sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">      <span class="type">Edge</span>(<span class="number">3</span>L, <span class="number">7</span>L, <span class="string">"collab"</span>),</span><br><span class="line">      <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">3</span>L, <span class="string">"advisor"</span>),</span><br><span class="line">      <span class="type">Edge</span>(<span class="number">2</span>L, <span class="number">5</span>L, <span class="string">"colleague"</span>),</span><br><span class="line">      <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">7</span>L, <span class="string">"pi"</span>))</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> defaultUser = (<span class="string">"John Doe"</span>, <span class="string">"Missing"</span>)</span><br><span class="line">  <span class="keyword">val</span> graph = <span class="type">Graph</span>(users, relationships, defaultUser)</span><br><span class="line">  <span class="keyword">val</span> facts: <span class="type">RDD</span>[<span class="type">String</span>] =</span><br><span class="line">    graph.triplets.map(triplet =&gt;</span><br><span class="line">      triplet.srcAttr._1 + <span class="string">" is the "</span> + triplet.attr + <span class="string">" of "</span> + triplet.dstAttr._1)</span><br><span class="line">  facts.collect.foreach(println(_))</span><br><span class="line"></span><br><span class="line">  <span class="comment">//关闭</span></span><br><span class="line">  sc.stop()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608153045.png" alt=""></p><h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><h3 id="创建操作"><a href="#创建操作" class="headerlink" title="创建操作"></a>创建操作</h3><p> 根据边和顶点的数据来创建。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>[<span class="type">VD</span>: <span class="type">ClassTag</span>, <span class="type">ED</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    vertices: <span class="type">RDD</span>[(<span class="type">VertexId</span>, <span class="type">VD</span>)],</span><br><span class="line">    edges: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">ED</span>]],</span><br><span class="line">    defaultVertexAttr: <span class="type">VD</span> = <span class="literal">null</span>.asInstanceOf[<span class="type">VD</span>],</span><br><span class="line">    edgeStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>,</span><br><span class="line">    vertexStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>): <span class="type">Graph</span>[<span class="type">VD</span>, <span class="type">ED</span>]</span><br></pre></td></tr></table></figure><p>根据边直接创建， 所有顶点的属性都一样为 defaultValue</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fromEdges</span></span>[<span class="type">VD</span>: <span class="type">ClassTag</span>, <span class="type">ED</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    edges: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">ED</span>]],</span><br><span class="line">    defaultValue: <span class="type">VD</span>,</span><br><span class="line">    edgeStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>,</span><br><span class="line">    vertexStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>): <span class="type">Graph</span>[<span class="type">VD</span>, <span class="type">ED</span>]</span><br></pre></td></tr></table></figure><p>根据裸边来进行创建，顶点的属性是 defaultValue  ，边的属性为1</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fromEdgeTuples</span></span>[<span class="type">VD</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rawEdges: <span class="type">RDD</span>[(<span class="type">VertexId</span>, <span class="type">VertexId</span>)],</span><br><span class="line">    defaultValue: <span class="type">VD</span>,</span><br><span class="line">    uniqueEdges: <span class="type">Option</span>[<span class="type">PartitionStrategy</span>] = <span class="type">None</span>,</span><br><span class="line">    edgeStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>,</span><br><span class="line">    vertexStorageLevel: <span class="type">StorageLevel</span> = <span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>): <span class="type">Graph</span>[<span class="type">VD</span>, <span class="type">Int</span>]</span><br></pre></td></tr></table></figure><h3 id="转换操作"><a href="#转换操作" class="headerlink" title="转换操作"></a>转换操作</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608153513.png" alt=""></p><p><code>numEdges</code>  返回边的个数</p><p><code>numVertices</code>  顶点的个数</p><p><code>inDegrees: VertexRDD[Int]</code>  返回顶点的入度， 返回类型为 <code>RDD[(VertexId, Int)] Int</code>就是入度的具体值</p><p><code>outDegrees: VertexRDD[Int]</code>  返回顶点的出度， 返回类型为 <code>RDD[(VertexId, Int)]</code> Int就是出度的具体值<br> <code>degrees: VertexRDD[Int]</code>  返回顶点的入度和出度之和。 返回类型为 <code>RDD[(VertexId, Int)]</code> Int就是出度的具体值</p><h3 id="结构操作"><a href="#结构操作" class="headerlink" title="结构操作"></a>结构操作</h3><p><code>def reverse: Graph[VD, ED]</code>   反转整个图  ，将边的方向调头</p><p><code>def subgraph( epred: EdgeTriplet[VD, ED] =&gt; Boolean = (x =&gt; true), vpred: (VertexId, VD) =&gt; Boolean = ((v, d) =&gt; true)) : Graph[VD, ED]</code>可以通过参数名来指定传参，  如果``subGraph`中有的边没有顶点对应，那么会自动将该边去除  。   没有边的顶点不会自动被删除</p><p><code>def mask[VD2: ClassTag, ED2: ClassTag](other:Graph[VD2, ED2]): Graph[VD, ED]</code>将当前图和Other图做交集，返回一个新图，如果other中的属性和原图的属性不同，那么保留原图的属性</p><p><code>def groupEdges(merge: (ED, ED) =&gt; ED): Graph[VD, ED]</code> 合并两条边，通过函数合并边的属性。</p><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p><code>def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]</code> 收集邻居节点的数据，根据指定的方向。返回的数据为RDD[(VertexId,  Array[(VertexId, VD)] )]   顶点的属性是 一个数组。数组中包含邻居节点的顶点</p><p><code>def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]]</code> 跟上一个相同，只不过只收集ID</p><p><code>def aggregateMessages[A: ClassTag]( sendMsg: EdgeContext[VD, ED, A] =&gt; Unit, mergeMsg: (A, A) =&gt; A, tripletFields: TripletFields = TripletFields.All) : VertexRDD[A]</code>  每一个边都会通过<code>sendMsg</code> 发送一个消息， 每一个顶点都会通过<code>mergeMsg</code> 来处理所有他收到的消息。  <code>TripletFields</code>存在主要用于定制 <code>EdgeContext</code>对象中的属性的值是否存在， 为了减少数据通信量。</p><h3 id="关联操作"><a href="#关联操作" class="headerlink" title="关联操作"></a>关联操作</h3><p><code>def joinVertices[U: ClassTag](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) =&gt; VD) : Graph[VD, ED]</code> 将相同顶点ID的数据进行加权，  将U这种类型的数据加入到 VD这种类型的数据上，但是不能修改VD的类型。</p><p><code>def outerJoinVertices[U: ClassTag, VD2: ClassTag](other: RDD[(VertexId, U)]) (mapFunc: (VertexId, VD, Option[U]) =&gt; VD2)(implicit eq: VD =:= VD2 = null) : Graph[VD2, ED]</code>   和<code>joinVertices</code>类似。，只不是如果没有相对应的节点，那么join的值默认为None。</p><h3 id="Pregel"><a href="#Pregel" class="headerlink" title="Pregel"></a>Pregel</h3><p>节点：  有两种状态：</p><p>1、钝化态【类似于休眠，不做任何事】 </p><p> 2、激活态【干活】<br>2、节点能够处于激活态需要有条件：</p><p>（1）、节点收到消息  </p><p> （2）、成功发送了任何一条消息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pregel</span></span>[<span class="type">A</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    initialMsg: <span class="type">A</span>,        <span class="comment">//     图初始化的时候，开始模型计算的时候，所有节点都会先收到一个消息。</span></span><br><span class="line">    maxIterations: <span class="type">Int</span> = <span class="type">Int</span>.<span class="type">MaxValue</span>,     <span class="comment">//最大迭代次数  </span></span><br><span class="line">    activeDirection: <span class="type">EdgeDirection</span> = <span class="type">EdgeDirection</span>.<span class="type">Either</span>)   <span class="comment">//规定了发送消息的方向</span></span><br><span class="line">   (</span><br><span class="line">    vprog: (<span class="type">VertexId</span>, <span class="type">VD</span>, <span class="type">A</span>) =&gt; <span class="type">VD</span>,  <span class="comment">//节点调用该消息将聚合后的数据和本节点进行属性的合并。  </span></span><br><span class="line">    sendMsg: <span class="type">EdgeTriplet</span>[<span class="type">VD</span>, <span class="type">ED</span>] =&gt; <span class="type">Iterator</span>[(<span class="type">VertexId</span>, <span class="type">A</span>)],   <span class="comment">//激活态的节点调用该方法发送消息</span></span><br><span class="line">    mergeMsg: (<span class="type">A</span>, <span class="type">A</span>) =&gt; <span class="type">A</span>)<span class="comment">//如果一个节点接收到多条消息，先用mergeMsg 来将多条消息聚合成为一条消息，如果节点只收到一条消息，则不调用该函数</span></span><br><span class="line">  : <span class="type">Graph</span>[<span class="type">VD</span>, <span class="type">ED</span>]</span><br></pre></td></tr></table></figure><h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.log4j.&#123;<span class="type">Level</span>, <span class="type">Logger</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.graphx.&#123;<span class="type">Edge</span>, _&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by wuyufei on 2017/9/22.</span></span><br><span class="line"><span class="comment">  *</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Practice</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//屏蔽日志</span></span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org.apache.spark"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org.eclipse.jetty.server"</span>).setLevel(<span class="type">Level</span>.<span class="type">OFF</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//设定一个SparkConf</span></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SimpleGraphX"</span>).setMaster(<span class="string">"local[4]"</span>)</span><br><span class="line">  <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//初始化顶点集合</span></span><br><span class="line">  <span class="keyword">val</span> vertexArray = <span class="type">Array</span>(</span><br><span class="line">    (<span class="number">1</span>L, (<span class="string">"Alice"</span>, <span class="number">28</span>)),</span><br><span class="line">    (<span class="number">2</span>L, (<span class="string">"Bob"</span>, <span class="number">27</span>)),</span><br><span class="line">    (<span class="number">3</span>L, (<span class="string">"Charlie"</span>, <span class="number">65</span>)),</span><br><span class="line">    (<span class="number">4</span>L, (<span class="string">"David"</span>, <span class="number">42</span>)),</span><br><span class="line">    (<span class="number">5</span>L, (<span class="string">"Ed"</span>, <span class="number">55</span>)),</span><br><span class="line">    (<span class="number">6</span>L, (<span class="string">"Fran"</span>, <span class="number">50</span>))</span><br><span class="line">  )</span><br><span class="line">  <span class="comment">//创建顶点的RDD表示</span></span><br><span class="line">  <span class="keyword">val</span> vertexRDD: <span class="type">RDD</span>[(<span class="type">Long</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = sc.parallelize(vertexArray)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//初始化边的集合</span></span><br><span class="line">  <span class="keyword">val</span> edgeArray = <span class="type">Array</span>(</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">2</span>L, <span class="number">1</span>L, <span class="number">7</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">2</span>L, <span class="number">4</span>L, <span class="number">2</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">3</span>L, <span class="number">2</span>L, <span class="number">4</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">3</span>L, <span class="number">6</span>L, <span class="number">3</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">4</span>L, <span class="number">1</span>L, <span class="number">1</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">2</span>L, <span class="number">5</span>L, <span class="number">2</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">3</span>L, <span class="number">8</span>),</span><br><span class="line">    <span class="type">Edge</span>(<span class="number">5</span>L, <span class="number">6</span>L, <span class="number">3</span>)</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建边的RDD表示</span></span><br><span class="line">  <span class="keyword">val</span> edgeRDD: <span class="type">RDD</span>[<span class="type">Edge</span>[<span class="type">Int</span>]] = sc.parallelize(edgeArray)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//创建一个图</span></span><br><span class="line">  <span class="keyword">val</span> graph: <span class="type">Graph</span>[(<span class="type">String</span>, <span class="type">Int</span>), <span class="type">Int</span>] = <span class="type">Graph</span>(vertexRDD, edgeRDD)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  图的属性    ****************************************</span></span><br><span class="line"></span><br><span class="line">  println(<span class="string">"属性演示"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  println(<span class="string">"找出图中年龄大于30的顶点："</span>)</span><br><span class="line">  graph.vertices.filter &#123; <span class="keyword">case</span> (id, (name, age)) =&gt; age &gt; <span class="number">30</span> &#125;.collect.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> (id, (name, age)) =&gt; println(<span class="string">s"<span class="subst">$name</span> is <span class="subst">$age</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println</span><br><span class="line">  <span class="comment">//</span></span><br><span class="line">  println(<span class="string">"找出图中属性大于5的边："</span>)</span><br><span class="line">  graph.edges.filter(e =&gt; e.attr &gt; <span class="number">5</span>).collect.foreach(e =&gt; println(<span class="string">s"<span class="subst">$&#123;e.srcId&#125;</span> to <span class="subst">$&#123;e.dstId&#125;</span> att <span class="subst">$&#123;e.attr&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//triplets操作，((srcId, srcAttr), (dstId, dstAttr), attr)</span></span><br><span class="line">  println(<span class="string">"列出边属性&gt;5的tripltes："</span>)</span><br><span class="line">  <span class="keyword">for</span> (triplet &lt;- graph.triplets.filter(t =&gt; t.attr &gt; <span class="number">5</span>).collect) &#123;</span><br><span class="line">    println(<span class="string">s"<span class="subst">$&#123;triplet.srcAttr._1&#125;</span> likes <span class="subst">$&#123;triplet.dstAttr._1&#125;</span>"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//Degrees操作</span></span><br><span class="line">  println(<span class="string">"找出图中最大的出度、入度、度数："</span>)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">max</span></span>(a: (<span class="type">VertexId</span>, <span class="type">Int</span>), b: (<span class="type">VertexId</span>, <span class="type">Int</span>)): (<span class="type">VertexId</span>, <span class="type">Int</span>) = &#123;</span><br><span class="line">    <span class="keyword">if</span> (a._2 &gt; b._2) a <span class="keyword">else</span> b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"max of outDegrees:"</span> + graph.outDegrees.reduce(max) + <span class="string">" max of inDegrees:"</span> + graph.inDegrees.reduce(max) + <span class="string">" max of Degrees:"</span> + graph.degrees.reduce(max))</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  转换操作    ****************************************</span></span><br><span class="line">  println(<span class="string">"转换操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  println(<span class="string">"顶点的转换操作，顶点age + 10："</span>)</span><br><span class="line">  graph.mapVertices &#123; <span class="keyword">case</span> (id, (name, age)) =&gt; (id, (name, age + <span class="number">10</span>)) &#125;.vertices.collect.foreach(v =&gt; println(<span class="string">s"<span class="subst">$&#123;v._2._1&#125;</span> is <span class="subst">$&#123;v._2._2&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line">  println(<span class="string">"边的转换操作，边的属性*2："</span>)</span><br><span class="line">  graph.mapEdges(e =&gt; e.attr * <span class="number">2</span>).edges.collect.foreach(e =&gt; println(<span class="string">s"<span class="subst">$&#123;e.srcId&#125;</span> to <span class="subst">$&#123;e.dstId&#125;</span> att <span class="subst">$&#123;e.attr&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line">  println(<span class="string">"三元组的转换操作，边的属性为端点的age相加："</span>)</span><br><span class="line">  graph.mapTriplets(tri =&gt; tri.srcAttr._2 * tri.dstAttr._2).triplets.collect.foreach(e =&gt; println(<span class="string">s"<span class="subst">$&#123;e.srcId&#125;</span> to <span class="subst">$&#123;e.dstId&#125;</span> att <span class="subst">$&#123;e.attr&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  结构操作    ****************************************</span></span><br><span class="line">  println(<span class="string">"结构操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  println(<span class="string">"顶点年纪&gt;30的子图："</span>)</span><br><span class="line">  <span class="keyword">val</span> subGraph = graph.subgraph(vpred = (id, vd) =&gt; vd._2 &gt;= <span class="number">30</span>)</span><br><span class="line">  println(<span class="string">"子图所有顶点："</span>)</span><br><span class="line">  subGraph.vertices.collect.foreach(v =&gt; println(<span class="string">s"<span class="subst">$&#123;v._2._1&#125;</span> is <span class="subst">$&#123;v._2._2&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line">  println(<span class="string">"子图所有边："</span>)</span><br><span class="line">  subGraph.edges.collect.foreach(e =&gt; println(<span class="string">s"<span class="subst">$&#123;e.srcId&#125;</span> to <span class="subst">$&#123;e.dstId&#125;</span> att <span class="subst">$&#123;e.attr&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line">  println(<span class="string">"反转整个图："</span>)</span><br><span class="line">  <span class="keyword">val</span> reverseGraph = graph.reverse</span><br><span class="line">  println(<span class="string">"子图所有顶点："</span>)</span><br><span class="line">  reverseGraph.vertices.collect.foreach(v =&gt; println(<span class="string">s"<span class="subst">$&#123;v._2._1&#125;</span> is <span class="subst">$&#123;v._2._2&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line">  println(<span class="string">"子图所有边："</span>)</span><br><span class="line">  reverseGraph.edges.collect.foreach(e =&gt; println(<span class="string">s"<span class="subst">$&#123;e.srcId&#125;</span> to <span class="subst">$&#123;e.dstId&#125;</span> att <span class="subst">$&#123;e.attr&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  连接操作    ****************************************</span></span><br><span class="line">  println(<span class="string">"连接操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  <span class="keyword">val</span> inDegrees: <span class="type">VertexRDD</span>[<span class="type">Int</span>] = graph.inDegrees</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span>, inDeg: <span class="type">Int</span>, outDeg: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">//创建一个新图，顶类点VD的数据型为User，并从graph做类型转换</span></span></span><br><span class="line"><span class="class">  <span class="title">val</span> <span class="title">initialUserGraph</span></span>: <span class="type">Graph</span>[<span class="type">User</span>, <span class="type">Int</span>] = graph.mapVertices &#123; <span class="keyword">case</span> (id, (name, age)) =&gt; <span class="type">User</span>(name, age, <span class="number">0</span>, <span class="number">0</span>) &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//initialUserGraph与inDegrees、outDegrees（RDD）进行连接，并修改initialUserGraph中inDeg值、outDeg值</span></span><br><span class="line">  <span class="keyword">val</span> userGraph = initialUserGraph.outerJoinVertices(initialUserGraph.inDegrees) &#123;</span><br><span class="line">    <span class="keyword">case</span> (id, u, inDegOpt) =&gt; <span class="type">User</span>(u.name, u.age, inDegOpt.getOrElse(<span class="number">0</span>), u.outDeg)</span><br><span class="line">  &#125;.outerJoinVertices(initialUserGraph.outDegrees) &#123;</span><br><span class="line">    <span class="keyword">case</span> (id, u, outDegOpt) =&gt; <span class="type">User</span>(u.name, u.age, u.inDeg, outDegOpt.getOrElse(<span class="number">0</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"连接图的属性："</span>)</span><br><span class="line">  userGraph.vertices.collect.foreach(v =&gt; println(<span class="string">s"<span class="subst">$&#123;v._2.name&#125;</span> inDeg: <span class="subst">$&#123;v._2.inDeg&#125;</span>  outDeg: <span class="subst">$&#123;v._2.outDeg&#125;</span>"</span>))</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"出度和入读相同的人员："</span>)</span><br><span class="line">  userGraph.vertices.filter &#123;</span><br><span class="line">    <span class="keyword">case</span> (id, u) =&gt; u.inDeg == u.outDeg</span><br><span class="line">  &#125;.collect.foreach &#123;</span><br><span class="line">    <span class="keyword">case</span> (id, property) =&gt; println(property.name)</span><br><span class="line">  &#125;</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  聚合操作    ****************************************</span></span><br><span class="line">  println(<span class="string">"聚合操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  println(<span class="string">"collectNeighbors：获取当前节点source节点的id和属性"</span>)</span><br><span class="line">  graph.collectNeighbors(<span class="type">EdgeDirection</span>.<span class="type">In</span>).collect.foreach(v =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"id: <span class="subst">$&#123;v._1&#125;</span>"</span>); <span class="keyword">for</span> (arr &lt;- v._2) &#123;</span><br><span class="line">      println(<span class="string">s"      <span class="subst">$&#123;arr._1&#125;</span> (name: <span class="subst">$&#123;arr._2._1&#125;</span>  age: <span class="subst">$&#123;arr._2._2&#125;</span>)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"aggregateMessages版本："</span>)</span><br><span class="line">  graph.aggregateMessages[<span class="type">Array</span>[(<span class="type">VertexId</span>, (<span class="type">String</span>, <span class="type">Int</span>))]](ctx =&gt; ctx.sendToDst(<span class="type">Array</span>((ctx.srcId.toLong, (ctx.srcAttr._1, ctx.srcAttr._2)))), _ ++ _).collect.foreach(v =&gt; &#123;</span><br><span class="line">    println(<span class="string">s"id: <span class="subst">$&#123;v._1&#125;</span>"</span>); <span class="keyword">for</span> (arr &lt;- v._2) &#123;</span><br><span class="line">      println(<span class="string">s"    <span class="subst">$&#123;arr._1&#125;</span> (name: <span class="subst">$&#123;arr._2._1&#125;</span>  age: <span class="subst">$&#123;arr._2._2&#125;</span>)"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"聚合操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line">  println(<span class="string">"找出年纪最大的追求者："</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> oldestFollower: <span class="type">VertexRDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = userGraph.aggregateMessages[(<span class="type">String</span>, <span class="type">Int</span>)](</span><br><span class="line">    <span class="comment">// 将源顶点的属性发送给目标顶点，map过程</span></span><br><span class="line">    ctx =&gt; ctx.sendToDst((ctx.srcAttr.name, ctx.srcAttr.age)),</span><br><span class="line">    <span class="comment">// 得到最大追求者，reduce过程</span></span><br><span class="line">    (a, b) =&gt; <span class="keyword">if</span> (a._2 &gt; b._2) a <span class="keyword">else</span> b</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  userGraph.vertices.leftJoin(oldestFollower) &#123; (id, user, optOldestFollower) =&gt;</span><br><span class="line">    optOldestFollower <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="string">s"<span class="subst">$&#123;user.name&#125;</span> does not have any followers."</span></span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>((name, age)) =&gt; <span class="string">s"<span class="subst">$&#123;name&#125;</span> is the oldest follower of <span class="subst">$&#123;user.name&#125;</span>."</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.collect.foreach &#123; <span class="keyword">case</span> (id, str) =&gt; println(str) &#125;</span><br><span class="line">  println</span><br><span class="line"></span><br><span class="line">  <span class="comment">//***************************  实用操作    ****************************************</span></span><br><span class="line">  println(<span class="string">"聚合操作"</span>)</span><br><span class="line">  println(<span class="string">"**********************************************************"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sourceId: <span class="type">VertexId</span> = <span class="number">5</span>L <span class="comment">// 定义源点</span></span><br><span class="line">  <span class="keyword">val</span> initialGraph = graph.mapVertices((id, _) =&gt; <span class="keyword">if</span> (id == sourceId) <span class="number">0.0</span> <span class="keyword">else</span> <span class="type">Double</span>.<span class="type">PositiveInfinity</span>)</span><br><span class="line"></span><br><span class="line">  initialGraph.triplets.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">  println(<span class="string">"找出5到各顶点的最短距离："</span>)</span><br><span class="line">  <span class="keyword">val</span> sssp = initialGraph.pregel(<span class="type">Double</span>.<span class="type">PositiveInfinity</span>, <span class="type">Int</span>.<span class="type">MaxValue</span>, <span class="type">EdgeDirection</span>.<span class="type">Out</span>)(</span><br><span class="line">    (id, dist, newDist) =&gt; &#123;</span><br><span class="line">      println(<span class="string">"||||"</span> + id); math.min(dist, newDist)</span><br><span class="line">    &#125;,</span><br><span class="line">    triplet =&gt; &#123; <span class="comment">// 计算权重</span></span><br><span class="line">      println(<span class="string">"&gt;&gt;&gt;&gt;"</span> + triplet.srcId)</span><br><span class="line">      <span class="keyword">if</span> (triplet.srcAttr + triplet.attr &lt; triplet.dstAttr) &#123;</span><br><span class="line">        <span class="comment">//发送成功</span></span><br><span class="line">        <span class="type">Iterator</span>((triplet.dstId, triplet.srcAttr + triplet.attr))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//发送不成功</span></span><br><span class="line">        <span class="type">Iterator</span>.empty</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    (a, b) =&gt; math.min(a, b) <span class="comment">// 当前节点所有输入的最短距离</span></span><br><span class="line">  )</span><br><span class="line">  sssp.triplets.collect().foreach(println)</span><br><span class="line"></span><br><span class="line">  println(sssp.vertices.collect.mkString(<span class="string">"\n"</span>))</span><br><span class="line"></span><br><span class="line">  sc.stop()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608155611.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608155624.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608155637.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.graphx.&#123;<span class="type">Graph</span>, <span class="type">VertexId</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>, graphx&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/8.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PageRank</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Spark Graphx PageRank"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> erdd = sc.textFile(<span class="string">"D:\\input\\graphx-wiki-edges.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> edges = erdd.map(x =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> para = x.split(<span class="string">"\t"</span>); graphx.<span class="type">Edge</span>(para(<span class="number">0</span>).trim.toLong, para(<span class="number">1</span>).trim.toLong, <span class="number">0</span>)</span><br><span class="line">    &#125;)</span><br><span class="line">    <span class="keyword">val</span> vrdd = sc.textFile(<span class="string">"D:\\input\\graphx-wiki-vertices.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> vertices = vrdd.map(x =&gt;&#123;<span class="keyword">val</span> para =x.split(<span class="string">"\t"</span>);(para(<span class="number">0</span>).trim.toLong,para(<span class="number">1</span>).trim)&#125;)</span><br><span class="line">    <span class="keyword">val</span> graph =<span class="type">Graph</span>(vertices,edges)</span><br><span class="line">    println(<span class="string">"*****************************************************"</span>)</span><br><span class="line">    println(<span class="string">"PageRank计算,获取最有价值的数据"</span>)</span><br><span class="line">    println(<span class="string">"*****************************************************"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> prGraph = graph.pageRank(<span class="number">0.001</span>).cache()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> titleAndPrGraph = graph.outerJoinVertices(prGraph.vertices) &#123;</span><br><span class="line">      (v, title, rank) =&gt; (rank.getOrElse(<span class="number">0.0</span>), title)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">   titleAndPrGraph.vertices.top(<span class="number">10</span>) &#123;</span><br><span class="line">      <span class="type">Ordering</span>.by((entry: (<span class="type">VertexId</span>, (<span class="type">Double</span>, <span class="type">String</span>))) =&gt; entry._2._1)</span><br><span class="line">    &#125;.foreach(t =&gt; println(t._2._2 + <span class="string">": "</span> + t._2._1))</span><br><span class="line">    </span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190608162612.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      GraphX相关学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="GraphX" scheme="https://www.hphblog.cn/tags/GraphX/"/>
    
  </entry>
  
  <entry>
    <title>Spark之StructuredStreaming</title>
    <link href="https://www.hphblog.cn/2019/06/07/Spark%E4%B9%8BStructuredStreaming/"/>
    <id>https://www.hphblog.cn/2019/06/07/Spark%E4%B9%8BStructuredStreaming/</id>
    <published>2019-06-07T14:57:46.000Z</published>
    <updated>2020-01-12T13:08:22.475Z</updated>
    
    <content type="html"><![CDATA[ Structured Streaming相关学习：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p> Structured Streaming是Spark2.0版本提出的新的实时流框架，是一种基于Spark SQL引擎的可扩展且容错的流处理引擎。在内部，默认情况下，结构化流式查询使用微批处理引擎进行处理，该引擎将数据流作为一系列小批量作业处理，从而实现低至100毫秒的端到端延迟和完全一次的容错保证。自Spark 2.3以来，引入了一种称为连续处理的新型低延迟处理模式，它可以实现低至1毫秒的端到端延迟，并且具有至少一次保证。</p><p>相比于Spark Streaming，优点如下：</p><p>支持多种数据源的输入和输出<br>以结构化的方式操作流式数据，能够像使用Spark SQL处理离线的批处理一样，处理流数据，代码更简洁，写法更简单<br>基于Event-Time，相比于Spark Streaming的Processing-Time更精确，更符合业务场景<br>解决了Spark Streaming存在的代码升级，DAG图变化引起的任务失败，无法断点续传的问题。</p><h2 id="WordCount"><a href="#WordCount" class="headerlink" title="WordCount"></a>WordCount</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/7.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"StructuredNetworkWordCount"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> lines = spark.readStream</span><br><span class="line">      .format(<span class="string">"socket"</span>)</span><br><span class="line">      .option(<span class="string">"host"</span>, <span class="string">"datanode1"</span>)</span><br><span class="line">      .option(<span class="string">"port"</span>, <span class="number">9999</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> words = lines.as[<span class="type">String</span>].flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = words.groupBy(<span class="string">"value"</span>).count()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> query = wordCounts.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .start()</span><br><span class="line"></span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/StructuredStreaming.gif" alt=""></p><h2 id="编程模型"><a href="#编程模型" class="headerlink" title="编程模型"></a>编程模型</h2><p>结构化流的关键思想是将活生生的数据流看作一张正在被连续追加数据的表。产生了一个与批处理模型非常相似的新的流处理模型。可以像在静态表之上的标准批处理查询一样，Spark是使用在一张无界的输入表之上的增量式查询来执行流计算的。</p><p><img src="http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png" alt="Stream as a Table"></p><p>数据流Data Stream看成了表的行数据，连续地往表中追加。结构化流查询将会产生一张结果表（Result Table）:</p><p><img src="http://spark.apache.org/docs/latest/img/structured-streaming-model.png" alt="Model"></p><p>第一行是Time，每秒有个触发器，第二行是输入流，对输入流执行查询后产生的结果最终会被更新到第三行的结果表中。第四行驶输出，图中显示的输出模式是完全模式（Complete Mode）。图中显示的是无论结果表何时得到更新，我们将希望将改变的结果行写入到外部存储。输出有三种不同的模式：</p><p>（1）完全模式（Complete Mode）</p><p>整个更新的结果表（Result Table）将被写入到外部存储。这取决于外部连接决定如何操作整个表的写入。</p><p>（2）追加模式（Append Mode）</p><p>只有从上一次触发后追加到结果表中新行会被写入到外部存储。适用于已经存在结果表中的行不期望被改变的查询。</p><p>（3）更新模式（Update Mode）</p><p>只有从上一次触发后在结果表中更新的行将会写入外部存储（Spark 2.1.1之后才可用）。这种模式不同于之前的完全模式，它仅仅输出上一次触发后改变的行。如果查询中不包含聚合，这种模式与追加模式等价的。每种模式适用于特定类型的查询。下面以单词计数的例子说明三种模式的区别（单词计数中使用了聚合）</p><p><img src="http://spark.apache.org/docs/latest/img/structured-streaming-example-model.png" alt="Model"></p><h3 id="Event-time-Late-Data"><a href="#Event-time-Late-Data" class="headerlink" title="Event-time Late Data"></a>Event-time Late Data</h3><p> Event-time是嵌入到数据本身的基于事件的时间。对于许多的应用来说，你可能希望操作这个事件-时间。例如，如果你想获得每分钟物联网设备产生的事件数量，然后想使用数据产生时的时间（也就是数据的event-time），而不是Spark接收他们的时间。每个设备中的事件是表中的一行，而事件-时间是行中的一个列值。这就允许将基于窗口的聚合（比如每分钟的事件数）看成是事件-时间列的分组和聚合的特殊类型——每个时间窗口是一个组，每行可以属于多个窗口/组。</p><p> 进一步，这个模型自然处理那些比期望延迟到达的事件-时间数据。当Spark正在更新结果表时，当有延迟数据，它就会完全控制更新旧的聚合，而且清理旧的聚合去限制中间状态数据的大小。从Spark 2.1开始，我们已经开始支持水印（watermarking ），它允许用户确定延迟的阈值，允许引擎相应地删除旧的状态。</p><h3 id="窗口操作"><a href="#窗口操作" class="headerlink" title="窗口操作"></a>窗口操作</h3><p>在滑动的事件-时间窗口上的聚合对于结构化流是简单的，非常类似于分组聚合。在分组聚合中，聚合的值对用户确定分组的列保持唯一的。在基于窗口的聚合中，聚合的值对每个窗口的事件-时间保持唯一的。</p><p> 修改我们前面的单词计数的例子，现在当产生一行句子时，附件一个时间戳。我们想每5分钟统计一次10分钟内的单词数。例如，12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20等。注意到12:00 - 12:10是一个窗口，表示数据12:00之后12:10之前到达。比如12:07到达的单词，这个单词应该在12:00 - 12:10和12:05 - 12:15两个窗口中都要被统计。如图：</p><p><img src="http://spark.apache.org/docs/latest/img/structured-streaming-window.png" alt="img"></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Timestamp</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/7.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WindowOnEventTime</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">TimeWord</span>(<span class="params">word: <span class="type">String</span>, timestamp: <span class="type">Timestamp</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"Structrued-Streaming"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">      </span><br><span class="line">    <span class="keyword">val</span> lines = spark.readStream</span><br><span class="line">      .format(<span class="string">"socket"</span>)</span><br><span class="line">      .option(<span class="string">"host"</span>, <span class="string">"datanode1"</span>)</span><br><span class="line">      .option(<span class="string">"port"</span>, <span class="number">9999</span>)</span><br><span class="line">      .option(<span class="string">"includeTimestamp"</span>, <span class="literal">true</span>) <span class="comment">//添加时间戳</span></span><br><span class="line">      .load()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> words = lines.as[(<span class="type">String</span>, <span class="type">Timestamp</span>)]</span><br><span class="line">      .flatMap(line =&gt; line._1.split(<span class="string">" "</span>)</span><br><span class="line">        .map(word =&gt; <span class="type">TimeWord</span>(word, line._2))).toDF()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计数</span></span><br><span class="line">    <span class="keyword">val</span> windowedCounts = words.groupBy(</span><br><span class="line">      window($<span class="string">"timestamp"</span>,<span class="string">"10 seconds"</span> ,<span class="string">"5 seconds"</span>), $<span class="string">"word"</span></span><br><span class="line">    ).count().orderBy(<span class="string">"window"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查询</span></span><br><span class="line">    <span class="keyword">val</span> query = windowedCounts.writeStream</span><br><span class="line">      .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">      .format(<span class="string">"console"</span>)</span><br><span class="line">      .option(<span class="string">"truncate"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .start()</span><br><span class="line">    query.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/StructuredStreaming_WindowandEvent.gif" alt=""></p><h3 id="容错语义"><a href="#容错语义" class="headerlink" title="容错语义"></a>容错语义</h3><p>提供end-to-end exactly-once语义是structured streaming设计背后的关键目标之一。 为了实现这一点，Spark设计了structured streaming的sources，sinks和执行引擎，可靠地跟踪处理进程的准确进度，以便它可以通过重新启动和/或重新处理来解决任何类型的故障。 假设每个Streaming源具有跟踪流中读取位置的偏移（类似于Kafka偏移或Kinesis序列号）。 引擎使用检查点和WAL（write ahead logs）记录每个触发器中正在处理的数据的偏移范围。 Streaming sinks为了解决重复计算被设计为幂等。 一起使用可重放sources和幂等sinks，Structured Streaming可以在任何故障下确保end-to-end exactly-once的语义。</p><h3 id="Watermarking"><a href="#Watermarking" class="headerlink" title="Watermarking"></a>Watermarking</h3><p>现在考虑如果其中一个事件延迟到达应用程序会发生什么。 例如，说在12:04（即事件时间）生成的一个单词可以在12:11被应用程序接收。 应用程序</p><p>应该使用时间12:04而不是12:11更新12:00 - 12:10的窗口的较旧计数。 这在我们基于窗口的分组中自然发生 - Structured Streaming可以长时间维持部分聚合的中间状态，以便迟到的数据可以正确地更新旧窗口的聚合，如下所示。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190608100551.png" alt=""></p><p>为了持续几天运行这个查询，系统必须限制其累积的内存中间状态的数量。这意味着系统需要知道什么时候可以从内存状态中删除旧的聚合，因为应用程序不会再为该集合接收到较晚的数据。为了实现这一点，在Spark 2.1中引入了watermarking，让引擎自动跟踪数据中的当前event-time，并尝试相应地清理旧状态。您可以通过指定事件时间列来定义查询的watermarking，并根据事件时间指定数据的延迟时间的阈值。对于从时间T开始的特定窗口，引擎将保持状态，并允许延迟数据更新状态，直到引擎看到最大事件时间-迟到的最大阈值。换句话说，阈值内的迟到数据将被聚合，但是比阈值晚的数据将被丢弃。让我们以一个例子来理解这一点。我们可以使用Watermark（）轻松定义上一个例子中的watermarking ，</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> windowedCounts = words</span><br><span class="line">    .withWatermark(<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>)</span><br><span class="line">    .groupBy(</span><br><span class="line">        window($<span class="string">"timestamp"</span>, <span class="string">"10 minutes"</span>, <span class="string">"5 minutes"</span>),</span><br><span class="line">        $<span class="string">"word"</span>)</span><br><span class="line">    .count()</span><br></pre></td></tr></table></figure><p>在这个例子中，我们正在定义“timestamp”列的查询的watermark ，并将“10分钟”定义为允许数据延迟的阈值。 如果此查询在更新输出模式下运行（稍后在“输出模式”部分中讨论），则引擎将继续更新Resule表中窗口的计数，直到窗口比watermark 旧，滞后于当前事件时间列“ timestamp“10分钟。 </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190608101146.png" alt=""></p><p>与之前的更新模式类似，引擎维护每个窗口的中间计数。 但是，部分计数不会更新到结果表，也不写入sink。 引擎等待“10分钟”接收迟到数据，然后丢弃窗口（watermark）的中间状态，并将最终计数附加到结果表sink。 例如，窗口12:00 - 12:10的最终计数仅在watermark更新到12:11之后才附加到结果表中。<br>watermarking 清理聚合状态的条件重要的是要注意，为了清理聚合查询中的状态，必须满足以下条件（从Spark 2.1.1开始，以后再进行更改）。 </p><ul><li>输出模式必须是追加或更新。 完整模式要求保留所有聚合数据，因此不能使用watermarking 去掉中间状态。 有关每种输出模式的语义的详细说明，请参见“输出模式”部分。 </li><li>聚合必须具有事件时间列或事件时间列上的窗口。 </li><li>必须在与聚合中使用的时间戳列相同的列上使用withWatermark 。 例如，df.withWatermark（“time”，“1 min”）.groupBy（“time2”）.count（）在附加输出模式中无效，因为watermark 在不同的列上定义为聚合列。 </li><li>必须在聚合之前调用withWatermark才能使用watermark 细节。 例如，在附加输出模式下，<code>df.groupBy（“time”）.count（）.withWatermark（“time”，“1 min”）</code>无效。</li></ul><h3 id="Join操作"><a href="#Join操作" class="headerlink" title="Join操作"></a>Join操作</h3><p>Streaming DataFrames可以与静态 DataFrames连接，以创建新的Streaming DataFrames。 例如下面的例子。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> staticDf = spark.read. ...</span><br><span class="line"><span class="keyword">val</span> streamingDf = spark.readStream. ... </span><br><span class="line"></span><br><span class="line">streamingDf.join(staticDf, <span class="string">"type"</span>)          <span class="comment">// inner equi-join with a static DF</span></span><br><span class="line">streamingDf.join(staticDf, <span class="string">"type"</span>, <span class="string">"right_join"</span>)  <span class="comment">// right outer join with a static DF</span></span><br></pre></td></tr></table></figure><p>在Spark 2.3中，Spark添加了对流 - 流 Join的支持，也就是说，您可以加入两个 streaming Datasets/DataFrames。 在两个数据流之间生成连接结果的挑战是，在任何时间点，dataset的view对于连接的两侧都是不完整的，这使得在输入之间找到匹配更加困难。 从一个输入流接收的任何行的数据都可以与来自另一个输入流的未来输入的任何一条数据匹配，尚未接收的行匹配。 因此，对于两个输入流，我们将过去的输入缓冲为流状态，以便我们可以将每个未来输入与过去的输入相匹配，从而生成Join结果。 此外，类似于流聚合，Spark自动处理迟到的无序数据，并可以使用水印限制状态。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">mport org.apache.spark.sql.functions.expr</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> impressions = spark.readStream. ...</span><br><span class="line"><span class="keyword">val</span> clicks = spark.readStream. ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Apply watermarks on event-time columns</span></span><br><span class="line"><span class="keyword">val</span> impressionsWithWatermark = impressions.withWatermark(<span class="string">"impressionTime"</span>, <span class="string">"2 hours"</span>)</span><br><span class="line"><span class="keyword">val</span> clicksWithWatermark = clicks.withWatermark(<span class="string">"clickTime"</span>, <span class="string">"3 hours"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// Join with event-time constraints</span></span><br><span class="line">impressionsWithWatermark.join(</span><br><span class="line">  clicksWithWatermark,</span><br><span class="line">  expr(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">    clickAdId = impressionAdId AND</span></span><br><span class="line"><span class="string">    clickTime &gt;= impressionTime AND</span></span><br><span class="line"><span class="string">    clickTime &lt;= impressionTime + interval 1 hour</span></span><br><span class="line"><span class="string">    "</span><span class="string">""</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="不支持的操作"><a href="#不支持的操作" class="headerlink" title="不支持的操作"></a>不支持的操作</h2><p>有几个DataFrame / Dataset操作不支持streaming DataFrames / Datasets。 其中一些如下。 </p><ul><li><p>streaming Datasets不支持多个streaming聚合（即streaming DF上的聚合链）。 </p></li><li><p>流数据集不支持limit和取前N行。 </p></li><li><p>Streaming Datasets不支持Distinct 操作。 </p></li><li><p>只有在在完全输出模式的聚合之后，streaming Datasets才支持排序操作。 </p></li><li><p>有条件地支持Streaming和静态Datasets之间的外连接。<br>不支持与 streaming Dataset的Full outer join<br>不支持streaming Dataset 在右侧的Left outer join<br>不支持streaming Dataset在左侧的Right outer join</p></li><li><p>两个streaming Datasets之间的任何种类型的join都不受支持</p></li></ul><p>此外，还有一些Dataset方法将不适用于streaming Datasets。 它们是立即运行查询并返回结果的操作，这在streaming Datasets上没有意义。 相反，这些功能可以通过显式启动streaming查询来完成（参见下一节）。<br>- count() - 无法从流数据集返回单个计数。 而是使用ds.group By.count（）返回一个包含running count的streaming Dataset 。<br>- foreach() - 而是使用ds.writeStream.foreach（…）（见下一节）。<br>- show() - Instead use the console sink (see next section).</p><h2 id="流式查询"><a href="#流式查询" class="headerlink" title="流式查询"></a>流式查询</h2><h3 id="输出模式"><a href="#输出模式" class="headerlink" title="输出模式"></a>输出模式</h3><ul><li>Append mode (default) - 这是默认模式，其中只有从上次触发后添加到结果表的新行将被输出到sink。 只有那些添加到“结果表”中并且从不会更改的行的查询才支持这一点。 因此，该模式保证每行只能输出一次（假定容错sink）。 例如，只有select，where，map，flatMap，filter，join等的查询将支持Append模式。 </li><li>Complete mode -每个触发后，整个结果表将被输出到sink。 聚合查询支持这一点。 </li><li>Update mode - （自Spark 2.1.1以来可用）只有结果表中自上次触发后更新的行才会被输出到sink。 更多信息将在以后的版本中添加。</li></ul><table><thead><tr><th>Type</th><th>Supported Output Modes</th><th>备注</th></tr></thead><tbody><tr><td>没有聚合的查询</td><td>Append, Update</td><td>不支持完整模式，因为将所有数据保存在结果表中是不可行的。</td></tr><tr><td>有聚合的查询：使用watermark对event-time进行聚合</td><td>Append, Update, Complete</td><td>附加模式使用watermark 来降低旧聚合状态。 但是，窗口化聚合的输出会延迟“withWatermark（）”中指定的晚期阈值，因为模式语义可以在结果表中定义后才能将结果表添加到结果表中（即在watermark 被交叉之后）。 有关详细信息，请参阅后期数据部分。更新模式使用水印去掉旧的聚合状态。完全模式不会丢弃旧的聚合状态，因为根据定义，此模式保留结果表中的所有数据。</td></tr><tr><td>有聚合的查询：其他聚合</td><td>Complete, Update</td><td>由于没有定义watermark （仅在其他类别中定义），旧的聚合状态不会被丢弃。不支持附加模式，因为聚合可以更新，从而违反了此模式的语义。</td></tr></tbody></table><h3 id="Output-Sinks"><a href="#Output-Sinks" class="headerlink" title="Output Sinks"></a>Output Sinks</h3><p>File sink-将输出存储到目录</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">"parquet"</span>)        <span class="comment">// 也可以是 "orc", "json", "csv", 等等.</span></span><br><span class="line">    .option(<span class="string">"path"</span>, <span class="string">"path/to/destination/dir"</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><p>Foreach sink - 对输出中的记录运行任意计算。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .foreach(...)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><p>Console sink (for debugging) </p><p>每次触发时将输出打印到控制台/ stdout。 都支持“Append ”和“Complete ”输出模式。 这应该用于低数据量的调试目的，因为在每次触发后，整个输出被收集并存储在驱动程序的内存中。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">"console"</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><p>Memory sink (for debugging) </p><p>输出作为内存表存储在内存中。 都支持“Append ”和“Complete ”输出模式。 由于整个输出被收集并存储在驱动程序的内存中，所以应用于低数据量的调试目的。 因此，请谨慎使用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">writeStream</span><br><span class="line">    .format(<span class="string">"memory"</span>)</span><br><span class="line">    .queryName(<span class="string">"tableName"</span>)</span><br><span class="line">    .start()</span><br></pre></td></tr></table></figure><table><thead><tr><th>sink</th><th>Supported Output Modes</th><th>Options</th><th>Fault-tolerant</th><th>Notes</th></tr></thead><tbody><tr><td>File Sink</td><td>Append</td><td>path：输出目录的路径，必须指定。 maxFilesPerTrigger：每个触发器中要考虑的最大新文件数（默认值：无最大值） latestFirst：是否首先处理最新的新文件，当有大量的文件积压（default：false）时很有用 有关特定于文件格式的选项，请参阅DataFrameWriter（Scala / Java / Python）中的相关方法。 例如。 对于“parquet”格式选项请参阅DataFrameWriter.parquet（）</td><td>yes</td><td>支持对分区表的写入。 按时间划分可能有用。</td></tr><tr><td>Foreach Sink</td><td>Append, Update, Compelete</td><td>None</td><td>取决于ForeachWriter的实现</td><td>更多细节在下一节</td></tr><tr><td>Console Sink</td><td>Append, Update, Complete</td><td>numRows：每次触发打印的行数（默认值：20）truncate：输出太长是否截断（默认值：true）</td><td>no</td><td></td></tr><tr><td>Memory Sink</td><td>Append, Complete</td><td>None</td><td>否。但在Complete模式下，重新启动的查询将重新创建整个表。</td><td>查询名就是表名</td></tr></tbody></table><p>您必须调用start（）来实际启动查询的执行。 这将返回一个StreamingQuery对象，它是连续运行执行的句柄。 您可以使用此对象来管理查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> noAggDF = deviceDataDf.select(<span class="string">"device"</span>).where(<span class="string">"signal &gt; 10"</span>)   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Print new data to console</span></span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Write new data to Parquet files</span></span><br><span class="line">noAggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .format(<span class="string">"parquet"</span>)</span><br><span class="line">  .option(<span class="string">"checkpointLocation"</span>, <span class="string">"path/to/checkpoint/dir"</span>)</span><br><span class="line">  .option(<span class="string">"path"</span>, <span class="string">"path/to/destination/dir"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ========== DF with aggregation ==========</span></span><br><span class="line"><span class="keyword">val</span> aggDF = df.groupBy(<span class="string">"device"</span>).count()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Print updated aggregations to console</span></span><br><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// Have all the aggregates in an in-memory table </span></span><br><span class="line">aggDF</span><br><span class="line">  .writeStream</span><br><span class="line">  .queryName(<span class="string">"aggregates"</span>)    <span class="comment">// this query name will be the table name</span></span><br><span class="line">  .outputMode(<span class="string">"complete"</span>)</span><br><span class="line">  .format(<span class="string">"memory"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">spark.sql(<span class="string">"select * from aggregates"</span>).show()   <span class="comment">// interactively query in-memory table</span></span><br></pre></td></tr></table></figure><h3 id="Foreach和ForeachBatch"><a href="#Foreach和ForeachBatch" class="headerlink" title="Foreach和ForeachBatch"></a>Foreach和ForeachBatch</h3><p>foreach和foreachBatch操作允许您在流式查询的输出上应用任意操作和编写逻辑。 它们的用例略有不同 - 虽然foreach允许在每一行上自定义写入逻辑，foreachBatch允许在每个微批量的输出上进行任意操作和自定义逻辑。</p><p>foreachBatch（）允许您指定在流式查询的每个微批次的输出数据上执行的函数。 从Spark 2.4开始，Scala，Java和Python都支持它。 它需要两个参数：DataFrame或Dataset，它具有微批次的输出数据和微批次的唯一ID。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt;</span><br><span class="line">  <span class="comment">// Transform and write batchDF </span></span><br><span class="line">&#125;.start()</span><br></pre></td></tr></table></figure><h4 id="foreachBatch"><a href="#foreachBatch" class="headerlink" title="foreachBatch"></a>foreachBatch</h4><p>重用现有的批处理数据源 - 对于许多存储系统，可能还没有可用的流式接收器，但可能已经存在用于批量查询的数据写入器。使用foreachBatch，您可以在每个微批次的输出上使用批处理数据编写器。</p><p>写入多个位置 - 如果要将流式查询的输出写入多个位置，则可以简单地多次写入输出DataFrame / Dataset。但是，每次写入尝试都会导致重新计算输出数据（包括可能重新读取输入数据）。要避免重新计算，您应该缓存输出DataFrame / Dataset，将其写入多个位置，然后将其解除。这是一个大纲。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">streamingDF.writeStream.foreachBatch &#123; (batchDF: <span class="type">DataFrame</span>, batchId: <span class="type">Long</span>) =&gt; batchDF.persist() batchDF.write.format(…).save(…) <span class="comment">// location 1 batchDF.write.format(…).save(…) // location 2 batchDF.unpersist() &#125;</span></span><br></pre></td></tr></table></figure><p>应用其他DataFrame操作 - 流式DataFrame中不支持许多DataFrame和Dataset操作，因为Spark不支持在这些情况下生成增量计划。使用foreachBatch，您可以在每个微批输出上应用其中一些操作。但是，您必须自己解释执行该操作的端到端语义。注意：默认情况下，foreachBatch仅提供至少一次写保证。但是，您可以使用提供给该函数的batchId作为重复数据删除输出并获得一次性保证的方法。 foreachBatch不适用于连续处理模式，因为它从根本上依赖于流式查询的微批量执行。如果以连续模式写入数据，请改用foreach。</p><p>注意：默认情况下，foreachBatch仅提供至少一次写保证。 但是，您可以使用提供给该函数的batchId作为重复数据删除输出并获得一次性保证的方法。 foreachBatch不适用于连续处理模式，因为它从根本上依赖于流式查询的微批量执行。 如果以连续模式写入数据，请改用foreach。</p><h4 id="Foreach"><a href="#Foreach" class="headerlink" title="Foreach"></a>Foreach</h4><p>如果foreachBatch不是一个选项（例如，相应的批处理数据写入器不存在，或连续处理模式），那么您可以使用foreach表达自定义编写器逻辑。 具体来说，您可以通过将数据划分为三种方法来表达数据写入逻辑：打开，处理和关闭。 从Spark 2.4开始，foreach可用于Scala，Java和Python。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">treamingDatasetOfString.writeStream.foreach(</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ForeachWriter</span>[<span class="type">String</span>] &#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open</span></span>(partitionId: <span class="type">Long</span>, version: <span class="type">Long</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">      <span class="comment">// Open connection</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span></span>(record: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// Write string to connection</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close</span></span>(errorOrNull: <span class="type">Throwable</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      <span class="comment">// Close the connection</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">).start()</span><br></pre></td></tr></table></figure><p>执行语义启动流式查询时，Spark以下列方式调用函数或对象的方法：</p><p>此对象的单个副本负责查询中单个任务生成的所有数据。换句话说，一个实例负责处理以分布式方式生成的数据的一个分区。</p><p>此对象必须是可序列化的，因为每个任务都将获得所提供对象的新的序列化反序列化副本。 因此，强烈建议在调用open（）方法之后完成用于写入数据的任何初始化（例如，打开连接或启动事务），这表示任务已准备好生成数据。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 方法的生命周期如下：</span><br><span class="line">-   <span class="type">For</span> each partition <span class="keyword">with</span> partition_id:</span><br><span class="line">    - <span class="type">For</span> each batch/epoch of streaming data <span class="keyword">with</span> epoch_id:</span><br><span class="line">        - open(partitionId, epochId) 被调用</span><br><span class="line">        - 如果open（...）返回<span class="literal">true</span>，则对于partition和 batch/epoch中的每一行，将调用方法process(row)</span><br><span class="line">        - 调用方法close（错误），在处理行时看到错误（如果有的话话）。</span><br></pre></td></tr></table></figure><p>当失败导致某些输入数据的重新处理时，open（）方法中的partitionId和epochId可用于对生成的数据进行重复数据删除。 这取决于查询的执行模式。 如果以微批处理模式执行流式查询，则保证由唯一元组（partition_id，epoch_id）表示的每个分区具有相同的数据。 因此，（partition_id，epoch_id）可用于对数据进行重复数据删除和/或事务提交，并实现一次性保证。 但是，如果正在以连续模式执行流式查询，则此保证不成立，因此不应用于重复数据删除。</p><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><p>流式查询的触发器设置定义了流式数据处理的时间，查询是作为具有固定批处理间隔的微批量查询还是作为连续处理查询来执行。 以下是支持的各种触发器。</p><table><thead><tr><th align="left">Trigger Type</th><th align="left">Description</th></tr></thead><tbody><tr><td align="left">未指定（默认）</td><td align="left">如果未明确指定触发设置，则默认情况下，查询将以微批处理模式执行，一旦前一个微批处理完成处理，将立即生成微批处理。</td></tr><tr><td align="left"><strong>Fixed interval micro-batches</strong></td><td align="left">查询将以微批处理模式执行，其中微批处理将以用户指定的间隔启动。<br/>如果先前的微批次在该间隔内完成，则引擎将等待该间隔结束，然后开始下一个微批次。<br/>如果前一个微批次需要的时间长于完成的间隔（即如果错过了间隔边界），则下一个微批次将在前一个完成后立即开始（即，它不会等待下一个间隔边界） ）。<br/>如果没有可用的新数据，则不会启动微批次。</td></tr><tr><td align="left">One-time micro-batch</td><td align="left">查询将执行<em>仅一个</em>微批处理所有可用数据，然后自行停止。 这在您希望定期启动集群，处理自上一个时间段以来可用的所有内容，然后关闭集群的方案中非常有用。 在某些情况下，这可能会显着节省成本。</td></tr><tr><td align="left"><strong>Continuous with fixed checkpoint interval</strong> <em>(实验)</em></td><td align="left">查询将以新的低延迟，连续处理模式执行。在下面的连续处理部分中阅读更多相关信息。 <a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing-experimental" target="_blank" rel="noopener">Continuous Processing section</a></td></tr></tbody></table><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.streaming.<span class="type">Trigger</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Default trigger (runs micro-batch as soon as it can)</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ProcessingTime trigger with two-seconds micro-batch interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">ProcessingTime</span>(<span class="string">"2 seconds"</span>))</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// One-time trigger</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Once</span>())</span><br><span class="line">  .start()</span><br><span class="line"></span><br><span class="line"><span class="comment">// Continuous trigger with one-second checkpointing interval</span></span><br><span class="line">df.writeStream</span><br><span class="line">  .format(<span class="string">"console"</span>)</span><br><span class="line">  .trigger(<span class="type">Trigger</span>.<span class="type">Continuous</span>(<span class="string">"1 second"</span>))</span><br><span class="line">  .start()</span><br></pre></td></tr></table></figure><h2 id="管理流式查询"><a href="#管理流式查询" class="headerlink" title="管理流式查询"></a>管理流式查询</h2><p>启动查询时创建的StreamingQuery对象可用于监视和管理查询。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> query = df.writeStream.format(<span class="string">"console"</span>).start()   <span class="comment">// get the query object</span></span><br><span class="line"></span><br><span class="line">query.id          <span class="comment">// get the unique identifier of the running query that persists across restarts from checkpoint data</span></span><br><span class="line"></span><br><span class="line">query.runId       <span class="comment">// get the unique id of this run of the query, which will be generated at every start/restart</span></span><br><span class="line"></span><br><span class="line">query.name        <span class="comment">// get the name of the auto-generated or user-specified name</span></span><br><span class="line"></span><br><span class="line">query.explain()   <span class="comment">// print detailed explanations of the query</span></span><br><span class="line"></span><br><span class="line">query.stop()      <span class="comment">// stop the query</span></span><br><span class="line"></span><br><span class="line">query.awaitTermination()   <span class="comment">// block until query is terminated, with stop() or with error</span></span><br><span class="line"></span><br><span class="line">query.exception       <span class="comment">// the exception if the query has been terminated with error</span></span><br><span class="line"></span><br><span class="line">query.recentProgress  <span class="comment">// an array of the most recent progress updates for this query</span></span><br><span class="line"></span><br><span class="line">query.lastProgress    <span class="comment">// the most recent progress update of this streaming query</span></span><br></pre></td></tr></table></figure><p>您可以在单个SparkSession中启动任意数量的查询。 它们将同时运行，共享群集资源,您可以使用<code>sparkSession.streams()</code>来获取可用于管理当前活动的查询的StreamingQueryManager</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = ...</span><br><span class="line"></span><br><span class="line">spark.streams.active    <span class="comment">// get the list of currently active streaming queries</span></span><br><span class="line"></span><br><span class="line">spark.streams.get(id)   <span class="comment">// get a query object by its unique id</span></span><br><span class="line"></span><br><span class="line">spark.streams.awaitAnyTermination()   <span class="comment">// block until any one of them terminates</span></span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190608115959.png" alt=""></p><p>更多请参考Spark官方网站</p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>Spark官方网站</p>]]></content>
    
    <summary type="html">
    
      Structured Streaming相关学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="Structured Streaming" scheme="https://www.hphblog.cn/tags/Structured-Streaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkStreaming的DStream操作</title>
    <link href="https://www.hphblog.cn/2019/06/06/Spark%E4%B9%8BSparkStreaming%E7%9A%84DStream%E6%93%8D%E4%BD%9C/"/>
    <id>https://www.hphblog.cn/2019/06/06/Spark%E4%B9%8BSparkStreaming%E7%9A%84DStream%E6%93%8D%E4%BD%9C/</id>
    <published>2019-06-06T02:58:21.000Z</published>
    <updated>2020-01-12T13:08:23.425Z</updated>
    
    <content type="html"><![CDATA[ DStream的转换操作和输出、累加器等：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="转换"><a href="#转换" class="headerlink" title="转换"></a>转换</h2><p>DStream上的原语分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。</p><table><thead><tr><th><strong>Transformation</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td>map(func)</td><td>将源DStream中的每个元素通过一个函数func从而得到新的DStreams。</td></tr><tr><td>flatMap(func)</td><td>和map类似，但是每个输入的项可以被映射为0或更多项。</td></tr><tr><td>filter(func)</td><td>选择源DStream中函数func判为true的记录作为新DStreams</td></tr><tr><td>repartition(numPartitions)</td><td>通过创建更多或者更少的partition来改变此DStream的并行级别。</td></tr><tr><td>union(otherStream)</td><td>联合源DStreams和其他DStreams来得到新DStream</td></tr><tr><td>count()</td><td>统计源DStreams中每个RDD所含元素的个数得到单元素RDD的新DStreams。</td></tr><tr><td>reduce(func)</td><td>通过函数func(两个参数一个输出)来整合源DStreams中每个RDD元素得到单元素RDD的DStreams。这个函数需要关联从而可以被并行计算。</td></tr><tr><td>countByValue()</td><td>对于DStreams中元素类型为K调用此函数，得到包含(K,Long)对的新DStream，其中Long值表明相应的K在源DStream中每个RDD出现的频率。</td></tr><tr><td>reduceByKey(func, [numTasks])</td><td>对(K,V)对的DStream调用此函数，返回同样（K,V)对的新DStream，但是新DStream中的对应V为使用reduce函数整合而来。<em>Note</em>：默认情况下，这个操作使用Spark默认数量的并行任务（本地模式为2，集群模式中的数量取决于配置参数spark.default.parallelism）。你也可以传入可选的参数numTaska来设置不同数量的任务。</td></tr><tr><td>join(otherStream, [numTasks])</td><td>两DStream分别为(K,V)和(K,W)对，返回(K,(V,W))对的新DStream。</td></tr><tr><td>cogroup(otherStream, [numTasks])</td><td>两DStream分别为(K,V)和(K,W)对，返回(K,(Seq[V],Seq[W])对新DStreams</td></tr><tr><td>transform(func)</td><td>将RDD到RDD映射的函数func作用于源DStream中每个RDD上得到新DStream。这个可用于在DStream的RDD上做任意操作。</td></tr><tr><td>updateStateByKey(func)</td><td>得到”状态”DStream，其中每个key状态的更新是通过将给定函数用于此key的上一个状态和新值而得到。这个可用于保存每个key值的任意状态数据。</td></tr></tbody></table><p>DStream 的转化操作可以分为无状态(stateless)和有状态(stateful)两种。 </p><p>在无状态转化操作中，每个批次的处理不依赖于之前批次的数据。常见的 RDD 转化操作，例如 map()、filter()、reduceByKey() 等，都是无状态转化操作。 </p><p>相对地，有状态转化操作需要使用之前批次的数据或者是中间结果来计算当前批次的数据。有状态转化操作包括基于滑动窗口的转化操作和追踪状态变化的转化操作。</p><h3 id="无状态转化"><a href="#无状态转化" class="headerlink" title="无状态转化"></a>无状态转化</h3><p>无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加import StreamingContext._ 才能在Scala中使用。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190606111311.png" alt=""></p><p>尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD(批次)组成，且无状态转化操作是分别应用到每个 RDD 上的。例如， reduceByKey() 会归约每个时间区间中的数据，但不会归约不同区间之间的数据。 </p><p>举个例子，在之前的wordcount程序中，我们只会统计1秒内接收到的数据的单词个数，而不会累加。 </p><p>无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。例如，键 值对 DStream 拥有和 RDD 一样的与连接相关的转化操作，也就是 cogroup()、join()、 leftOuterJoin() 等。我们可以在 DStream 上使用这些操作，这样就对每个批次分别执行了对应的 RDD 操作。</p><p>我们还可以像在常规的 Spark 中一样使用 DStream 的 union() 操作将它和另一个 DStream 的内容合并起来，也可以使用 StreamingContext.union() 来合并多个流。 </p><h3 id="有状态转化"><a href="#有状态转化" class="headerlink" title="有状态转化"></a>有状态转化</h3><p>UpdateStateByKey原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件 更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。 </p><p>updateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。</p><p>updateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateStateByKey</span></span>[<span class="type">S</span>: <span class="type">ClassTag</span>](   </span><br><span class="line">    updateFunc: (<span class="type">Seq</span>[<span class="type">V</span>], <span class="type">Option</span>[<span class="type">S</span>]) =&gt; <span class="type">Option</span>[<span class="type">S</span>]</span><br><span class="line">  ): <span class="type">DStream</span>[(<span class="type">K</span>, <span class="type">S</span>)] = ssc.withScope &#123;</span><br><span class="line">  updateStateByKey(updateFunc, defaultPartitioner())</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>状态存储在CheckPoint中，类似于一个HashMap，key就是KV结构的Key，updateFunc中的seq[V]是Rdd中所有Value的集合，第一个Option[s]是上一次的状态，第二个Option[S]是新产生的状态。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/6.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StateFulWordCount</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Stateful WordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  ssc.sparkContext.setCheckpointDir(<span class="string">"./checkpoint"</span>)</span><br><span class="line">  <span class="keyword">val</span> line = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line">  <span class="keyword">val</span> words = line.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> state = word2Count.updateStateByKey[<span class="type">Int</span>] &#123; (values: <span class="type">Seq</span>[<span class="type">Int</span>], state: <span class="type">Option</span>[<span class="type">Int</span>]) =&gt;</span><br><span class="line">    state <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt; <span class="type">Some</span>(values.sum)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(pre) =&gt; <span class="type">Some</span>(values.sum + pre)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  state.print()</span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/StateFUlSparkStreaming.gif" alt=""></p><h2 id="Window-Operations"><a href="#Window-Operations" class="headerlink" title="Window Operations"></a>Window Operations</h2><p>Window Operations 以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。</p><p>基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。 </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190606195914.png" alt=""></p><p>所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍。窗口时长控制每次计算最近的多少个批次的数据，其实就是最近的 windowDuration/batchInterval 个批次。如果有一个以 10 秒为批次间隔的源 DStream，要创建一个最近 30 秒的时间窗口(即最近 3 个批次)，就应当把 windowDuration 设为 30 秒。而滑动步长的默认值与批次间隔相等，用来控制对新的 DStream 进行计算的间隔。如果源 DStream 批次间隔为 10 秒，并且我们只希望每两个批次计算一次窗口结果， 就应该把滑动步长设置为 20 秒。 </p><p>假设，你想拓展前例从而每隔十秒对持续30秒的数据生成word count。为做到这个，我们需要在持续30秒数据的(word,1)对DStream上应用reduceByKey。使用操作reduceByKeyAndWindow.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/6.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PairDstreamFunctions</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"stateful"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  ssc.sparkContext.setCheckpointDir(<span class="string">"./checkpoint"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">val</span> state = word2Count.reduceByKeyAndWindow((a: <span class="type">Int</span>, b: <span class="type">Int</span>) =&gt; a + b, <span class="type">Seconds</span>(<span class="number">15</span>), <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  state.print()</span><br><span class="line"></span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/SparkStreamingWindow.gif" alt=""></p><h2 id="DStreams输出"><a href="#DStreams输出" class="headerlink" title="DStreams输出"></a>DStreams输出</h2><p>输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。 </p><table><thead><tr><th><strong>Output Operation</strong></th><th><strong>Meaning</strong></th></tr></thead><tbody><tr><td><strong>print</strong>()</td><td>在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫pprint()。</td></tr><tr><td><strong>saveAsTextFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td>以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”.</td></tr><tr><td><strong>saveAsObjectFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td>以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”. Python中目前不可用。</td></tr><tr><td><strong>saveAsHadoopFiles</strong>(<em>prefix</em>, [<em>suffix</em>])</td><td>将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为”prefix-TIME_IN_MS[.suffix]”.     Python API Python中目前不可用。</td></tr><tr><td><strong>foreachRDD</strong>(<em>func</em>)</td><td>这是最通用的输出操作，即将函数func用于产生于stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。注意：函数func在运行流应用的驱动中被执行，同时其中一般函数RDD操作从而强制其对于流RDD的运算。</td></tr></tbody></table><p>通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算。这和transform() 有些类似，都可以让我们访问任意 RDD。在 foreachRDD() 中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。 </p><p>需要注意的：</p><p>连接不能写在driver层面</p><p>如果写在foreach则每个RDD都创建，得不偿失</p><p>增加foreachPartition，在分区创建</p><p>可以考虑使用连接池优化</p><p>我们写一个Mysql连接池</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.<span class="type">Connection</span>;</span><br><span class="line"><span class="keyword">import</span> java.sql.<span class="type">DriverManager</span>;</span><br><span class="line"><span class="keyword">import</span> java.util.<span class="type">LinkedList</span>;</span><br><span class="line"></span><br><span class="line">public <span class="class"><span class="keyword">class</span> <span class="title">ConnectionPool</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> static <span class="type">LinkedList</span>&lt;<span class="type">Connection</span>&gt; connectionQueue;</span><br><span class="line"></span><br><span class="line">    static &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="type">Class</span>.forName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (<span class="type">ClassNotFoundException</span> e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public synchronized static <span class="type">Connection</span> getConnection() &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (connectionQueue == <span class="literal">null</span>) &#123;</span><br><span class="line">                connectionQueue = <span class="keyword">new</span> <span class="type">LinkedList</span>&lt;<span class="type">Connection</span>&gt;();</span><br><span class="line">                <span class="keyword">for</span> (int i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">                    <span class="type">Connection</span> conn = <span class="type">DriverManager</span>.getConnection(</span><br><span class="line">                            <span class="string">"jdbc:mysql://192.168.1.101:3306/sparkstreaming"</span>,</span><br><span class="line">                            <span class="string">"root"</span>,</span><br><span class="line">                            <span class="string">"123456"</span>);</span><br><span class="line">                    connectionQueue.push(conn);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (<span class="type">Exception</span> e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> connectionQueue.poll();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void returnConnection(<span class="type">Connection</span> conn) &#123;</span><br><span class="line">        connectionQueue.push(conn);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/6.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DStream2Mysql</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"stateful"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line">  word2Count.print()</span><br><span class="line">  word2Count.foreachRDD &#123; rdd =&gt;</span><br><span class="line">    rdd.foreachPartition &#123; partitionOfRecords =&gt; &#123;</span><br><span class="line">      <span class="comment">// ConnectionPool is a static, lazily initialized pool of connections</span></span><br><span class="line">      <span class="keyword">val</span> connection = <span class="type">ConnectionPool</span>.getConnection()</span><br><span class="line">      partitionOfRecords.foreach(record =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> sql = <span class="string">"insert into streaming_wordCount(item,count) values('"</span> + record._1 + <span class="string">"',"</span> + record._2 + <span class="string">")"</span></span><br><span class="line">        <span class="keyword">val</span> stmt = connection.createStatement();</span><br><span class="line">        stmt.executeUpdate(sql);</span><br><span class="line">      &#125;)</span><br><span class="line">      <span class="type">ConnectionPool</span>.returnConnection(connection) <span class="comment">// return to the pool for future reuse</span></span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/SparkStreaming2ysql.gif" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190606234958.png" alt=""></p><h2 id="累加器广播变量"><a href="#累加器广播变量" class="headerlink" title="累加器广播变量"></a>累加器广播变量</h2><p>累加器(Accumulators)和广播变量(Broadcast variables)不能从Spark Streaming的检查点中恢复。如果你启用检查并也使用了累加器和广播变量，那么你必须创建累加器和广播变量的延迟单实例从而在驱动因失效重启后他们可以被重新实例化。如下例述：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkContext</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.broadcast.<span class="type">Broadcast</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.util.<span class="type">LongAccumulator</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordBlacklist</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">Broadcast</span>[<span class="type">Seq</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">      synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">          <span class="keyword">val</span> wordBlacklist = <span class="type">Seq</span>(<span class="string">"flink"</span>, <span class="string">"spark"</span>, <span class="string">"hadoop"</span>)</span><br><span class="line">          instance = sc.broadcast(wordBlacklist)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    instance</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DroppedWordsCounter</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> instance: <span class="type">LongAccumulator</span> = <span class="literal">null</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">getInstance</span></span>(sc: <span class="type">SparkContext</span>): <span class="type">LongAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">      synchronized &#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="literal">null</span>) &#123;</span><br><span class="line">          instance = sc.longAccumulator(<span class="string">"WordsInBlacklistCounter"</span>)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    instance</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>, <span class="type">Time</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/7.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Accumulators</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"Streaming_AccumulatorAndBroadcast"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">    <span class="keyword">val</span> wordCounts = words.map((_, <span class="number">1</span>)).reduceByKey(_ + _) <span class="comment">//转换为二元组进行累加操作</span></span><br><span class="line"></span><br><span class="line">    wordCounts.foreachRDD &#123; (rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)], time: <span class="type">Time</span>) =&gt;</span><br><span class="line">      <span class="comment">// Get or register the blacklist Broadcast</span></span><br><span class="line">      <span class="keyword">val</span> blacklist = <span class="type">WordBlacklist</span>.getInstance(rdd.sparkContext)</span><br><span class="line">      <span class="comment">// Get or register the droppedWordsCounter Accumulator</span></span><br><span class="line">      <span class="keyword">val</span> droppedWordsCounter = <span class="type">DroppedWordsCounter</span>.getInstance(rdd.sparkContext)</span><br><span class="line">      <span class="comment">// Use blacklist to drop words and use droppedWordsCounter to count them</span></span><br><span class="line">      <span class="keyword">val</span> counts = rdd.filter &#123; <span class="keyword">case</span> (word, count) =&gt;</span><br><span class="line">        <span class="keyword">if</span> (blacklist.value.contains(word)) &#123;</span><br><span class="line">          droppedWordsCounter.add(count)</span><br><span class="line">          <span class="literal">false</span></span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;.collect().mkString(<span class="string">"["</span>, <span class="string">", "</span>, <span class="string">"]"</span>)</span><br><span class="line">      <span class="keyword">val</span> output = <span class="string">"Counts at time "</span> + time + <span class="string">" "</span> + counts</span><br><span class="line">      println(output)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    wordCounts.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/SparkStreamingAccumlators.gif" alt=""></p><h2 id="DataFrame-ans-SQL-Operations"><a href="#DataFrame-ans-SQL-Operations" class="headerlink" title="DataFrame ans SQL Operations"></a>DataFrame ans SQL Operations</h2><p>你可以很容易地在流数据上使用DataFrames和SQL。你必须使用SparkContext来创建StreamingContext要用的SQLContext。此外，这一过程可以在驱动失效后重启。我们通过创建一个实例化的SQLContext单实例来实现这个工作。如下例所示。我们对前例word count进行修改从而使用DataFrames和SQL来产生word counts。每个RDD被转换为DataFrame，以临时表格配置并用SQL进行查询</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Minutes</span>, <span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/7.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingSQL</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreaming SQL"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> linesDStream = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">    linesDStream.foreachRDD &#123; rdd =&gt;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Get the singleton instance of SparkSession</span></span><br><span class="line">      <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder.config(rdd.sparkContext.getConf).getOrCreate()</span><br><span class="line">      <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Convert RDD[String] to DataFrame</span></span><br><span class="line">      <span class="keyword">val</span> wordsDataFrame = rdd.toDF(<span class="string">"word"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Create a temporary view</span></span><br><span class="line">      wordsDataFrame.createOrReplaceTempView(<span class="string">"words"</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// Do word count on DataFrame using SQL and print it</span></span><br><span class="line">      <span class="keyword">val</span> wordCountsDataFrame =</span><br><span class="line">        spark.sql(<span class="string">"select word, count(*) as total from words group by word"</span>)</span><br><span class="line">      wordCountsDataFrame.show()</span><br><span class="line">    &#125;</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/SparkStreamingSQL.gif" alt=""></p><h2 id="Caching-Persistence"><a href="#Caching-Persistence" class="headerlink" title="Caching / Persistence"></a>Caching / Persistence</h2><p>DStreams允许开发者将流数据保存在内存中。也就是说，在DStream上使用persist()方法将会自动把DStreams中的每个RDD保存在内存中。当DStream中的数据要被多次计算时，这个非常有用（如在同样数据上的多次操作）。对于像reduceByWindow和reduceByKeyAndWindow以及基于状态的(updateStateByKey)这种操作，保存是隐含默认的。因此，即使开发者没有调用persist()，由基于窗操作产生的DStreams会自动保存在内存中。 </p>]]></content>
    
    <summary type="html">
    
      DStream的转换操作和输出、累加器等：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://www.hphblog.cn/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkStreaming数据源</title>
    <link href="https://www.hphblog.cn/2019/06/05/Spark%E4%B9%8BSparkStreaming%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>https://www.hphblog.cn/2019/06/05/Spark%E4%B9%8BSparkStreaming%E6%95%B0%E6%8D%AE%E6%BA%90/</id>
    <published>2019-06-05T14:52:19.000Z</published>
    <updated>2020-01-12T13:08:22.081Z</updated>
    
    <content type="html"><![CDATA[ SparkStreaming的数据源 文件 Flume Kafka：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="DStreams输入"><a href="#DStreams输入" class="headerlink" title="DStreams输入"></a>DStreams输入</h2><p>Spark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，而其他的一些则可以通过 <code>spark-streaming-kafka</code> 等附加工件获取。每个接收器都以 Spark 执行器程序中一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。此外，我们还需要有可用的 CPU 核心来处理数据。这意味着如果要运行多个接收器，就必须至少有和接收器数目相同的核心数，还要加上用来完成计算所需要的核心数。例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，不要使用local或者local[1]。</p><h3 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h3><p>文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取。</p><p>Spark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件，目前不支持嵌套目录。</p><p>文件需要有相同的数据格式。</p><p>文件进入 dataDirectory的方式需要通过移动或者重命名来实现。</p><p>一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据。</p><p>如果文件比较简单，则可以使用 streamingContext.textFileStream(dataDirectory)方法来读取文件。文件流不需要接收器，不需要单独分配CPU核。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingHDFS</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"streaming data from HDFS"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="comment">//创建StreamingContext</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    <span class="comment">//从HDFS接口数据</span></span><br><span class="line">    <span class="keyword">val</span> lines = ssc.textFileStream(<span class="string">"hdfs://datanode1:9000/input/streaming/"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordCounts = words.map((_, <span class="number">1</span>)).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    wordCounts.print()</span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/HDFSStreaming.gif" alt=""></p><h3 id="自定义配置"><a href="#自定义配置" class="headerlink" title="自定义配置"></a>自定义配置</h3><p>通过继承Receiver，并实现onStart、onStop方法来自定义数据源采集。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.&#123;<span class="type">BufferedReader</span>, <span class="type">InputStreamReader</span>&#125;</span><br><span class="line"><span class="keyword">import</span> java.net.<span class="type">Socket</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.receiver.<span class="type">Receiver</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/3.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerRecevicer</span>(<span class="params">host: <span class="type">String</span>, port: <span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Receiver</span>[<span class="type">String</span>](<span class="params"><span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span></span>) </span>&#123;</span><br><span class="line">  <span class="comment">//接收器启动的时候子自动调用</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建线程</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">Thread</span>(<span class="string">"receiver"</span>) &#123;</span><br><span class="line">      <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">        <span class="comment">//接受数据并提交给框架</span></span><br><span class="line">        receive()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;.start()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">var</span> socket: <span class="type">Socket</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">var</span> input: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      socket = <span class="keyword">new</span> <span class="type">Socket</span>(host, port)</span><br><span class="line">      <span class="comment">//生成输入流</span></span><br><span class="line">      <span class="keyword">val</span> reader = <span class="keyword">new</span> <span class="type">BufferedReader</span>(<span class="keyword">new</span> <span class="type">InputStreamReader</span>(socket.getInputStream))</span><br><span class="line"></span><br><span class="line">      <span class="comment">//接收数据</span></span><br><span class="line">      <span class="comment">//            input = reader.readLine()</span></span><br><span class="line">      <span class="keyword">while</span> (!isStopped() &amp;&amp; (input = reader.readLine()) != <span class="literal">null</span>) &#123;</span><br><span class="line">        store(input)</span><br><span class="line">      &#125;</span><br><span class="line">      restart(<span class="string">"restart"</span>)</span><br><span class="line"></span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> e: java.net.<span class="type">ConnectException</span> =&gt; restart(<span class="string">"restart"</span>)</span><br><span class="line">      <span class="keyword">case</span> t:<span class="type">Throwable</span> =&gt; restart(<span class="string">"restart"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//接收器关闭的时候调用</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">CustomerStreamingWordCount</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">//创建配置</span></span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"streaming word count"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">  <span class="comment">//创建StreamingContext</span></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  <span class="comment">//从socket接口数据   </span></span><br><span class="line">  <span class="keyword">val</span> lineDStream = ssc.receiverStream(<span class="keyword">new</span> <span class="type">CustomerRecevicer</span>(<span class="string">"datanode1"</span>, <span class="number">9999</span>))  <span class="comment">//自定义的使用的是receiverStream</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordDStream = lineDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> word2CountDStream = wordDStream.map((_, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">val</span> result = word2CountDStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  result.print()</span><br><span class="line">  <span class="comment">//启动</span></span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/CustomersparkStreamingWordCount.gif" alt=""></p><h3 id="RDD队列"><a href="#RDD队列" class="headerlink" title="RDD队列"></a>RDD队列</h3><p>Spark Streaming也可以使用 streamingContext.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.mutable</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">QueueRdd</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"QueueRdd"</span>)</span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建RDD队列</span></span><br><span class="line">    <span class="keyword">val</span> rddQueue = <span class="keyword">new</span> mutable.<span class="type">SynchronizedQueue</span>[<span class="type">RDD</span>[<span class="type">Int</span>]]()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建QueueInputDStream</span></span><br><span class="line">    <span class="keyword">val</span> inputStream = ssc.queueStream(rddQueue)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//处理队列中的RDD数据</span></span><br><span class="line">    <span class="keyword">val</span> mappedStream = inputStream.map(x =&gt; (x % <span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> reducedStream = mappedStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//打印结果</span></span><br><span class="line">    reducedStream.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//启动计算</span></span><br><span class="line">    ssc.start()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create and push some RDDs into</span></span><br><span class="line">    <span class="keyword">for</span> (i &lt;- <span class="number">1</span> to <span class="number">30</span>) &#123;</span><br><span class="line">      rddQueue += ssc.sparkContext.makeRDD(<span class="number">1</span> to <span class="number">300</span>, <span class="number">10</span>)</span><br><span class="line">      <span class="type">Thread</span>.sleep(<span class="number">5000</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">//通过程序停止StreamingContext的运行</span></span><br><span class="line">      <span class="comment">//ssc.stop()</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/QueueRdd.gif" alt=""></p><h3 id="高级数据源-Kafka等"><a href="#高级数据源-Kafka等" class="headerlink" title="高级数据源(Kafka等)"></a>高级数据源(Kafka等)</h3><p>这一类的来源需要外部接口，其中一些有复杂的依赖关系（如Kafka和Flume),因此通过这些来源创建DStreams需要明确其依赖。在工程中需要引入 Maven 工件 spark- streaming-kafka_2.10 来使用它。包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream。由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。要创建出一个流数据，需 要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字(唯一名字)，以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190604111236.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka.<span class="type">KafkaUtils</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/4.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">KafkaStreming</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//配置</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"KafkaStreaming"</span>) setMaster (<span class="string">"local[*]"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment">//kafka的参数</span></span><br><span class="line">    <span class="keyword">val</span> brokers = <span class="string">"datanode1:9092,datanode2:9092,datanode3:9092"</span></span><br><span class="line">    <span class="keyword">val</span> zookeeper = <span class="string">"datanode1:2181,datanode2:2181,datanode3:2181"</span></span><br><span class="line">    <span class="keyword">val</span> sourceTopic = <span class="string">"source"</span></span><br><span class="line">    <span class="keyword">val</span> targetTopic = <span class="string">"target"</span></span><br><span class="line">    <span class="keyword">val</span> consumerGroup = <span class="string">"consumer"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//封装kafka参数</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] &#123;</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; brokers</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">KEY_DESERIALIZER_CLASS_CONFIG</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">VALUE_DESERIALIZER_CLASS_CONFIG</span> -&gt; <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; consumerGroup</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> kafkaDStream = <span class="type">KafkaUtils</span>.x(ssc, kafkaParams, <span class="type">Set</span>(sourceTopic))</span><br><span class="line">    kafkaDStream.print()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nohup /opt/module/kafka/bin/kafka-server-start.sh /opt/module/kafka/config/server.properties &amp; #启动kafka</span><br><span class="line">[hadoop@datanode1 bin]$ nohup ./kafka-manager  -java-home /opt/module/jdk1.8.0_162/  -Dconfig.file=../conf/application.conf &gt;/dev/null 2&gt;&amp;1 &amp;  #启动kafkamanager</span><br><span class="line"> /opt/module/kafka/bin/kafka-topics.sh --zookeeper datanode1:2181 --create --replication-factor 2 --partitions 2 --topic source #创建一个topic</span><br><span class="line">  /opt/module/kafka/bin/kafka-console-producer.sh --broker-list datanode1:9092 --topic source #启动生产者</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/KafkaSpark.gif" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.<span class="type">ConsumerConfig</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.<span class="type">StringDeserializer</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">TaskContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">ConsumerStrategies</span>.<span class="type">Subscribe</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.<span class="type">LocationStrategies</span>.<span class="type">PreferConsistent</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.kafka010.&#123;<span class="type">HasOffsetRanges</span>, <span class="type">KafkaUtils</span>, <span class="type">OffsetRange</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/5.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingWithKafka</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> brokeList = <span class="string">"datanode1:9092,datanode2:9092,datanode2:9092"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> topic = <span class="string">"topic-spark"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> group = <span class="string">"group-spark"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> checkpointDir = <span class="string">"/opt/kafka/checkpoint"</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"StreamingWithKafka"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">    ssc.checkpoint(checkpointDir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建kafka的连接对象</span></span><br><span class="line">    <span class="keyword">val</span> kafkaParams = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Object</span>](</span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">BOOTSTRAP_SERVERS_CONFIG</span> -&gt; brokeList, <span class="comment">//Kafka集群</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">KEY_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//序列化</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">VALUE_DESERIALIZER_CLASS_CONFIG</span> -&gt; classOf[<span class="type">StringDeserializer</span>], <span class="comment">//序列化</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">GROUP_ID_CONFIG</span> -&gt; group, <span class="comment">//消费者组</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">AUTO_OFFSET_RESET_CONFIG</span> -&gt; <span class="string">"latest"</span>, <span class="comment">//latest自动重置偏移量为最新的偏移量</span></span><br><span class="line">      <span class="type">ConsumerConfig</span>.<span class="type">ENABLE_AUTO_COMMIT_CONFIG</span> -&gt; (<span class="literal">false</span>: java.lang.<span class="type">Boolean</span>) <span class="comment">//是否自动提交</span></span><br><span class="line">    )</span><br><span class="line">    <span class="comment">//创建DStream,发挥接受的消息</span></span><br><span class="line">    <span class="keyword">val</span> stream = <span class="type">KafkaUtils</span>.createDirectStream[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">      ssc,</span><br><span class="line">      <span class="type">PreferConsistent</span>,</span><br><span class="line">      <span class="type">Subscribe</span>[<span class="type">String</span>, <span class="type">String</span>](<span class="type">List</span>(topic), kafkaParams))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> value = stream.map(record =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> intVal = <span class="type">Integer</span>.valueOf(record.value())</span><br><span class="line">      println(intVal)        <span class="comment">// 打印输入数字</span></span><br><span class="line">      intVal</span><br><span class="line">    &#125;).reduce(_ + _)   <span class="comment">//相加</span></span><br><span class="line">    value.print()   <span class="comment">//输出</span></span><br><span class="line"></span><br><span class="line">    stream.foreachRDD(rdd =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> offsetRanges = rdd.asInstanceOf[<span class="type">HasOffsetRanges</span>].offsetRanges</span><br><span class="line">      rdd.foreachPartition &#123; iter =&gt;</span><br><span class="line">        <span class="keyword">val</span> o: <span class="type">OffsetRange</span> = offsetRanges(<span class="type">TaskContext</span>.getPartitionId())</span><br><span class="line">        print(<span class="string">s"<span class="subst">$&#123;o.topic&#125;</span> <span class="subst">$&#123;o.partition&#125;</span> <span class="subst">$&#123;o.fromOffset&#125;</span> <span class="subst">$&#123;o.untilOffset&#125;</span>"</span>)</span><br><span class="line"></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    ssc.start</span><br><span class="line">    ssc.awaitTermination</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --create --replication-factor 3 --partitions 1 --topic-spark</span><br><span class="line">bin/kafka-console-producer.sh --broker-list datanode1:9092 --topic topic-spark</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/KafkaSparkStreaming.gif" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190605200456.png" alt=""></p><p>因为在本地运行E盘放了我们程序相当于E盘就是根目录了,可以指定HDFS.</p><h4 id="两种连接方式"><a href="#两种连接方式" class="headerlink" title="两种连接方式"></a>两种连接方式</h4><p>Spark对于Kafka的连接主要有两种方式，一种是DirectKafkaInputDStream，另外一种是KafkaInputDStream。DirectKafkaInputDStream 只在 driver 端接收数据，所以继承了 InputDStream，是没有 receivers 的。</p><p>主要通过KafkaUtils.createDirectStream以及KafkaUtils.createStream这两个 API 来创建，除了要传入的参数不同外，接收 kafka 数据的节点、拉取数据的时机也完全不同。</p><h5 id="createStream-Receiver-based"><a href="#createStream-Receiver-based" class="headerlink" title="createStream[Receiver-based]"></a>createStream[Receiver-based]</h5><p>这种方法使用一个 Receiver 来接收数据。在该 Receiver 的实现中使用了 Kafka high-level consumer API。Receiver 从 kafka 接收的数据将被存储到 Spark executor 中，随后启动的 job 将处理这些数据。</p><p>在默认配置下，该方法失败后会丢失数据（保存在 executor 内存里的数据在 application 失败后就没了），若要保证数据不丢失，需要启用 WAL（即预写日志至 HDFS、S3等），这样再失败后可以从日志文件中恢复数据。</p><p>在该函数中，会新建一个 KafkaInputDStream对象，KafkaInputDStream继承于 ReceiverInputDStream。KafkaInputDStream实现了getReceiver方法，返回接收器的实例：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getReceiver</span></span>(): <span class="type">Receiver</span>[(<span class="type">K</span>, <span class="type">V</span>)] = &#123;</span><br><span class="line">  <span class="keyword">if</span> (!useReliableReceiver) &#123;</span><br><span class="line">    <span class="comment">//&lt; 不启用 WAL</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">KafkaReceiver</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">U</span>, <span class="type">T</span>](kafkaParams, topics, storageLevel)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">//&lt; 启用 WAL</span></span><br><span class="line">    <span class="keyword">new</span> <span class="type">ReliableKafkaReceiver</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">U</span>, <span class="type">T</span>](kafkaParams, topics, storageLevel)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>根据是否启用 WAL，receiver 分为KafkaReceiver 和 ReliableKafkaReceiver。下图描述了 KafkaReceiver 接收数据的具体流程：</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190605201219.png" alt=""></p><p>Kafka Topic 的 partitions 与RDD 的 partitions 没有直接关系，不能一一对应。如果增加 topic 的 partition 个数的话仅仅会增加单个 Receiver 接收数据的线程数。事实上，使用这种方法只会在一个 executor 上启用一个 Receiver，该 Receiver 包含一个线程池，线程池的线程个数与所有 topics 的 partitions 个数总和一致，每条线程接收一个 topic 的一个 partition 的数据。而并不会增加处理数据时的并行度。</p><p>对于一个 topic，可以使用多个 groupid 相同的 input DStream 来使用多个 Receivers 来增加并行度，然后 union 他们；对于多个 topics，除了可以用上个办法增加并行度外，还可以对不同的 topic 使用不同的 input DStream 然后 union 他们来增加并行度</p><p>如果你启用了 WAL，为能将接收到的数据将以 log 的方式在指定的存储系统备份一份，需要指定输入数据的存储等级为 StorageLevel.MEMORY_AND_DISK_SER 或 StorageLevel.MEMORY_AND_DISK_SER_2</p><h5 id="createDirectStream-WithOut-Receiver"><a href="#createDirectStream-WithOut-Receiver" class="headerlink" title="createDirectStream[WithOut Receiver]"></a>createDirectStream[WithOut Receiver]</h5><p>自 Spark-1.3.0 起，提供了不需要 Receiver 的方法。替代了使用 receivers 来接收数据，该方法定期查询每个 topic+partition 的 lastest offset，并据此决定每个 batch 要接收的 offsets 范围。</p><p>createDirectStream调用中，会新建DirectKafkaInputDStream，DirectKafkaInputDStream#compute(validTime: Time)会从 kafka 拉取数据并生成 RDD，流程如下：</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190605201632.png" alt=""></p><p>该函数主要做了以下三个事情：</p><p>确定要接收的 partitions 的 offsetRange，以作为第2步创建的 RDD 的数据来源</p><p>创建 RDD 并执行 count 操作，使 RDD 真实具有数据</p><p>以 streamId、数据条数，offsetRanges 信息初始化 inputInfo 并添加到 JobScheduler 中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">进一步看 <span class="type">KafkaRDD</span> 的 getPartitions 实现：</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartitions</span></span>: <span class="type">Array</span>[<span class="type">Partition</span>] = &#123;</span><br><span class="line"></span><br><span class="line">    offsetRanges.zipWithIndex.map &#123; <span class="keyword">case</span> (o, i) =&gt;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">val</span> (host, port) = leaders(<span class="type">TopicAndPartition</span>(o.topic, o.partition))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> <span class="type">KafkaRDDPartition</span>(i, o.topic, o.partition, o.fromOffset, o.untilOffset, host, port)</span><br><span class="line"></span><br><span class="line">    &#125;.toArray</span><br><span class="line"></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>从上面的代码可以很明显看到，KafkaRDD 的 partition 数据与 Kafka topic 的某个 partition 的 o.fromOffset 至 o.untilOffset 数据是相对应的，也就是说 KafkaRDD 的 partition 与 Kafka partition 是一一对应的</p><p>该方式相比使用 Receiver 的方式有以下好处：</p><p>简化并行：不再需要创建多个 kafka input DStream 然后再 union 这些 input DStream。使用 directStream，Spark Streaming会创建与 Kafka partitions 相同数量的 paritions 的 RDD，RDD 的 partition与 Kafka 的 partition 一一对应，这样更易于理解及调优</p><p>高效：在方式一中要保证数据零丢失需要启用 WAL（预写日志），这会占用更多空间。而在方式二中，可以直接从 Kafka 指定的 topic 的指定 offsets 处恢复数据，不需要使用 WAL</p><p>恰好一次语义保证：基于Receiver方式使用了 Kafka 的 high level API 来在 Zookeeper 中存储已消费的 offsets。这在某些情况下会导致一些数据被消费两次，比如 streaming app 在处理某个 batch  内已接受到的数据的过程中挂掉，但是数据已经处理了一部分，但这种情况下无法将已处理数据的 offsets 更新到 Zookeeper 中，下次重启时，这批数据将再次被消费且处理。基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。这种方式中，只要将 output 操作和保存 offsets 操作封装成一个原子操作就能避免失败后的重复消费和处理，从而达到恰好一次的语义（Exactly-once）</p><p>通过以上分析，我们可以对这两种方式的区别做一个总结：</p><p>createStream会使用 Receiver；而createDirectStream不会</p><p>createStream使用的 Receiver 会分发到某个 executor 上去启动并接受数据；而createDirectStream直接在 driver 上接收数据</p><p>createStream使用 Receiver 源源不断的接收数据并把数据交给 ReceiverSupervisor 处理最终存储为 blocks 作为 RDD 的输入，从 kafka 拉取数据与计算消费数据相互独立；而createDirectStream会在每个 batch 拉取数据并就地消费，到下个 batch 再次拉取消费，周而复始，从 kafka 拉取数据与计算消费数据是连续的，没有独立开</p><p>createStream中创建的KafkaInputDStream 每个 batch 所对应的 RDD 的 partition 不与 Kafka partition 一 一对应；而createDirectStream中创建的 DirectKafkaInputDStream 每个 batch 所对应的 RDD 的 partition 与 Kafka partition 一 一对应</p><h4 id="Flume"><a href="#Flume" class="headerlink" title="Flume"></a>Flume</h4><p>Spark提供两个不同的接收器来使用Apache Flum 两个接收器简介如下。 </p><p>推式接收器该接收器以 Avro 数据池的方式工作，由 Flume 向其中推数据。 </p><p>拉式接收器该接收器可以从自定义的中间数据池中拉数据，而其他进程可以使用 Flume 把数据推进 该中间数据池。 </p><p>两种方式都需要重新配置 Flume，并在某个节点配置的端口上运行接收器(不是已有的 Spark 或者 Flume 使用的端口)。要使用其中任何一种方法，都需要在工程中引入 Maven 工件 spark-streaming-flume_2.10。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190605202028.png" alt=""></p><p>Avro 数据池的方式工作，我们需要配置 Flume 来把数据发到 Avro 数据池。我们提供的 FlumeUtils 对象会把接收器配置在一个特定的工作节点的主机名及端口号上。这些设置必须和 Flume 配置相匹配。 </p><p>虽然这种方式很简洁，但缺点是没有事务支持。这会增加运行接收器的工作节点发生错误 时丢失少量数据的几率。不仅如此，如果运行接收器的工作节点发生故障，系统会尝试从 另一个位置启动接收器，这时需要重新配置 Flume 才能将数据发给新的工作节点。这样配 置会比较麻烦。 </p><p>较新的方式是拉式接收器(在Spark 1.1中引入)，它设置了一个专用的Flume数据池供 Spark Streaming读取，并让接收器主动从数据池中拉取数据。这种方式的优点在于弹性较好，Spark Streaming通过事务从数据池中读取并复制数据。在收到事务完成的通知前，这些数据还保留在数据池中。 </p><p>我们需要先把自定义数据池配置为 Flume 的第三方插件。安装插件的最新方法请参考 Flume 文档的相关部分([链接](<a href="https://flume.apache.org/FlumeUserGuide.html#installing-third-party-" target="_blank" rel="noopener">https://flume.apache.org/FlumeUserGuide.html#installing-third-party-</a> plugins))。由于插件是用 Scala 写的，因此需要把插件本身以及 Scala 库都添加到 Flume 插件 中。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming-flume-sink_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.11<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>当你把自定义 Flume 数据池添加到一个节点上之后，就需要配置 Flume 来把数据推送到这个数据池中， </p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">a1.sources</span> = <span class="string">r1</span></span><br><span class="line"><span class="meta">a1.sinks</span> = <span class="string">k1</span></span><br><span class="line"><span class="meta">a1.channels</span> = <span class="string">c1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># source</span></span><br><span class="line"><span class="meta">a1.sources.r1.type</span> = <span class="string">spooldir</span></span><br><span class="line"><span class="meta">a1.sources.r1.spoolDir</span> = <span class="string">/home/hadoop/flumedata</span></span><br><span class="line"><span class="meta">a1.sources.r1.fileHeader</span> = <span class="string">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">a1.sinks.k1.type</span> = <span class="string">org.apache.spark.streaming.flume.sink.SparkSink</span></span><br><span class="line"><span class="comment">#表示从这里拉数据</span></span><br><span class="line"><span class="meta">a1.sinks.k1.hostname</span> = <span class="string">192.168.1.101</span></span><br><span class="line"><span class="meta">a1.sinks.k1.port</span> = <span class="string">4444</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">a1.channels.c1.capacity</span> = <span class="string">1000</span></span><br><span class="line"><span class="meta">a1.channels.c1.transactionCapacity</span> = <span class="string">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">a1.sources.r1.channels</span> = <span class="string">c1</span></span><br><span class="line"><span class="meta">a1.sinks.k1.channel</span> = <span class="string">c1</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.storage.<span class="type">StorageLevel</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.dstream.&#123;<span class="type">DStream</span>, <span class="type">ReceiverInputDStream</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.flume.&#123;<span class="type">FlumeUtils</span>, <span class="type">SparkFlumeEvent</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/6/5.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">FlumeStream</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//spark配置</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"Flume Spark Streaming"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> inputDstream:<span class="type">ReceiverInputDStream</span>[<span class="type">SparkFlumeEvent</span>]</span><br><span class="line">    = <span class="type">FlumeUtils</span>.createPollingStream(ssc, <span class="string">"192.168.1.101"</span>, <span class="number">4444</span>, <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span>)</span><br><span class="line">    <span class="keyword">val</span> words = inputDstream.flatMap(x =&gt; <span class="keyword">new</span> <span class="type">String</span>(x.event.getBody.array()).split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> wordAndOne : <span class="type">DStream</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = words.map((_,<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> result : <span class="type">DStream</span>[(<span class="type">String</span>,<span class="type">Int</span>)] = wordAndOne.reduceByKey(_+_)</span><br><span class="line"></span><br><span class="line">    result.print()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 启动流</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/flume_Spark.gif" alt=""></p>]]></content>
    
    <summary type="html">
    
      SparkStreaming的数据源 文件 Flume Kafka：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://www.hphblog.cn/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkStreaming理论篇</title>
    <link href="https://www.hphblog.cn/2019/06/03/Spark%E4%B9%8BSparkStreaming%E7%90%86%E8%AE%BA%E7%AF%87/"/>
    <id>https://www.hphblog.cn/2019/06/03/Spark%E4%B9%8BSparkStreaming%E7%90%86%E8%AE%BA%E7%AF%87/</id>
    <published>2019-06-03T06:15:45.000Z</published>
    <updated>2020-01-12T13:08:22.973Z</updated>
    
    <content type="html"><![CDATA[ SparkStreaming的相关理论学习：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Spark Streaming用于流式数据的处理。Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。结果也能保存在很多地方，如HDFS，数据库等。Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603142207.png" alt=""></p><p>和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此 得名“离散化”)。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603142349.png" alt=""></p><p>DStream 可以从各种输入源创建，比如 Flume、Kafka 或者 HDFS。创建出来的DStream。支持两种操作，一种是转化操作(transformation)，会生成一个新的DStream，另一种是输出操作(output operation)，可以把数据写入外部系统中。DStream 提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。</p><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><h3 id="批处理比较"><a href="#批处理比较" class="headerlink" title="批处理比较"></a>批处理比较</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603142911.png" alt=""></p><h3 id="流处理比较"><a href="#流处理比较" class="headerlink" title="流处理比较"></a>流处理比较</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603143002.png" alt=""></p><p>后续会更新Flink的学习笔记。</p><h2 id="HelloWorld"><a href="#HelloWorld" class="headerlink" title="HelloWorld"></a>HelloWorld</h2><h3 id="pom"><a href="#pom" class="headerlink" title="pom"></a>pom</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>sparkstreaming<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>helloworld<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">SparkConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.streaming.&#123;<span class="type">Seconds</span>, <span class="type">StreamingContext</span>&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">StreamingWordCount</span> <span class="keyword">extends</span> <span class="title">App</span> </span>&#123;</span><br><span class="line">  <span class="comment">//创建配置</span></span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"streaming word count"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">  <span class="comment">//创建StreamingContext</span></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line">  <span class="comment">//从socket接口数据</span></span><br><span class="line">  <span class="keyword">val</span> lineDStream = ssc.socketTextStream(<span class="string">"datanode1"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> wordDStream = lineDStream.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">  <span class="keyword">val</span> word2CountDStream = wordDStream.map((_, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">val</span> result = word2CountDStream.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  result.print()</span><br><span class="line"></span><br><span class="line">  <span class="comment">//启动</span></span><br><span class="line">  ssc.start()</span><br><span class="line">  ssc.awaitTermination()</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/sparkStreamingWordCount.gif" alt=""></p><h3 id="模式"><a href="#模式" class="headerlink" title="模式"></a>模式</h3><p>Spark Streaming使用“微批次”的架构，把流式计算看作一系列连续的小规模批处理来对待。Spark Streaming从各种输入源中读取数据，并把数据分组为小的批次。新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中。在时间区间结束时，批次停止增长。时间区间的大小是由批次间隔这个参数决定的。批次间隔一般设在500毫秒到几秒之间，由应用开发者配置。每个输入批次都形成一个RDD，以 Spark 作业的方式处理并生成其他的 RDD。 处理的结果可以以批处理的方式传给外部系统。因此严格意义上来说Spark Streaming并不是一个真正的实时计算框架。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603160340.png" alt=""></p><p>Spark Streaming的编程抽象是离散化流，也就是DStream。它是一个 RDD 序列，每个RDD代表数据流中一个时间片内的数据。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603161713.png" alt=""></p><p>Spark Streaming 在 Spark 的驱动器程序 工作节点的结构的执行过程如下图所示。Spark Streaming 为每个输入源启动对应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为 RDD。它们收集到输入数据后会把数据复制到另一个执行器进程来保障容错性(默认行为)。数据保存在执行器进程的内存中，和缓存 RDD 的方式一样。驱动器程序中的 StreamingContext 会周期性地运行 Spark 作业来处理这些数据，把数据与之前时间区间中的 RDD 进行整合。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603164340.png" alt=""></p><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><ol><li><p>StreamingContext一旦启动，对DStreams的操作就不能修改了。</p></li><li><p>在同一时间一个JVM中只有一个StreamingContext可以启动</p></li><li><p>stop() 方法将同时停止SparkContext，可以传入参数stopSparkContext用于只停止StreamingContext</p></li><li><p>在Spark1.4版本后，如何优雅的停止SparkStreaming而不丢失数据，通过设置sparkConf.set(“spark.streaming.stopGracefullyOnShutdown”,”true”) 即可。在StreamingContext的start方法中已经注册了Hook方法。</p></li></ol><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603162825.png" alt=""></p><figure class="highlight scala"><figcaption><span>def start(): Unit </span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">start</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">   state <span class="keyword">match</span> &#123;</span><br><span class="line">     <span class="keyword">case</span> <span class="type">INITIALIZED</span> =&gt;</span><br><span class="line">       startSite.set(<span class="type">DStream</span>.getCreationSite())</span><br><span class="line">       <span class="type">StreamingContext</span>.<span class="type">ACTIVATION_LOCK</span>.synchronized &#123;</span><br><span class="line">         <span class="type">StreamingContext</span>.assertNoOtherContextIsActive()</span><br><span class="line">         <span class="keyword">try</span> &#123;</span><br><span class="line">           validate()</span><br><span class="line"></span><br><span class="line">           <span class="comment">// Start the streaming scheduler in a new thread, so that thread local properties</span></span><br><span class="line">           <span class="comment">// like call sites and job groups can be reset without affecting those of the</span></span><br><span class="line">           <span class="comment">// current thread.</span></span><br><span class="line">           <span class="type">ThreadUtils</span>.runInNewThread(<span class="string">"streaming-start"</span>) &#123;</span><br><span class="line">             sparkContext.setCallSite(startSite.get)</span><br><span class="line">             sparkContext.clearJobGroup()</span><br><span class="line">             sparkContext.setLocalProperty(<span class="type">SparkContext</span>.<span class="type">SPARK_JOB_INTERRUPT_ON_CANCEL</span>, <span class="string">"false"</span>)</span><br><span class="line">             savedProperties.set(<span class="type">SerializationUtils</span>.clone(sparkContext.localProperties.get()))</span><br><span class="line">             scheduler.start()</span><br><span class="line">           &#125;</span><br><span class="line">           state = <span class="type">StreamingContextState</span>.<span class="type">ACTIVE</span></span><br><span class="line">         &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">           <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">             logError(<span class="string">"Error starting the context, marking it as stopped"</span>, e)</span><br><span class="line">             scheduler.stop(<span class="literal">false</span>)</span><br><span class="line">             state = <span class="type">StreamingContextState</span>.<span class="type">STOPPED</span></span><br><span class="line">             <span class="keyword">throw</span> e</span><br><span class="line">         &#125;</span><br><span class="line">         <span class="type">StreamingContext</span>.setActiveContext(<span class="keyword">this</span>)</span><br><span class="line">       &#125;</span><br><span class="line">       logDebug(<span class="string">"Adding shutdown hook"</span>) <span class="comment">// force eager creation of logger</span></span><br><span class="line">       shutdownHookRef = <span class="type">ShutdownHookManager</span>.addShutdownHook(</span><br><span class="line">         <span class="type">StreamingContext</span>.<span class="type">SHUTDOWN_HOOK_PRIORITY</span>)(stopOnShutdown)</span><br><span class="line">       <span class="comment">// Registering Streaming Metrics at the start of the StreamingContext</span></span><br><span class="line">       assert(env.metricsSystem != <span class="literal">null</span>)</span><br><span class="line">       env.metricsSystem.registerSource(streamingSource)</span><br><span class="line">       uiTab.foreach(_.attach())</span><br><span class="line">       logInfo(<span class="string">"StreamingContext started"</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">ACTIVE</span> =&gt;</span><br><span class="line">       logWarning(<span class="string">"StreamingContext has already been started"</span>)</span><br><span class="line">     <span class="keyword">case</span> <span class="type">STOPPED</span> =&gt;</span><br><span class="line">       <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">"StreamingContext has already been stopped"</span>)</span><br><span class="line">   &#125;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h2 id="DStreams"><a href="#DStreams" class="headerlink" title="DStreams"></a>DStreams</h2><p>Discretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603163741.png" alt=""></p><p>对数据的操作也是按照RDD为单位来进行的</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603163808.png" alt=""></p><p>计算过程由Spark engine来完成</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SparkStreaming/20190603163827.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      SparkStreaming的相关理论学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparkStreaming" scheme="https://www.hphblog.cn/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkSQL数据源</title>
    <link href="https://www.hphblog.cn/2019/06/01/Spark%E4%B9%8BSparkSQL%E6%95%B0%E6%8D%AE%E6%BA%90/"/>
    <id>https://www.hphblog.cn/2019/06/01/Spark%E4%B9%8BSparkSQL%E6%95%B0%E6%8D%AE%E6%BA%90/</id>
    <published>2019-06-01T08:38:41.000Z</published>
    <updated>2020-01-12T13:08:23.674Z</updated>
    
    <content type="html"><![CDATA[ SparkSQL数据源:parquet Json Mysql  Hive：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="SparkSQL数据源"><a href="#SparkSQL数据源" class="headerlink" title="SparkSQL数据源"></a>SparkSQL数据源</h2><h3 id="手动指定选项"><a href="#手动指定选项" class="headerlink" title="手动指定选项"></a>手动指定选项</h3><p>Spark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDD的方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。</p><p>Spark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.load(<span class="string">"/input/sparksql/users.parquet"</span>) </span><br><span class="line">df.show()</span><br><span class="line">df.select(<span class="string">"name"</span>,<span class="string">"favorite_color"</span>).write.save(<span class="string">"/output/sparksql_out/namesAndFavColors.parquet"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601204039.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601204231.png" alt=""></p><p>当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：<code>org.apache.spark.sql.parquet</code>），如果数据源格式为内置格式，则只需要指定简称定<code>json, parquet, jdbc, orc, libsvm, csv,</code> text来指定数据的格式。可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.format(<span class="string">"json"</span>).load(<span class="string">"/input/sparksql/people.json"</span>)</span><br><span class="line">peopleDF.show()</span><br><span class="line">peopleDF.write.format(<span class="string">"parquet"</span>).save(<span class="string">"/output/sparksql_out/namesAndAges.parquet"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601204334.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601204445.png" alt=""></p><p>同时也可以直接运行SQL在文件上：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`/output/sparksql_out/namesAndAges.parquet`"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM parquet.`/output/sparksql_out/namesAndAges.parquet`"</span>)</span><br><span class="line">sqlDF.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601204928.png" alt=""></p><h3 id="文件保存选项"><a href="#文件保存选项" class="headerlink" title="文件保存选项"></a>文件保存选项</h3><p>SaveMode定义了对数据的处理模式，这些保存模式不使用任何锁定，f非原子操作。当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。</p><table><thead><tr><th>Scala/Java</th><th>Any Language</th><th>Meaning</th></tr></thead><tbody><tr><td>SaveMode.ErrorIfExists(default)</td><td>“error”(default)</td><td>如果文件存在，则报错</td></tr><tr><td>SaveMode.Append</td><td>“append”</td><td>追加</td></tr><tr><td>SaveMode.Overwrite</td><td>“overwrite”</td><td>覆写</td></tr><tr><td>SaveMode.Ignore</td><td>“ignore”</td><td>数据存在，则忽略</td></tr></tbody></table><h3 id="Parquet格式"><a href="#Parquet格式" class="headerlink" title="Parquet格式"></a>Parquet格式</h3><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。</p><p><a href="https://s2.ax1x.com/2019/01/18/k9A97d.png" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/01/18/k9A97d.png" alt="k9A97d.png"></a></p><p>Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h4 id="Parquet读写"><a href="#Parquet读写" class="headerlink" title="Parquet读写"></a>Parquet读写</h4><p>Parquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleDF = spark.read.json(<span class="string">"/input/sparksql/people.json"</span>)</span><br><span class="line">peopleDF.collect</span><br><span class="line">peopleDF.write.parquet(<span class="string">"/output/sparksql_out/people.parquet"</span>)</span><br><span class="line"><span class="keyword">val</span> parquetFileDF = spark.read.parquet(<span class="string">"/output/sparksql_out/people.parquet"</span>)</span><br><span class="line">parquetFileDF.createOrReplaceTempView(<span class="string">"parquetFile"</span>)</span><br><span class="line"><span class="keyword">val</span> namesDF = spark.sql(<span class="string">"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">namesDF.show()</span><br><span class="line">namesDF.map(attributes =&gt; <span class="string">"Name: "</span> + attributes(<span class="number">0</span>)).show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602131629.png" alt=""></p><h4 id="解析分区信息"><a href="#解析分区信息" class="headerlink" title="解析分区信息"></a>解析分区信息</h4><p>对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">path</span><br><span class="line">└── to</span><br><span class="line">    └── table</span><br><span class="line">        ├── gender=male</span><br><span class="line">        │   ├── ...</span><br><span class="line">        │   │</span><br><span class="line">        │   ├── country=<span class="type">US</span></span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   ├── country=<span class="type">CN</span></span><br><span class="line">        │   │   └── data.parquet</span><br><span class="line">        │   └── ...</span><br><span class="line">        └── gender=female</span><br><span class="line">            ├── ...</span><br><span class="line">            │</span><br><span class="line">            ├── country=<span class="type">US</span></span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            ├── country=<span class="type">CN</span></span><br><span class="line">            │   └── data.parquet</span><br><span class="line">            └── ...</span><br></pre></td></tr></table></figure><p>通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root</span><br><span class="line">|-- name: string (nullable &#x3D; true)</span><br><span class="line">|-- age: long (nullable &#x3D; true)</span><br><span class="line">|-- gender: string (nullable &#x3D; true)</span><br><span class="line">|-- country: string (nullable &#x3D; true)</span><br></pre></td></tr></table></figure><p>数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：<code>spark.sql.sources.partitionColumnTypeInference.enabled</code>，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。</p><h4 id="Schema合并"><a href="#Schema合并" class="headerlink" title="Schema合并"></a>Schema合并</h4><p>Parquet也支持Schema evolution（Schema演变）。用户先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。</p><p>因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：</p><p>当数据源为Parquet文件时，将数据源选项mergeSchema设置为true</p><p>设置全局SQL选项spark.sql.parquet.mergeSchema为true</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>).map(i =&gt; (i, i * <span class="number">2</span>)).toDF(<span class="string">"single"</span>, <span class="string">"double"</span>)</span><br><span class="line">df1.write.parquet(<span class="string">"/data/test_table/key=1"</span>)</span><br><span class="line"><span class="keyword">val</span> df2 = sc.makeRDD(<span class="number">6</span> to <span class="number">10</span>).map(i =&gt; (i, i * <span class="number">3</span>)).toDF(<span class="string">"single"</span>, <span class="string">"triple"</span>)</span><br><span class="line">df2.write.parquet(<span class="string">"/data/test_table/key=2"</span>)</span><br><span class="line"><span class="keyword">val</span> df3 = spark.read.option(<span class="string">"mergeSchema"</span>, <span class="string">"true"</span>).parquet(<span class="string">"/data/test_table"</span>)</span><br><span class="line">df3.printSchema()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602132456.png" alt="">.</p><h3 id="Hive数据库"><a href="#Hive数据库" class="headerlink" title="Hive数据库"></a>Hive数据库</h3><p>Apache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 </p><p>若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。</p><h4 id="内嵌Hive"><a href="#内嵌Hive" class="headerlink" title="内嵌Hive"></a>内嵌Hive</h4><p>内嵌的Hive可以直接使用,为了方便演示我们尽量使用指定master为lcoal</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">spark.sql(<span class="string">"show tables"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"CREATE TABLE IF NOT EXISTS test (key INT, value STRING)"</span>)</span><br><span class="line">spark.sql(<span class="string">"LOAD DATA LOCAL INPATH '/home/hadoop/data/kv1.txt' INTO TABLE test"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM test"</span>).show()</span><br><span class="line">spark.sql(<span class="string">"SELECT COUNT(*) FROM test"</span>).show()</span><br><span class="line"><span class="keyword">val</span> sqlDF = sql(<span class="string">"SELECT key, value FROM test WHERE key &lt; 10 ORDER BY key"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> stringsDS = sqlDF.map &#123;</span><br><span class="line"><span class="keyword">case</span> <span class="type">Row</span>(key: <span class="type">Int</span>, value: <span class="type">String</span>) =&gt; <span class="string">s"Key: <span class="subst">$key</span>, Value: <span class="subst">$value</span>"</span></span><br><span class="line">&#125;</span><br><span class="line">stringsDS.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">key: <span class="type">Int</span>, value: <span class="type">String</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">recordsDF</span> </span>= spark.createDataFrame((<span class="number">1</span> to <span class="number">100</span>).map(i =&gt; <span class="type">Record</span>(i, <span class="string">s"val_<span class="subst">$i</span>"</span>)))</span><br><span class="line">recordsDF.createOrReplaceTempView(<span class="string">"records"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM records r JOIN test s ON r.key = s.key"</span>).show()</span><br></pre></td></tr></table></figure><p><font color='red'> 注意:</font>如果使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果使用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到<code>Spark conf</code>目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。</p><h4 id="外部Hive"><a href="#外部Hive" class="headerlink" title="外部Hive"></a>外部Hive</h4><p>1)  将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp /opt/module/hadoop/etc/hadoop/core-site.xml /opt/module/spark/conf/</span><br><span class="line">cp /opt/module/hadoop/etc/hadoop/hdfs-site.xml /opt/module/spark/conf/</span><br><span class="line">cd /opt/module/spark/conf</span><br><span class="line">ln -s /opt/module/hive/conf/hive-site.xml</span><br></pre></td></tr></table></figure><p>2) 打开spark shell，注意带上访问Hive元数据库的JDBC客户端</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-shell --master spark:<span class="comment">//datanode1:7077 --jars mysql-connector-java-5.1.27-bin.jar</span></span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602152130.png" alt=""></p><p>配置比较简单。</p><h4 id="Windos开发"><a href="#Windos开发" class="headerlink" title="Windos开发"></a>Windos开发</h4><p> 在本地windos上需要将<code>core-site.xml</code>, <code>hive-site.xml</code>,<code>hdfs-site.xml的</code>配置拷贝到resources中去</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602155649.png" alt=""></p><p>同时pom文件配置如下</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>sparksql<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>sparksql-helloword<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.27<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-hive_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LocalHive</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .enableHiveSupport()</span><br><span class="line">      .appName(<span class="string">"Spark Hive"</span>)</span><br><span class="line">      .master(<span class="string">"local[2]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"show tables"</span>).limit(<span class="number">5</span>).show()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602160113.png" alt=""></p><h3 id="JSON数据集"><a href="#JSON数据集" class="headerlink" title="JSON数据集"></a>JSON数据集</h3><p>Spark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取数据</span></span><br><span class="line"><span class="keyword">val</span> peopleDF  = spark.read.json(<span class="string">"/input/sparksql/people.json"</span>)</span><br><span class="line">peopleDF.printSchema()</span><br><span class="line"><span class="comment">//创建临时表</span></span><br><span class="line">peopleDF.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="comment">//查询</span></span><br><span class="line"><span class="keyword">val</span> teenagerNamesDF = spark.sql(<span class="string">"SELECT name FROM people WHERE age BETWEEN 13 AND 19"</span>)</span><br><span class="line">teenagerNamesDF.show()</span><br><span class="line"><span class="comment">//隐式转换</span></span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="comment">//创建Dataset</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleDataset = spark.createDataset(<span class="string">""</span><span class="string">"&#123;"</span><span class="string">name":"</span><span class="type">Yin</span><span class="string">","</span><span class="string">address":&#123;"</span><span class="string">city":"</span><span class="type">Columbus</span><span class="string">","</span><span class="string">state":"</span><span class="type">Ohio</span><span class="string">"&#125;&#125;"</span><span class="string">""</span> :: <span class="type">Nil</span>)</span><br><span class="line"><span class="comment">//官方网站案例 可以直接将读取otherPeopleDataset,然而spark2.1需要转化一下成为javaRDD</span></span><br><span class="line"><span class="keyword">val</span> otherPeopleRdd = otherPeopleDataset.toJavaRDD</span><br><span class="line"><span class="keyword">val</span> otherPeople = spark.read.json(otherPeopleRdd)</span><br><span class="line">otherPeople.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602193248.png" alt=""></p><h3 id="JDBC"><a href="#JDBC" class="headerlink" title="JDBC"></a>JDBC</h3><p>Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.<span class="type">Properties</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Test</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL Strong Type UDF example"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//配置JDBC</span></span><br><span class="line">    <span class="comment">//配置JDBC</span></span><br><span class="line">    <span class="keyword">val</span> jdbcDF = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">" rddtable"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123456"</span>)</span><br><span class="line">      .load()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> connectionProperties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">    connectionProperties.put(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">    connectionProperties.put(<span class="string">"password"</span>, <span class="string">"123456"</span>)</span><br><span class="line">    <span class="keyword">val</span> jdbcDF2 = spark.read.jdbc(<span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>, <span class="string">"rddtable"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 写入数据库</span></span><br><span class="line">    jdbcDF.write</span><br><span class="line">      .format(<span class="string">"jdbc"</span>)</span><br><span class="line">      .option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>)</span><br><span class="line">      .option(<span class="string">"dbtable"</span>, <span class="string">"Spark_2_Mysql"</span>)</span><br><span class="line">      .option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">      .option(<span class="string">"password"</span>, <span class="string">"123456"</span>)</span><br><span class="line">      .save()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    jdbcDF2.write.jdbc(<span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>, <span class="string">"Spark_2_Mysql_1"</span>, connectionProperties)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 创建表时指定数据类别</span></span><br><span class="line">    jdbcDF.write.</span><br><span class="line">      option(<span class="string">"createTableColumnTypes"</span>, <span class="string">"name CHAR(64), comments VARCHAR(1024)"</span>)</span><br><span class="line">      .jdbc(<span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>, <span class="string">"SparkRDD2Mysql_Type"</span>, connectionProperties)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602194541.png" alt=""></p><p>原始数据</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602200638.png" alt=""></p><h3 id="JDBC-ODBC服务器"><a href="#JDBC-ODBC服务器" class="headerlink" title="JDBC/ODBC服务器"></a>JDBC/ODBC服务器</h3><p>Spark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。 </p><p>Spark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。 </p><p>服务器可以通过 Spark 目录中的 <code>sbin/start-thriftserver.sh</code> 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数–hiveconf property=value来设置Hive选项。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">./sbin/start-thriftserver.sh \</span><br><span class="line">--hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \</span><br><span class="line">--hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \</span><br><span class="line">--master &lt;master-uri&gt;</span><br><span class="line">...</span><br><span class="line">./bin/beeline</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-thriftserver.sh</span><br><span class="line">./bin/beeline</span><br><span class="line">!connect jdbc:hive2://datanode1:10000</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602201059.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602201658.png" alt=""></p><h3 id="Spark-SQL-CLI"><a href="#Spark-SQL-CLI" class="headerlink" title="Spark SQL CLI"></a>Spark SQL CLI</h3><p>Spark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。<br> 在Spark目录下执行如下命令启动Spark SQL CLI：<code>spark-sql</code> 配置Hive需要替换 conf/ 下的 hive-site.xml 。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190602202234.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      SparkSQL数据源:parquet Json Mysql  Hive：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparKSQL" scheme="https://www.hphblog.cn/tags/SparKSQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkSQL实战</title>
    <link href="https://www.hphblog.cn/2019/05/30/Spark%E4%B9%8BSparkSQL%E5%AE%9E%E6%88%98/"/>
    <id>https://www.hphblog.cn/2019/05/30/Spark%E4%B9%8BSparkSQL%E5%AE%9E%E6%88%98/</id>
    <published>2019-05-30T14:09:06.000Z</published>
    <updated>2020-01-12T13:08:24.048Z</updated>
    
    <content type="html"><![CDATA[ DataFrames 基本操作和 DSL SQL风格 UDF函数 以及数据源：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="SparkSQL查询"><a href="#SparkSQL查询" class="headerlink" title="SparkSQL查询"></a>SparkSQL查询</h2><p>Json数据准备</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Michael"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Andy"</span>, <span class="attr">"age"</span>:<span class="number">30</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Justin"</span>, <span class="attr">"age"</span>:<span class="number">19</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df =spark.read.json(<span class="string">"/input/sparksql/json/people.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line">df.filter($<span class="string">"age"</span>&gt;<span class="number">21</span>).show();</span><br><span class="line">df.createOrReplaceTempView(<span class="string">"person"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM person"</span>).show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530221923.png" alt=""></p><h2 id="IDEA创建SparkSQL程序"><a href="#IDEA创建SparkSQL程序" class="headerlink" title="IDEA创建SparkSQL程序"></a>IDEA创建SparkSQL程序</h2><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">scope</span>&gt;</span>provided<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// For implicit conversions like converting RDDs to DataFrames</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"F:\\spark\\examples\\src\\main\\resources\\people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190531121301.png" alt=""></p><h2 id="SparkSession"><a href="#SparkSession" class="headerlink" title="SparkSession"></a>SparkSession</h2><p>老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">HelloWorld</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL basic example"</span>)</span><br><span class="line">      .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。</span></span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"hdfs://datanode1:9000/input/sparksql/people.json"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Displays the content of the DataFrame to stdout</span></span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"persons"</span>)</span><br><span class="line"></span><br><span class="line">    spark.sql(<span class="string">"SELECT * FROM persons where age &gt; 21"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601104448.png" alt=""></p><h2 id="创建DataFrames"><a href="#创建DataFrames" class="headerlink" title="创建DataFrames"></a>创建DataFrames</h2><p>SparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = spark.read.json(<span class="string">"/input/sparksql/people.json"</span>)</span><br><span class="line">df.show()</span><br><span class="line"><span class="keyword">val</span> peopleRdd = sc.textFile(<span class="string">"/input/sparksql/people.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> peopleDF = peopleRdd.map(_.split(<span class="string">","</span>)).map(paras =&gt; (paras(<span class="number">0</span>),paras(<span class="number">1</span>).trim().toInt)).toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">peopleDF.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601123919.png" alt=""></p><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><h3 id="DSL风格语法"><a href="#DSL风格语法" class="headerlink" title="DSL风格语法"></a>DSL风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">df.printSchema() </span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line">df.select($<span class="string">"name"</span>, $<span class="string">"age"</span> + <span class="number">1</span>).show()  </span><br><span class="line">df.filter($<span class="string">"age"</span> &gt; <span class="number">21</span>).show()</span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601124441.png" alt=""></p><h3 id="SQL风格语法"><a href="#SQL风格语法" class="headerlink" title="SQL风格语法"></a>SQL风格语法</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"><span class="keyword">val</span> sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line">df.createGlobalTempView(<span class="string">"people"</span>)</span><br><span class="line">spark.sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br><span class="line">spark.newSession().sql(<span class="string">"SELECT * FROM global_temp.people"</span>).show()</span><br></pre></td></tr></table></figure><p>临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601124700.png" alt=""></p><h2 id="创建DataSet"><a href="#创建DataSet" class="headerlink" title="创建DataSet"></a>创建DataSet</h2><p>注意: Case classes in Scala 2.10只支持 22 字段. 你可以使用自定义的 classes 来实现对字段的映射<br><code>case class Person(name: String, age: Long)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">caseClassDS</span> </span>= <span class="type">Seq</span>(<span class="type">Person</span>(<span class="string">"Andy"</span>, <span class="number">32</span>)).toDS()</span><br><span class="line">caseClassDS.show()</span><br><span class="line"><span class="keyword">import</span> spark.implicits._ </span><br><span class="line"><span class="keyword">val</span> primitiveDS = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>).toDS() </span><br><span class="line">primitiveDS.map(_ + <span class="number">1</span>).collect()</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> path = <span class="string">"/input/sparksql/people.json"</span></span><br><span class="line"><span class="keyword">val</span> peopleDS = spark.read.json(path).as[<span class="type">Person</span>]</span><br><span class="line">peopleDS.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601151413.png" alt=""></p><h2 id="相互转化"><a href="#相互转化" class="headerlink" title="相互转化"></a>相互转化</h2><p>具体的转换可以参考: <a href="https://www.hphblog.cn/2019/05/30/Spark之SparkSQL/#三者共性">三者共性</a></p><h2 id="UDF函数"><a href="#UDF函数" class="headerlink" title="UDF函数"></a>UDF函数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL UDF example"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="comment">//读取数据</span></span><br><span class="line">    <span class="keyword">val</span> df = spark.read.json(<span class="string">"hdfs://datanode1:9000/input/sparksql/people.json"</span>)</span><br><span class="line">    df.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment">//注册UDF函数</span></span><br><span class="line">    spark.udf.register(<span class="string">"AddOne"</span>, (age: <span class="type">Int</span>) =&gt; age + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line">    <span class="comment">//SQL语句</span></span><br><span class="line">    spark.sql(<span class="string">"Select name,AddOne(age), age from people"</span>).show()</span><br><span class="line"></span><br><span class="line">    spark.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601162725.png" alt=""></p><h2 id="自定义聚合函数"><a href="#自定义聚合函数" class="headerlink" title="自定义聚合函数"></a>自定义聚合函数</h2><p>强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。</p><p>弱类型用户自定义聚合函数：通过继承<code>UserDefinedAggregateFunction</code>来实现用户自定义聚</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Row</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.&#123;<span class="type">MutableAggregationBuffer</span>, <span class="type">UserDefinedAggregateFunction</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"></span><br><span class="line"><span class="comment">//自定义UDAF函数需要继承UserDefinedAggregateFunction</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AverageSal</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  <span class="comment">//输入数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"salary"</span>, <span class="type">LongType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//定义每一个分区中的共享变量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = <span class="type">StructType</span>(<span class="type">StructField</span>(<span class="string">"sum"</span>, <span class="type">LongType</span>) :: <span class="type">StructField</span>(<span class="string">"count"</span>, <span class="type">IntegerType</span>) :: <span class="type">Nil</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//表示UDAF函数的输出类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">DoubleType</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//如果有相同的输入是否会存在相同的输出,如果是则为True</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//初始化 每一个分区中的共享变量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>L; <span class="comment">//  sum</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0</span> <span class="comment">//   sum</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//每一个分区中的每一条数据聚合的时候需要调用该方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//获取这一行中的工资,然后将工资加入到sum里</span></span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getLong(<span class="number">0</span>) + input.getLong(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//需要将工资的个数加1</span></span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getInt(<span class="number">1</span>) + <span class="number">1</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//将每一个没去的输出合并形成最后的数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//合并总的工资</span></span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getLong(<span class="number">0</span>) + buffer2.getLong(<span class="number">0</span>)</span><br><span class="line">    <span class="comment">//合并总的工资个数</span></span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getInt(<span class="number">1</span>) + buffer2.getInt(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//给出计算结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = &#123;</span><br><span class="line">    <span class="comment">//取出总的工资  / 总的工资个数</span></span><br><span class="line">    buffer.getLong(<span class="number">0</span>).toDouble / buffer.getInt(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">AverageSal</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL UDF example"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> employee = spark.read.json(<span class="string">"hdfs://datanode1:9000/input/sparksql/employees.json"</span>)</span><br><span class="line">    employee.createOrReplaceTempView(<span class="string">"employee"</span>)</span><br><span class="line">      </span><br><span class="line">    spark.udf.register(<span class="string">"average"</span>, <span class="keyword">new</span> <span class="type">AverageSal</span>)</span><br><span class="line">    spark.sql(<span class="string">"select average(salary)  from employee"</span>).show()</span><br><span class="line">    spark.stop()</span><br><span class="line">    </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="强类型函数"><a href="#强类型函数" class="headerlink" title="强类型函数"></a>强类型函数</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph.sql</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.&#123;<span class="type">Encoder</span>, <span class="type">Encoders</span>, <span class="type">SparkSession</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.expressions.<span class="type">Aggregator</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Employee</span>(<span class="params">name: <span class="type">String</span>, salary: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">case</span> <span class="title">class</span> <span class="title">Aver</span>(<span class="params">var sum: <span class="type">Long</span>, var count: <span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="title">class</span> <span class="title">Average</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Employee</span>, <span class="type">Aver</span>, <span class="type">Double</span>] </span>&#123;</span><br><span class="line">  <span class="comment">//初始化方法:初始化每一个分区的共享变量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">Aver</span> = <span class="type">Aver</span>(<span class="number">0</span>L, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//每一个分区的每一条数据聚合的时候需要回调该方法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">Aver</span>, a: <span class="type">Employee</span>): <span class="type">Aver</span> = &#123;</span><br><span class="line">    b.sum = b.sum + a.salary</span><br><span class="line">    b.count = b.count + <span class="number">1</span></span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//将每一个分区的输出 合并 形成最后的数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">Aver</span>, b2: <span class="type">Aver</span>): <span class="type">Aver</span> = &#123;</span><br><span class="line">    b1.sum = b1.sum + b2.sum</span><br><span class="line">    b1.count = b1.count + b2.count</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//输出计算结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">Aver</span>): <span class="type">Double</span> = &#123;</span><br><span class="line">    reduction.sum.toDouble / reduction.count</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Aver</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">Double</span>] = <span class="type">Encoders</span>.scalaDouble</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Average</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//创建SparkConf()并设置App名称</span></span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span>.builder()</span><br><span class="line">      .appName(<span class="string">"Spark SQL Strong Type UDF example"</span>)</span><br><span class="line">      .master(<span class="string">"local[*]"</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line">    <span class="keyword">import</span> spark.implicits._</span><br><span class="line">    <span class="keyword">val</span> employee = spark.read.json(<span class="string">"hdfs://datanode1:9000/input/sparksql/employees.json"</span>).as[<span class="type">Employee</span>]</span><br><span class="line">    <span class="keyword">val</span> aver = <span class="keyword">new</span> <span class="type">Average</span>().toColumn.name(<span class="string">"average"</span>)</span><br><span class="line">    employee.select(aver).show()</span><br><span class="line"></span><br><span class="line">    spark.close()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601200128.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      DataFrames 基本操作和 DSL SQL风格 UDF函数 以及数据源：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparKSQL" scheme="https://www.hphblog.cn/tags/SparKSQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark之SparkSQL理论篇</title>
    <link href="https://www.hphblog.cn/2019/05/30/Spark%E4%B9%8BSparkSQL/"/>
    <id>https://www.hphblog.cn/2019/05/30/Spark%E4%B9%8BSparkSQL/</id>
    <published>2019-05-30T11:09:55.000Z</published>
    <updated>2020-01-12T13:08:24.956Z</updated>
    
    <content type="html"><![CDATA[ Spark SQL 理论学习：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191139.png" alt=""></p><p>Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。</p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><p>1）易整合</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191724.png" alt=""></p><p>2) 统一的数据访问方式</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191703.png" alt=""></p><p>3）兼容Hive</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191746.png" alt=""></p><p>4）标准的数据连接</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191824.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530191859.png" alt=""></p><p> SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。</p><p> 在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet</p><p><code>RDD (Spark1.0)</code> —&gt;<code>Dataframe(Spark1.3)</code> —&gt;<code>Dataset(Spark1.6)</code></p><p>同样的数据都给到这三个数据结构有相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530192332.png" alt=""></p><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><p>RDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。简单，API设计友好。但它是一个JVM驻内存对象，受GC的限制和数据增加时Java序列化成本的升高。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file:///opt/module/spark/README.md"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530193912.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530194214.png" alt=""></p><h2 id="Dataframe"><a href="#Dataframe" class="headerlink" title="Dataframe"></a>Dataframe</h2><p>在Spark中DataFrame与RDD类似，也是一个分布式数据容器。但是DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息（schema），与Hive类似，DataFrame也支持嵌套数据类型<code>（struct、array和map）</code>。DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。与R和Pandas的DataFrame类似。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530194628.png" alt=""></p><p>RDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。</p><p>性能上比RDD要高，主要有两方面原因： </p><p>DataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待是同时DataFrame也是懒执行的。</p><p>定制化内存管理数据以二进制的方式存在于<code>非堆内存</code>，节省了大量空间之外，还摆脱了GC的限制。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530194934.png" alt=""></p><p>优化的执行计划查询计划通过Spark catalyst optimiser进行优化. </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530195026.png" alt=""></p><p>举个例子</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530195702.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530195713.png" alt=""></p><p>上图展示的人口数据分析的示例，构造了两个DataFrame，join之后做了filter操作。直接地执行这个执行计划，执行效率很差。join是代价较大的操作，如果能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，可以缩短执行时间。Spark SQL的查询优化器是这样做的：逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 </p><p>得到的优化执行计划在转换成物理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 </p><p>对于普通开发者而言：即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。但是由于在编译期缺少类型安全检查，导致运行时容易出错。</p><h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>1）是Dataframe API的一个扩展，是Spark最新的数据抽象</p><p>2）用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。</p><p>3）Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</p><p>4）样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。</p><p>5） Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。</p><p>6）DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].</p><p>7）DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。</p><p>数据准备</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Hadoop"</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Spark"</span>, <span class="attr">"Year"</span>:<span class="number">2015</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Flink"</span>, <span class="attr">"Year"</span>:<span class="number">2018</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span>  <span class="title">Bigdata</span>(<span class="params">name:<span class="type">String</span>,<span class="type">Year</span>:<span class="type">Int</span></span>) </span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= spark.sqlContext.read.json(<span class="string">"file:///opt/module/spark/json/bigdata.json"</span>)</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530203550.png" alt=""></p><p>RDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么,控制的粒度不一样。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530203640.png" alt=""></p><h2 id="三者共性"><a href="#三者共性" class="headerlink" title="三者共性"></a>三者共性</h2><p>1）都是spark平台下的分布式弹性数据集。</p><p>2）三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过</p><p>3）都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd=spark.sparkContext.parallelize(<span class="type">Seq</span>((<span class="string">"a"</span>, <span class="number">1</span>), (<span class="string">"b"</span>, <span class="number">1</span>), (<span class="string">"a"</span>, <span class="number">1</span>)))</span><br><span class="line"><span class="comment">// map不运行</span></span><br><span class="line"><span class="comment">// map不运行</span></span><br><span class="line">rdd.map&#123;line=&gt;</span><br><span class="line">  println(<span class="string">"运行"</span>)</span><br><span class="line">  line._1</span><br><span class="line">&#125;.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530204902.png" alt="">+</p><p>4)都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出</p><p>5)都有partition的概念</p><p>6)都有partition的概念</p><p>7)对DataFrame和Dataset进行操作许多操作都需要这个包进行支持 <code>import spark.implicits._</code></p><p>8)DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型</p><h2 id="三者区别"><a href="#三者区别" class="headerlink" title="三者区别"></a>三者区别</h2><p>1) RDD一般和spark mlib同时使用，但是不支持sparksql操作</p><p>2) DataFrame:与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，每一列的值没法直接访问</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> testDF = spark.read.json(<span class="string">"file:///opt/module/spark/json/bigdata.json"</span>)</span><br><span class="line">testDF.foreach&#123;</span><br><span class="line">  line =&gt;</span><br><span class="line">    <span class="keyword">val</span> col1=line.getAs[<span class="type">String</span>](<span class="string">"name"</span>)</span><br><span class="line">    <span class="keyword">val</span> col2=line.getAs[<span class="type">Long</span>](<span class="string">"Year"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>3) DataFrame与Dataset一般不与spark ml同时使用</p><p>4) DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> testDF = spark.read.json(<span class="string">"file:///opt/module/spark/json/bigdata.json"</span>)</span><br><span class="line">testDF.createOrReplaceTempView(<span class="string">"tmp"</span>)</span><br><span class="line">spark.sql(<span class="string">"select * from tmp"</span>).show(<span class="number">100</span>,<span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530211720.png" alt=""></p><p>5) DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""</span>,<span class="string">"Sepal.Length"</span>,<span class="string">"Sepal.Width"</span>,<span class="string">"Petal.Length"</span>,<span class="string">"Petal.Width"</span>,<span class="string">"Species"</span></span><br><span class="line"><span class="string">"1"</span>,<span class="number">5.1</span>,<span class="number">3.5</span>,<span class="number">1.4</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"2"</span>,<span class="number">4.9</span>,<span class="number">3</span>,<span class="number">1.4</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"3"</span>,<span class="number">4.7</span>,<span class="number">3.2</span>,<span class="number">1.3</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"4"</span>,<span class="number">4.6</span>,<span class="number">3.1</span>,<span class="number">1.5</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"5"</span>,<span class="number">5</span>,<span class="number">3.6</span>,<span class="number">1.4</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"6"</span>,<span class="number">5.4</span>,<span class="number">3.9</span>,<span class="number">1.7</span>,<span class="number">0.4</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"7"</span>,<span class="number">4.6</span>,<span class="number">3.4</span>,<span class="number">1.4</span>,<span class="number">0.3</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"8"</span>,<span class="number">5</span>,<span class="number">3.4</span>,<span class="number">1.5</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"9"</span>,<span class="number">4.4</span>,<span class="number">2.9</span>,<span class="number">1.4</span>,<span class="number">0.2</span>,<span class="string">"setosa"</span></span><br><span class="line"><span class="string">"10"</span>,<span class="number">4.9</span>,<span class="number">3.1</span>,<span class="number">1.5</span>,<span class="number">0.1</span>,<span class="string">"setosa"</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取</span></span><br><span class="line"><span class="keyword">val</span> options = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"file:///opt/module/spark/csv/iris.csv"</span>)</span><br><span class="line"><span class="keyword">val</span> datarDF= spark.read.options(options).format(<span class="string">"csv"</span>).load()</span><br><span class="line">datarDF.show()</span><br><span class="line"><span class="comment">//保存</span></span><br><span class="line"><span class="keyword">val</span> saveoptions = <span class="type">Map</span>(<span class="string">"header"</span> -&gt; <span class="string">"true"</span>, <span class="string">"delimiter"</span> -&gt; <span class="string">"\t"</span>, <span class="string">"path"</span> -&gt; <span class="string">"hdfs://datanode1:9000/test/saveToCSV"</span>)</span><br><span class="line">datarDF.write.format(<span class="string">"csv"</span>).mode(org.apache.spark.sql.<span class="type">SaveMode</span>.<span class="type">Overwrite</span>).options(saveoptions).save()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530214629.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190530214810.png" alt=""></p><h3 id="三者转换"><a href="#三者转换" class="headerlink" title="三者转换"></a>三者转换</h3><h3 id="RDD-gt-DataFrame"><a href="#RDD-gt-DataFrame" class="headerlink" title="RDD =&gt; DataFrame"></a>RDD =&gt; DataFrame</h3><h4 id="手动确定"><a href="#手动确定" class="headerlink" title="手动确定"></a>手动确定</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"/input/sparksql/people.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> name2AgeRDD = peopleRDD.map&#123;x =&gt; <span class="keyword">val</span> para = x.split(<span class="string">","</span>);(para(<span class="number">0</span>).trim, para(<span class="number">1</span>).trim.toInt) &#125;</span><br><span class="line">name2AgeRDD.collect</span><br><span class="line"><span class="keyword">import</span> spark.implicits._</span><br><span class="line"><span class="keyword">val</span> df = name2AgeRDD.toDF(<span class="string">"name"</span>,<span class="string">"age"</span>)</span><br><span class="line">df.show</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190531202646.png" alt=""></p><h4 id="反射确定"><a href="#反射确定" class="headerlink" title="反射确定"></a>反射确定</h4><p>利用case class</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"/input/sparksql/people.txt"</span>)</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">classPeople</span>(<span class="params">name:<span class="type">String</span>,age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">df</span> </span>= peopleRDD.map&#123;x =&gt; <span class="keyword">val</span> para = x.split(<span class="string">","</span>);<span class="type">People</span>(para(<span class="number">0</span>).trim, para(<span class="number">1</span>).trim.toInt) &#125;.toDS</span><br><span class="line">df.show()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601095101.png" alt=""></p><h4 id="编程方式"><a href="#编程方式" class="headerlink" title="编程方式"></a>编程方式</h4><p>1)准备Scheam</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types._</span><br><span class="line"><span class="keyword">val</span> schema = <span class="type">StructType</span>( <span class="type">StructField</span>(<span class="string">"name"</span>,<span class="type">StringType</span>)::<span class="type">StructField</span>(<span class="string">"age"</span>,org.apache.spark.sql.types.<span class="type">IntegerType</span>)::<span class="type">Nil</span>)</span><br></pre></td></tr></table></figure><p>2)准备Data   【需要Row类型】</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"/input/sparksql/people.txt"</span>)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql._</span><br><span class="line"><span class="keyword">val</span> data = peopleRDD.map&#123; x =&gt; <span class="keyword">val</span> para = x.split(<span class="string">","</span>);<span class="type">Row</span>(para(<span class="number">0</span>),para(<span class="number">1</span>).trim.toInt)&#125;</span><br></pre></td></tr></table></figure><p>3)生成DataFrame</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataFrame = spark.createDataFrame(data, schema)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601101135.png" alt=""></p><h3 id="DataFrame-gt-RDD"><a href="#DataFrame-gt-RDD" class="headerlink" title="DataFrame =&gt; RDD"></a>DataFrame =&gt; RDD</h3><p>直接DataFrame.rdd即可</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataFrame.rdd</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601101408.png" alt=""></p><h3 id="RDD-lt-gt-DataSet"><a href="#RDD-lt-gt-DataSet" class="headerlink" title="RDD  &lt;==&gt;DataSet"></a>RDD  &lt;==&gt;DataSet</h3><h4 id="RDD-》-DataSet"><a href="#RDD-》-DataSet" class="headerlink" title="RDD -》  DataSet"></a>RDD -》  DataSet</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> peopleRDD = sc.textFile(<span class="string">"/input/sparksql/people.txt"</span>)</span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)  <span class="title">//case</span> <span class="title">class</span> <span class="title">确定schema</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">ds</span> </span>= peopleRDD.map&#123;x =&gt; <span class="keyword">val</span> para = x.split(<span class="string">","</span>);<span class="type">People</span>(para(<span class="number">0</span>), para(<span class="number">1</span>).trim.toInt)&#125;.toDS</span><br><span class="line">ds.show()</span><br></pre></td></tr></table></figure><p><img src="C:%5CUsers%5CSchindler%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1559355499746.png" alt="1559355499746"></p><h4 id="DataSet-》-RDD"><a href="#DataSet-》-RDD" class="headerlink" title="DataSet -》 RDD"></a>DataSet -》 RDD</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dsRDD = ds.rdd</span><br><span class="line">dsRDD.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601102125.png" alt=""></p><h3 id="DataFrame-lt-gt-DataSet"><a href="#DataFrame-lt-gt-DataSet" class="headerlink" title="DataFrame  &lt;==&gt; DataSet"></a>DataFrame  &lt;==&gt; DataSet</h3><h4 id="DataSet-gt-DataFrame"><a href="#DataSet-gt-DataFrame" class="headerlink" title="DataSet  =&gt; DataFrame"></a>DataSet  =&gt; DataFrame</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> ds2df = ds.totoDF</span><br><span class="line">ds2df.show</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601102502.png" alt=""></p><h4 id="DataFrame-gt-DataSet"><a href="#DataFrame-gt-DataSet" class="headerlink" title="DataFrame  =&gt;DataSet"></a>DataFrame  =&gt;DataSet</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">People</span>(<span class="params">name:<span class="type">String</span>, age:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">df2ds</span> </span>= ds2df.as[<span class="type">People</span>]</span><br><span class="line">df2ds.show</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/SQL/20190601102723.png" alt=""></p><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>关于SparkSQL原理深入的学习可以参考《图解Spark》</p>]]></content>
    
    <summary type="html">
    
      Spark SQL 理论学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="SparKSQL" scheme="https://www.hphblog.cn/tags/SparKSQL/"/>
    
  </entry>
  
  <entry>
    <title>Spark之RDD实战篇3</title>
    <link href="https://www.hphblog.cn/2019/05/29/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%873/"/>
    <id>https://www.hphblog.cn/2019/05/29/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%873/</id>
    <published>2019-05-29T11:06:25.000Z</published>
    <updated>2020-01-12T13:08:24.264Z</updated>
    
    <content type="html"><![CDATA[ 键值对RDD、数据读取与保存、累加器、广播变量：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="键值对RDD"><a href="#键值对RDD" class="headerlink" title="键值对RDD"></a>键值对RDD</h2><p>Spark 为包含键值对类型的 RDD 提供了一些专有的操作 在PairRDDFunctions专门进行了定义。这些 RDD 被称为 pair RDD。有很多种方式创建<code>pair RDD</code>，在输入输出章节会讲解。一般如果从一个普通的RDD转 为pair RDD时，可以调用map()函数来实现，传递的函数需要返回键值对。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529191639.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529191720.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529191855.png" alt=""></p><p>果从一个普通的RDD转为<code>pair RDD</code>时，可以调用map()函数来实现，传递的函数需要返回键值对。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529192923.png" alt=""></p><h3 id="转化操作列表"><a href="#转化操作列表" class="headerlink" title="转化操作列表"></a>转化操作列表</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529193023.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529193034.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529193100.png" alt=""></p><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p>当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pair RDD上则 有相应的针对键的转化操作。Spark 有一组类似的操作，可以组合具有相同键的值。这些 操作返回 RDD，因此它们是转化操作而不是行动操作。 </p><p>reduceByKey() 与 reduce() 相当类似;它们都接收一个函数，并使用该函数对值进行合并。 reduceByKey() 会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合 并起来。因为数据集中可能有大量的键，所以 reduceByKey() 没有被实现为向用户程序返回一 个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的 RDD。 </p><p>foldByKey() 则与 fold() 相当类似;它们都使用一个与 RDD 和合并函数中的数据类型相 同的零值作为初始值。与 fold() 一样，foldByKey() 操作所使用的合并函数对零值与另一 个元素进行合并，结果仍为该元素。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd =sc.parallelize(<span class="type">Array</span>((<span class="string">"panda"</span>,<span class="number">0</span>),(<span class="string">"pink"</span>,<span class="number">3</span>),(<span class="string">"pirate"</span>,<span class="number">3</span>),(<span class="string">"panda"</span>,<span class="number">1</span>),(<span class="string">"pink"</span>,<span class="number">4</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> result =rdd.mapValues(x =&gt; (x, <span class="number">1</span>)).reduceByKey((x, y) =&gt; (x._1 + y._1, x._2 + y._2))</span><br><span class="line">result.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529195651.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529195711.png" alt=""></p><p>combineByKey() 是常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和 aggregate() 一样，combineByKey() 可以让用户返回与输入数据的类型不同的 返回值。 </p><p>由于 combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。 </p><p>如果这是一个新的元素，combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个 RDD 中第一次出现一个键时发生。 </p><p>如果这是一个在处理当前分区之前已经遇到的键，它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。 </p><p>由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更 多的分区都有对应同一个键的累加器，就需要使用用户提供的 mergeCombiners() 方法将各 个分区的结果进行合并。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd =sc.parallelize(<span class="type">Array</span>((<span class="string">"panda"</span>,<span class="number">0</span>),(<span class="string">"pink"</span>,<span class="number">3</span>),(<span class="string">"pirate"</span>,<span class="number">3</span>),(<span class="string">"panda"</span>,<span class="number">1</span>),(<span class="string">"pink"</span>,<span class="number">4</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> result = rdd.combineByKey(</span><br><span class="line">  (v) =&gt; (v, <span class="number">1</span>),</span><br><span class="line">  (acc: (<span class="type">Int</span>, <span class="type">Int</span>), v) =&gt; (acc._1 + v, acc._2 + <span class="number">1</span>),</span><br><span class="line">  (acc1: (<span class="type">Int</span>, <span class="type">Int</span>), acc2: (<span class="type">Int</span>, <span class="type">Int</span>)) =&gt; (acc1._1 + acc2._1, acc1._2 + acc2._2)</span><br><span class="line">)</span><br><span class="line">result.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529200208.png" alt=""></p><h3 id="数据分组"><a href="#数据分组" class="headerlink" title="数据分组"></a>数据分组</h3><p>如果数据已经以预期的方式提取了键，groupByKey() 就会使用 RDD 中的键来对数据进行 分组。对于一个由类型 K 的键和类型 V 的值组成的 RDD，所得到的结果 RDD 类型会是 [K, Iterable[V]]。 </p><p>多个RDD分组，可以使用cogroup函数，cogroup() 的函数对多个共享同 一个键的 RDD 进行分组。对两个键的类型均为 K 而值的类型分别为 V 和 W 的 RDD 进行 cogroup() 时，得到的结果 RDD 类型为 [(K, (Iterable[V], Iterable[W]))]。如果其中的 一个 RDD 对于另一个 RDD 中存在的某个键没有对应的记录，那么对应的迭代器则为空。 cogroup() 提供了为多个 RDD 进行数据分组的方法。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rddl = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="number">0</span>), (<span class="string">"A"</span>,<span class="number">2</span>), (<span class="string">"B"</span>,<span class="number">1</span>), (<span class="string">"B"</span>,<span class="number">2</span>), (<span class="string">"Cn"</span>,<span class="number">1</span>)))</span><br><span class="line">rdd1.groupByKey().collect</span><br><span class="line"><span class="comment">//使用reduceByKey操作将RDD[K,V]中每个K对应的V值根据映射函数来运算                 </span></span><br><span class="line"><span class="keyword">var</span> rdd2 = rdd1.reduceByKey((x,y) =&gt; x + y)</span><br><span class="line"><span class="comment">//对rddl使用reduceByKey操作进行重新分区</span></span><br><span class="line"><span class="keyword">var</span> rdd2 = rdd1.reduceByKey (<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>),(x, y) =&gt; x + y)</span><br><span class="line">rdd2.collect</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"a"</span>),(<span class="string">"C"</span>,<span class="string">"c"</span>),(<span class="string">"D"</span>,<span class="string">"d"</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">var</span> rdd3 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="string">"A"</span>),(<span class="string">"E"</span>,<span class="string">"E"</span>)),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">var</span> rdd4 = rddl.cogroup(rdd2,rdd3)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529203008.png" alt=""></p><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><p>连接主要用于多个Pair RDD的操作，连接方式多种多样:右外连接、左外连接、交 叉连接以及内连接。 </p><p>普通的 join 操作符表示内连接 2。只有在两个 pair RDD 中都存在的键才叫输出。当一个输 入对应的某个键有多个值时，生成的pair RDD会包括来自两个输入RDD的每一组相对应 的记录。 </p><p>leftOuterJoin()产生的pair RDD中，源RDD的每一个键都有对应的记录。每个 键相应的值是由一个源 RDD 中的值与一个包含第二个 RDD 的值的 Option(在 Java 中为 Optional)对象组成的二元组。 </p><p>rightOuterJoin() 几乎与 leftOuterJoin() 完全一样，只不过预期结果中的键必须出现在第二个 RDD 中，而二元组中的可缺失的部分则来自于源 RDD 而非第二个 RDD。这些连接操作都是继承了cgroup</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd1 = sc.makeRDD (<span class="type">Array</span>((<span class="string">"A"</span>, <span class="string">"1"</span>), (<span class="string">"B"</span>,<span class="string">"2"</span>) , (<span class="string">"C"</span>,<span class="string">"3"</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">var</span> rdd2 = sc.makeRDD (<span class="type">Array</span>((<span class="string">"A"</span>, <span class="string">"a"</span>), (<span class="string">"C"</span>,<span class="string">"c"</span>) , (<span class="string">"D"</span>,<span class="string">"d"</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="comment">//进行内连接操作</span></span><br><span class="line">rdd1.join(rdd2).collect</span><br><span class="line"><span class="comment">//进行左连接操作</span></span><br><span class="line">rdd1.leftOuterJoin(rdd2).collect</span><br><span class="line">rdd1.leftOuterJoin(rdd2).collect                      </span><br><span class="line"><span class="comment">//进行右连接操作</span></span><br><span class="line">rdd1.rightOuterJoin(rdd2).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529204438.png" alt=""></p><h3 id="数据排序"><a href="#数据排序" class="headerlink" title="数据排序"></a>数据排序</h3><p>sortByKey() 函数接收一个叫作 ascending 的参数，表示我们是否想要让结果按升序排序(默认值为 true)。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">··<span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"hadoop"</span>),(<span class="number">6</span>,<span class="string">"hohblog"</span>),(<span class="number">2</span>,<span class="string">"flink"</span>),(<span class="number">1</span>,<span class="string">"spark"</span>)))</span><br><span class="line">rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">rdd.sortByKey(<span class="literal">false</span>).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559015691057.png" alt="img"></p><h3 id="行动操作"><a href="#行动操作" class="headerlink" title="行动操作"></a>行动操作</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529204942.png" alt=""></p><h3 id="数据分区"><a href="#数据分区" class="headerlink" title="数据分区"></a>数据分区</h3><p>Spark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数，注意：</p><p>(1)只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None。<br>(2)每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> pairs =sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">1</span>),(<span class="number">2</span>,<span class="number">2</span>),(<span class="number">3</span>,<span class="number">3</span>)))</span><br><span class="line">pairs.partitioner</span><br><span class="line"><span class="keyword">val</span> partitioned = pairs.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">partitioned.partitioner</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529210421.png" alt=""></p><h4 id="Hash分区方式"><a href="#Hash分区方式" class="headerlink" title="Hash分区方式"></a>Hash分区方式</h4><p>对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nopar = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">8</span>)</span><br><span class="line">nopar.partitioner</span><br><span class="line">nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; <span class="type">Iterator</span>(index.toString+<span class="string">" : "</span>+iter.mkString(<span class="string">"|"</span>)) &#125;).collect</span><br><span class="line"><span class="keyword">val</span> hashpar = nopar.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">7</span>))</span><br><span class="line">hashpar.count</span><br><span class="line">hashpar.partitioner</span><br><span class="line">hashpar.mapPartitions(iter =&gt; <span class="type">Iterator</span>(iter.length)).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529210826.png" alt=""></p><h4 id="Ranger分区方式"><a href="#Ranger分区方式" class="headerlink" title="Ranger分区方式"></a>Ranger分区方式</h4><p>HashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。</p><p>RangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；</p><p>但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。</p><p>RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法用到了<a href="https://www.cnblogs.com/krcys/p/9121487.html" target="_blank" rel="noopener">水塘抽样算法</a>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> nopar = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">2</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">8</span>)</span><br><span class="line">nopar.partitioner</span><br><span class="line">nopar.mapPartitionsWithIndex((index,iter)=&gt;&#123; <span class="type">Iterator</span>(index.toString+<span class="string">" : "</span>+iter.mkString(<span class="string">"|"</span>)) &#125;).collect</span><br><span class="line"><span class="keyword">val</span> rangepar = nopar.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">RangePartitioner</span>(<span class="number">2</span>,nopar))</span><br><span class="line">rangepar.count</span><br><span class="line">rangepar.partitioner</span><br><span class="line">rangepar.mapPartitions(iter =&gt; <span class="type">Iterator</span>(iter.length)).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529211534.png" alt=""></p><h4 id="自定义分区"><a href="#自定义分区" class="headerlink" title="自定义分区"></a>自定义分区</h4><p>要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法。 </p><p>numPartitions: Int:返回创建出来的分区数。</p><p>getPartition(key: Any): Int:返回给定键的分区编号(0到numPartitions-1)。 </p><p>equals():Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。  </p><p>假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>(<span class="string">"aa.2"</span>,<span class="string">"bb.2"</span>,<span class="string">"cc.3"</span>,<span class="string">"dd.3"</span>,<span class="string">"ee.5"</span>).zipWithIndex,<span class="number">2</span>)</span><br><span class="line">data.collect</span><br><span class="line">data.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529215147.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">Partitioner</span>, <span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CustomerPartitioner</span>(<span class="params">numParts:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">Partitioner</span></span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = numParts</span><br><span class="line"></span><br><span class="line">  <span class="comment">//覆盖分区号获取函数</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> ckey: <span class="type">String</span> = key.toString</span><br><span class="line">    ckey.substring(ckey.length<span class="number">-1</span>).toInt%numParts</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = data.partitionBy(<span class="keyword">new</span> <span class="type">CustomerPartitioner</span>(<span class="number">4</span>))</span><br><span class="line">result.mapPartitionsWithIndex((index,iter)=&gt;<span class="type">Iterator</span>(index.toString +<span class="string">" : "</span>+ iter.mkString(<span class="string">"|"</span>))).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529215215.png" alt=""></p><p>使用自定义的 Partitioner 是很容易的:只要把它传给 partitionBy() 方法即可。Spark 中有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，它们也可以接收一个可选的 Partitioner 对象来控制输出数据的分区方式。</p><h4 id="分区shuffle优化"><a href="#分区shuffle优化" class="headerlink" title="分区shuffle优化"></a>分区shuffle优化</h4><p>在分布式程序中， 通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。 </p><p>Spark 中所有的键值对 RDD 都可以进行分区。系统会根据一个针对键的函数对元素进行分组。 主要有哈希分区和范围分区，当然用户也可以自定义分区函数。</p><p>通过分区可以有效提升程序性能。如下例子：</p><p>它在内存中保存着一张很大的用户信息表—— 也就是一个由 (UserID, UserInfo) 对组成的 RDD，其中 UserInfo 包含一个该用户所订阅 的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过 去五分钟内发生的事件——其实就是一个由 (UserID, LinkInfo) 对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面 的情况进行统计。 </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529215423.png" alt=""></p><p>代码可以正确运行，但是不够高效。这是因为在每次调用 processNewLogs() 时都会用到 join() 操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为 userData 表比每五分钟出现的访问日志表 events 要大得多，所以要浪费时间做很多额外工作:在每次调用时都对 userData 表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529215459.png" alt=""></p><p>我们在构建 userData 时调用了 partitionBy()，Spark 就知道了该 RDD 是根据键的哈希值来分区的，这样在调用 join() 时，Spark 就会利用到这一点。具体来说，当调用 userData. join(events) 时，Spark 只会对 events 进行数据混洗操作，将 events 中特定 UserID 的记 录发送到 userData 的对应分区所在的那台机器上。这样，需要通过网络传输的 数据就大大减少了，程序运行速度也可以显著提升了。 </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529215606.png" alt=""></p><h4 id="基于分区进行操作"><a href="#基于分区进行操作" class="headerlink" title="基于分区进行操作"></a>基于分区进行操作</h4><p>基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开 数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的 工作。Spark 提供基于分区的 mapPartition 和 foreachPartition，让你的部分代码只对 RDD 的每个分区运行 一次，这样可以帮助降低这些操作的代价。</p><h4 id="从分区中获益的操作"><a href="#从分区中获益的操作" class="headerlink" title="从分区中获益的操作"></a>从分区中获益的操作</h4><p>能够从数据分区中获得性能提升的操作有cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等。</p><h3 id="数据读取与保存"><a href="#数据读取与保存" class="headerlink" title="数据读取与保存"></a>数据读取与保存</h3><h4 id="文本文件"><a href="#文本文件" class="headerlink" title="文本文件"></a>文本文件</h4><p>当我们将一个文本文件读取为RDD 时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pair RDD，其中键是文件名，值是文件内容。</p><p>如果传递目录，则将目录下的所有文件读取作为RDD。</p><p>文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。</p><p>Spark通过saveAsTextFile() 进行文本文件的输出，该方法接收一个路径，并将 RDD 中的内容都输入到路径对应的文件中。Spark 将传入的路径作为目录对待，会在那个 目录下输出多个文件。这样，Spark 就可以从多个节点上并行输出了。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"/input/test.txt"</span>)</span><br><span class="line">rdd.collect</span><br><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"/input/*"</span>)</span><br><span class="line">rdd.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529220140.png" alt=""></p><h4 id="JSON文件"><a href="#JSON文件" class="headerlink" title="JSON文件"></a>JSON文件</h4><p>JSON文件中每一行就是一个JSON记录，可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Hadoop"</span>,<span class="attr">"age"</span>:<span class="number">13</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Spark"</span>, <span class="attr">"age"</span>:<span class="number">11</span>&#125;</span><br><span class="line">&#123;<span class="attr">"name"</span>:<span class="string">"Flink"</span>, <span class="attr">"age"</span>:<span class="number">3</span>&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.json4s.<span class="type">ShortTypeHints</span></span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">JsonMethods</span>._</span><br><span class="line"><span class="keyword">import</span> org.json4s.jackson.<span class="type">Serialization</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">TestJson</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">BigData</span>(<span class="params">name:<span class="type">String</span>,year:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class">  <span class="title">def</span> <span class="title">main</span>(<span class="params">args: <span class="type">Array</span>[<span class="type">String</span>]</span>)</span>: <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"JSON"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">implicit</span> <span class="keyword">val</span> formats = <span class="type">Serialization</span>.formats(<span class="type">ShortTypeHints</span>(<span class="type">List</span>()))</span><br><span class="line">    <span class="keyword">val</span> input = sc.textFile(<span class="string">"D:\\input\\people.json"</span>)</span><br><span class="line"></span><br><span class="line">    input.collect().foreach(x =&gt; &#123;<span class="keyword">var</span> c = parse(x).extract[<span class="type">BigData</span>];println(c.name + <span class="string">","</span> + c.year)&#125;)</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529225006.png" alt=""></p><h4 id="CSV文件"><a href="#CSV文件" class="headerlink" title="CSV文件"></a>CSV文件</h4><p>读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用 Spark 的文本文件 API 写出去。</p><h4 id="Sequence文件"><a href="#Sequence文件" class="headerlink" title="Sequence文件"></a>Sequence文件</h4><p> SequenceFile文件是<a href="http://lib.csdn.net/base/hadoop" target="_blank" rel="noopener">Hadoop</a>用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。</p><p> Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用 sequenceFile<a href="path"> keyClass, valueClass</a>。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529225226.png" alt=""></p><h4 id="对象文件"><a href="#对象文件" class="headerlink" title="对象文件"></a>对象文件</h4><p>对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile<a href="path">k,v</a> 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile() 实现对对象文件的输出。因为是序列化所以要指定类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data=sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"hphblog"</span>),(<span class="number">2</span>,<span class="string">"Spark"</span>),(<span class="number">3</span>,<span class="string">"Flink"</span>),(<span class="number">4</span>,<span class="string">"SpringBoot"</span>),(<span class="number">5</span>,<span class="string">"SpringCloud"</span>)))</span><br><span class="line">data.saveAsObjectFile(<span class="string">"hdfs://datanode1:9000/objfile"</span>)</span><br><span class="line"><span class="keyword">val</span> objrdd:org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>,<span class="type">String</span>)] = sc.objectFile[(<span class="type">Int</span>,<span class="type">String</span>)](<span class="string">"hdfs://datanode1:9000/objfile/p*"</span>)</span><br><span class="line">objrdd.collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529231917.png" alt=""></p><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>Spark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数.</p><p>1）输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)</p><p>2）键类型: 指定[K,V]键值对中K的类型</p><p>3）值类型: 指定[K,V]键值对中V的类型</p><p>4）分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits</p><p>其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值</p><p>分为新旧API，<strong>注意:</strong></p><p>1.在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压.</p><p>2.如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成上面的hadoopRDD和newAPIHadoopRDD两个类就行了</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"Hadoop"</span>), (<span class="number">2</span>,<span class="string">"Spark"</span>), (<span class="number">3</span>,<span class="string">"Flink"</span>)))</span><br><span class="line">data.saveAsHadoopFile(<span class="string">"hdfs://datanode1:9000/output/hdfs_spark"</span>,classOf[<span class="type">Text</span>],classOf[<span class="type">IntWritable</span>],classOf[<span class="type">TextOutputFormat</span>[<span class="type">Text</span>,<span class="type">IntWritable</span>]])</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529233324.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="type">Array</span>((<span class="string">"Hadoop"</span>,<span class="number">1</span>), (<span class="string">"Spark"</span>,<span class="number">2</span>), (<span class="string">"Flink"</span>,<span class="number">3</span>)))</span><br><span class="line">data.saveAsNewAPIHadoopFile(<span class="string">"hdfs://datanod1:9000/output/NewAPI/"</span>,classOf[<span class="type">Text</span>],classOf[<span class="type">IntWritable</span>] , classOf[org.apache.hadoop.mapreduce.<span class="type">OutputFormat</span>[<span class="type">Text</span>,<span class="type">IntWritable</span>]])</span><br></pre></td></tr></table></figure><h4 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h4><p>Spark 支持读写很多种文件系统， 像本地文件系统、Amazon S3、HDFS等甚至是腾讯和阿里的COS等。</p><h4 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h4><p>支持通过Java JDBC访问关系型数据库。需要通过JdbcRDD进行，不过需要我们把驱动包放入Spark的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/5/30.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCRdd</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span> </span>(args: <span class="type">Array</span>[<span class="type">String</span>] ) &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span> ().setMaster (<span class="string">"local[*]"</span>).setAppName (<span class="string">"JdbcApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span> (sparkConf)</span><br><span class="line">    <span class="keyword">val</span> rdd = <span class="keyword">new</span> org.apache.spark.rdd.<span class="type">JdbcRDD</span> (</span><br><span class="line">      sc,</span><br><span class="line">      () =&gt; &#123;</span><br><span class="line">        <span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">        java.sql.<span class="type">DriverManager</span>.getConnection (<span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"123456"</span>)</span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="string">"select * from rddtable where id &gt;= ? and id &lt;= ?;"</span>,  <span class="comment">//SQL</span></span><br><span class="line">      <span class="number">1</span>,   <span class="comment">// 下界</span></span><br><span class="line">      <span class="number">10</span>, <span class="comment">//上界</span></span><br><span class="line">      <span class="number">1</span>, <span class="comment">//分区数</span></span><br><span class="line">      r =&gt; (r.getInt(<span class="number">1</span>), r.getString(<span class="number">2</span>)))</span><br><span class="line"></span><br><span class="line">    println (rdd.count () )</span><br><span class="line">    rdd.foreach (println (_) )</span><br><span class="line">    sc.stop ()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530094256.png" alt="">)<img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530094311.png" alt=""></p><p>Mysql写入：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/5/30.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">JDBCRDD2MySQL</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="keyword">val</span> data = sc.parallelize(<span class="type">List</span>(<span class="string">"JDBC2Mysql"</span>, <span class="string">"JDBCSaveToMysql"</span>,<span class="string">"RDD2Mysql"</span>))</span><br><span class="line"></span><br><span class="line">    data.foreachPartition(insertData)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">insertData</span></span>(iterator: <span class="type">Iterator</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="type">Class</span>.forName (<span class="string">"com.mysql.jdbc.Driver"</span>).newInstance()</span><br><span class="line">    <span class="keyword">val</span> conn = java.sql.<span class="type">DriverManager</span>.getConnection(<span class="string">"jdbc:mysql://datanode1:3306/rdd"</span>, <span class="string">"root"</span>, <span class="string">"hive"</span>)</span><br><span class="line">    iterator.foreach(data =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> ps = conn.prepareStatement(<span class="string">"insert into rddtable(name) values (?)"</span>)</span><br><span class="line">      ps.setString(<span class="number">1</span>, data)</span><br><span class="line">      ps.executeUpdate()</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530095104.png" alt=""></p><p>JdbcRDD 接收这样几个参数。 </p><p>• 首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配置后创建自己读取数据的连接。 </p><p>• 接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中<code>lowerBound</code>和 <code>upperBound</code> 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。</p><p>• 这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create <span class="symbol">'frui</span>t',<span class="symbol">'inf</span>o'</span><br><span class="line">put <span class="symbol">'frui</span>t',<span class="symbol">'100</span>1',<span class="symbol">'info</span>:name',<span class="symbol">'Appl</span>e'</span><br><span class="line">put <span class="symbol">'frui</span>t',<span class="symbol">'100</span>1',<span class="symbol">'info</span>:color',<span class="symbol">'Rea</span>d'</span><br><span class="line">put <span class="symbol">'frui</span>t',<span class="symbol">'100</span>2',<span class="symbol">'info</span>:name',<span class="symbol">'Banan</span>a'</span><br><span class="line">put <span class="symbol">'frui</span>t',<span class="symbol">'100</span>2',<span class="symbol">'info</span>:color',<span class="symbol">'Yelo</span>w'</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.<span class="type">HBaseConfiguration</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapreduce.<span class="type">TableInputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/5/30.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">ReadHBase</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[*]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"192.168.1.101"</span>);</span><br><span class="line">    conf.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>)</span><br><span class="line">    <span class="comment">//HBase中的表名</span></span><br><span class="line">    conf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, <span class="string">"fruit"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">      classOf[org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">      classOf[org.apache.hadoop.hbase.client.<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> count = hBaseRDD.count()</span><br><span class="line">    println(<span class="string">"hBaseRDD RDD Count:"</span> + count)</span><br><span class="line">    hBaseRDD.cache()</span><br><span class="line">    hBaseRDD.foreach &#123;</span><br><span class="line">      <span class="keyword">case</span> (_, result) =&gt;</span><br><span class="line">        <span class="keyword">val</span> key = <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">        <span class="keyword">val</span> name = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"name"</span>.getBytes))</span><br><span class="line">        <span class="keyword">val</span> color = <span class="type">Bytes</span>.toString(result.getValue(<span class="string">"info"</span>.getBytes, <span class="string">"color"</span>.getBytes))</span><br><span class="line">        println(<span class="string">"Row key:"</span> + key + <span class="string">" Name:"</span> + name + <span class="string">" Color:"</span> + color)</span><br><span class="line">    &#125;</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530104837.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.&#123;<span class="type">HBaseAdmin</span>, <span class="type">Put</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.&#123;<span class="type">HBaseConfiguration</span>, <span class="type">HColumnDescriptor</span>, <span class="type">HTableDescriptor</span>, <span class="type">TableName</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.io.<span class="type">ImmutableBytesWritable</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.mapred.<span class="type">TableOutputFormat</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.<span class="type">Bytes</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapred.<span class="type">JobConf</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Created by 清风笑丶 Cotter on 2019/5/30.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Write2Hbase</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"HBaseApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    sc.setLogLevel(<span class="string">"ERROR"</span>)</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">    conf.set(<span class="string">"hbase.zookeeper.quorum"</span>, <span class="string">"192.168.1.101"</span>);</span><br><span class="line">    conf.set(<span class="string">"hbase.zookeeper.property.clientPort"</span>, <span class="string">"2181"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> jobConf = <span class="keyword">new</span> <span class="type">JobConf</span>(conf)</span><br><span class="line">    jobConf.setOutputFormat(classOf[<span class="type">TableOutputFormat</span>])</span><br><span class="line">    jobConf.set(<span class="type">TableOutputFormat</span>.<span class="type">OUTPUT_TABLE</span>, <span class="string">"fruit_spark"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> fruitTable = <span class="type">TableName</span>.valueOf(<span class="string">"fruit_spark"</span>)</span><br><span class="line">    <span class="keyword">val</span> tableDescr = <span class="keyword">new</span> <span class="type">HTableDescriptor</span>(fruitTable)</span><br><span class="line">    tableDescr.addFamily(<span class="keyword">new</span> <span class="type">HColumnDescriptor</span>(<span class="string">"info"</span>.getBytes))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> admin = <span class="keyword">new</span> <span class="type">HBaseAdmin</span>(conf)</span><br><span class="line">    <span class="keyword">if</span> (admin.tableExists(fruitTable)) &#123;</span><br><span class="line">      admin.disableTable(fruitTable)</span><br><span class="line">      admin.deleteTable(fruitTable)</span><br><span class="line">    &#125;</span><br><span class="line">    admin.createTable(tableDescr)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">convert</span></span>(triple: (<span class="type">Int</span>, <span class="type">String</span>, <span class="type">Int</span>)) = &#123;</span><br><span class="line">      <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(triple._1))</span><br><span class="line">      put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"name"</span>), <span class="type">Bytes</span>.toBytes(triple._2))</span><br><span class="line">      put.addImmutable(<span class="type">Bytes</span>.toBytes(<span class="string">"info"</span>), <span class="type">Bytes</span>.toBytes(<span class="string">"price"</span>), <span class="type">Bytes</span>.toBytes(triple._3))</span><br><span class="line">      (<span class="keyword">new</span> <span class="type">ImmutableBytesWritable</span>, put)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> initialRDD = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="string">"apple"</span>,<span class="number">11</span>), (<span class="number">2</span>,<span class="string">"banana"</span>,<span class="number">12</span>), (<span class="number">3</span>,<span class="string">"pear"</span>,<span class="number">13</span>)))</span><br><span class="line">    <span class="keyword">val</span> localData = initialRDD.map(convert)</span><br><span class="line"></span><br><span class="line">    localData.saveAsHadoopDataset(jobConf)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530105359.png" alt=""></p><h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><h4 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h4><p>一个全局共享变量,可以完成对信息进行操作,相当于MapReduce中的计数器, Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本， 更新这些副本的值也不会影响驱动器中的对应变量。 如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> notice = sc.textFile(<span class="string">"file:///opt/module/spark/README.md"</span>)</span><br><span class="line"> <span class="keyword">val</span> blanklines = sc.accumulator(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">val</span> tmp = notice.flatMap(line =&gt; &#123;</span><br><span class="line">         <span class="keyword">if</span> (line == <span class="string">""</span>) &#123;</span><br><span class="line">            blanklines += <span class="number">1</span></span><br><span class="line">         &#125;</span><br><span class="line">         line.split(<span class="string">" "</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">tmp.count()</span><br><span class="line">blanklines.value</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530111120.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@deprecated</span>(<span class="string">"use AccumulatorV2"</span>, <span class="string">"2.0.0"</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accumulator</span></span>[<span class="type">T</span>](initialValue: <span class="type">T</span>)(<span class="keyword">implicit</span> param: <span class="type">AccumulatorParam</span>[<span class="type">T</span>]): <span class="type">Accumulator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> acc = <span class="keyword">new</span> <span class="type">Accumulator</span>(initialValue, param)</span><br><span class="line">  cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))</span><br><span class="line">  acc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过在驱动器中调用<code>SparkContext.accumulator(initialValue)</code>方法，创建出存有初始值的累加器。返回值为<br><code>org.apache.spark.Accumulator[T]</code>对象，T 是初始值 initialValue 的类型。Spark闭包里的执行器代码可以使用累加器的 += 方法(在Java中是 add)增加累加器的值。 驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。 </p><p>为什么有了reduce()这样的聚合操作了,还要累加器呢?因为RDD本身提供的同步机制力度太粗,尤其是在转换操作中变量状态不能同步,累加器可以对那些与RDD本身的范围和粒度不一样的值进行聚合,只不过它是一个只写变量,无法读取这个值,只能在驱动程序中读取累加器的值。</p><h4 id="自定义累加器"><a href="#自定义累加器" class="headerlink" title="自定义累加器"></a>自定义累加器</h4><p>在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.collection.<span class="type">JavaConversions</span>._</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LogAccumulator</span> <span class="keyword">extends</span> <span class="title">org</span>.<span class="title">apache</span>.<span class="title">spark</span>.<span class="title">util</span>.<span class="title">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] </span>&#123;</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> _logArray: java.util.<span class="type">Set</span>[<span class="type">String</span>] = <span class="keyword">new</span> java.util.<span class="type">HashSet</span>[<span class="type">String</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">    _logArray.isEmpty</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.clear()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(v: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    _logArray.add(v)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    other <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> o: <span class="type">LogAccumulator</span> =&gt; _logArray.addAll(o.value)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: java.util.<span class="type">Set</span>[<span class="type">String</span>] = &#123;</span><br><span class="line">    java.util.<span class="type">Collections</span>.unmodifiableSet(_logArray)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>():org.apache.spark.util.<span class="type">AccumulatorV2</span>[<span class="type">String</span>, java.util.<span class="type">Set</span>[<span class="type">String</span>]] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newAcc = <span class="keyword">new</span> <span class="type">LogAccumulator</span>()</span><br><span class="line">    _logArray.synchronized&#123;</span><br><span class="line">      newAcc._logArray.addAll(_logArray)</span><br><span class="line">    &#125;</span><br><span class="line">    newAcc</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 过滤掉带字母的</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">LogAccumulator</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf=<span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"LogAccumulator"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc=<span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> accum = <span class="keyword">new</span> <span class="type">LogAccumulator</span></span><br><span class="line">    sc.register(accum, <span class="string">"logAccum"</span>)</span><br><span class="line">    <span class="keyword">val</span> sum = sc.parallelize(<span class="type">Array</span>(<span class="string">"1"</span>, <span class="string">"2a"</span>, <span class="string">"3"</span>, <span class="string">"4b"</span>, <span class="string">"5"</span>, <span class="string">"6"</span>, <span class="string">"7cd"</span>, <span class="string">"8"</span>, <span class="string">"9"</span>), <span class="number">2</span>).filter(line =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> pattern = <span class="string">""</span><span class="string">"^-?(\d+)"</span><span class="string">""</span></span><br><span class="line">      <span class="keyword">val</span> flag = line.matches(pattern)</span><br><span class="line">      <span class="keyword">if</span> (!flag) &#123;</span><br><span class="line">        accum.add(line)</span><br><span class="line">      &#125;</span><br><span class="line">      flag</span><br><span class="line">    &#125;).map(_.toInt).reduce(_ + _)  <span class="comment">//1+3+5+6+7+8+9 =32</span></span><br><span class="line"></span><br><span class="line">    println(<span class="string">"sum: "</span> + sum)</span><br><span class="line">    <span class="keyword">for</span> (v &lt;- accum.value) print(v + <span class="string">""</span>)</span><br><span class="line">    println()</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530124720.png" alt=""></p><h4 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h4><p>Spark的算子逻辑是发送到Executor中运行的，数据是分区的，因此当Executor中需要引用外部变量的时候，就需要我们用到广播变量(Broadcast)</p><p>累加器相当于统筹大变量，通常用于计数，统计广播变量允许程序员缓存一个只读的变量在每一台机器上（worker）上，而不是每一个任务保存一份备份。利用广播变量可以以更有效的方式将大数据量输入集合的副本分配到每一个节点。</p><p>广播变量通过两方面提高数据共享效率：</p><p>1)集群重的每一个节点(物理机器)只有一个副本，默认的闭包是每一个任务一个副本；</p><p>2)广播传输时通过BT下载模式实现的，也就是P2P下载的，在集群很多的情况下可以极大地提高数据传输速率。广播变量修改后，不会反馈到其他节点。</p><p>在Spark中，它会自动把所有音容变量发送到工作节点是，虽然很方便，但是效率比较低：</p><p>1)默认地任务发射机制时专门为小任务进行优化的。</p><p>2)实际过程中可能会在多个并行操作中使用同一个变量，而Spark会分别为每个操作发送这个变量。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcastVar = sc.broadcast(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">broadcastVar.value</span><br><span class="line">sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>)).flatMap(x =&gt; (broadcastVar.value)).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530182120.png" alt=""></p><p>广播变量内部存储地数据量较小地时候可以进行高效地广播，当这个变量变得非常大地时候，例如:在广播规则库的时候，规则库比较大，从主节点发送这样的一个规则数组非常消耗内存，如果之后还需要用到规则库这个变量，则需要再向每个节点发送一遍，同时如果一个节点的Executor中多个Task都用到这个变量，那么每个Task中都需要从driver端发送一份规则库的变量，最终导致占用的内存空间很大，如果变量为外部变量，进行广播前要进行collect操作。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> broadcas = sc.textFile(<span class="string">"file:///opt/module/spark/README.md"</span>)</span><br><span class="line"><span class="keyword">val</span> broadcasRDD = broadcas.collect</span><br><span class="line"><span class="keyword">val</span> c = sc.broadcast(broadcasRDD)</span><br><span class="line">c.value</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190530184944.png" alt=""></p><p>我们通过调用一个对象SparkContext.broadcast创建一个Broadcast对象，任何可以序列化对象都可以这样实现。需要注意的是，如果变量是从外部读取的，需要先进行collect操作，再进行广播，给如果广播的值比较大，可以选择即快又好的序列化格式。在Scala和Java API中默认使用Java序列化库，对于除基本的数组以外的任何对象都比较低效，我们可以使用<code>spark.serialler</code>属性选择另外一种序列化库来优化序列化的过程(也可以使用reduce()方法为Python的pickle库自定义序列化)</p>]]></content>
    
    <summary type="html">
    
      键值对RDD、数据读取与保存、累加器、广播变量：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="RDD" scheme="https://www.hphblog.cn/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark之RDD实战2</title>
    <link href="https://www.hphblog.cn/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/"/>
    <id>https://www.hphblog.cn/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/</id>
    <published>2019-05-28T14:57:46.000Z</published>
    <updated>2020-01-12T13:08:24.638Z</updated>
    
    <content type="html"><![CDATA[ 宽窄依赖、DAG RDD相关概念：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h2><p>RDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528225959.png" alt=""></p><h3 id="窄依赖"><a href="#窄依赖" class="headerlink" title="窄依赖"></a>窄依赖</h3><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。</p><h3 id="宽依赖"><a href="#宽依赖" class="headerlink" title="宽依赖"></a>宽依赖</h3><p>窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用,窄依赖我们形象的比喻为独生子女</p><h3 id="Lineage"><a href="#Lineage" class="headerlink" title="Lineage"></a>Lineage</h3><p>RDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528230120.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> text = sc.textFile(<span class="string">"/input/test.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> words = text.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"><span class="keyword">val</span> date = words.map((_,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> result = date.reduceByKey(_+_)</span><br><span class="line">date.dependencies</span><br><span class="line">result.dependencies</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528231029.png" alt=""></p><h2 id="DAG的生成"><a href="#DAG的生成" class="headerlink" title="DAG的生成"></a>DAG的生成</h2><p>DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528231121.png" alt=""></p><h2 id="RDD相关概念关系"><a href="#RDD相关概念关系" class="headerlink" title="RDD相关概念关系"></a>RDD相关概念关系</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528231142.png" alt=""></p><p>输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。</p><p>1)      每个节点可以起一个或多个Executor。</p><p>2)      每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。</p><p>3)      每个Task执行的结果就是生成了目标RDD的一个partiton。</p><p>注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而 Task被执行的并发度 = Executor数目 * 每个Executor核数。至于partition的数目：</p><p>1)      对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。</p><p>2)      在Map阶段partition数目保持不变。</p><p>3)      在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。</p><p>RDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。</p><p>比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。</p>]]></content>
    
    <summary type="html">
    
      宽窄依赖、DAG RDD相关概念：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="RDD" scheme="https://www.hphblog.cn/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark之RDD实战篇</title>
    <link href="https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/"/>
    <id>https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/</id>
    <published>2019-05-27T13:38:53.000Z</published>
    <updated>2020-01-12T13:08:25.363Z</updated>
    
    <content type="html"><![CDATA[ Spark RDD创建、转换、行动算子、RDD的持久化：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用action触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p><p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527214645.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215048.png" alt=""></p><h2 id="解析器集成"><a href="#解析器集成" class="headerlink" title="解析器集成"></a>解析器集成</h2><p>与Ruby和Python类似，Scala提供了一个交互式Shell (解析器)，借助内存数据带来的低延迟特性，可以让用户通过解析器对大数据进行交互式查询。Spark解析器将用户输入的多行命令解析为相应Java对象的示例如图所示</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215246.png" alt=""></p><p>Scala解析器处理过程一般为：</p><p>①将用户输入的每一行编译成一个类；</p><p>②将该类载入到JVM 中；</p><p>③调用该类的某个函数。在该类中包含一个单利对象，对象中包含当前行的变量或函数，在初始化方法中包含处理该行的代码。例如，如果用户输入“varx=5”，在换行输入primln(x),那解析器会定义一个叫Linel的类，该类包含X，第二行编译成println (Linel.getlnstance().x)。</p><p>Spark中做了以下两个改变。<br>(1)类传输：为了让工作节点能够从各行生成的类中获取到字节码，通过HTTP传输。<br>(2)代码生成器的改动：通常各种代码生成的单例对象是由类的静态方法来提供的。也就是说，当序列化一个引用上一行定义变量的闭包（例如上面例子的Linel.x), Java不会通过检索对象树的方式去传输包含x的Linel实例。因此工作节点不能够得到x,在Spark中修改了代码生成器的逻辑，让各行对象的实例可以被字节应用。在图中显示了 Spark修改之后解析器是如何把用户输入的每一行变成Java对象的。</p><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Spark提供了 3种持久化RDD的存储策略：</p><p>1.未序列化Java对象存在内存中、</p><p>2.序列化的数据存于内存中</p><p>3.存储在磁盘中</p><p>第一个选项的性能是最优的，因为可以直接访问在Java虚拟机内存里的RDD对象；在空间有限的情况下，第二种方式可以让用户釆用比Java对象更有效的内存组织方式，但代价是降低了性能；第三种策略使用于RDD太大的情形，每次重新计算该RDD会带来额外的资源开销（如I/O等)。对于内存使用LRU回收算法来进行管理，当计算得到一个新的RDD分区，但没有足够空间来存储时，系统会从最近最少使用的RDD回收其一个分区的空间。除非该RDD是新分区对应的RDD，这种情况下Spark会将旧的分区继续保留在内存中，防止同一个RDD的分区被循环调入/调出。这点很关键，因为大部分的操作会在一个RDD的所有分区上进行，那么很有可能己经存在内存中的分区将再次被使用。</p><h2 id="多用户管理"><a href="#多用户管理" class="headerlink" title="多用户管理"></a>多用户管理</h2><p>RDD模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群能够支持多种资源共享算法。特别地，每个RDD应用可以在执行过程中动态调整访问资源。<br>在每个应用程序中，Spark运行多线程同时提交作业，并通过一种等级公平调度器来实现多个作业对集群资源的共享，这种调度器和Hadoop Fair Scheduler类似。该算法主 要用于创建基于针对相同内存数据的多用户应用，例如：Spark SQL引擎有一个服务 模式支持多用户并行查询。公平调度算法确保短的作业能够在即使长作业占满集群资源的情况下尽早完成。</p><p>Spark的公平调度也使用延迟调度，通过轮询每台机器的数据，在保持公平的情况下给予作业高的本地性。Spark支持多级本地化访问策略（本地化)，包括内存、磁盘和机 架。</p><p>由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。Spark中可以使用Mesos来实现细粒度的资源共享，这使得Spark应用能相互之间或在不同的计算框架之间实现资源的动态共享。Spark使用Sparrow系统扩展支持分布式调度，该调度允许多个Spark应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。</p><h2 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h2><h3 id="集合中创建RDD"><a href="#集合中创建RDD" class="headerlink" title="集合中创建RDD"></a>集合中创建RDD</h3><p>从已有的集合中创建RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527222920.png" alt=""></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//并行化操作  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123; <span class="comment">//默认是多少呢</span></span><br><span class="line">    assertNotStopped()</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//本地模式下  </span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> =</span><br><span class="line">    scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)  </span><br><span class="line"><span class="comment">//CoarseGrainedSchedulerBackend</span></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    conf.getInt(<span class="string">"spark.default.parallelism"</span>, math.max(totalCoreCount.get(), <span class="number">2</span>))</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//stanlone继承了CoarseGrainedSchedulerBackend 因此绝大部分的情况下并行化处理数据的并行度为CPU的核数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//makeRDD本质上还是调用了parallelize</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Distribute a local Scala collection to form an RDD, with one or more</span></span><br><span class="line"><span class="comment"> * location preferences (hostnames of Spark nodes) for each object.</span></span><br><span class="line"><span class="comment"> * Create a new partition for each collection item.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), math.max(seq.size, <span class="number">1</span>), indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> test1 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(<span class="string">"datanode1"</span>)),(<span class="number">2</span>,<span class="type">List</span>(<span class="string">"datanode2"</span>)))  <span class="comment">//可以提供位置信息</span></span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527230912.png" alt=""></p><p><code>def parallelize[T: ClassTag]</code> 和<code>def makeRDD[T: ClassTag]</code>返回的都是ParallelCollectionRDD,而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p><h3 id="外部存储系统的数据集创建"><a href="#外部存储系统的数据集创建" class="headerlink" title="外部存储系统的数据集创建"></a>外部存储系统的数据集创建</h3><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> datasets =sc.textFile(<span class="string">"hdfs://datanode1:9000/input/test.txt"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232248.png" alt=""></p><h2 id="RDD转换"><a href="#RDD转换" class="headerlink" title="RDD转换"></a>RDD转换</h2><h3 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h3><p>map操作时对RDD中的每一个数都执行一个指定的函数来产生一个新的RDD,任何元RDD中的元素在新RDD中都有且只有一个元素与之对应。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>).collect()</span><br><span class="line"><span class="keyword">val</span> map = data.map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232843.png" alt=""></p><h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions()"></a>mapPartitions()</h3><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是<code>Iterator[T] =&gt; Iterator[U]</code>。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。其中<code>preservesPartitioning</code>表示是否保留父RDD的partitiones分区信息，如果在映射过程中需要频繁创建对象，使用mapPartitions操作要比map操作高 效得多。比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection,这样开销很大。如果使用mapPartitions，那么只需要针对每一个分区建立一个connectiono mapPartitionsWithlndex操作作用类似于mapPartitions,只是输入参数多了一个分区索引。W</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">     f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">     preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">     <span class="keyword">this</span>,</span><br><span class="line">     (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">     preservesPartitioning)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD使RDD有两个分区</span></span><br><span class="line"> <span class="keyword">var</span> rdd1= sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">///使用mapPartitions对rddl进行重新分区</span></span><br><span class="line">  <span class="keyword">var</span> rdd2 = rdd1.mapPartitions&#123; x =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span>(x.hasNext)&#123;</span><br><span class="line">          i += x.next()</span><br><span class="line">    &#125;</span><br><span class="line">     result.::(i).iterator</span><br><span class="line">   &#125;&#125;</span><br><span class="line">  <span class="comment">//rdd2将rddl中每个分区中的数值累加</span></span><br><span class="line">  rdd2.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">//重新对rdd1分区</span></span><br><span class="line"><span class="keyword">var</span> rdd3 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     (x,iter) =&gt; &#123;</span><br><span class="line">       <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">       <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">       <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">          i += iter.next()</span><br><span class="line">         &#125;</span><br><span class="line">        result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png" alt=""></p><h3 id="glom"><a href="#glom" class="headerlink" title="glom()"></a>glom()</h3><p>RDD中每一个分区所有类型为T的数据转变成元素类型为T的数组[Array[T]].</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd.glom().collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083705.png" alt=""></p><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap()"></a>flatMap()</h3><p>flatMap操作原RDD中的每一个元素生成一个或多个元素来构建新的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> flatMap = rdd1.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084629.png" alt=""></p><h3 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h3><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"flink"</span>,<span class="string">"hphblog"</span>))</span><br><span class="line"><span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"h"</span>))</span><br><span class="line">filter.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084943.png" alt=""></p><h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex()"></a>mapPartitionsWithIndex()</h3><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是<code>(Int, Interator[T]) =&gt;Iterator[U]</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD使RDD有两个分区</span></span><br><span class="line"> <span class="keyword">var</span> rdd1= sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">///使用mapPartitions对rddl进行重新分区</span></span><br><span class="line">  <span class="keyword">var</span> rdd2 = rdd1.mapPartitions&#123; x =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span>(x.hasNext)&#123;</span><br><span class="line">          i += x.next()</span><br><span class="line">    &#125;</span><br><span class="line">     result.::(i).iterator</span><br><span class="line">   &#125;&#125;</span><br><span class="line">  <span class="comment">//rdd2将rddl中每个分区中的数值累加</span></span><br><span class="line">  rdd2.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">//重新对rdd1分区</span></span><br><span class="line"><span class="keyword">var</span> rdd3 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     (x,iter) =&gt; &#123;</span><br><span class="line">       <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">       <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">       <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">          i += iter.next()</span><br><span class="line">         &#125;</span><br><span class="line">        result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png" alt=""></p><h3 id="sample-withReplacement-fraction-seed"><a href="#sample-withReplacement-fraction-seed" class="headerlink" title="sample(withReplacement, fraction, seed)"></a>sample(withReplacement, fraction, seed)</h3><p>以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为2（即可能以1 2 3的其中一个起始值）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd.collect</span><br><span class="line"><span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.5</span>,<span class="number">2</span>)</span><br><span class="line">sample1.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528091220.png" alt=""></p><h3 id="distinct-numTasks"><a href="#distinct-numTasks" class="headerlink" title="distinct([numTasks]))"></a>distinct([numTasks]))</h3><p>对原来RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.distinct()</span><br><span class="line">rdd1.collect</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.distinct(<span class="number">10</span>)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093127.png" alt=""></p><h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><p>对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"hadoop"</span>),(<span class="number">2</span>,<span class="string">"spark"</span>),(<span class="number">3</span>,<span class="string">"flink"</span>),(<span class="number">4</span>,<span class="string">"hphblog"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd.partitions.size</span><br><span class="line"><span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093534.png" alt=""></p><h3 id="coalesce-numPartitions-shuffle"><a href="#coalesce-numPartitions-shuffle" class="headerlink" title="coalesce((numPartitions, shuffle)"></a>coalesce((numPartitions, shuffle)</h3><p>缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。shuffle默认关闭.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10000</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> shuffleRDD = rdd.coalesce(<span class="number">2</span>,<span class="literal">true</span>)</span><br><span class="line">shuffleRDD.collect</span><br><span class="line">rdd.collect</span><br><span class="line">coalesceRDD.partitions.size</span><br><span class="line">shuffleRDD.partitions.size</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528094743.png" alt=""></p><h3 id="repartition-numPartitions"><a href="#repartition-numPartitions" class="headerlink" title="repartition(numPartitions)"></a>repartition(numPartitions)</h3><p>根据分区数，从新通过网络随机洗牌所有数据。底层调用的是<code>coalesce(numPartitions, shuffle = true)</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10000</span>,<span class="number">4</span>)</span><br><span class="line">rdd.partitions.size</span><br><span class="line"><span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd.partitions.size</span><br><span class="line"><span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd.partitions.size</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528100745.png" alt=""></p><h3 id="repartitionAndSortWithinPartitions"><a href="#repartitionAndSortWithinPartitions" class="headerlink" title="repartitionAndSortWithinPartitions"></a>repartitionAndSortWithinPartitions</h3><p>repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartitionAndSortWithinPartitions</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, partitioner).setKeyOrdering(ordering)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="sortBy-ascending-numTasks"><a href="#sortBy-ascending-numTasks" class="headerlink" title="sortBy([ascending], [numTasks])"></a>sortBy([ascending], [numTasks])</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">    f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">    ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd =sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>))</span><br><span class="line">rdd.sortBy(x =&gt; x ,ascending=<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102152.png" alt=""></p><h3 id="union-otherDataset"><a href="#union-otherDataset" class="headerlink" title="union(otherDataset)"></a>union(otherDataset)</h3><p>对源RDD和参数RDD求并集后返回一个新的RDD <code>不去重</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102902.png" alt=""></p><h3 id="subtract-otherDataset"><a href="#subtract-otherDataset" class="headerlink" title="subtract (otherDataset)"></a>subtract (otherDataset)</h3><p>计算差的一种函数，去除两个RDD中相同的元素，不同的RDD将保留下来 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.subtract(rdd2)</span><br><span class="line">rdd3.collect</span><br><span class="line"><span class="keyword">val</span> rdd4 =rdd2.subtract(rdd1)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559010918532.png" alt=""></p><h3 id="intersection-otherDataset"><a href="#intersection-otherDataset" class="headerlink" title="intersection(otherDataset)"></a>intersection(otherDataset)</h3><p>对源RDD和参数RDD求交集后返回一个新的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3.collect</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.intersection(rdd1)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528103820.png" alt=""></p><h3 id="cartesian-otherDataset"><a href="#cartesian-otherDataset" class="headerlink" title="cartesian(otherDataset)"></a>cartesian(otherDataset)</h3><p>笛卡尔积</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd1.cartesian(rdd2).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104041.png" alt=""></p><h3 id="pipe-command-envVars"><a href="#pipe-command-envVars" class="headerlink" title="pipe(command, [envVars])"></a>pipe(command, [envVars])</h3><p>管道，对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "hello Spark This is Linux bash"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"flink"</span>,<span class="string">"hphblog"</span>),<span class="number">1</span>)</span><br><span class="line">rdd.pipe(<span class="string">"/home/hadoop/pipe.sh"</span>).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104554.png" alt=""></p><h3 id="join-otherDataset-numTasks"><a href="#join-otherDataset-numTasks" class="headerlink" title="join(otherDataset, [numTasks])"></a>join(otherDataset, [numTasks])</h3><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104857.png" alt=""></p><h3 id="cogroup-otherDataset-numTasks"><a href="#cogroup-otherDataset-numTasks" class="headerlink" title="cogroup(otherDataset,[numTasks])"></a>cogroup(otherDataset,[numTasks])</h3><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个<code>(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))</code>类型的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.cogroup(rdd1).collect()</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.cogroup(rdd2).collect()</span><br><span class="line"><span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3.cogroup(rdd2).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110138.png" alt=""></p><h3 id="reduceByKey-func-numTasks"><a href="#reduceByKey-func-numTasks" class="headerlink" title="reduceByKey(func, [numTasks])"></a>reduceByKey(func, [numTasks])</h3><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">1</span>),(<span class="string">"spark"</span>,<span class="number">5</span>),(<span class="string">"spark"</span>,<span class="number">5</span>),(<span class="string">"flink"</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> reduce = rdd.reduceByKey((x,y)=&gt;(x+y))</span><br><span class="line">reduce.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110533.png" alt=""></p><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>groupByKey也是对每个key进行操作，但只生成一个sequence</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"spark"</span>, <span class="string">"flink"</span>, <span class="string">"flink"</span>, <span class="string">"flink"</span>)</span><br><span class="line"><span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group.collect()</span><br><span class="line"><span class="keyword">val</span> result = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">result.collect</span><br><span class="line"><span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map.collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111123.png" alt=""></p><h3 id="combineByKey-C"><a href="#combineByKey-C" class="headerlink" title="combineByKey[C]"></a>combineByKey[C]</h3><p>(  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C,  mergeCombiners: (C, C) =&gt; C) 对相同K，把V合并成一个集合。</p><p>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br> 那个键对应的累加器的初始值</p><p>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</p><p>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line"><span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line"><span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">          (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">          (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">          (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">         <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)</span><br><span class="line">&#125;</span><br><span class="line">result.collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111935.png" alt=""></p><h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><p><code>(zeroValue:U,[partitioner: Partitioner]) (seqOp:(U, V) =&gt; U,combOp: (U, U) =&gt; U)</code></p><p>在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。</p><p>seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg.collect()</span><br><span class="line">agg.partitions.size</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528114417.png" alt=""></p><h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><p><code>(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code>aggregateByKey的简化操作，seqop和combop相同</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg.collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115239.png" alt=""></p><h3 id="sortByKey-ascending-numTasks"><a href="#sortByKey-ascending-numTasks" class="headerlink" title="sortByKey([ascending], [numTasks])"></a>sortByKey([ascending], [numTasks])</h3><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"hadoop"</span>),(<span class="number">6</span>,<span class="string">"hohblog"</span>),(<span class="number">2</span>,<span class="string">"flink"</span>),(<span class="number">1</span>,<span class="string">"spark"</span>)))</span><br><span class="line">rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">rdd.sortByKey(<span class="literal">false</span>).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559015691057.png" alt=""></p><h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p>针对于(K,V)形式的类型只对V进行操作 </p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"hadoop"</span>),(<span class="number">6</span>,<span class="string">"hohblog"</span>),(<span class="number">2</span>,<span class="string">"flink"</span>),(<span class="number">1</span>,<span class="string">"spark"</span>)))</span><br><span class="line">rdd.mapValues(_+<span class="string">"==&gt; www.hphblog.cn"</span>).collect()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115636.png" alt=""></p><h2 id="RDD行动算子"><a href="#RDD行动算子" class="headerlink" title="RDD行动算子"></a>RDD行动算子</h2><h3 id="reduce-func"><a href="#reduce-func" class="headerlink" title="reduce(func)"></a>reduce(func)</h3><p>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.reduce(_+_)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd1.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120250.png" alt=""></p><h3 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.collect()</span><br></pre></td></tr></table></figure><p>在驱动程序中，以数组的形式返回数据集的所有元素</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120451.png" alt=""></p><h3 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h3><p>返回RDD的元素个数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd. count()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120547.png" alt=""></p><h3 id="first"><a href="#first" class="headerlink" title="first()"></a>first()</h3><p>返回RDD的第一个元素（类似于take(1)）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.first()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120819.png" alt=""></p><h3 id="take-n"><a href="#take-n" class="headerlink" title="take(n)"></a>take(n)</h3><p>返回一个由数据集的前n个元素组成的数组</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120952.png" alt=""></p><h3 id="takeSample-withReplacement-num-seed"><a href="#takeSample-withReplacement-num-seed" class="headerlink" title="takeSample(withReplacement,num, [seed])"></a>takeSample(withReplacement,num, [seed])</h3><p>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.takeSample(<span class="literal">true</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.takeSample(<span class="literal">false</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121504.png" alt=""></p><h3 id="takeOrdered-n"><a href="#takeOrdered-n" class="headerlink" title="takeOrdered(n)"></a>takeOrdered(n)</h3><p>返回前几个的排序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121702.png" alt=""></p><h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p><code>(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</code>aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;, </span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528183859.png" alt=""></p><p>为什么是58呢:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rdd.mapPartitionsWithIndex&#123;</span><br><span class="line">    (partid,iter)=&gt;&#123;</span><br><span class="line">        <span class="keyword">var</span> part_map = scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">List</span>[<span class="type">Int</span>]]()</span><br><span class="line">        <span class="keyword">var</span> part_name = <span class="string">"part_"</span> + partid</span><br><span class="line">        part_map(part_name) = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">        <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">            part_map(part_name) :+= iter.next()<span class="comment">//:+= 列表尾部追加元素</span></span><br><span class="line">        &#125;</span><br><span class="line">        part_map.iterator</span><br><span class="line">    &#125;</span><br><span class="line">&#125;.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528185402.png" alt=""></p><p>遍历第一个分区的数据我们知道第一个分区的数据是(1,2,3,4,5),第二个分区的数据是(6,7,8,9,10)首先在每一个分区执行<code>(x : Int,y : Int) =&gt; x + y</code>我们传入的zeroValue的值为1,即在<code>part_0中zeroValue+5+4+3+2+1=19</code>,在<code>part_1中zeroValue+6+7+8+9+10=41</code>,在将连个分局的结果合并<code>(a : Int,b : Int) =&gt; a + b</code>,并且使用zeroValue的值1即<code>zeroValue+part_0+part_1=1+16+41=58</code>因此结果为58.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br></pre></td></tr></table></figure><p>相同的我们可以刻分析出来</p><p>首先在每一个分区执行<code>(x : Int,y : Int) =&gt; x * y</code>我们传入的zeroValue的值为1,即在<code>part_0中zeroValue*5*4*3*2*1=120</code>,在<code>part_1中zeroValue*6*7*8*9*10=30240</code>,在将连个分局的结果合并<code>(a : Int,b : Int) =&gt; a + b</code>,并且使用zeroValue的值1即<code>zeroValue+part_0+part_1=1+120+30240=30361</code>因此结果为30361.</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528192803.png" alt=""></p><h3 id="fold-num-func"><a href="#fold-num-func" class="headerlink" title="fold(num)(func)"></a>fold(num)(func)</h3><p>折叠操作，aggregate的简化操作，seqop和combop一样。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br><span class="line">rdd.fold(<span class="number">1</span>)(_+_)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201005.png" alt=""></p><h3 id="saveAsTextFile-path"><a href="#saveAsTextFile-path" class="headerlink" title="saveAsTextFile(path)"></a>saveAsTextFile(path)</h3><p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.saveAsTextFile(<span class="string">"hdfs://datanode1:9000/spark/saveAsTextFile/"</span>)</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201529.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201656.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201735.png" alt=""></p><h3 id="saveAsSequenceFile-path"><a href="#saveAsSequenceFile-path" class="headerlink" title="saveAsSequenceFile(path)"></a>saveAsSequenceFile(path)</h3><p>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</p><h3 id="saveAsObjectFile-path"><a href="#saveAsObjectFile-path" class="headerlink" title="saveAsObjectFile(path)"></a>saveAsObjectFile(path)</h3><p>用于将RDD中的元素序列化成对象，存储到文件中。</p><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey()"></a>countByKey()</h3><p>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">3</span>),(<span class="string">"spark"</span>,<span class="number">2</span>),(<span class="string">"hphblog"</span>,<span class="number">3</span>),(<span class="string">"flink"</span>,<span class="number">9</span>),(<span class="string">"flink"</span>,<span class="number">9</span>),(<span class="string">"spark"</span>,<span class="number">10</span>)),<span class="number">3</span>)</span><br><span class="line">rdd.countByKey()</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202419.png" alt=""></p><h3 id="foreach-func"><a href="#foreach-func" class="headerlink" title="foreach(func)"></a>foreach(func)</h3><p>在数据集的每一个元素上，运行函数func进行更新。注意foreach遍历RDD,将函数f应用于每一个元素.要注意如果对RDD执行foreach,只会在Executor端有效,而不是Driver.比如rdd.collect().foreach(println),只会在Executor端有效,Driver端是看不到的.</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202856.png" alt=""></p><h3 id="sortBy-funct"><a href="#sortBy-funct" class="headerlink" title="sortBy(funct)"></a>sortBy(funct)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="number">2</span>),(<span class="string">"D"</span>,<span class="number">5</span>), (<span class="string">"A"</span>,<span class="number">1</span>), (<span class="string">"B"</span>,<span class="number">6</span>), (<span class="string">"B"</span>,<span class="number">3</span>), (<span class="string">"E"</span>, <span class="number">7</span>),(<span class="string">"C"</span>,<span class="number">4</span>)))</span><br><span class="line">rdd.sortBy(x =&gt; x).collect</span><br><span class="line">rdd.sortBy(x =&gt; x._2,<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528204821.png" alt=""></p><h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果 希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p><h3 id="缓存方式"><a href="#缓存方式" class="headerlink" title="缓存方式"></a>缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空 间中。 </p><p>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)  <span class="comment">//默认的持久化是内存中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= persist()   <span class="comment">//cache最终也是调用了persist方法</span></span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224007.png" alt=""></p><p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224100.png" alt=""></p><p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line"><span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache.cache</span><br><span class="line">nocache.collect</span><br><span class="line">nocache.collect</span><br><span class="line">cache.collect</span><br><span class="line">cache.collect</span><br><span class="line">cache.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224651.png" alt=""></p><p>我们发现持久化的内存时间戳没有变化,未持久化的内存时间戳是有变化的</p><h2 id="RDD检查点机制"><a href="#RDD检查点机制" class="headerlink" title="RDD检查点机制"></a>RDD检查点机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。</p><p>cache 和 checkpoint 是有显著区别的，  缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志）， 也不能丢掉， 当某个点某个 executor 宕了，上面cache 的RDD就会丢掉， 需要通过依赖链重放计算出来， 不同的是， checkpoint是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。</p><p>如果存在以下场景，则比较适合使用检查点机制：</p><p>1）DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</p><p>2）在宽依赖上做Checkpoint获得的收益更大。</p><p>为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">1000</span> , <span class="number">5</span>)</span><br><span class="line">sc.setCheckpointDir(<span class="string">"hdfs://datanode1:9000/checkpoint"</span>)</span><br><span class="line">data.checkpoint</span><br><span class="line">data.count</span><br><span class="line"><span class="keyword">val</span> ch1 = sc.parallelize(<span class="number">1</span> to <span class="number">20</span>)</span><br><span class="line"><span class="keyword">val</span> ch2 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line"><span class="keyword">val</span> ch3 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch3.checkpoint</span><br><span class="line">ch2.collect</span><br><span class="line">ch2.collect</span><br><span class="line">ch2.collect</span><br><span class="line">ch3.collect</span><br><span class="line">ch3.collect</span><br><span class="line">ch3.collect</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529115207.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      Spark RDD创建、转换、行动算子、RDD的持久化：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="RDD" scheme="https://www.hphblog.cn/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark之RDD理论篇</title>
    <link href="https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD/"/>
    <id>https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD/</id>
    <published>2019-05-27T11:57:45.000Z</published>
    <updated>2020-01-12T13:08:25.949Z</updated>
    
    <content type="html"><![CDATA[ Spark的基石RDD：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="RDD与MapReduce"><a href="#RDD与MapReduce" class="headerlink" title="RDD与MapReduce"></a>RDD与MapReduce</h2><p>Spark的编程模型是弹性分布式数据集(Resilient Distributed Dataset,RDD),它是MapReduce的扩展和延申,解决了MapReduce的缺陷:在并行计算阶段高效地进行数据共享.运行高效的数据共享概念和类似于MapReduce操作方式,使并行计算高效运行。Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527204519.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527204541.png" alt=""></p><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><p>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 </p><p>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><p>一组分片（Partition）：数据集的基本组成单位。对于RDD每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>一个计算每个分区的函数：Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>RDD之间的依赖关系：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>一个Partitioner：即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p> 一个列表：存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><h4 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h4><p><code>自动进行内存和磁盘数据存储的切换</code>：  Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。<br><code>基于血统的高效容错机制</code>： 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</p><p><code>Task如果失败会自动进行特定次数的重试</code>：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</p><p><code>Stage如果失败会自动进行特定次数的重试</code>：  如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</p><p><code>如果Job的某个Stage阶段计算失败</code>，框架也会自动进行任务的重新计算，默认次数也是4次：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除</p><p><code>数据调度弹性</code>：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</p><p><code>数据分片的高度弹性</code>： 可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</p><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527210835.png" alt=""></p><h5 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h5><p>RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211304.png" alt=""></p><p> 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211347.png" alt=""></p><p> RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211453.png" alt=""></p><h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。依赖包括两种，一种是窄依赖，RDDs之间分区是一 一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211742.png" alt=""></p><p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p><h5 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h5><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><h5 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h5><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>给定一个RDD我们至少可以知道如下几点信息：</p><p>1、分区数以及分区方式；</p><p>2、由父RDDs衍生而来的相关依赖信息；</p><p>3、计算每个分区的数据，计算步骤为：</p><p>1）如果被缓存，则从缓存中取的分区的数据；</p><p>2）如果被checkpoint，则从checkpoint处恢复数据；</p><p>3）根据血缘关系计算分区的数据。</p>]]></content>
    
    <summary type="html">
    
      Spark的基石RDD：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="RDD" scheme="https://www.hphblog.cn/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark之RDD理论篇</title>
    <link href="https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/"/>
    <id>https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/</id>
    <published>2019-05-27T11:57:45.000Z</published>
    <updated>2020-01-12T13:08:25.688Z</updated>
    
    <content type="html"><![CDATA[ Spark的基石RDD：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="RDD与MapReduce"><a href="#RDD与MapReduce" class="headerlink" title="RDD与MapReduce"></a>RDD与MapReduce</h2><p>Spark的编程模型是弹性分布式数据集(Resilient Distributed Dataset,RDD),它是MapReduce的扩展和延申,解决了MapReduce的缺陷:在并行计算阶段高效地进行数据共享.运行高效的数据共享概念和类似于MapReduce操作方式,使并行计算高效运行。Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527204519.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527204541.png" alt=""></p><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。</p><p>RDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 </p><p>Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。</p><h4 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h4><p>一组分片（Partition）：数据集的基本组成单位。对于RDD每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</p><p>一个计算每个分区的函数：Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</p><p>RDD之间的依赖关系：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</p><p>一个Partitioner：即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</p><p> 一个列表：存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</p><h4 id="弹性"><a href="#弹性" class="headerlink" title="弹性"></a>弹性</h4><p><code>自动进行内存和磁盘数据存储的切换</code>：  Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。<br><code>基于血统的高效容错机制</code>： 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。</p><p><code>Task如果失败会自动进行特定次数的重试</code>：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。</p><p><code>Stage如果失败会自动进行特定次数的重试</code>：  如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。</p><p><code>如果Job的某个Stage阶段计算失败</code>，框架也会自动进行任务的重新计算，默认次数也是4次：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除</p><p><code>数据调度弹性</code>：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。</p><p><code>数据分片的高度弹性</code>： 可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。</p><h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><h5 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h5><p>RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527210835.png" alt=""></p><h5 id="只读"><a href="#只读" class="headerlink" title="只读"></a>只读</h5><p>RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211304.png" alt=""></p><p> 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211347.png" alt=""></p><p> RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211453.png" alt=""></p><h5 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h5><p>RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。依赖包括两种，一种是窄依赖，RDDs之间分区是一 一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527211742.png" alt=""></p><p>通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。</p><h5 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h5><p>如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。</p><h5 id="CheckPoint"><a href="#CheckPoint" class="headerlink" title="CheckPoint"></a>CheckPoint</h5><p>虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。</p><p>给定一个RDD我们至少可以知道如下几点信息：</p><p>1、分区数以及分区方式；</p><p>2、由父RDDs衍生而来的相关依赖信息；</p><p>3、计算每个分区的数据，计算步骤为：</p><p>1）如果被缓存，则从缓存中取的分区的数据；</p><p>2）如果被checkpoint，则从checkpoint处恢复数据；</p><p>3）根据血缘关系计算分区的数据。</p>]]></content>
    
    <summary type="html">
    
      Spark的基石RDD：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
      <category term="RDD" scheme="https://www.hphblog.cn/tags/RDD/"/>
    
  </entry>
  
  <entry>
    <title>Spark生态圈及安装</title>
    <link href="https://www.hphblog.cn/2019/05/26/Spark%E7%94%9F%E6%80%81%E5%9C%88%E5%8F%8A%E5%AE%89%E8%A3%85/"/>
    <id>https://www.hphblog.cn/2019/05/26/Spark%E7%94%9F%E6%80%81%E5%9C%88%E5%8F%8A%E5%AE%89%E8%A3%85/</id>
    <published>2019-05-26T13:43:03.000Z</published>
    <updated>2020-01-12T13:08:26.741Z</updated>
    
    <content type="html"><![CDATA[ Spark生态圈的简单介绍和安装：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526214751.png" alt=""></p><p>2009年由马泰·扎哈里亚在加州伯克利分校的AMPLab实现开发的子项目,经过开源捐给了Apache基金会,最后成为了我们熟悉的Apache Spark,Spark式式由Scala语言实现的专门为大规模数据处理而设计的快速通用的计算引擎,经过多年的发展势头迅猛,当然,Flink的出现,也将打破Spark在流式计算的一些短板.后续会更新FLink相关的学习记录.</p><p>Spark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是<code>基于内存计算的大数据并行计算框架</code>。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分别管理的负担。 </p><p>大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：</p><p>1、软件栈中所有的程序库和高级组件都可以从下层的改进中获益。</p><p>2、运行整个软件栈的代价变小了。不需要运 行 5 到10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。</p><p>3、能够构建出无缝整合不同处理模型的应用。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526215518.png" alt=""></p><h3 id="Spark-Core"><a href="#Spark-Core" class="headerlink" title="Spark Core"></a><font color='red'>Spark Core</font></h3><p>实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。</p><h3 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a><font color='red'>Spark SQL</font></h3><p>是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比如 Hive 表、Parquet 以及 JSON 等。</p><h3 id="Spark-Streaming"><a href="#Spark-Streaming" class="headerlink" title="Spark Streaming"></a><font color='red'>Spark Streaming</font></h3><p>是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。</p><h3 id="Spark-MLlib"><a href="#Spark-MLlib" class="headerlink" title="Spark MLlib"></a><font color='red'>Spark MLlib</font></h3><p>提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 </p><h3 id="集群管理器"><a href="#集群管理器" class="headerlink" title="集群管理器"></a><font color='red'>集群管理器</font></h3><p>Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度器，叫作独立调度器。 </p><h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="快"><a href="#快" class="headerlink" title="快"></a><font color='red'>快</font></h3><p>与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。</p><h3 id="易用"><a href="#易用" class="headerlink" title="易用"></a><font color='red'>易用</font></h3><p>Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。</p><h3 id="通用"><a href="#通用" class="headerlink" title="通用"></a><font color='red'>通用</font></h3><p>Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。</p><h3 id="兼容性"><a href="#兼容性" class="headerlink" title="兼容性"></a><font color='red'>兼容性</font></h3><p>Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管 理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。</p><h2 id="用户"><a href="#用户" class="headerlink" title="用户"></a>用户</h2><p>我们大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。</p><h3 id="数据科学任务"><a href="#数据科学任务" class="headerlink" title="数据科学任务"></a><font color='red'>数据科学任务</font></h3><p>主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。</p><h3 id="数据处理应用"><a href="#数据处理应用" class="headerlink" title="数据处理应用"></a><font color='red'>数据处理应用</font></h3><p>工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。</p><h2 id="集群角色"><a href="#集群角色" class="headerlink" title="集群角色"></a>集群角色</h2><p>从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点：Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526220332.png" alt=""></p><p>从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。</p><h2 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h2><h3 id="Local"><a href="#Local" class="headerlink" title="Local"></a>Local</h3><p> 所有计算都运行在一个线程当中，没有任何并行计算，测试学习练习使用。</p><p>local[K]: 指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力;</p><p><code>local[*]</code>: 这种模式直接帮你按照cpu最多cores来设置线程数了。</p><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526220740.png" alt=""></p><h3 id="YarnSpark"><a href="#YarnSpark" class="headerlink" title="YarnSpark"></a>YarnSpark</h3><p>客户端直接连接Yarn；不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，</p><p>主要区别在于：Driver程序的运行节点。</p><p>yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。</p><p>yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526220903.png" alt=""></p><h3 id="Mesos"><a href="#Mesos" class="headerlink" title="Mesos"></a>Mesos</h3><p>Spark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。</p><h2 id="Spark2-X新特性"><a href="#Spark2-X新特性" class="headerlink" title="Spark2.X新特性"></a>Spark2.X新特性</h2><h3 id="精简的API"><a href="#精简的API" class="headerlink" title="精简的API"></a>精简的API</h3><ol><li>统一的DataFrame和DataSet接口。统</li><li>一Scala和Java的DataFrame、Dataset接口，在R和Python中缺乏安全类型,DataFrame成为主要的程序接口。</li><li>新增SparkSession入口，SparkSession替代原来的SQLContext和HiveContext作为DataFrame和Dataset的入口函数。SQLContext和HiveContext保持向后兼容。</li><li>为SparkSession通过全新的工作流式配置。</li><li>更易用、更高效的计算接口。</li><li>DataSet中的聚合操作有全新的、改进的聚合接口。</li></ol><h3 id="Spark作为编译器"><a href="#Spark作为编译器" class="headerlink" title="Spark作为编译器"></a>Spark作为编译器</h3><p>Spark2.0搭载了第二代Tungsten引擎，该引擎根据现代编译器与MPP数据库的理念来构建的，它将这些理念用于数据处理中，其中的主要的思想就是在运行时使用优化的字节码，将整体查询合称为单个函数，不再使用虚拟函数调用，而是利用CPU来注册中间数据。效果得到了很大的提升</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526222559.png" alt=""></p><h3 id="智能化程度"><a href="#智能化程度" class="headerlink" title="智能化程度"></a>智能化程度</h3><p>为了实现Spark更快、更轻松、更智能的目标、Spark2.X再许多模块上都做了更新，比如Structred Streaming 引入了低延迟的连续处理(Continuous Processing)、支持Stream-steam Joins、通过Pandas UDFs的性能提升PySpark、支持4种调度引擎：Kubernets Clusters 、Standalone、YARN、Mesos。</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>上传并解压spark安装包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/ #解压到指定目录</span><br><span class="line">mv spark-2.1.1-bin-hadoop2.7/ spark #重命名spark</span><br></pre></td></tr></table></figure><p>进入spark安装目录下的conf文件夹</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> spark/conf/</span><br></pre></td></tr></table></figure><p>修改slave文件，添加work节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode1 conf]$ vim slaves</span><br><span class="line">datanode1</span><br><span class="line">datanode2</span><br><span class="line">datanode3</span><br></pre></td></tr></table></figure><p>修改spark-env.sh文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode1 conf]$ cp spark-env.sh.template spark-env.sh</span><br><span class="line">[hadoop@datanode1 conf]$ vim spark-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#####################                     配置如下                     ######################</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Options <span class="keyword">for</span> the daemons used <span class="keyword">in</span> the standalone deploy mode</span></span><br><span class="line">SPARK_MASTER_HOST=datanode1  #指定Master</span><br><span class="line">SPARK_MASTER_PORT=7077      #指定Master端口</span><br></pre></td></tr></table></figure><p>分发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync spark/</span><br></pre></td></tr></table></figure><p>如果遇到这样的问题</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526224233.png" alt=""></p><p>我们需要设置一下JAVA_HOME，需要再sbin目录下的spark-config.sh 文件中加入JAVA_HOME的路径</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /opt/module/spark/sbin/spark-config.sh </span><br><span class="line"></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_162</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526225637.png" alt=""></p><p>访问datanode1:8080即可访问</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527115650.png" alt=""></p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> bin/spark-submit \       </span><br><span class="line">--class org.apache.spark.examples.SparkPi \            #主类</span><br><span class="line">--master spark://datanode1:7077 \                      #master</span><br><span class="line">--executor-memory 1G \    #任务的资源指定内存为1G</span><br><span class="line">--total-executor-cores 2 \#使用cpu核数</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \  #jar包</span><br><span class="line">100  #蒙特卡罗算法迭代次数</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526230433.png" alt=""></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit \ </span><br><span class="line">--class &lt;main-class&gt; # 应用的启动类 (如 org.apache.spark.examples.SparkPi)</span><br><span class="line">--master &lt;master-url&gt; \ #指定Master的地址</span><br><span class="line">--deploy-mode &lt;deploy-mode&gt; \  #是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*</span><br><span class="line">--conf &lt;key&gt;=&lt;value&gt; \ # 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” </span><br><span class="line">... # other options</span><br><span class="line">&lt;application-jar&gt; \ #打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar</span><br><span class="line">application-arguments: 传给main()方法的参数</span><br><span class="line">--executor-memory 1G 指定每个executor可用内存为1G</span><br><span class="line">--total-executor-cores 2 指定每个executor使用的cup核数为2个</span><br></pre></td></tr></table></figure><p>可以粗略的计算出PI大致为</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526230454.png" alt=""></p><p>再启动spark shell的时候我们也可以指定Spark的Master如果我们不指定的话，则使用的使local模式</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-shell \</span><br><span class="line">--master spark:<span class="comment">//datanode1:7077 \</span></span><br><span class="line">--executor-memory <span class="number">1</span>g \</span><br><span class="line">--total-executor-cores <span class="number">2</span></span><br></pre></td></tr></table></figure><p>Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 </p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526231126.png" alt=""></p><p>因此我们可以</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sc.textFile(<span class="string">"file:///opt/module/spark/word.txt"</span>).flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_).collect</span><br></pre></td></tr></table></figure><p>准备数据</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526232053.png" alt=""></p><p>由于使分布式启动我们需要把数据同步一下</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526233811.png" alt=""></p><h2 id="JobHistoryServer"><a href="#JobHistoryServer" class="headerlink" title="JobHistoryServer"></a>JobHistoryServer</h2><p>修改spark-default.conf.template名称</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode1 conf]$  mv spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"><span class="meta">#</span><span class="bash">修改下面配置 确保HDFS开启</span></span><br><span class="line">spark.master                     spark://datanode1:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://datanode1:9000/sparklog</span><br></pre></td></tr></table></figure><p><font color='red'>注意：HDFS上的目录需要提前存在。</font></p><p>修改spark-env.sh文件，添加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_HISTORY_OPTS="-Dspark.history.ui.port=4000 </span><br><span class="line">-Dspark.history.retainedApplications=3 </span><br><span class="line">-Dspark.history.fs.logDirectory=hdfs://datanode1:9000/sparklog"</span><br></pre></td></tr></table></figure><p>启动历史服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-history-server.sh</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190526235029.png" alt=""></p><p>我们再次执行任务</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit \</span><br><span class="line">--class org.apache.spark.examples.SparkPi \</span><br><span class="line">--master spark://datanode1:7077 \</span><br><span class="line">--executor-memory 1G \</span><br><span class="line">--total-executor-cores 2 \</span><br><span class="line">./examples/jars/spark-examples_2.11-2.1.1.jar \</span><br><span class="line">100</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527000603.png" alt=""></p><p>任务过程中会出现这样的界面，任务完成后我们可以查看</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527000746.png" alt=""></p><p>同时在HDFS上也会生成日志</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527000818.png" alt=""></p><h2 id="HA高可用"><a href="#HA高可用" class="headerlink" title="HA高可用"></a>HA高可用</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527001032.png" alt=""></p><p>1.首先我们要确保zookeeper正常安装并启动(具体参阅本人博客)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkstart</span><br></pre></td></tr></table></figure><p>修改spark-env.sh的配置文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">注释以下内容</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_MASTER_HOST=datanode1</span></span><br><span class="line"><span class="meta">#</span><span class="bash">SPARK_MASTER_PORT=7077</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">添加以下内容</span></span><br><span class="line">export SPARK_DAEMON_JAVA_OPTS="</span><br><span class="line">-Dspark.deploy.recoveryMode=ZOOKEEPER</span><br><span class="line">-Dspark.deploy.zookeeper.url=datanode1,datanode2,datanode3</span><br><span class="line">-Dspark.deploy.zookeeper.dir=/spark"</span><br></pre></td></tr></table></figure><p>分发配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xsync spark-env.sh</span><br></pre></td></tr></table></figure><p>datanode1节点上启动所有节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode1 spark]$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527121554.png" alt=""></p><p>datanode2启动master</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@datanode2 spark]$ sbin/start-all.sh</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527121833.png" alt=""></p><p>sparkHA访问集群</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/spark/bin/spark-shell --master spark://datanode1:7077,datanode2:7077 --executor-memory 1g --total-executor-cores 1</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527122708.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527122807.png" alt=""></p><p>我们在datanode2节点模拟节点出现故障</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527123554.png" alt=""></p><p>任务依旧可以执行。</p><h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><p>修改hadoop配置文件yarn-site.xml,添加如下内容：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>修改spark-env.sh，添加如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">YARN_CONF_DIR=/opt/module/hadoop/etc/hadoop  </span><br><span class="line">HADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop</span><br></pre></td></tr></table></figure><p>同步以下配置文件(脚本参考Hadoop篇)</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xsync /opt/module/hadoop/etc/hadoop/yarn-site.xml</span><br><span class="line">xsync /opt/module/spark/conf/spark-env.sh</span><br></pre></td></tr></table></figure><p>启动HDFS和YARN集群</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527131327.png" alt=""></p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527131423.png" alt=""></p><h2 id="IDEA环境配置"><a href="#IDEA环境配置" class="headerlink" title="IDEA环境配置"></a>IDEA环境配置</h2><p>spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。</p><p>首先我们先创建一个Maven的父项目</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>sparkcore<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>sparksql<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>sparkGraphx<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 表明当前项目是一个父项目，没有具体代码，只有声明的共有信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 声明公有的属性 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">properties</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark.version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">spark.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scala.version</span>&gt;</span>2.11.8<span class="tag">&lt;/<span class="name">scala.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">log4j.version</span>&gt;</span>1.2.17<span class="tag">&lt;/<span class="name">log4j.version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">slf4j.version</span>&gt;</span>1.7.22<span class="tag">&lt;/<span class="name">slf4j.version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">properties</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 声明并引入公有的依赖 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Logging --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>jcl-over-slf4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-api<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.slf4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>slf4j-log4j12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;slf4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>log4j<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;log4j.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- Logging End --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;scala.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 仅声明公有的依赖 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 编译环境能用，运行环境不可用 --&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 编译环境能用，运行环境不可用 --&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-streaming_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-graphx_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;spark.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 编译环境能用，运行环境不可用 --&gt;</span></span><br><span class="line">                <span class="comment">&lt;!--&lt;scope&gt;provided&lt;/scope&gt;--&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencyManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 配置构建信息 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 声明并引入构建的插件 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 设置项目的编译版本 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-compiler-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.6.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">source</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">source</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">target</span>&gt;</span>1.8<span class="tag">&lt;/<span class="name">target</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="comment">&lt;!-- 用于编译Scala代码到class --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>net.alchim31.maven<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-maven-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>compile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goal</span>&gt;</span>testCompile<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">&lt;!-- 仅声明构建的插件 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.0.0<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">executions</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">execution</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">id</span>&gt;</span>make-assembly<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">phase</span>&gt;</span>package<span class="tag">&lt;/<span class="name">phase</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">goals</span>&gt;</span></span><br><span class="line">                                <span class="tag">&lt;<span class="name">goal</span>&gt;</span>single<span class="tag">&lt;/<span class="name">goal</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;/<span class="name">goals</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">execution</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">executions</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">pluginManagement</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><p>在创建一个Maven的子项目sparkcore，在sparkcore中创建spark-wordcount项目</p><h3 id="sparkcore"><a href="#sparkcore" class="headerlink" title="sparkcore"></a>sparkcore</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">packaging</span>&gt;</span>pom<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modules</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">module</span>&gt;</span>spark-wordcount<span class="tag">&lt;/<span class="name">module</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">modules</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="spark-wordcount"><a href="#spark-wordcount" class="headerlink" title="spark-wordcount"></a>spark-wordcount</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">project</span> <span class="attr">xmlns</span>=<span class="string">"http://maven.apache.org/POM/4.0.0"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xmlns:xsi</span>=<span class="string">"http://www.w3.org/2001/XMLSchema-instance"</span></span></span><br><span class="line"><span class="tag">         <span class="attr">xsi:schemaLocation</span>=<span class="string">"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.hph<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0-SNAPSHOT<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">modelVersion</span>&gt;</span>4.0.0<span class="tag">&lt;/<span class="name">modelVersion</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-wordcount<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">build</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">plugins</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">plugin</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.maven.plugins<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>maven-assembly-plugin<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">archive</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">manifest</span>&gt;</span></span><br><span class="line">                            <span class="tag">&lt;<span class="name">mainClass</span>&gt;</span>com.hph.WordCount<span class="tag">&lt;/<span class="name">mainClass</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;/<span class="name">manifest</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">archive</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                        <span class="tag">&lt;<span class="name">descriptorRef</span>&gt;</span>jar-with-dependencies<span class="tag">&lt;/<span class="name">descriptorRef</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">descriptorRefs</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">plugin</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">plugins</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">build</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">project</span>&gt;</span></span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//声明配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置日志等级</span></span><br><span class="line">    sc.setLogLevel(<span class="string">"INFO"</span>)</span><br><span class="line">    <span class="comment">//读取输入的文件路径</span></span><br><span class="line">    <span class="keyword">val</span> file = sc.textFile(args(<span class="number">0</span>))</span><br><span class="line">    <span class="comment">//对输入的文本信息进行分割压平</span></span><br><span class="line">    <span class="keyword">val</span> words = file.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="comment">//对文本信息进行映射成K,1  </span></span><br><span class="line">    <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">//相同的Key相加  </span></span><br><span class="line">    <span class="keyword">val</span> result = word2Count.reduceByKey(_ + _)</span><br><span class="line">     <span class="comment">//输入存储路径</span></span><br><span class="line">    result.saveAsTextFile(args(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">     <span class="comment">//关闭资源</span></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>打包将我们的包更名为wordcunt.jar执行命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">spark-submit --class com.hph.WordCount --master spark://datanode1:7077 --executor-memory 1G --total-executor-cores 2 spark-wordcount-1.0-SNAPSHOT.jar hdfs://datanode1:9000//input/test.txt  hdfs://datanode1:9000//output/SPARK_WordCount</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527193259.png" alt=""></p><p>当然这种就打包就比较麻烦因此我们可以尝试以下别的方法来运行以下。</p><h4 id="远程运行"><a href="#远程运行" class="headerlink" title="远程运行"></a>远程运行</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//配置用户名</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="type">System</span>.getProperties</span><br><span class="line">    properties.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>)</span><br><span class="line">    <span class="comment">//声明配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>).setMaster(<span class="string">"spark://datanode1:7077"</span>)</span><br><span class="line">      .setJars(<span class="type">List</span>(<span class="string">"E:\\spark2\\sparkcore\\spark-wordcount\\target\\spark-wordcount-1.0-SNAPSHOT.jar"</span>))</span><br><span class="line">      .setIfMissing(<span class="string">"spark.driver.host"</span>, <span class="string">"192.168.1.1"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置日志等级</span></span><br><span class="line">    sc.setLogLevel(<span class="string">"INFO"</span>)</span><br><span class="line">    <span class="comment">//业务处理</span></span><br><span class="line">    <span class="keyword">val</span> file = sc.textFile(<span class="string">"hdfs://datanode1:9000/input/test.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> words = file.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> result = word2Count.reduceByKey(_ + _)</span><br><span class="line">    result.saveAsTextFile(<span class="string">"hdfs://datanode1:9000/output/Spark_Driver_On_W10"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527194605.png" alt=""></p><p>这个相当于在W10上执行了任务，宿主机Windos当作了Driver。</p><h4 id="本地调试"><a href="#本地调试" class="headerlink" title="本地调试"></a>本地调试</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.hph</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">//配置用户名</span></span><br><span class="line">    <span class="keyword">val</span> properties = <span class="type">System</span>.getProperties</span><br><span class="line">    properties.setProperty(<span class="string">"HADOOP_USER_NAME"</span>, <span class="string">"hadoop"</span>)</span><br><span class="line">    <span class="comment">//声明配置</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"WordCount"</span>).setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line"><span class="comment">//      .setJars(List("E:\\spark2\\sparkcore\\spark-wordcount\\target\\spark-wordcount-1.0-SNAPSHOT.jar"))</span></span><br><span class="line"><span class="comment">//      .setIfMissing("spark.driver.host", "192.168.1.1")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建SparkContext</span></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置日志等级</span></span><br><span class="line">    sc.setLogLevel(<span class="string">"INFO"</span>)</span><br><span class="line">    <span class="comment">//业务处理</span></span><br><span class="line">    <span class="keyword">val</span> file = sc.textFile(<span class="string">"D:\\input\\words.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> words = file.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="keyword">val</span> word2Count = words.map((_, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">val</span> result = word2Count.reduceByKey(_ + _)</span><br><span class="line">    result.saveAsTextFile(<span class="string">"D:\\output\\SPARK_ON_local"</span>)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527195233.png" alt=""></p><p>如果你遇到了错误</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527195302.png" alt=""></p><p>可以尝试以下方法修复</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527195331.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      Spark生态圈的简单介绍和安装：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="大数据" scheme="https://www.hphblog.cn/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    
      <category term="Spark" scheme="https://www.hphblog.cn/tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>数据结构之哈希表</title>
    <link href="https://www.hphblog.cn/2019/05/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    <id>https://www.hphblog.cn/2019/05/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%93%88%E5%B8%8C%E8%A1%A8/</id>
    <published>2019-05-26T02:26:19.000Z</published>
    <updated>2020-01-12T13:08:02.949Z</updated>
    
    <content type="html"><![CDATA[ 哈希表的相关学习：<Excerpt in index | 首页摘要><a id="more"></a> <h2 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h2><p>散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。</p><p>给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。</p><h2 id="哈希函数"><a href="#哈希函数" class="headerlink" title="哈希函数"></a>哈希函数</h2><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526104304.png" alt=""></p><p>哈希表是时间与空间之间的平衡,因此哈希函数的设计很重要。所以哈希函数应该尽量减少Hash冲突。也就是说“键”通过哈希函数得到的“索引”分布越均匀越好。</p><h3 id="整形"><a href="#整形" class="headerlink" title="整形"></a>整形</h3><p>对于小范围的整数比如<code>-100~100</code>,我们完全可以对整数直接使用把它映射到<code>0~200</code>之间,而对于身份证这种的大整数,通常我们采用的是取模,比如大整数的后四位相当于<code>mod 10000</code>,这里有一个小问题,如果我们选择的数字如果不好的话,就有可能可能导致数据映射分布不均匀,因此我们最好寻找一个素数.</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526105544.png" alt=""></p><p>如果选择一个合适的素数呢,这里有一个选择:</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526105723.png" alt=""></p><p>图片来源:<a href="https://planetmath.org/goodhashtableprimes" target="_blank" rel="noopener">https://planetmath.org/goodhashtableprimes</a></p><h3 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a>浮点型</h3><p>在计算机中都是32位或者64位的二进制表表示,只不过计算j级解析成了浮点数</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526115339.png" alt=""></p><h3 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h3><p>字符串我们需要把它也转成整型处理</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526115537.png" alt=""></p><p>我们对上面的方法进行优化一下,这就涉及到数学方面的知识了</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526115755.png" alt=""></p><p>对于字符串来说,计算出来的大整形如果特别大的话可能会出现内存溢出,因此我们可以对取模的过程分别挪到式子里面.</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526120023.png" alt=""></p><p>对于整个字符串来说我们可以写程序也是十分容易地写出来他的处理函数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> hash = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length; i++) &#123;</span><br><span class="line">    hash = (hash * B + s.charAt(i)) % M;</span><br></pre></td></tr></table></figure><h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> grade;</span><br><span class="line">    <span class="keyword">int</span> cls;</span><br><span class="line">    String firstName;</span><br><span class="line">    String lastName;</span><br><span class="line"></span><br><span class="line">    Student(<span class="keyword">int</span> grade, <span class="keyword">int</span> cls, String firstName, String lastName)&#123;</span><br><span class="line">        <span class="keyword">this</span>.grade = grade;</span><br><span class="line">        <span class="keyword">this</span>.cls = cls;</span><br><span class="line">        <span class="keyword">this</span>.firstName = firstName;</span><br><span class="line">        <span class="keyword">this</span>.lastName = lastName;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashCode</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> B = <span class="number">31</span>;</span><br><span class="line">        <span class="keyword">int</span> hash = <span class="number">0</span>;</span><br><span class="line">        hash = hash * B + ((Integer)grade).hashCode();</span><br><span class="line">        hash = hash * B + ((Integer)cls).hashCode();</span><br><span class="line">        hash = hash * B + firstName.toLowerCase().hashCode();</span><br><span class="line">        hash = hash * B + lastName.toLowerCase().hashCode();</span><br><span class="line">        <span class="keyword">return</span> hash;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">equals</span><span class="params">(Object o)</span></span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(<span class="keyword">this</span> == o)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(o == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(getClass() != o.getClass())</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">        Student another = (Student)o;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">this</span>.grade == another.grade &amp;&amp;</span><br><span class="line">                <span class="keyword">this</span>.cls == another.cls &amp;&amp;</span><br><span class="line">                <span class="keyword">this</span>.firstName.toLowerCase().equals(another.firstName.toLowerCase()) &amp;&amp;</span><br><span class="line">                <span class="keyword">this</span>.lastName.toLowerCase().equals(another.lastName.toLowerCase());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Student&#123;"</span> +</span><br><span class="line">                <span class="string">"grade="</span> + grade +</span><br><span class="line">                <span class="string">", cls="</span> + cls +</span><br><span class="line">                <span class="string">", firstName='"</span> + firstName + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">", lastName='"</span> + lastName + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.HashSet;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> a = <span class="number">42</span>;</span><br><span class="line">        System.out.println(((Integer)a).hashCode());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> b = -<span class="number">42</span>;</span><br><span class="line">        System.out.println(((Integer)b).hashCode());</span><br><span class="line"></span><br><span class="line">        <span class="keyword">double</span> c = <span class="number">3.1415926</span>;</span><br><span class="line">        System.out.println(((Double)c).hashCode());</span><br><span class="line"></span><br><span class="line">        String d = <span class="string">"imooc"</span>;</span><br><span class="line">        System.out.println(d.hashCode());</span><br><span class="line"></span><br><span class="line">        System.out.println(Integer.MAX_VALUE + <span class="number">1</span>);</span><br><span class="line">        System.out.println();</span><br><span class="line"></span><br><span class="line">        Student student = <span class="keyword">new</span> Student(<span class="number">3</span>, <span class="number">2</span>, <span class="string">"penghui"</span>, <span class="string">"Han"</span>);</span><br><span class="line">        System.out.println(student.hashCode());</span><br><span class="line"></span><br><span class="line">        HashSet&lt;Student&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        set.add(student);</span><br><span class="line">        <span class="keyword">for</span> (Student s : set) &#123;</span><br><span class="line">            System.out.println(s.toString());</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        HashMap&lt;Student, Integer&gt; scores = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        scores.put(student, <span class="number">100</span>);</span><br><span class="line">        System.out.println(scores.toString());</span><br><span class="line"></span><br><span class="line">        Student student2 = <span class="keyword">new</span> Student(<span class="number">3</span>, <span class="number">2</span>, <span class="string">"Penghui"</span>, <span class="string">"han"</span>);</span><br><span class="line">        System.out.println(student2.hashCode());</span><br><span class="line">        System.out.println(student.equals(student2));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526152303.png" alt=""></p><p>我们可以看到在实际的运行过程中对于整数的负数来说,依旧存在整数类型int中,对于浮点数和字符串来说都有都i是按照上面的方法.来进行计算。对于hashCode值相同我们并没有办法取判断是否属于一个对象，因此在equals和hashCode相同鼓的时候我们才能够说这个两个对象是相同的。</p><h2 id="哈希冲突处理"><a href="#哈希冲突处理" class="headerlink" title="哈希冲突处理"></a>哈希冲突处理</h2><h3 id="链地址法"><a href="#链地址法" class="headerlink" title="链地址法"></a>链地址法</h3><p>哈希表本质就是一个数组，</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526153109.png" alt=""></p><p>对于哈希表我们只需要让他求出K1然后在模于M，当然这个大M是一个素数。对于负数来说，可以直接用绝对值来解决负数的问题。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526153139.png" alt=""></p><p>当然我们有时候看别人的代码或者源码的时候会看到</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526153453.png" alt=""></p><p>用16进制法表示的整型，先和一个16进制表示的<code>0x7fffffff</code>的结果我们在对M取模，这个表示的是用二进制表示的话是31个<code>1</code>,整型有32位，最高位是符号位，32位和31位相与，这样做是吧最高位的32位，模成了0，符号位的问题我们就解决了。因此如果我们记录的<code>k1</code>的值为<code>4</code>，那么我们就可可以把k1存储到地址4这个位置中去。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526154202.png" alt=""></p><p>如果k2的索引位置为1，那么假设k3的位置也为1，那么我们就产生了hash冲突，如何解决呢？</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526154417.png" alt=""></p><p>这里我们可以采用链表的方式，对于整个哈希表我们开M个空间，由于会出现hash冲突，我们可以把它做成链表，这种方法也叫<em>separate chaining</em>，当然我们已可以存红黑树，或者TreeMap。</p><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E5%93%88%E5%B8%8C%E8%A1%A8/20190526154901.png" alt=""></p><p>本质上来说，HashMap就是一个TreeMap数组，HashSet就是一个TreeSet数组。对于Java8来说，Java8之前每一个位置对应的是一个链表，Java8开始之后，当哈希冲突达到了一定的程度，每一个位置从链表转化为红黑树，这个阈值为8；</p><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.LinkedList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HashTable</span>&lt;<span class="title">T</span>&gt;</span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashTable</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>(DEFAULT_TABLE_SIZE);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HashTable</span><span class="params">(<span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">        theLists=<span class="keyword">new</span> LinkedList[nextPrime(size)];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;theLists.length;i++) &#123;</span><br><span class="line">            theLists[i]=<span class="keyword">new</span> LinkedList&lt;&gt;();<span class="comment">//初始化链表数组</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 哈希表插入元素</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">        List&lt;T&gt; whichList=theLists[myhash(x)];</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 如果当前哈希地址的链表不含有元素，则链表中添加该元素</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="keyword">if</span>(!whichList.contains(x)) &#123;</span><br><span class="line">            whichList.add(x);</span><br><span class="line">            <span class="keyword">if</span>(++currentSize&gt;theLists.length)<span class="comment">//如果表长度不够，则扩容</span></span><br><span class="line">                rehash();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">remove</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">        List&lt;T&gt; whichList=theLists[myhash(x)];</span><br><span class="line">        <span class="keyword">if</span>(whichList.contains(x)) &#123;</span><br><span class="line">            whichList.remove(x);</span><br><span class="line">            currentSize--;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">contains</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">        List&lt;T&gt; whilchList=theLists[myhash(x)];</span><br><span class="line">        <span class="keyword">return</span> whilchList.contains(x);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">makeEmpty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;theLists.length;i++)</span><br><span class="line">            theLists[i].clear();</span><br><span class="line">        currentSize=<span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">int</span> DEFAULT_TABLE_SIZE=<span class="number">101</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">private</span> List&lt;T&gt; [] theLists;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> currentSize;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 哈希表扩容，表长度为下一个素数</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">rehash</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        List&lt;T&gt;[] oldLists=theLists;</span><br><span class="line">        theLists=<span class="keyword">new</span> List[nextPrime(<span class="number">2</span>*theLists.length)];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;theLists.length;j++)</span><br><span class="line">            theLists[j]=<span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">        </span><br><span class="line">        currentSize=<span class="number">0</span>;</span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * 更新哈希表</span></span><br><span class="line"><span class="comment">         * */</span></span><br><span class="line">        <span class="keyword">for</span>(List&lt;T&gt; list:oldLists)</span><br><span class="line">            <span class="keyword">for</span>(T item:list)</span><br><span class="line">                insert(item);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">     * myhash()方法获得哈希表的地址</span></span><br><span class="line"><span class="comment">     * */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">myhash</span><span class="params">(T x)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> hashVal=x.hashCode();<span class="comment">//hashCode()方法返回该对象的哈希码值</span></span><br><span class="line">        hashVal%=theLists.length;<span class="comment">//对哈希表长度取余数</span></span><br><span class="line">        <span class="keyword">if</span>(hashVal&lt;<span class="number">0</span>)</span><br><span class="line">            hashVal+=theLists.length;</span><br><span class="line">        <span class="keyword">return</span> hashVal;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//下一个素数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">nextPrime</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>( n % <span class="number">2</span> == <span class="number">0</span> )</span><br><span class="line">            n++;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>( ; !isPrime( n ); n += <span class="number">2</span> )</span><br><span class="line">            ;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> n;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//判断是否是素数</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">         <span class="keyword">if</span>( n == <span class="number">2</span> || n == <span class="number">3</span> )</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>( n == <span class="number">1</span> || n % <span class="number">2</span> == <span class="number">0</span> )</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span>( <span class="keyword">int</span> i = <span class="number">3</span>; i * i &lt;= n; i += <span class="number">2</span> )</span><br><span class="line">                <span class="keyword">if</span>( n % i == <span class="number">0</span> )</span><br><span class="line">                    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="开放地址法"><a href="#开放地址法" class="headerlink" title="开放地址法"></a>开放地址法</h3><p>这个方法的基本思想是：当发生地址冲突时，按照某种方法继续探测哈希表中的其他存储单元，直到找到空位置为止。这个过程可用下式描述：<br>H i ( key ) = ( H ( key )+ d i ) mod m ( i = 1,2,…… ， k ( k ≤ m – 1))<br>其中： H ( key ) 为关键字 key 的直接哈希地址， m 为哈希表的长度， di 为每次再探测时的地址增量。<br>采用这种方法时，首先计算出元素的直接哈希地址 H ( key ) ，如果该存储单元已被其他元素占用，则继续查看地址为 H ( key ) + d 2 的存储单元，如此重复直至找到某个存储单元为空时，将关键字为 key 的数据元素存放到该单元。<br>增量 d 可以有不同的取法，并根据其取法有不同的称呼：<br>（ 1 ） d i ＝ 1 ， 2 ， 3 ， …… 线性探测再散列；<br>（ 2 ） d i ＝ 1^2 ，－ 1^2 ， 2^2 ，－ 2^2 ， k^2， -k^2…… 二次探测再散列；<br>（ 3 ） d i ＝ 伪随机序列 伪随机再散列； </p><p>例1设有哈希函数 H ( key ) = key mod 7 ，哈希表的地址空间为 0 ～ 6 ，对关键字序列（ 32 ， 13 ， 49 ， 55 ， 22 ， 38 ， 21 ）按线性探测再散列和二次探测再散列的方法分别构造哈希表。<br>解：<br>（ 1 ）线性探测再散列：<br>32 ％ 7 = 4 ； 13 ％ 7 = 6 ； 49 ％ 7 = 0 ；<br>55 ％ 7 = 6 发生冲突，下一个存储地址（ 6 ＋ 1 ）％ 7 ＝ 0 ，仍然发生冲突，再下一个存储地址：（ 6 ＋ 2 ）％ 7 ＝ 1 未发生冲突，可以存入。<br>22 ％ 7 ＝ 1 发生冲突，下一个存储地址是：（ 1 ＋ 1 ）％ 7 ＝ 2 未发生冲突；<br>38 ％ 7 ＝ 3 ；<br>21 ％ 7 ＝ 0 发生冲突，按照上面方法继续探测直至空间 5 ，不发生冲突，所得到的哈希表对应存储位置：<br>下标： 0 1 2 3 4 5 6 </p><p>49 55 22 38 32 21 13 </p><p>当然还有其他的方法比如<code>再哈希法</code>，Coalesced Hashing法（综合了Seperate Chainging 和 Open Addressiing）等。</p><h3 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DataItem</span> </span>&#123; <span class="comment">//数据                             </span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">int</span> iData;    <span class="comment">// data item (key)  </span></span><br><span class="line">  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">DataItem</span><span class="params">(<span class="keyword">int</span> ii)</span> </span>&#123;   </span><br><span class="line">    iData = ii;   </span><br><span class="line">  &#125;  </span><br><span class="line">      <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getKey</span><span class="params">()</span></span>&#123;  </span><br><span class="line">       <span class="keyword">return</span> iData;   </span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   &#125;    </span><br><span class="line">  </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashTable</span></span>&#123;<span class="comment">//数组实现的哈希表，开放地址法之线性探测  </span></span><br><span class="line">   <span class="keyword">private</span> DataItem[] hashArray; <span class="comment">//存放数据的数组  </span></span><br><span class="line">   <span class="keyword">private</span> <span class="keyword">int</span> arraySize;  </span><br><span class="line">   <span class="keyword">private</span> DataItem nonItem; <span class="comment">//用作删除标志  </span></span><br><span class="line">  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="title">HashTable</span><span class="params">(<span class="keyword">int</span> size)</span> </span>&#123;<span class="comment">//构造函数  </span></span><br><span class="line">      arraySize = size;  </span><br><span class="line">      hashArray = <span class="keyword">new</span> DataItem[arraySize];  </span><br><span class="line">      nonItem = <span class="keyword">new</span> DataItem(-<span class="number">1</span>);   <span class="comment">// deleted item key is -1  </span></span><br><span class="line">   &#125;  </span><br><span class="line">  </span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">displayTable</span><span class="params">()</span></span>&#123;<span class="comment">//显示哈希表  </span></span><br><span class="line">      System.out.print(<span class="string">"Table: "</span>);  </span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;arraySize; j++)  </span><br><span class="line">         &#123;  </span><br><span class="line">         <span class="keyword">if</span>(hashArray[j] != <span class="keyword">null</span>)  </span><br><span class="line">            System.out.print(hashArray[j].getKey() + <span class="string">" "</span>);  </span><br><span class="line">         <span class="keyword">else</span>  </span><br><span class="line">            System.out.print(<span class="string">"** "</span>);  </span><br><span class="line">         &#125;  </span><br><span class="line">      System.out.println(<span class="string">""</span>);  </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//哈希函数  </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">hashFunc</span><span class="params">(<span class="keyword">int</span> key)</span>  </span></span><br><span class="line"><span class="function">      </span>&#123;  </span><br><span class="line">      <span class="keyword">return</span> key % arraySize;        </span><br><span class="line">      &#125;  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//在哈希表中插入数据  </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">insert</span><span class="params">(DataItem item)</span></span>&#123;  </span><br><span class="line">      <span class="keyword">int</span> key = item.getKey();      <span class="comment">// 获取数据的键值  </span></span><br><span class="line">      <span class="keyword">int</span> hashVal = hashFunc(key);  <span class="comment">// 计算其哈希值  </span></span><br><span class="line">                                    </span><br><span class="line">      <span class="keyword">while</span>(hashArray[hashVal] != <span class="keyword">null</span> &amp;&amp; hashArray[hashVal].getKey() != -<span class="number">1</span>)&#123;  </span><br><span class="line">         ++hashVal;                 <span class="comment">// 插入位置被占，线性探测下一位置  </span></span><br><span class="line">         hashVal %= arraySize;   <span class="comment">// 不让超过数组的大小  </span></span><br><span class="line">     &#125;  </span><br><span class="line">      hashArray[hashVal] = item;  <span class="comment">// 找到空位后插入  </span></span><br><span class="line">   &#125;    </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//在哈希表中删除  </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> DataItem <span class="title">delete</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;  </span><br><span class="line">      <span class="keyword">int</span> hashVal = hashFunc(key);  <span class="comment">// 计算其哈希值  </span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">while</span>(hashArray[hashVal] != <span class="keyword">null</span>)&#123;                               </span><br><span class="line">         <span class="keyword">if</span>(hashArray[hashVal].getKey() == key)&#123;  </span><br><span class="line">            DataItem temp = hashArray[hashVal]; <span class="comment">// 记录已删除的数据  </span></span><br><span class="line">            hashArray[hashVal] = nonItem;       <span class="comment">// 删除它  </span></span><br><span class="line">            <span class="keyword">return</span> temp;                          </span><br><span class="line">         &#125;  </span><br><span class="line">         ++hashVal;  <span class="comment">// 到下一单元找  </span></span><br><span class="line">         hashVal %= arraySize;      </span><br><span class="line">      &#125;  </span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;    <span class="comment">// 没有找到要删除的数据  </span></span><br><span class="line">      &#125;   </span><br><span class="line">  </span><br><span class="line">   <span class="comment">//在哈希表中查找  </span></span><br><span class="line">   <span class="function"><span class="keyword">public</span> DataItem <span class="title">find</span><span class="params">(<span class="keyword">int</span> key)</span> </span>&#123;  </span><br><span class="line">      <span class="keyword">int</span> hashVal = hashFunc(key);  <span class="comment">//哈希这个键  </span></span><br><span class="line">  </span><br><span class="line">      <span class="keyword">while</span>(hashArray[hashVal] != <span class="keyword">null</span>) &#123; <span class="comment">// 直到空的单元                       </span></span><br><span class="line">         <span class="keyword">if</span>(hashArray[hashVal].getKey() == key)  </span><br><span class="line">            <span class="keyword">return</span> hashArray[hashVal];   <span class="comment">// 找到  </span></span><br><span class="line">         ++hashVal;                 <span class="comment">// 去下一单元找  </span></span><br><span class="line">         hashVal %= arraySize;      <span class="comment">// 不让超过数组的大小  </span></span><br><span class="line">         &#125;  </span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;  <span class="comment">// 没有找到  </span></span><br><span class="line">  &#125;  </span><br><span class="line">  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.cnblogs.com/vincentme/p/7920237.html" target="_blank" rel="noopener">https://www.cnblogs.com/vincentme/p/7920237.html</a></p><p><a href="https://blog.csdn.net/w_fenghui/article/details/2010387" target="_blank" rel="noopener">https://blog.csdn.net/w_fenghui/article/details/2010387</a> </p><p><a href="https://128kj.iteye.com/blog/1744810" target="_blank" rel="noopener">https://128kj.iteye.com/blog/1744810</a></p>]]></content>
    
    <summary type="html">
    
      哈希表的相关学习：&lt;Excerpt in index | 首页摘要&gt;
    
    </summary>
    
    
      <category term="数据结构" scheme="https://www.hphblog.cn/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
      <category term="哈希表" scheme="https://www.hphblog.cn/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/"/>
    
  </entry>
  
</feed>
