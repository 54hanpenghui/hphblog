[{"title":"Flink运行架构","url":"/2020/01/20/Flink运行架构/","content":"\n {{ \"Flink运行架构学习\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n我们已经知道了IDEA下如何快速搭建一个Flink的项目，让我们来了解一下Flink的架构。在了解架构之前，先熟悉几个名词。\n\n## 组件\n\n### JobManager\n\n1. 控制一个应用程序执行的主进程，每个应用程序都会被一个不同的JobManager所控制。\n\n2. JobManager会先接收到应用程序，应用程序包括：作业图(JobGraph)、逻辑数据流图和打包的所有类库和其他资源的Jar包。\n\n3. JobManager会把JobGraph转换成一个物理层面的数据流图，这个图被叫做“执行图”（ExecutionGraph）,包含了所有可以并发执行的任务。\n\n4. JobManager会向资源管理器（ResourceManager）请求执行任务必要的资源，也就是任务管理器上的slot。一旦获取到足够的资源，就会将执行图分发到真正运行的TaskManager上。\n\n### TaskManager\n\n1. 每一个TaskManager都包含了一定数量的插槽（slots）。插槽的数量限制了TaskManager能够执行的任务数量。\n\n2. 启动后，TaskManager回向资源管理器注册它的插槽，收到资源管理器的指令后，TaskManager就会将一个或者多个插槽提供给JobManager调用。JobManager就可以向插槽分配任务(tasks)来执行了。\n\n3. 在执行过程中，一个TaskManager可以跟其他运行同一个应用程序的TaskManager交换数据。\n\n### ResourceManager\n\n1. 负责管理任务管理器(TaskManager)的插槽（slot）,TaskManager插槽是Flink中定义的处理资源单元。\n\n2. Flink为不同的环境和资源管理工具提供了不同的资源管理器，比如yarn，mesos，k8s\n\n3. 当jobManager申请插槽资源时，resourceManager会将有空闲的插槽的TaskManager分配给JobManager。如果ResourceManager没有足够的插槽来满足jobManager的请求，它还可以向资源提供平台发起会话，提供启动TaskManager进程的容器。\n\n### Dispatcher\n\n1. 可以跨作业运行，它为应用提交提供了rest接口。\n\n2. 当一个应用被提交执行时，分发器就会启动并将应用移交给一个jobManager。\n\n3. Dispatcher也会启动一个web UI，用来方便展示和监控作业的执行信息。\n\n4. Dispatcher在架构中可能并不是必须的，这取决于应用提交运行的方式。\n\n\n\n## YARN 提交\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200120120550.png)\n\nFlink任务提交后，Client向HDFS上传Flink的Jar包和配置，之后向Yarn ResourceManager提交任务，ResourceManager分配Container资源并通知对应的NodeManager启动ApplicationMaster，ApplicationMaster启动后加载Flink的Jar包和配置构建环境，然后启动JobManager，之后ApplicationMaster向ResourceManager申请资源启动TaskManager，ResourceManager分配Container资源后，由ApplicationMaster通知资源所在节点的NodeManager启动TaskManager，NodeManager加载Flink的Jar包和配置构建环境并启动TaskManager，TaskManager启动后向JobManager发送心跳包，并等待JobManager向其分配任务。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200120112358.png)\n\n当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。Client提交任务给JobManager，JobManager调度任务到各个TaskManager执行，TaskManager将心跳和统计信息汇报给JobManager。TaskManager之间以流的形式进行数据的传输，JobManger,TaskManager,Client均为独立的JVM进程。\n\n- **Client** 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可），负责接收用户的程序代码，为其创建数据流，将数据流提交给JobManager。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。\n- **JobManager** 主要负责调度 Job 并协调 Task 做 checkpoint，从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。\n- **TaskManager** 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。\n\n\n\n","tags":["Flink"],"categories":["大数据"]},{"title":"Flink初识","url":"/2020/01/05/Flink初识/","content":"\n {{ \"Flink简介与安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nFlink起源于2010~2014的柏林工业大学、柏林洪堡大学、哈索·普拉特纳研究所联名发起的Stratosphere项目，该项目于2014年捐赠给了Apache软件基金会。2014年12月成为Apache软件基金会的顶级项目。\n\n在德语中Flink表示快速和灵巧。\n\nFlink  Log：\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200105222031.png)\n\n与Spark相比Flink是更加纯粹的流式计算，对于Spark来讲、Spark本质上还是基于批计算、即使是Spark Streaming 也是基于微批次计算。\n\n## 快速体验\n\n安装好Maven执行下面这条命令我们就可以快速开发Flink了\n\n```\nmvn archetype:generate -DarchetypeGroupId=org.apache.flink -DarchetypeArtifactId=flink-quickstart-scala -DarchetypeVersion=1.7.0  -DarchetypeCatlog=local\n```\n\n这样会生成两个Scala类，流作业和批作业。\n\n### 批作业\n\n```scala\n\npackage com.hph.flink\n\nimport org.apache.flink.api.scala.ExecutionEnvironment\n\n\nobject BatchJob {\n\n  def main(args: Array[String]) {\n    // set up the batch execution environment\n    val env = ExecutionEnvironment.getExecutionEnvironment\n\n    val dataset= env.readTextFile(\"E:\\\\Words.txt\")\n\n    import org.apache.flink.api.scala._\n    val result =     dataset.flatMap(_.split(\" \")).map((_,1)).groupBy(0).sum(1).print()\n    \n  }\n}\n```\n\n如果运行出现这种情况，\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106192645.png)\n\n我们需要把IDEA中的![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200112220745.png)勾选上去。\n\n\n\n再次运行结果如图所示：\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106192820.png)\n\n这样我们就轻松的完成了MapReduce中的WordCount。\n\n文件文本如下\n\n```\nHadoop\nSpark\nFlink\nFlink\nSpark\nHadoop\nSpark\nhphblog\nClickhouse\nI love Flink\n```\n\n### 流作业\n\n```scala\npackage com.hph.flink\n\nimport org.apache.flink.streaming.api.scala._\n\n\nobject StreamingJob {\n  def main(args: Array[String]) {\n\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n    var StreamData  = env.socketTextStream(\"58.87.70.124\",9999)\n    var result = StreamData.flatMap(_.split(\" \")).map((_,1)).keyBy(0).sum(1)\n\n    result.print()\n\n    env.execute(\"Flink Streaming Scala API Skeleton\")\n  }\n}\n```\n\n这段代码则是监控hadoop102这台服务器端口为9999的数据信息。\n\n运行一下\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/FlinkStreamingWordCount.gif)\n\n我们轻松的实现了流的有状态统计而且和Spark Streaming 相比Flink 显得更加实时。什么是所谓的状态呢？所谓状态就是计算过程中产生的中间计算结果，每次计算新的数据进入到流式系统中都是基于中间状态结果的基础上进行运算，最终产生正确的统计结果。基于有状态计算的方式最大的优势是不需要将原始数据重新从外部存储中拿出来，从而进行全量计算，因为这种计算方式的代价可能是非常高的。从另一个角度讲，用户无须通过调度和协调各种批量计算工具，从数据仓库中获取数据统计结果，然后再落地存储，这些操作全部都可以基于流式计算完成，可以极大地减轻系统对其他框架的依赖，减少数据计算过程中的时间损耗以及硬件存储。\n\n## 集群安装\n\n由于虚拟机安装过Haoop Spark 所以我们选择安装的时候可以选择安装与我们Hadoop版本匹配的安装包。\n\n```shell\ntar -zxvf flink-1.7.2-bin-hadoop27-scala_2.11.tgz  -C /opt/module/\n```\n\n切换到 `/opt/module/flink-1.7.2`下，执行\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106214003.png)\n\n我们可以看到 Flink无需任何配置就可以完成安装，当然这个只是单机版的。访问 hadoop102:8081。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106214157.png)\n\n这就是Flink的Web界面。\n\n那么完全集群模式 集成YARN怎么安装呢。\n\n对于Flink来说集群安装十分简单。只需要更改**flink-conf.yaml** 和**slave** 文件即可\n\n修改f**flink-conf.yaml**文件\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106214803.png)\n\n修改salav文件\n\n```properties\nhadoop102\nhadoop103\nhadoop104\n```\n\n修改masters文件\n\n```properties\nhadoop102:8081\n```\n\n同步脚本如下\n\n```shell\n#!/bin/bash\n#1 获取输入参数个数，如果没有参数，直接退出\npcount=$#\nif((pcount==0)); then\necho no args;\nexit;\nfi\n\n#2 获取文件名称\np1=$1\nfname=`basename $p1`\necho fname=$fname\n\n#3 获取上级目录到绝对路径\npdir=`cd -P $(dirname $p1); pwd`\necho pdir=$pdir\n\n#4 获取当前用户名称\nuser=`whoami`\n\n#5 循环\nfor((host=102; host<105; host++)); do\n        echo ------------------- hadoop$host --------------\n        rsync -rvl $pdir/$fname $user@hadoop$host:$pdir\ndone\n```\n\n同步发送flink配置\n\n```shell\n xsync /opt/module/flink-1.7.2/\n```\n\n重新启动Flink\n\n```shell\nbin/start-cluster.sh\n```\n\n打开web界面\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106215411.png)\n\n集群安装完成。\n\n## 作业提交\n\n### shell \n\n执行`bin/start-scala-shell.sh local ` 我们就可以进入类似于Spark-shell的界面，这里也出现了可爱的小松鼠\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200113220903.png)\n\n在shell中执行以下命令\n\n```scala\n \t//绑定端口数据\n    var dataStream = senv.socketTextStream(\"hadoop102\",9999)\n      \n    //处理数据\n    import  org.apache.flink.api.scala._\n    var result =dataStream.flatMap(_.split(\" \")).map((_,1)).keyBy(0).sum(1)\n\n    result.print()\n    senv.execute(\"Stream Job\")\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/flinkStreamingJob.gif)\n\nWeb界面如下\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200113222800.png)\n\n看到了Flink的单机版的Job作业调试如此方便。和Spark-shell一样如此友好，下面我们可以尝试一些常规的生产中的经常使用到的Jar包提交的方式 。\n\n### Mavn依赖\n\n创建Maven项目pom包如下\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.hph.flink.</groupId>\n    <artifactId>FlinkJob</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-scala_2.11</artifactId>\n            <version>1.7.2</version>\n        </dependency>\n        <!-- https://mvnrepository.com/artifact/org.apache.flink/flink-streaming-scala -->\n        <dependency>\n            <groupId>org.apache.flink</groupId>\n            <artifactId>flink-streaming-scala_2.11</artifactId>\n            <version>1.7.2</version>\n        </dependency>\n\n        <dependency>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n            <version>2.7.2</version>\n        </dependency>\n    </dependencies>\n\n    <build>\n        <plugins>\n            <!-- 该插件用于将Scala代码编译成class文件 -->\n            <plugin>\n                <groupId>net.alchim31.maven</groupId>\n                <artifactId>scala-maven-plugin</artifactId>\n                <version>3.4.6</version>\n                <executions>\n                    <execution>\n                        <!-- 声明绑定到maven的compile阶段 -->\n                        <goals>\n                            <goal>compile</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <version>3.0.0</version>\n                <configuration>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n                <executions>\n                    <execution>\n                        <id>make-assembly</id>\n                        <phase>package</phase>\n                        <goals>\n                            <goal>single</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\n### 流作业\n\n```scala\npackage com.hph.job\n\nimport org.apache.flink.api.java.utils.ParameterTool\nimport org.apache.flink.api.scala.ExecutionEnvironment\nimport org.apache.flink.streaming.api.scala.StreamExecutionEnvironment\n\nobject StreamJob {\n  def main(args: Array[String]): Unit = {\n\n    // 从外部命令中获取参数\n    val params: ParameterTool =  ParameterTool.fromArgs(args)\n    val host: String = params.get(\"host\")\n    val port: Int = params.getInt(\"port\")\n\n    // 创建流处理环境\n    val env = StreamExecutionEnvironment.getExecutionEnvironment\n\n    //绑定端口数据\n    var dataStream = env.socketTextStream(\"hadoop102\",9999)\n      \n    //处理数据\n    import  org.apache.flink.api.scala._\n    var result =dataStream.flatMap(_.split(\" \")).map((_,1)).keyBy(0).sum(1)\n\n    result.print()\n    env.execute(\"Stream Job\")\n  }\n}\n\n```\n\n### WebUI 提交\n\n我们把打好的jar包提交到WebUI上看一下\n\n\n\n提交jar包\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106224233.png)\n\n指定一下类名和参数\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106225643.png)\n\n提交作业后则会\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Flink/20200113220036.png)\n\n(因为最近电脑出现了问题，这张图今天给大家补上的实在不好意思)\n\n我们在hadoop102服务器上输入几个字符\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106225805.png)\n\n你一定会很好奇结果出现在了哪里，我想你已经猜到了就在Task Manage中这里的TM就相当于干活的人，也就相当于Spark中的Executor。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106230151.png)\n\n就这样flink 的流式wordcount就部署起来了。\n\n### 命令行提交\n\n当然我们也可以使用命令行的方式提交作业这样做起来会更酷。\n\n```shell\n ./flink run -c com.hph.job.StreamJob  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar --host hadoop102 --port 9999\n```\n\n我们刚才取消掉了那个流式任务现在看一下这个任务\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106231156.png)\n\n输入几个数据测试一下\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200106231334.png)\n\nTaskManager下我们发现了刚才的输入的数据计算的结果。\n\n###  Yarn提交\n\n````shell\n./flink run  -m yarn-cluster -c  com.hph.job.StreamJob  /ext/flink0503-1.0-SNAPSHOT.jar  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar --host hadoop102 --port 9999\n````\n\n然而一直再报出\n\n```\n2020-01-06 23:27:44,167 INFO  org.apache.flink.yarn.AbstractYarnClusterDescriptor           - Deployment took more than 60 seconds. Please check if the requested resources are available in the YARN cluster\n```\n\n这是需要我们调整分配的资源因为虚拟机的资源不够所以导致无法申请到相应的资源\n\n```shell\n./flink run -m yarn-cluster -nm FinkStreamWordCount  -c  com.hph.job.StreamJob  /opt/module/jars/FlinkJob-1.0-SNAPSHOT-jar-with-dependencies.jar    -n 1 -s 1 -jm 768 -tm 768  --host hadoop102 --port 9999\n```\n\n我们把资源调小，在YARN界面上就可以看到\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200107001803.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200107002024.png)\n\n点击ApplicationMaster即可进入Flink Web UI\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com//大数据/Flink/20200107002328.png)\n\n输出结果如上所述。\n\n这样Flink的搭建以及提交作业到Yarn就基本完成了。\n\n\n\n","tags":["Flink"],"categories":["大数据"]},{"title":"Spark内核解析3","url":"/2019/06/10/Spark内核解析3/","content":"\n {{ \"Spark的交互流程 - 应用提交\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n\n## 步骤\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190610123821.png)\n\n`橙色：提交用户Spark程序`\n1) 用户提交一个Spark程序，主要的流程如下所示：\n2) 用户spark-submit脚本提交一个Spark程序，会创建一个ClientEndpoint对象，该对象负责与Master通信交互\n3) ClientEndpoint向Master发送一个RequestSubmitDriver消息，表示提交用户程序\n4) Master收到RequestSubmitDriver消息，向ClientEndpoint回复SubmitDriverResponse，表示用户程序已经完成注册\nClientEndpoint向Master发送RequestDriverStatus消息，请求Driver状态\n5) 如果当前用户程序对应的Driver已经启动，则ClientEndpoint直接退出，完成提交用户程序\n紫色：启动Driver进程\n当用户提交用户Spark程序后，需要启动Driver来处理用户程序的计算逻辑，完成计算任务，这时Master协调需要启动一个Driver，具体流程如下所示：\n1) Maser内存中维护着用户提交计算的任务Application，每次内存结构变更都会触发调度，向Worker发送LaunchDriver请求\n2) Worker收到LaunchDriver消息，会启动一个DriverRunner线程去执行LaunchDriver的任务\n3) DriverRunner线程在Worker上启动一个新的JVM实例，该JVM实例内运行一个Driver进程，该Driver会创建SparkContext对象\n`红色：注册Application`\nDirver启动以后，它会创建SparkContext对象，初始化计算过程中必需的基本组件，并向Master注册Application，流程描述如下：\n1) 创建SparkEnv对象，创建并管理一些基本组件\n2) 创建TaskScheduler，负责Task调度\n3) 创建StandaloneSchedulerBackend，负责与ClusterManager进行资源协商\n4) 创建DriverEndpoint，其它组件可以与Driver进行通信\n5) 在StandaloneSchedulerBackend内部创建一个StandaloneAppClient，负责处理与Master的通信交互\n6) StandaloneAppClient创建一个ClientEndpoint，实际负责与Master通信\n7) ClientEndpoint向Master发送RegisterApplication消息，注册Application\n8) Master收到RegisterApplication请求后，回复ClientEndpoint一个RegisteredApplication消息，表示已经注册成功\n`蓝色：启动Executor进程`\n1)Master向Worker发送LaunchExecutor消息，请求启动Executor；同时Master会向Driver发送ExecutorAdded消息，表示Master已经新增了一个Executor（此时还未启动）\n2) Worker收到LaunchExecutor消息，会启动一个ExecutorRunner线程去执行LaunchExecutor的任务\n3) Worker向Master发送ExecutorStageChanged消息，通知Executor状态已发生变化\n4) Master向Driver发送ExecutorUpdated消息，此时Executor已经启动\n`粉色：启动Task执行`\n1) StandaloneSchedulerBackend启动一个DriverEndpoint\n2) DriverEndpoint启动后，会周期性地检查Driver维护的Executor的状态，如果有空闲的Executor便会调度任务执行\n3) DriverEndpoint向TaskScheduler发送Resource Offer请求\n4) 如果有可用资源启动Task，则DriverEndpoint向Executor发送LaunchTask请求\n5) Executor进程内部的CoarseGrainedExecutorBackend调用内部的Executor线程的launchTask方法启动Task\n6) Executor线程内部维护一个线程池，创建一个TaskRunner线程并提交到线程池执行\n`绿色：Task运行完成`\n1) Executor进程内部的Executor线程通知CoarseGrainedExecutorBackend，Task运行完成\n2) CoarseGrainedExecutorBackend向DriverEndpoint发送StatusUpdated消息，通知Driver运行的Task状态发生变更\n3) StandaloneSchedulerBackend调用TaskScheduler的updateStatus方法更新Task状态\n4) StandaloneSchedulerBackend继续调用TaskScheduler的resourceOffers方法，调度其他任务运行\n\n## 应用提交\n\n### SparkSubumit\n\n提交任务需要用到SparkSubumit这个类我们先看一下`main`方法\n\n```scala\n  def main(args: Array[String]): Unit = {\n    val appArgs = new SparkSubmitArguments(args) //解析参数\n    if (appArgs.verbose) {\n      // scalastyle:off println\n      printStream.println(appArgs)\n      // scalastyle:on println\n    }\n    appArgs.action match {\n      case SparkSubmitAction.SUBMIT => submit(appArgs) //提交\n      case SparkSubmitAction.KILL => kill(appArgs) //杀死\n      case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs) //查看状态\n    }\n  }\n```\n\n我们可以看一下SparkSubmitArguments里面的参数是什么\n\n```scala\n  var master: String = null\n  var deployMode: String = null\n  var executorMemory: String = null\n  var executorCores: String = null\n  var totalExecutorCores: String = null\n  var propertiesFile: String = null\n  var driverMemory: String = null\n  var driverExtraClassPath: String = null\n  var driverExtraLibraryPath: String = null\n  var driverExtraJavaOptions: String = null\n  var queue: String = null\n  var numExecutors: String = null\n  var files: String = null\n  var archives: String = null\n  var mainClass: String = null\n  var primaryResource: String = null\n  var name: String = null\n  var childArgs: ArrayBuffer[String] = new ArrayBuffer[String]()\n  var jars: String = null\n  var packages: String = null\n  var repositories: String = null\n  var ivyRepoPath: String = null\n  var packagesExclusions: String = null\n  var verbose: Boolean = false\n  var isPython: Boolean = false\n  var pyFiles: String = null\n  var isR: Boolean = false\n  var action: SparkSubmitAction = null\n  val sparkProperties: HashMap[String, String] = new HashMap[String, String]()\n  var proxyUser: String = null\n  var principal: String = null\n  var keytab: String = null\n\n  // Standalone cluster mode only\n  var supervise: Boolean = false\n  var driverCores: String = null\n  var submissionToKill: String = null\n  var submissionToRequestStatusFor: String = null\n  var useRest: Boolean = true // used internally\n```\n\n这些参数在运行的时候我们都可以指定比如\n\n```shell\n./bin/spark-submit --class org.apache.spark.examples.SparkPi \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 1g \\\n--executor-memory 1g \\\n--executor-cores 1 \\\n--queue thequeue \\\nexamples/target/scala-2.11/jars/spark-examples*.jar 10\n```\n\n\n\n| 参数名                 | 参数说明                                                     |\n| ---------------------- | ------------------------------------------------------------ |\n| --master               | master 的地址，提交任务到哪里执行，例如 spark://host:port,  yarn,  local |\n| --deploy-mode          | 在本地 (client) 启动 driver 或在 cluster 上启动，默认是 client |\n| --class                | 应用程序的主类，仅针对 java 或 scala 应用                    |\n| --name                 | 应用程序的名称                                               |\n| --jars                 | 用逗号分隔的本地 jar 包，设置后，这些 jar 将包含在 driver 和 executor 的 classpath 下 |\n| --packages             | 包含在driver 和executor 的 classpath 中的 jar 的 maven 坐标  |\n| --exclude-packages     | 为了避免冲突 而指定不包含的 package                          |\n| --repositories         | 远程 repository                                              |\n| --conf PROP=VALUE      | 指定 spark 配置属性的值， 例如 -conf spark.executor.extraJavaOptions=\"-XX:MaxPermSize=256m\" |\n| --properties-file      | 加载的配置文件，默认为 conf/spark-defaults.conf              |\n| --driver-memory        | Driver内存，默认 1G                                          |\n| --driver-java-options  | 传给 driver 的额外的 Java 选项                               |\n| --driver-library-path  | 传给 driver 的额外的库路径                                   |\n| --driver-class-path    | 传给 driver 的额外的类路径                                   |\n| --driver-cores         | Driver 的核数，默认是1。在 yarn 或者 standalone 下使用       |\n| --executor-memory      | 每个 executor 的内存，默认是1G                               |\n| --total-executor-cores | 所有 executor 总共的核数。仅仅在 mesos 或者 standalone 下使用 |\n| --num-executors        | 启动的 executor 数量。默认为2。在 yarn 下使用                |\n| --executor-core        | 每个 executor 的核数。在yarn或者standalone下使用             |\n\n### submit\n\n```scala\n  @tailrec\n  private def submit(args: SparkSubmitArguments): Unit = {\n      //根据参数准备提交应用时所需的环境\n    val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)\n\n    def doRunMain(): Unit = { \n      if (args.proxyUser != null) {\n        val proxyUser = UserGroupInformation.createProxyUser(args.proxyUser,\n          UserGroupInformation.getCurrentUser())\n        try {\n          proxyUser.doAs(new PrivilegedExceptionAction[Unit]() {\n            override def run(): Unit = {\n              runMain(childArgsm, childClasspath, sysProps, childMainClass, args.verbose)\n            }\n          })\n        } catch {\n          case e: Exception =>\n            // Hadoop's AuthorizationException suppresses the exception's stack trace, which\n            // makes the message printed to the output by the JVM not very helpful. Instead,\n            // detect exceptions with empty stack traces here, and treat them differently.\n            if (e.getStackTrace().length == 0) {\n              // scalastyle:off println\n              printStream.println(s\"ERROR: ${e.getClass().getName()}: ${e.getMessage()}\")\n              // scalastyle:on println\n              exitFn(1)\n            } else {\n              throw e\n            }\n        }\n      } else {\n        runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)\n      }\n    }\n\n     // In standalone cluster mode, there are two submission gateways:\n     //   (1) The traditional RPC gateway using o.a.s.deploy.Client as a wrapper\n     //   (2) The new REST-based gateway introduced in Spark 1.3\n     // The latter is the default behavior as of Spark 1.3, but Spark submit will fail over\n     // to use the legacy gateway if the master endpoint turns out to be not a REST server.\n    if (args.isStandaloneCluster && args.useRest) {\n      try {\n        // scalastyle:off println\n        printStream.println(\"Running Spark using the REST application submission protocol.\")\n        // scalastyle:on println\n        doRunMain()  //调用doRunMain\n      } catch {\n        // Fail over to use the legacy submission gateway\n        case e: SubmitRestConnectionException =>\n          printWarning(s\"Master endpoint ${args.master} was not a REST server. \" +\n            \"Falling back to legacy submission gateway instead.\")\n          args.useRest = false\n          submit(args)\n      }\n    // In all other modes, just run the main class as prepared\n    } else {\n      doRunMain() //doRunMain\n    }\n  }\n```\n\n### runMain\n\n```scala\n private def runMain(\n      childArgs: Seq[String],\n      childClasspath: Seq[String],\n      sysProps: Map[String, String],\n      childMainClass: String,\n      verbose: Boolean): Unit = {\n    // scalastyle:off println\n    if (verbose) { //如果打开调试 则会输出\n      printStream.println(s\"Main class:\\n$childMainClass\")\n      printStream.println(s\"Arguments:\\n${childArgs.mkString(\"\\n\")}\")\n      printStream.println(s\"System properties:\\n${sysProps.mkString(\"\\n\")}\")\n      printStream.println(s\"Classpath elements:\\n${childClasspath.mkString(\"\\n\")}\")\n      printStream.println(\"\\n\")\n    }\n    // scalastyle:on println\n\n    val loader =\n      if (sysProps.getOrElse(\"spark.driver.userClassPathFirst\", \"false\").toBoolean) {\n        new ChildFirstURLClassLoader(new Array[URL](0),\n          Thread.currentThread.getContextClassLoader)\n      } else {\n        new MutableURLClassLoader(new Array[URL](0),\n          Thread.currentThread.getContextClassLoader)\n      }\n    Thread.currentThread.setContextClassLoader(loader)\n\n    for (jar <- childClasspath) {\n      addJarToClasspath(jar, loader) //添加jar包到类路径下\n    }\n\n    for ((key, value) <- sysProps) {\n      System.setProperty(key, value) //sysProps 添加到 System\n    }\n\n    var mainClass: Class[_] = null \n\n    try {\n      mainClass = Utils.classForName(childMainClass)  //利用了反射childMainClass得到mainClass\n    } catch {\n      case e: ClassNotFoundException => //异常ClassNotFoundException\n        e.printStackTrace(printStream)\n        if (childMainClass.contains(\"thriftserver\")) {\n          // scalastyle:off println\n          printStream.println(s\"Failed to load main class $childMainClass.\")\n          printStream.println(\"You need to build Spark with -Phive and -Phive-thriftserver.\")\n          // scalastyle:on println\n        }\n        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)\n      case e: NoClassDefFoundError =>\n        e.printStackTrace(printStream)\n        if (e.getMessage.contains(\"org/apache/hadoop/hive\")) {\n          // scalastyle:off println\n          printStream.println(s\"Failed to load hive class.\")\n          printStream.println(\"You need to build Spark with -Phive and -Phive-thriftserver.\")\n          // scalastyle:on println\n        }\n        System.exit(CLASS_NOT_FOUND_EXIT_STATUS)\n    }\n\n    // SPARK-4170   BUG 尚未解决的  缺陷跟踪\n    if (classOf[scala.App].isAssignableFrom(mainClass)) {\n      printWarning(\"Subclasses of scala.App may not work correctly. Use a main() method instead.\")\n    } \n\t//get 一个  main 方法\n    val mainMethod = mainClass.getMethod(\"main\", new Array[String](0).getClass)\n    if (!Modifier.isStatic(mainMethod.getModifiers)) {\n      throw new IllegalStateException(\"The main method in the given main class must be static\")\n    }\n\n    @tailrec\n    def findCause(t: Throwable): Throwable = t match {\n      case e: UndeclaredThrowableException =>\n        if (e.getCause() != null) findCause(e.getCause()) else e\n      case e: InvocationTargetException =>\n        if (e.getCause() != null) findCause(e.getCause()) else e\n      case e: Throwable =>\n        e\n    }\n\n    try {\n        //反射执行通过childArgs\n      mainMethod.invoke(null, childArgs.toArray)\n    } catch {\n      case t: Throwable =>\n        findCause(t) match {\n          case SparkUserAppException(exitCode) =>\n            System.exit(exitCode)\n\n          case t: Throwable =>\n            throw t\n        }\n    }\n  }\n```\n\n由于`mainClass`利用了反射我们需要寻找一下`childMainClass` 注意观察 submit方法中`    val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args)`我们查看一下`prepareSubmitEnvironment`\n\n### prepareSubmitEnvironment\n\n```scala\n  private[deploy] def prepareSubmitEnvironment(args: SparkSubmitArguments)\n      : (Seq[String], Seq[String], Map[String, String], String) = {\n    // Return values\n    val childArgs = new ArrayBuffer[String]()\n    val childClasspath = new ArrayBuffer[String]()\n    val sysProps = new HashMap[String, String]()\n    var childMainClass = \"\"  //目标出现\n\n    // Set the cluster manager  \n    val clusterManager: Int = args.master match {\n      case \"yarn\" => YARN\n      case \"yarn-client\" | \"yarn-cluster\" =>\n        printWarning(s\"Master ${args.master} is deprecated since 2.0.\" +\n          \" Please use master \\\"yarn\\\" with specified deploy mode instead.\")\n        YARN\n      case m if m.startsWith(\"spark\") => STANDALONE\n      case m if m.startsWith(\"mesos\") => MESOS\n      case m if m.startsWith(\"local\") => LOCAL\n      case _ =>\n        printErrorAndExit(\"Master must either be yarn or start with spark, mesos, local\")\n        -1\n    }\n\n    // Set the deploy mode; default is client mode  设置 deploy 模式\n    var deployMode: Int = args.deployMode match {\n      case \"client\" | null => CLIENT // 如果是NULL 或者 \"client\" 默认为CLIENT\n      case \"cluster\" => CLUSTER\n      case _ => printErrorAndExit(\"Deploy mode must be either client or cluster\"); -1\n    }\n\n    // Because the deprecated way of specifying \"yarn-cluster\" and \"yarn-client\" encapsulate both\n    // the master and deploy mode, we have some logic to infer the master and deploy mode\n    // from each other if only one is specified, or exit early if they are at odds.\n    if (clusterManager == YARN) {   //如果是YARN模式\n      (args.master, args.deployMode) match {\n        case (\"yarn-cluster\", null) =>\n          deployMode = CLUSTER\n          args.master = \"yarn\"\n        case (\"yarn-cluster\", \"client\") =>\n          printErrorAndExit(\"Client deploy mode is not compatible with master \\\"yarn-cluster\\\"\")\n        case (\"yarn-client\", \"cluster\") =>\n          printErrorAndExit(\"Cluster deploy mode is not compatible with master \\\"yarn-client\\\"\")\n        case (_, mode) =>\n          args.master = \"yarn\"\n      }\n\n      // Make sure YARN is included in our build if we're trying to use it\n      if (!Utils.classIsLoadable(\"org.apache.spark.deploy.yarn.Client\") && !Utils.isTesting) {\n        printErrorAndExit(\n          \"Could not load YARN classes. \" +\n          \"This copy of Spark may not have been compiled with YARN support.\")\n      }\n    }\n\n    // Update args.deployMode if it is null. It will be passed down as a Spark property later.\n    (args.deployMode, deployMode) match {\n      case (null, CLIENT) => args.deployMode = \"client\"\n      case (null, CLUSTER) => args.deployMode = \"cluster\"\n      case _ =>\n    }\n    val isYarnCluster = clusterManager == YARN && deployMode == CLUSTER\n    val isMesosCluster = clusterManager == MESOS && deployMode == CLUSTER\n\n   \n     .....\n    // A list of rules to map each argument to system properties or command-line options in\n    // each deploy mode; we iterate through these below \n    val options = List[OptionAssigner]( // 整理一下 options 把参数聚集到了一块\n\n      // All cluster managers\n      OptionAssigner(args.master, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = \"spark.master\"),\n      OptionAssigner(args.deployMode, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.submit.deployMode\"),\n      OptionAssigner(args.name, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES, sysProp = \"spark.app.name\"),\n      OptionAssigner(args.ivyRepoPath, ALL_CLUSTER_MGRS, CLIENT, sysProp = \"spark.jars.ivy\"),\n      OptionAssigner(args.driverMemory, ALL_CLUSTER_MGRS, CLIENT,\n        sysProp = \"spark.driver.memory\"),\n      OptionAssigner(args.driverExtraClassPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.driver.extraClassPath\"),\n      OptionAssigner(args.driverExtraJavaOptions, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.driver.extraJavaOptions\"),\n      OptionAssigner(args.driverExtraLibraryPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.driver.extraLibraryPath\"),\n\n      // Yarn only\n      OptionAssigner(args.queue, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.queue\"),\n      OptionAssigner(args.numExecutors, YARN, ALL_DEPLOY_MODES,\n        sysProp = \"spark.executor.instances\"),\n      OptionAssigner(args.jars, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.dist.jars\"),\n      OptionAssigner(args.files, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.dist.files\"),\n      OptionAssigner(args.archives, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.dist.archives\"),\n      OptionAssigner(args.principal, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.principal\"),\n      OptionAssigner(args.keytab, YARN, ALL_DEPLOY_MODES, sysProp = \"spark.yarn.keytab\"),\n\n      // Other options\n      OptionAssigner(args.executorCores, STANDALONE | YARN, ALL_DEPLOY_MODES,\n        sysProp = \"spark.executor.cores\"),\n      OptionAssigner(args.executorMemory, STANDALONE | MESOS | YARN, ALL_DEPLOY_MODES,\n        sysProp = \"spark.executor.memory\"),\n      OptionAssigner(args.totalExecutorCores, STANDALONE | MESOS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.cores.max\"),\n      OptionAssigner(args.files, LOCAL | STANDALONE | MESOS, ALL_DEPLOY_MODES,\n        sysProp = \"spark.files\"),\n      OptionAssigner(args.jars, LOCAL, CLIENT, sysProp = \"spark.jars\"),\n      OptionAssigner(args.jars, STANDALONE | MESOS, ALL_DEPLOY_MODES, sysProp = \"spark.jars\"),\n      OptionAssigner(args.driverMemory, STANDALONE | MESOS | YARN, CLUSTER,\n        sysProp = \"spark.driver.memory\"),\n      OptionAssigner(args.driverCores, STANDALONE | MESOS | YARN, CLUSTER,\n        sysProp = \"spark.driver.cores\"),\n      OptionAssigner(args.supervise.toString, STANDALONE | MESOS, CLUSTER,\n        sysProp = \"spark.driver.supervise\"),\n      OptionAssigner(args.ivyRepoPath, STANDALONE, CLUSTER, sysProp = \"spark.jars.ivy\")\n    )\n\n    // In client mode, launch the application main class directly\n    // In addition, add the main application jar and any added jars (if any) to the classpath\n    // Also add the main application jar and any added jars to classpath in case YARN client\n    // requires these jars.\n    if (deployMode == CLIENT || isYarnCluster) {\n      childMainClass = args.mainClass\n      if (isUserJar(args.primaryResource)) {\n        childClasspath += args.primaryResource\n      }\n      if (args.jars != null) { childClasspath ++= args.jars.split(\",\") }\n    }\n\n    if (deployMode == CLIENT) {\n      if (args.childArgs != null) { childArgs ++= args.childArgs }\n    }\n\n    // Map all arguments to command-line options or system properties for our chosen mode\n    for (opt <- options) {\n      if (opt.value != null &&\n          (deployMode & opt.deployMode) != 0 &&\n          (clusterManager & opt.clusterManager) != 0) {\n        if (opt.clOption != null) { childArgs += (opt.clOption, opt.value) }\n        if (opt.sysProp != null) { sysProps.put(opt.sysProp, opt.value) }\n      }\n    }\n\n    // Add the application jar automatically so the user doesn't have to call sc.addJar\n    // For YARN cluster mode, the jar is already distributed on each node as \"app.jar\"\n    // For python and R files, the primary resource is already distributed as a regular file\n    if (!isYarnCluster && !args.isPython && !args.isR) {\n      var jars = sysProps.get(\"spark.jars\").map(x => x.split(\",\").toSeq).getOrElse(Seq.empty)\n      if (isUserJar(args.primaryResource)) {\n        jars = jars ++ Seq(args.primaryResource)\n      }\n      sysProps.put(\"spark.jars\", jars.mkString(\",\"))\n    }\n\n    // In standalone cluster mode, use the REST client to submit the application (Spark 1.3+).\n    // All Spark parameters are expected to be passed to the client through system properties.\n    if (args.isStandaloneCluster) { //如果使用的是Standalone集群模式\n      if (args.useRest) {  //如果用了useRest\n        childMainClass = \"org.apache.spark.deploy.rest.RestSubmissionClient\" \n        childArgs += (args.primaryResource, args.mainClass)\n      } else { //如果没有用如useRest\n        // In legacy standalone cluster mode, use Client as a wrapper around the user class\n        childMainClass = \"org.apache.spark.deploy.Client\"  //这个类使用到了Client 类\n        if (args.supervise) { childArgs += \"--supervise\" }\n        Option(args.driverMemory).foreach { m => childArgs += (\"--memory\", m) }\n        Option(args.driverCores).foreach { c => childArgs += (\"--cores\", c) }\n        childArgs += \"launch\"\n        childArgs += (args.master, args.primaryResource, args.mainClass)\n      }\n      if (args.childArgs != null) {\n        childArgs ++= args.childArgs\n      }\n    }\n\n    // Let YARN know it's a pyspark app, so it distributes needed libraries.\n    if (clusterManager == YARN) {\n      if (args.isPython) {\n        sysProps.put(\"spark.yarn.isPython\", \"true\")\n      }\n\n      if (args.pyFiles != null) {\n        sysProps(\"spark.submit.pyFiles\") = args.pyFiles\n      }\n    }\n\n    // assure a keytab is available from any place in a JVM\n    if (clusterManager == YARN || clusterManager == LOCAL) {\n      if (args.principal != null) {\n        require(args.keytab != null, \"Keytab must be specified when principal is specified\")\n        if (!new File(args.keytab).exists()) {\n          throw new SparkException(s\"Keytab file: ${args.keytab} does not exist\")\n        } else {\n          // Add keytab and principal configurations in sysProps to make them available\n          // for later use; e.g. in spark sql, the isolated class loader used to talk\n          // to HiveMetastore will use these settings. They will be set as Java system\n          // properties and then loaded by SparkConf\n          sysProps.put(\"spark.yarn.keytab\", args.keytab)\n          sysProps.put(\"spark.yarn.principal\", args.principal)\n\n          UserGroupInformation.loginUserFromKeytab(args.principal, args.keytab)\n        }\n      }\n    }\n\n    // In yarn-cluster mode, use yarn.Client as a wrapper around the user class\n    if (isYarnCluster) {\n      childMainClass = \"org.apache.spark.deploy.yarn.Client\"  //我们重点关注一下\n      if (args.isPython) {\n        childArgs += (\"--primary-py-file\", args.primaryResource)\n        childArgs += (\"--class\", \"org.apache.spark.deploy.PythonRunner\")\n      } else if (args.isR) {\n        val mainFile = new Path(args.primaryResource).getName\n        childArgs += (\"--primary-r-file\", mainFile)\n        childArgs += (\"--class\", \"org.apache.spark.deploy.RRunner\")\n      } else {\n        if (args.primaryResource != SparkLauncher.NO_RESOURCE) {\n          childArgs += (\"--jar\", args.primaryResource)\n        }\n        childArgs += (\"--class\", args.mainClass)\n      }\n      if (args.childArgs != null) {\n        args.childArgs.foreach { arg => childArgs += (\"--arg\", arg) }\n      }\n    }\n\n    if (isMesosCluster) {\n     .....\n    }\n\n    // Load any properties specified through --conf and the default properties file\n    for ((k, v) <- args.sparkProperties) {\n      sysProps.getOrElseUpdate(k, v)\n    }\n\n    // Ignore invalid spark.driver.host in cluster modes.\n    if (deployMode == CLUSTER) {\n      sysProps -= \"spark.driver.host\"\n    }\n\n    // Resolve paths in certain spark properties\n    val pathConfigs = Seq(\n      \"spark.jars\",\n      \"spark.files\",\n      \"spark.yarn.dist.files\",\n      \"spark.yarn.dist.archives\",\n      \"spark.yarn.dist.jars\")\n    pathConfigs.foreach { config =>\n      // Replace old URIs with resolved URIs, if they exist\n      sysProps.get(config).foreach { oldValue =>\n        sysProps(config) = Utils.resolveURIs(oldValue)\n      }\n    }\n\n    // Resolve and format python file paths properly before adding them to the PYTHONPATH.\n    // The resolving part is redundant in the case of --py-files, but necessary if the user\n    // explicitly sets `spark.submit.pyFiles` in his/her default properties file.\n    sysProps.get(\"spark.submit.pyFiles\").foreach { pyFiles =>\n      val resolvedPyFiles = Utils.resolveURIs(pyFiles)\n      val formattedPyFiles = if (!isYarnCluster && !isMesosCluster) {\n        PythonRunner.formatPaths(resolvedPyFiles).mkString(\",\")\n      } else {\n        // Ignoring formatting python path in yarn and mesos cluster mode, these two modes\n        // support dealing with remote python files, they could distribute and add python files\n        // locally.\n        resolvedPyFiles\n      }\n      sysProps(\"spark.submit.pyFiles\") = formattedPyFiles\n    }\n\n    (childArgs, childClasspath, sysProps, childMainClass)\n  }\n```\n\n### Client\n\n反射执行的main方法应该是Client的main方法\n\n```scala\nobject Client {\n  def main(args: Array[String]) {\n    // scalastyle:off println\n    if (!sys.props.contains(\"SPARK_SUBMIT\")) {\n      println(\"WARNING: This client is deprecated and will be removed in a future version of Spark\") //未来client 可能会被弃用\n      println(\"Use ./bin/spark-submit with \\\"--master spark://host:port\\\"\")\n    }\n    // scalastyle:on println\n\n    val conf = new SparkConf()\n    val driverArgs = new ClientArguments(args)\n\n    if (!conf.contains(\"spark.rpc.askTimeout\")) {\n      conf.set(\"spark.rpc.askTimeout\", \"10s\") //设置rpc的超时时间\n    }\n    Logger.getRootLogger.setLevel(driverArgs.logLevel)\n\n    val rpcEnv =  //创建 rpcEnv   需要主动和Master 发消息\n      RpcEnv.create(\"driverClient\", Utils.localHostName(), 0, conf, new SecurityManager(conf)) \n\n    val masterEndpoints = driverArgs.masters.map(RpcAddress.fromSparkURL). //创建masterEndpoints \n      map(rpcEnv.setupEndpointRef(_, Master.ENDPOINT_NAME)) //获取master的Ref\n      //给自己启动一个ClientEndpoint\n    rpcEnv.setupEndpoint(\"client\", new ClientEndpoint(rpcEnv, driverArgs, masterEndpoints, conf))\n\n    rpcEnv.awaitTermination()\n  }\n}\n```\n\n### ClientEndpoint\n\n我们需要查看一下ClientEndpoint\n\n````scala\nprivate class ClientEndpoint(\n    override val rpcEnv: RpcEnv,\n    driverArgs: ClientArguments,\n    masterEndpoints: Seq[RpcEndpointRef],\n    conf: SparkConf)\n  extends ThreadSafeRpcEndpoint with Logging {  //继承了ThreadSafeRpcEndpoint 我们关注一下具体方法\n      ....\n      \n override def onStart(): Unit = {\n    driverArgs.cmd match {\n      case \"launch\" => //是launch\n        // TODO: We could add an env variable here and intercept it in `sc.addJar` that would\n        //       truncate filesystem paths similar to what YARN does. For now, we just require\n        //       people call `addJar` assuming the jar is in the same directory.\n        val mainClass = \"org.apache.spark.deploy.worker.DriverWrapper\"  //图中紫色的DriverWorker\n\n        val classPathConf = \"spark.driver.extraClassPath\"\n        val classPathEntries = sys.props.get(classPathConf).toSeq.flatMap { cp =>\n          cp.split(java.io.File.pathSeparator)\n        }\n\n        val libraryPathConf = \"spark.driver.extraLibraryPath\"\n        val libraryPathEntries = sys.props.get(libraryPathConf).toSeq.flatMap { cp =>\n          cp.split(java.io.File.pathSeparator)\n        }\n\n        val extraJavaOptsConf = \"spark.driver.extraJavaOptions\"\n        val extraJavaOpts = sys.props.get(extraJavaOptsConf)\n          .map(Utils.splitCommandString).getOrElse(Seq.empty)\n        val sparkJavaOpts = Utils.sparkJavaOpts(conf)\n        val javaOpts = sparkJavaOpts ++ extraJavaOpts\n        val command = new Command(mainClass,  //command是一个case Class 这段代码相当于组装进去了配置\n          Seq(\"{{WORKER_URL}}\", \"{{USER_JAR}}\", driverArgs.mainClass) ++ driverArgs.driverOptions,\n          sys.env, classPathEntries, libraryPathEntries, javaOpts)\n\n        val driverDescription = new DriverDescription( //创建了DriverDescription的描述\n          driverArgs.jarUrl,\n          driverArgs.memory, //Driver内存小慎用Collect会把Executor所有的数据load进Driver的JVM中\n          driverArgs.cores,\n          driverArgs.supervise,\n          command)\n        ayncSendToMasterAndForwardReply[SubmitDriverResponse]( //异步地发送给Master Reply\n          RequestSubmitDriver(driverDescription)) //发送driverDescription信息\n\n      case \"kill\" =>\n        val driverId = driverArgs.driverId\n        ayncSendToMasterAndForwardReply[KillDriverResponse](RequestKillDriver(driverId))\n    }\n  }\n ....\n   override def receive: PartialFunction[Any, Unit] = {\n\n    case SubmitDriverResponse(master, success, driverId, message) =>\n      logInfo(message)\n      if (success) {\n        activeMasterEndpoint = master\n        pollAndReportStatus(driverId.get)\n      } else if (!Utils.responseFromBackup(message)) {\n        System.exit(-1)\n      }\n\n\n    case KillDriverResponse(master, driverId, success, message) =>\n      logInfo(message)\n      if (success) {\n        activeMasterEndpoint = master\n        pollAndReportStatus(driverId)\n      } else if (!Utils.responseFromBackup(message)) {\n        System.exit(-1)\n      }\n  }\n````\n\n### 回到Master\n\n```scala\n override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {     \n\t...\n    case RequestSubmitDriver(description) =>  \n      if (state != RecoveryState.ALIVE) { //如果没有存活\n        val msg = s\"${Utils.BACKUP_STANDALONE_MASTER_PREFIX}: $state. \" +\n          \"Can only accept driver submissions in ALIVE state.\"\n        context.reply(SubmitDriverResponse(self, false, None, msg)) //设置为fasle\n      } else {\n        logInfo(\"Driver submitted \" + description.command.mainClass)\n        val driver = createDriver(description) //创建一个driver\n        persistenceEngine.addDriver(driver)\n        waitingDrivers += driver  //将driver添加到waitingDrivers:集群不止一个jar在提交还有其它地,\n        drivers.add(driver)\n        schedule()\n\n        // TODO: It might be good to instead have the submission client poll the master to determine\n        //       the current status of the driver. For now it's simply \"fire and forget\".\n\n        context.reply(SubmitDriverResponse(self, true, Some(driver.id), //返回给Client\n          s\"Driver successfully submitted as ${driver.id}\"))\n      }\n```\n\n```scala\n private def createDriver(desc: DriverDescription): DriverInfo = {\n    val now = System.currentTimeMillis()\n    val date = new Date(now)\n    new DriverInfo(now, newDriverId(date), desc, date)  //创建了一个DriverInfo\n  }\n```\n\n### 回到Clinet\n\n```scala\n  override def receive: PartialFunction[Any, Unit] = {\n\n    case SubmitDriverResponse(master, success, driverId, message) =>\n      logInfo(message) \n      if (success) { //如果成功了\n        activeMasterEndpoint = master\n        pollAndReportStatus(driverId.get) //调用此方法\n      } else if (!Utils.responseFromBackup(message)) {\n        System.exit(-1)\n      }\n```\n\n### pollAndReportStatus\n\n```scala\n  def pollAndReportStatus(driverId: String): Unit = {\n    // Since ClientEndpoint is the only RpcEndpoint in the process, blocking the event loop thread\n    // is fine.\n    logInfo(\"... waiting before polling master for driver state\")\n    Thread.sleep(5000)\n    logInfo(\"... polling master for driver state\")\n    val statusResponse = //如果没有发送成功   重试\n      activeMasterEndpoint.askWithRetry[DriverStatusResponse](RequestDriverStatus(driverId))\n    if (statusResponse.found) { //如果 没有问题\n      logInfo(s\"State of $driverId is ${statusResponse.state.get}\")\n      // Worker node, if present\n      (statusResponse.workerId, statusResponse.workerHostPort, statusResponse.state) match {\n        case (Some(id), Some(hostPort), Some(DriverState.RUNNING)) =>\n          logInfo(s\"Driver running on $hostPort ($id)\")\n        case _ =>\n      }\n      // Exception, if present\n      statusResponse.exception match {\n        case Some(e) =>\n          logError(s\"Exception from cluster was: $e\")\n          e.printStackTrace()\n          System.exit(-1)\n        case _ =>\n          System.exit(0)  //退出\n      }\n    } else { //如果错误\n      logError(s\"ERROR: Cluster master did not recognize $driverId\")\n      System.exit(-1)  //错误退出\n    }\n  }\n```\n\n### 回到Master\n\n```scala\noverride def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n    case RegisterWorker(\n        id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =>\n      logInfo(\"Registering worker %s:%d with %d cores, %s RAM\".format(\n        workerHost, workerPort, cores, Utils.megabytesToString(memory)))\n      if (state == RecoveryState.STANDBY) {\n        context.reply(MasterInStandby)\n      } else if (idToWorker.contains(id)) {\n        context.reply(RegisterWorkerFailed(\"Duplicate worker ID\"))\n      } else {\n        val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory,\n          workerRef, workerWebUiUrl)\n        if (registerWorker(worker)) {\n          persistenceEngine.addWorker(worker)\n          context.reply(RegisteredWorker(self, masterWebUiUrl))\n          schedule()  // 调用了schedule()方法\n        } else {\n          val workerAddress = worker.endpoint.address\n          logWarning(\"Worker registration failed. Attempted to re-register worker at same \" +\n            \"address: \" + workerAddress)\n          context.reply(RegisterWorkerFailed(\"Attempted to re-register worker at same address: \"\n            + workerAddress))\n        }\n      }\n```\n\n### schedule\n\n````scala\n/**\n   * Schedule the currently available resources among waiting apps. This method will be called\n   * every time a new app joins or resource availability changes.\n   */  \nprivate def schedule(): Unit = {\n    if (state != RecoveryState.ALIVE) {  //如果当前地状态不是ALIVE\n      return\n    }\n    // Drivers take strict precedence over executors\n    val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))\n    val numWorkersAlive = shuffledAliveWorkers.size\n    var curPos = 0\n    for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers\n      // We assign workers to each waiting driver in a round-robin fashion. For each driver, we\n      // start from the last worker that was assigned a driver, and continue onwards until we have\n      // explored all alive workers.\n      var launched = false   //launched为false\n      var numWorkersVisited = 0\n      while (numWorkersVisited < numWorkersAlive && !launched) {\n        val worker = shuffledAliveWorkers(curPos)\n        numWorkersVisited += 1   //如果worker中的内存.CPU,\n        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {\n          launchDriver(worker, driver)  //调用launchDriver\n          waitingDrivers -= driver    //将这个driver从 waitingDrivers 中去掉\n          launched = true    //设置 launched 为true  \n        }                       //现在控制权应该走到了launchDriver\n        curPos = (curPos + 1) % numWorkersAlive\n      }\n    }\n    startExecutorsOnWorkers()\n  }\n````\n\n### launchDriver\n\n```scala\n  private def launchDriver(worker: WorkerInfo, driver: DriverInfo) {\n    logInfo(\"Launching driver \" + driver.id + \" on worker \" + worker.id) \n    worker.addDriver(driver) //这个worker在WorkerInfo中把当前的driver加上 Master可以知道那个Application运行在你的节点上\n    driver.worker = Some(worker)\n   \tworker.endpoint.send(LaunchDriver(driver.id, driver.desc))  // 这个时候控制权应该在在Worker\n    driver.state = DriverState.RUNNING\n  }\n```\n\n### 回到Worker\n\n### LaunchDriver \n\nDriverRunner包装了一下信息\n\n```scala\n  case LaunchDriver(driverId, driverDesc) => // Client中的\n      logInfo(s\"Asked to launch driver $driverId\") DriverRunner\n      val driver = new DriverRunner( // new 了一个 \n        conf,\n        driverId,\n        workDir,\n        sparkHome, \n        driverDesc.copy(command = Worker.maybeUpdateSSLSettings(driverDesc.command, conf)), //copy一下\n        self,\n        workerUri,\n        securityMgr)\n      drivers(driverId) = driver //这里的drivers是一个HashMap:drivers保存每一个application的ID和DricerRunneer\n      driver.start()  //调用一下方法\n\n      coresUsed += driverDesc.cores  //粗粒度 统计 core 的使用情况\n      memoryUsed += driverDesc.me\n```\n\n### 调转DriverRunner\n\n### driver.star\n\n```scala\n  /** Starts a thread to run and manage the driver. */\n  private[worker] def start() = {\n    new Thread(\"DriverRunner for \" + driverId) {    //创建一个线程\n      override def run() { \n        var shutdownHook: AnyRef = null\n        try {\n          shutdownHook = ShutdownHookManager.addShutdownHook { () =>\n            logInfo(s\"Worker shutting down, killing driver $driverId\")\n            kill()\n          }\n\n          // prepare driver jars and run driver\n          val exitCode = prepareAndRunDriver() \n\n          // set final state depending on if forcibly killed and process exit code\n          finalState = if (exitCode == 0) {\n            Some(DriverState.FINISHED)\n          } else if (killed) {\n            Some(DriverState.KILLED)\n          } else {\n            Some(DriverState.FAILED)\n          }\n        } catch {\n          case e: Exception =>\n            kill()\n            finalState = Some(DriverState.ERROR)\n            finalException = Some(e)\n        } finally {\n          if (shutdownHook != null) {\n            ShutdownHookManager.removeShutdownHook(shutdownHook)\n          }\n        }\n\t\t//没有出异常就Worker发送信息\n        // notify worker of final driver state, possible exception  \n        worker.send(DriverStateChanged(driverId, finalState.get, finalException))\n      }\n    }.start()\n  }\n```\n\n### prepareAndRunDriver\n\n```scala\n\n  private[worker] def prepareAndRunDriver(): Int = {   \n    val driverDir = createWorkingDirectory()  //创建一个工作目录\n    val localJarFilename = downloadUserJar(driverDir) //在DriverClinet有个小的http服务器,就是为了提供jar包的下载 这时候把jar包下载到我们的Worker里面了\n\n    def substituteVariables(argument: String): String = argument match {\n      case \"{{WORKER_URL}}\" => workerUrl\n      case \"{{USER_JAR}}\" => localJarFilename\n      case other => other\n    }\n\n    // TODO: If we add ability to submit multiple jars they should also be added here\n      //启动一个进程\n    val builder = CommandUtils.buildProcessBuilder(driverDesc.command, securityManager,\n      driverDesc.mem, sparkHome.getAbsolutePath, substituteVariables)\n\t//这是\n    runDriver(builder, driverDir, driverDesc.supervise)\n  }\n```\n\n我们查看一下`buildProcessBuilder`\n\n```scala\n  def buildProcessBuilder(\n      command: Command,\n      securityMgr: SecurityManager,\n      memory: Int,\n      sparkHome: String,\n      substituteArguments: String => String,\n      classPaths: Seq[String] = Seq[String](),\n      env: Map[String, String] = sys.env): ProcessBuilder = {\n    val localCommand = buildLocalCommand(\n      command, securityMgr, substituteArguments, classPaths, env)\n    val commandSeq = buildCommandSeq(localCommand, memory, sparkHome)\n    val builder = new ProcessBuilder(commandSeq: _*)    //Java中与运行本地命令的\n    val environment = builder.environment()\n    for ((key, value) <- localCommand.environment) {\n      environment.put(key, value)  \n    }\n    builder\n  }\n```\n\n### runDriver\n\n```scala\n  private def runDriver(builder: ProcessBuilder, baseDir: File, supervise: Boolean): Int = {\n    builder.directory(baseDir)\n    def initialize(process: Process): Unit = {\n      // Redirect stdout and stderr to files\n      val stdout = new File(baseDir, \"stdout\")  //获取正确输出\n      CommandUtils.redirectStream(process.getInputStream, stdout)\n\t\n    //获取异常\n      val stderr = new File(baseDir, \"stderr\")\n      val formattedCommand = builder.command.asScala.mkString(\"\\\"\", \"\\\" \\\"\", \"\\\"\")\n      val header = \"Launch Command: %s\\n%s\\n\\n\".format(formattedCommand, \"=\" * 40)\n      Files.append(header, stderr, StandardCharsets.UTF_8)   //把日志输出到文件\n      CommandUtils.redirectStream(process.getErrorStream, stderr)\n    }\n      //重试\n    runCommandWithRetry(ProcessBuilderLike(builder), initialize, supervise)\n  }\n```\n\n### runCommandWithRetry\n\n```scala\n  private[worker] def runCommandWithRetry(\n      command: ProcessBuilderLike, initialize: Process => Unit, supervise: Boolean): Int = {\n    var exitCode = -1  //退出代码\n    // Time to wait between submission retries.\n    var waitSeconds = 1\n    // A run of this many seconds resets the exponential back-off.\n    val successfulRunDuration = 5\n    var keepTrying = !killed\n\t//重试\n    while (keepTrying) {\n      logInfo(\"Launch Command: \" + command.command.mkString(\"\\\"\", \"\\\" \\\"\", \"\\\"\"))\n\n      synchronized {\n        if (killed) { return exitCode }  \n        process = Some(command.start()) //这个process可能就是你的DriverWrapper\n        initialize(process.get)\n      }\n\n      val processStart = clock.getTimeMillis()\n      exitCode = process.get.waitFor()\n\n      // check if attempting another run\n      keepTrying = supervise && exitCode != 0 && !killed\n      if (keepTrying) {\n        if (clock.getTimeMillis() - processStart > successfulRunDuration * 1000) {\n          waitSeconds = 1\n        }\n        logInfo(s\"Command exited with status $exitCode, re-launching after $waitSeconds s.\")\n        sleeper.sleep(waitSeconds)\n        waitSeconds = waitSeconds * 2 // exponential back-off\n      }\n    }\n\n    exitCode\n  }\n```\n\n### 进入DriverWrapper\n\n```scala\nobject DriverWrapper {\n  def main(args: Array[String]) {\n    args.toList match {\n      /*\n       * IMPORTANT: Spark 1.3 provides a stable application submission gateway that is both\n       * backward and forward compatible across future Spark versions. Because this gateway\n       * uses this class to launch the driver, the ordering and semantics of the arguments\n       * here must also remain consistent across versions.\n       */\n      case workerUrl :: userJar :: mainClass :: extraArgs =>\n        val rpcEnv = RpcEnv.create(\"Driver\",    //创建rpcEnv\n          Utils.localHostName(), 0, conf, new SecurityManager(conf))\n        rpcEnv.setupEndpoint(\"workerWatcher\", new WorkerWatcher(rpcEnv, workerUrl))  \n\n        val currentLoader = Thread.currentThread.getContextClassLoader\n        val userJarUrl = new File(userJar).toURI().toURL() //userJarUrl 是我们的Jar包\n        val loader = //获得线程\n          if (sys.props.getOrElse(\"spark.driver.userClassPathFirst\", \"false\").toBoolean) {\n            new ChildFirstURLClassLoader(Array(userJarUrl), currentLoader)\n          } else {\n            new MutableURLClassLoader(Array(userJarUrl), currentLoader)\n          }\n        Thread.currentThread.setContextClassLoader(loader)\n\n        // Delegate to supplied main class\n        //把main线程加载进来\n        val clazz = Utils.classForName(mainClass)\n        //反射jar包中的main方法\n        val mainMethod = clazz.getMethod(\"main\", classOf[Array[String]])\n        //执行jar包中的main方法\n        mainMethod.invoke(null, extraArgs.toArray[String])\n\n        rpcEnv.shutdown()  //最后Drive shutdown  因此我们可以有理由断定 Driver程序就是Jar包\n\n      case _ =>\n        // scalastyle:off println\n        System.err.println(\"Usage: DriverWrapper <workerUrl> <userJar> <driverMainClass> [options]\")\n        // scalastyle:on println\n        System.exit(-1)\n    }\n  }\n}\n```\n\n我们回到driver.start看`worker.send(DriverStateChanged(driverId, finalState.get, finalException))` 给worker发送\n\nDriverStateChanged\n\n```scala\n  /** Starts a thread to run and manage the driver. */\n  private[worker] def start() = {\n    new Thread(\"DriverRunner for \" + driverId) {\n      override def run() {\n        var shutdownHook: AnyRef = null\n        try {\n          shutdownHook = ShutdownHookManager.addShutdownHook { () =>\n            logInfo(s\"Worker shutting down, killing driver $driverId\")\n            kill()\n          }\n```\n\n### 回到Driver\n\n```scala\n    case driverStateChanged @ DriverStateChanged(driverId, state, exception) =>\n      handleDriverStateChanged(driverStateChanged)\n```\n\n我们进入handleDriverStateChanged\n\n### handleDriverStateChanged\n\n```scala\n  private[worker] def handleDriverStateChanged(driverStateChanged: DriverStateChanged): Unit = {\n    val driverId = driverStateChanged.driverId\n    val exception = driverStateChanged.exception\n    val state = driverStateChanged.state\n    state match {\n      case DriverState.ERROR => //打印一些消息\n        logWarning(s\"Driver $driverId failed with unrecoverable exception: ${exception.get}\")\n      case DriverState.FAILED =>\n        logWarning(s\"Driver $driverId exited with failure\")\n      case DriverState.FINISHED =>\n        logInfo(s\"Driver $driverId exited successfully\")\n      case DriverState.KILLED =>\n        logInfo(s\"Driver $driverId was killed by user\")\n      case _ =>\n        logDebug(s\"Driver $driverId changed state to $state\")\n    }\n    sendToMaster(driverStateChanged) //发送给Master driverStateChanged\n    val driver = drivers.remove(driverId).get\n    finishedDrivers(driverId) = driver\n    trimFinishedDriversIfNecessary()\n    memoryUsed -= driver.driverDesc.mem\n    coresUsed -= driver.driverDesc.cores\n  }\n```\n\n### sendToMaster\n\n```scala\n  private def sendToMaster(message: Any): Unit = {\n    master match {\n      case Some(masterRef) => masterRef.send(message)\n      case None =>\n        logWarning(\n          s\"Dropping $message because the connection to master has not yet been established\")\n    }\n  }\n```\n\n### 回到Master(Driver启动完毕)\n\n```scala\n   case DriverStateChanged(driverId, state, exception) =>\n      state match {\n        case DriverState.ERROR | DriverState.FINISHED | DriverState.KILLED | DriverState.FAILED =>\n          removeDriver(driverId, state, exception)\n        case _ =>\n          throw new Exception(s\"Received unexpected state update for driver $driverId: $state\")\n      }\n```\n\n到这一步Driver已经基本启动\n\n##  注册Application \n\n我们需要看SparkContext的代码代码有3000多行我们跳着看吧\n\n```scala\nclass SparkContext(config: SparkConf) extends Logging {  //SparkConf 一个配置类\n   ....\n    //这里是Spark内部的一些变量\n  private var _conf: SparkConf = _\n  private var _eventLogDir: Option[URI] = None\n  private var _eventLogCodec: Option[String] = None\n  private var _env: SparkEnv = _ \n  private var _jobProgressListener: JobProgressListener = _\n  private var _statusTracker: SparkStatusTracker = _\n  private var _progressBar: Option[ConsoleProgressBar] = None\n  private var _ui: Option[SparkUI] = None\n  private var _hadoopConfiguration: Configuration = _\n  private var _executorMemory: Int = _\n  private var _schedulerBackend: SchedulerBackend = _\n  private var _taskScheduler: TaskScheduler = _\n  private var _heartbeatReceiver: RpcEndpointRef = _\n  @volatile private var _dagScheduler: DAGScheduler = _ //DAG任务分解其\n  private var _applicationId: String = _\n  private var _applicationAttemptId: Option[String] = None\n  private var _eventLogger: Option[EventLoggingListener] = None\n  private var _executorAllocationManager: Option[ExecutorAllocationManager] = None\n  private var _cleaner: Option[ContextCleaner] = None\n  private var _listenerBusStarted: Boolean = false\n  private var _jars: Seq[String] = _\n  private var _files: Seq[String] = _\n  private var _shutdownHookRef: AnyRef = _\n```\n\n我们首先看一下SparkEnv吧\n\n### SparkEnv\n\n```scala\n@DeveloperApi\nclass SparkEnv (\n    val executorId: String,\n    private[spark] val rpcEnv: RpcEnv,\n    val serializer: Serializer,\n    val closureSerializer: Serializer,\n    val serializerManager: SerializerManager,\n    val mapOutputTracker: MapOutputTracker,\n    val shuffleManager: ShuffleManager,\n    val broadcastManager: BroadcastManager,\n    val blockManager: BlockManager,\n    val securityManager: SecurityManager,\n    val metricsSystem: MetricsSystem,\n    val memoryManager: MemoryManager,\n    val outputCommitCoordinator: OutputCommitCoordinator,\n    val conf: SparkConf) extends Logging {\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190610213205.png)\n\n### 主要初始化方法\n\n```scala\n  //主要初始化方法\n  try {\n    _conf = config.clone()\n    _conf.validateSettings()\n\n    if (!_conf.contains(\"spark.master\")) {\n      throw new SparkException(\"A master URL must be set in your configuration\")\n    }\n    if (!_conf.contains(\"spark.app.name\")) {\n      throw new SparkException(\"An application name must be set in your configuration\")\n    }\n\n    // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster\n    if (master == \"yarn\" && deployMode == \"cluster\" && !_conf.contains(\"spark.yarn.app.id\")) {\n      throw new SparkException(\"Detected yarn cluster mode, but isn't running on a cluster. \" +\n        \"Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.\")\n    }\n\n    if (_conf.getBoolean(\"spark.logConf\", false)) {\n      logInfo(\"Spark configuration:\\n\" + _conf.toDebugString)\n    }\n\n    // Set Spark driver host and port system properties. This explicitly sets the configuration\n    // instead of relying on the default value of the config constant.\n    _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS))\n    _conf.setIfMissing(\"spark.driver.port\", \"0\")\n\n    _conf.set(\"spark.executor.id\", SparkContext.DRIVER_IDENTIFIER)\n\n    _jars = Utils.getUserJars(_conf)\n    _files = _conf.getOption(\"spark.files\").map(_.split(\",\")).map(_.filter(_.nonEmpty))\n      .toSeq.flatten\n\n    _eventLogDir =\n      if (isEventLogEnabled) {\n        val unresolvedDir = conf.get(\"spark.eventLog.dir\", EventLoggingListener.DEFAULT_LOG_DIR)\n          .stripSuffix(\"/\")\n        Some(Utils.resolveURI(unresolvedDir))\n      } else {\n        None\n      }\n\n    _eventLogCodec = {\n      val compress = _conf.getBoolean(\"spark.eventLog.compress\", false)\n      if (compress && isEventLogEnabled) {\n        Some(CompressionCodec.getCodecName(_conf)).map(CompressionCodec.getShortName)\n      } else {\n        None\n      }\n    }\n\n    if (master == \"yarn\" && deployMode == \"client\") System.setProperty(\"SPARK_YARN_MODE\", \"true\")\n\n    // \"_jobProgressListener\" should be set up before creating SparkEnv because when creating\n    // \"SparkEnv\", some messages will be posted to \"listenerBus\" and we should not miss them.\n    _jobProgressListener = new JobProgressListener(_conf)\n    listenerBus.addListener(jobProgressListener)\n\n    // Create the Spark execution environment (cache, map output tracker, etc)\n    _env = createSparkEnv(_conf, isLocal, listenerBus)\n    SparkEnv.set(_env)\n\n    // If running the REPL, register the repl's output dir with the file server.\n    _conf.getOption(\"spark.repl.class.outputDir\").foreach { path =>\n      val replUri = _env.rpcEnv.fileServer.addDirectory(\"/classes\", new File(path))\n      _conf.set(\"spark.repl.class.uri\", replUri)\n    }\n\n    _statusTracker = new SparkStatusTracker(this)\n\n    _progressBar =\n      if (_conf.getBoolean(\"spark.ui.showConsoleProgress\", true) && !log.isInfoEnabled) {\n        Some(new ConsoleProgressBar(this))\n      } else {\n        None\n      }\n\n    _ui =\n      if (conf.getBoolean(\"spark.ui.enabled\", true)) {\n        Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,\n          _env.securityManager, appName, startTime = startTime))\n      } else {\n        // For tests, do not enable the UI\n        None\n      }\n    // Bind the UI before starting the task scheduler to communicate\n    // the bound port to the cluster manager properly\n    _ui.foreach(_.bind())\n\n    _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)\n\n    // Add each JAR given through the constructor\n    if (jars != null) {\n      jars.foreach(addJar)\n    }\n\n    if (files != null) {\n      files.foreach(addFile)\n    }\n\t//获取配置\n    _executorMemory = _conf.getOption(\"spark.executor.memory\")\n      .orElse(Option(System.getenv(\"SPARK_EXECUTOR_MEMORY\")))\n      .orElse(Option(System.getenv(\"SPARK_MEM\"))\n      .map(warnSparkMem))\n      .map(Utils.memoryStringToMb)\n      .getOrElse(1024)\n\n    // Convert java options to env vars as a work around\n    // since we can't set env vars directly in sbt.\n    for { (envKey, propKey) <- Seq((\"SPARK_TESTING\", \"spark.testing\"))\n      value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} {\n      executorEnvs(envKey) = value\n    }\n    Option(System.getenv(\"SPARK_PREPEND_CLASSES\")).foreach { v =>\n      executorEnvs(\"SPARK_PREPEND_CLASSES\") = v\n    }\n    // The Mesos scheduler backend relies on this environment variable to set executor memory.\n    // TODO: Set this only in the Mesos scheduler.\n    executorEnvs(\"SPARK_EXECUTOR_MEMORY\") = executorMemory + \"m\"\n    executorEnvs ++= _conf.getExecutorEnv\n    executorEnvs(\"SPARK_USER\") = sparkUser\n\n    // We need to register \"HeartbeatReceiver\" before \"createTaskScheduler\" because Executor will\n    // retrieve \"HeartbeatReceiver\" in the constructor. (SPARK-6640)\n    _heartbeatReceiver = env.rpcEnv.setupEndpoint( //心跳\n      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))\n\n    // Create and start the scheduler  \n    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)\n    _schedulerBackend = sched   //创建 _schedulerBackend\n    _taskScheduler = ts  //创建 _taskScheduler\n    _dagScheduler = new DAGScheduler(this) //创建 DAGScheduler\n    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)\n\n    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's\n    // constructor\n    _taskScheduler.start()\n\n    _applicationId = _taskScheduler.applicationId()\n    _applicationAttemptId = taskScheduler.applicationAttemptId()\n    _conf.set(\"spark.app.id\", _applicationId)\n    if (_conf.getBoolean(\"spark.ui.reverseProxy\", false)) {\n      System.setProperty(\"spark.ui.proxyBase\", \"/proxy/\" + _applicationId)\n    }\n    _ui.foreach(_.setAppId(_applicationId))\n    _env.blockManager.initialize(_applicationId)\n\n    // The metrics system for Driver need to be set spark.app.id to app ID.\n    // So it should start after we get app ID from the task scheduler and set spark.app.id.\n    _env.metricsSystem.start()\n    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.\n    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))\n\n    _eventLogger =\n      if (isEventLogEnabled) {\n        val logger =\n          new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,\n            _conf, _hadoopConfiguration)\n        logger.start()\n        listenerBus.addListener(logger)\n        Some(logger)\n      } else {\n        None\n      }\n\n    // Optionally scale number of executors dynamically based on workload. Exposed for testing.\n    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)\n    _executorAllocationManager =\n      if (dynamicAllocationEnabled) {\n        schedulerBackend match {\n          case b: ExecutorAllocationClient =>\n            Some(new ExecutorAllocationManager(\n              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf))\n          case _ =>\n            None\n        }\n      } else {\n        None\n      }\n    _executorAllocationManager.foreach(_.start())\n\n    _cleaner =\n      if (_conf.getBoolean(\"spark.cleaner.referenceTracking\", true)) {\n        Some(new ContextCleaner(this))\n      } else {\n        None\n      }\n    _cleaner.foreach(_.start())\n\n    setupAndStartListenerBus()\n    postEnvironmentUpdate()\n    postApplicationStart()\n\n    // Post init\n    _taskScheduler.postStartHook()\n    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)\n    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))\n    _executorAllocationManager.foreach { e =>\n      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)\n    }\n\n    // Make sure the context is stopped if the user forgets about it. This avoids leaving\n    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM\n    // is killed, though.\n    logDebug(\"Adding shutdown hook\") // force eager creation of logger\n    _shutdownHookRef = ShutdownHookManager.addShutdownHook(\n      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =>\n      logInfo(\"Invoking stop() from shutdown hook\")\n      stop()\n    }\n  } catch {\n    case NonFatal(e) =>\n      logError(\"Error initializing SparkContext.\", e)\n      try {\n        stop()\n      } catch {\n        case NonFatal(inner) =>\n          logError(\"Error stopping SparkContext after init error.\", inner)\n      } finally {\n        throw e\n      }\n  }\n```\n\n### createTaskScheduler\n\n```scala\n  private def createTaskScheduler(\n      sc: SparkContext,\n      master: String,\n      deployMode: String): (SchedulerBackend, TaskScheduler) = {\n    import SparkMasterRegex._\n\n    // When running locally, don't try to re-execute tasks on failure.\n    val MAX_LOCAL_TASK_FAILURES = 1\n\n    master match {\n      case \"local\" =>  //如果是local模式\n        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)\n        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)\n        scheduler.initialize(backend)\n        (backend, scheduler)\n\n      case LOCAL_N_REGEX(threads) => //或者是多线程的local模式\n        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()\n        // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.\n        val threadCount = if (threads == \"*\") localCpuCount else threads.toInt\n        if (threadCount <= 0) {\n          throw new SparkException(s\"Asked to run locally with $threadCount threads\")\n        }\n        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)\n        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)\n        scheduler.initialize(backend)\n        (backend, scheduler)\n\n      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>\n        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()\n        // local[*, M] means the number of cores on the computer with M failures\n        // local[N, M] means exactly N threads with M failures\n        val threadCount = if (threads == \"*\") localCpuCount else threads.toInt\n        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)\n        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)\n        scheduler.initialize(backend)\n        (backend, scheduler)\n\n      case SPARK_REGEX(sparkUrl) => //StandaloneSchedulerBackend 模式\n        val scheduler = new TaskSchedulerImpl(sc)\n        val masterUrls = sparkUrl.split(\",\").map(\"spark://\" + _)\n        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)\n        scheduler.initialize(backend)\n        (backend, scheduler)\n\n      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>\n        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.\n        val memoryPerSlaveInt = memoryPerSlave.toInt\n        if (sc.executorMemory > memoryPerSlaveInt) {\n          throw new SparkException(\n            \"Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker\".format(\n              memoryPerSlaveInt, sc.executorMemory))\n        }\n\n        val scheduler = new TaskSchedulerImpl(sc)\n        val localCluster = new LocalSparkCluster(\n          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)\n        val masterUrls = localCluster.start()\n        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)\n        scheduler.initialize(backend)\n        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {\n          localCluster.stop()\n        }\n        (backend, scheduler)\n \n      case masterUrl => //根据你的提交模式 会创建不同的 scheduler backend \n        val cm = getClusterManager(masterUrl) match {\n          case Some(clusterMgr) => clusterMgr\n          case None => throw new SparkException(\"Could not parse Master URL: '\" + master + \"'\")\n        }\n        try {\n          val scheduler = cm.createTaskScheduler(sc, masterUrl)\n          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler) \n          cm.initialize(scheduler, backend)\n          (backend, scheduler)\n        } catch {\n          case se: SparkException => throw se\n          case NonFatal(e) =>\n            throw new SparkException(\"External scheduler cannot be instantiated\", e)\n        }\n    }  \n```\n\n### 启动\n\n```scala\n    // Create and start the scheduler\n    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)\n    _schedulerBackend = sched\n    _taskScheduler = ts\n    _dagScheduler = new DAGScheduler(this)\n    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)\n\n    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's\n    // constructor\n    _taskScheduler.start()  //启动任务\n\n    _applicationId = _taskScheduler.applicationId()\n    _applicationAttemptId = taskScheduler.applicationAttemptId()\n    _conf.set(\"spark.app.id\", _applicationId)\n   if (_conf.getBoolean(\"spark.ui.reverseProxy\", false)) {\n      System.setProperty(\"spark.ui.proxyBase\", \"/proxy/\" + _applicationId)\n    }\n    _ui.foreach(_.setAppId(_applicationId))\n    _env.blockManager.initialize(_applicationId)\n\n    // The metrics system for Driver need to be set spark.app.id to app ID.\n    // So it should start after we get app ID from the task scheduler and set spark.app.id.\n    _env.metricsSystem.start()\n    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.\n    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))\n\n    _eventLogger =\n      if (isEventLogEnabled) {\n        val logger =\n          new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,\n            _conf, _hadoopConfiguration)\n        logger.start()\n        listenerBus.addListener(logger)\n        Some(logger)\n      } else {\n        None\n      }\n\n    // Optionally scale number of executors dynamically based on workload. Exposed for testing.\n    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)\n    _executorAllocationManager =\n      if (dynamicAllocationEnabled) {\n        schedulerBackend match {\n          case b: ExecutorAllocationClient =>\n            Some(new ExecutorAllocationManager(\n              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf))\n          case _ =>\n            None\n        }\n      } else {\n        None\n      }\n    _executorAllocationManager.foreach(_.start())\n\n    _cleaner =\n      if (_conf.getBoolean(\"spark.cleaner.referenceTracking\", true)) {\n        Some(new ContextCleaner(this))\n      } else {\n        None\n      }\n    _cleaner.foreach(_.start())\n\n    setupAndStartListenerBus()\n    postEnvironmentUpdate()  \n    postApplicationStart()\n\n    // Post init\n    _taskScheduler.postStartHook()\n    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)\n    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))\n    _executorAllocationManager.foreach { e =>\n      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)\n    }\n\n    // Make sure the context is stopped if the user forgets about it. This avoids leaving\n    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM\n    // is killed, though.\n    logDebug(\"Adding shutdown hook\") // force eager creation of logger\n    _shutdownHookRef = ShutdownHookManager.addShutdownHook( //注册一个ShutdownHook (钩子函数)\n      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =>\n      logInfo(\"Invoking stop() from shutdown hook\")\n      stop()\n    }\n  } catch {\n    case NonFatal(e) =>\n      logError(\"Error initializing SparkContext.\", e)\n      try {\n        stop()\n      } catch {\n        case NonFatal(inner) =>\n          logError(\"Error stopping SparkContext after init error.\", inner)\n      } finally {\n        throw e\n      }\n  }\n```\n\n\n\n## Excutor分配\n\n### StandaloneSchedulerBackend\n\n```scala\n/**\n * A [[SchedulerBackend]] implementation for Spark's standalone cluster manager.\n */\nprivate[spark] class StandaloneSchedulerBackend(\n    scheduler: TaskSchedulerImpl,\n    sc: SparkContext,\n    masters: Array[String])\n  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv)//继承了一个粗粒度schedulerBackend  在应用运行之间就把资源计算好,在应用运行期间资源是不动的固定的. Messon有粗细粒度之分再YARN 集群和Stanlone中 都为粗粒度的\n  with StandaloneAppClientListener\n  with Logging {\n\n  private var client: StandaloneAppClient = null\n  private val stopping = new AtomicBoolean(false)\n  private val launcherBackend = new LauncherBackend() {\n    override protected def onStopRequest(): Unit = stop(SparkAppHandle.State.KILLED)\n  }\n\n  @volatile var shutdownCallback: StandaloneSchedulerBackend => Unit = _\n  @volatile private var appId: String = _\n\n  private val registrationBarrier = new Semaphore(0)\n\n  private val maxCores = conf.getOption(\"spark.cores.max\").map(_.toInt)\n  private val totalExpectedCores = maxCores.getOrElse(0)\n\n  override def start() { //我们关注一下start的方法\n    super.start()\n    launcherBackend.connect()\n\n    // The endpoint for executors to talk to us\n    val driverUrl = RpcEndpointAddress(\n      sc.conf.get(\"spark.driver.host\"),\n      sc.conf.get(\"spark.driver.port\").toInt,\n      CoarseGrainedSchedulerBackend.ENDPOINT_NAME).toString\n    val args = Seq(\n      \"--driver-url\", driverUrl,\n      \"--executor-id\", \"{{EXECUTOR_ID}}\",\n      \"--hostname\", \"{{HOSTNAME}}\",\n      \"--cores\", \"{{CORES}}\",\n      \"--app-id\", \"{{APP_ID}}\",\n      \"--worker-url\", \"{{WORKER_URL}}\")\n    val extraJavaOpts = sc.conf.getOption(\"spark.executor.extraJavaOptions\")\n      .map(Utils.splitCommandString).getOrElse(Seq.empty)\n    val classPathEntries = sc.conf.getOption(\"spark.executor.extraClassPath\")\n      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)\n    val libraryPathEntries = sc.conf.getOption(\"spark.executor.extraLibraryPath\")\n      .map(_.split(java.io.File.pathSeparator).toSeq).getOrElse(Nil)\n\n    // When testing, expose the parent class path to the child. This is processed by\n    // compute-classpath.{cmd,sh} and makes all needed jars available to child processes\n    // when the assembly is built with the \"*-provided\" profiles enabled.\n    val testingClassPath =\n      if (sys.props.contains(\"spark.testing\")) {\n        sys.props(\"java.class.path\").split(java.io.File.pathSeparator).toSeq\n      } else {\n        Nil\n      }\n\n    // Start executors with a few necessary configs for registering with the scheduler\n    val sparkJavaOpts = Utils.sparkJavaOpts(conf, SparkConf.isExecutorStartupConf)\n    val javaOpts = sparkJavaOpts ++ extraJavaOpts\n    val command = Command(\"org.apache.spark.executor.CoarseGrainedExecutorBackend\",\n      args, sc.executorEnvs, classPathEntries ++ testingClassPath, libraryPathEntries, javaOpts)\n    val appUIAddress = sc.ui.map(_.appUIAddress).getOrElse(\"\")\n    val coresPerExecutor = conf.getOption(\"spark.executor.cores\").map(_.toInt)\n    // If we're using dynamic allocation, set our initial executor limit to 0 for now.\n    // ExecutorAllocationManager will send the real initial limit to the Master later.\n    val initialExecutorLimit =\n      if (Utils.isDynamicAllocationEnabled(conf)) {\n        Some(0)\n      } else {\n        None\n      }\n    val appDesc = new ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,\n      appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)\n    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)\n    client.start()\n    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)\n    waitForRegistration()\n    launcherBackend.setState(SparkAppHandle.State.RUNNING)\n  }\n\n  override def stop(): Unit = {\n    stop(SparkAppHandle.State.FINISHED)\n  }\n\n  override def connected(appId: String) {\n    logInfo(\"Connected to Spark cluster with app ID \" + appId)\n    this.appId = appId\n    notifyContext()\n    launcherBackend.setAppId(appId)\n  }\n\n  override def disconnected() {\n    notifyContext()\n    if (!stopping.get) {\n      logWarning(\"Disconnected from Spark cluster! Waiting for reconnection...\")\n    }\n  }\n\n  override def dead(reason: String) {\n    notifyContext()\n    if (!stopping.get) {\n      launcherBackend.setState(SparkAppHandle.State.KILLED)\n      logError(\"Application has been killed. Reason: \" + reason)\n      try {\n        scheduler.error(reason)\n      } finally {\n        // Ensure the application terminates, as we can no longer run jobs.\n        sc.stopInNewThread()\n      }\n    }\n  }\n```\n\n### CoarseGrainedSchedulerBackend\n\n```scala\n  override def start() {\n    val properties = new ArrayBuffer[(String, String)]\n    for ((key, value) <- scheduler.sc.conf.getAll) {\n      if (key.startsWith(\"spark.\")) {\n        properties += ((key, value))\n      }\n    }\n\n    // TODO (prashant) send conf instead of properties\n    driverEndpoint = createDriverEndpointRef(properties)  //创建了一个driverEndpoint\n  }\n```\n\n### 回到StandaloneSchedulerBackend\n\n我们看一下再start方法中的\n\n```scala\n\t//创建了一个ApplicationDescription\n\tval appDesc = new ApplicationDescription(sc.appName, maxCores, sc.executorMemory, command,\n      appUIAddress, sc.eventLogDir, sc.eventLogCodec, coresPerExecutor, initialExecutorLimit)\n\t//创建了一个StandaloneAppClient\n    client = new StandaloneAppClient(sc.env.rpcEnv, masters, appDesc, this, conf)  //控制权限转到StandaloneAppClient\n    client.start() \n    launcherBackend.setState(SparkAppHandle.State.SUBMITTED)\n    waitForRegistration()\n    launcherBackend.setState(SparkAppHandle.State.RUNNING)\n```\n\n### 跳转StandaloneAppClient\n\n```scala\nprivate[spark] class StandaloneAppClient(\n    rpcEnv: RpcEnv,\n    masterUrls: Array[String],\n    appDescription: ApplicationDescription,\n    listener: StandaloneAppClientListener,\n    conf: SparkConf)\n  extends Logging { //Client \n\n  private val masterRpcAddresses = masterUrls.map(RpcAddress.fromSparkURL(_))\n\n  private val REGISTRATION_TIMEOUT_SECONDS = 20\n  private val REGISTRATION_RETRIES = 3\n\n  private val endpoint = new AtomicReference[RpcEndpointRef]\n  private val appId = new AtomicReference[String]\n  private val registered = new AtomicBoolean(false)\n\t//也是个端点,\n  private class ClientEndpoint(override val rpcEnv: RpcEnv) extends ThreadSafeRpcEndpoint\n    with Logging {\n\n    private var master: Option[RpcEndpointRef] = None\n    // To avoid calling listener.disconnected() multiple times\n    private var alreadyDisconnected = false\n    // To avoid calling listener.dead() multiple times\n    private val alreadyDead = new AtomicBoolean(false)\n    private val registerMasterFutures = new AtomicReference[Array[JFuture[_]]]\n    private val registrationRetryTimer = new AtomicReference[JScheduledFuture[_]]\n\t\t....\n  \tdef start() {\n    // Just launch an rpcEndpoint; it will call back into the listener.\n\t//创建了一个ClientEndpoint\n    endpoint.set(rpcEnv.setupEndpoint(\"AppClient\", new ClientEndpoint(rpcEnv))) \n  }\n   ....\n            override def receive: PartialFunction[Any, Unit] = {\n      case RegisteredApplication(appId_, masterRef) =>\n        // FIXME How to handle the following cases?\n        // 1. A master receives multiple registrations and sends back multiple\n        // RegisteredApplications due to an unstable network.\n        // 2. Receive multiple RegisteredApplication from different masters because the master is\n        // changing.\n        appId.set(appId_)\n        registered.set(true)\n        master = Some(masterRef)\n        listener.connected(appId.get)\n\n      case ApplicationRemoved(message) =>\n        markDead(\"Master removed our application: %s\".format(message))\n        stop()\n\n      case ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) =>\n        val fullId = appId + \"/\" + id\n        logInfo((\"Ex\" +\n          \"ecutor added: %s on %s (%s) with %d cores\").format(fullId, workerId, hostPort,\n          cores))\n        listener.executorAdded(fullId, workerId, hostPort, cores, memory)\n\n      case ExecutorUpdated(id, state, message, exitStatus, workerLost) =>\n        val fullId = appId + \"/\" + id\n        val messageText = message.map(s => \" (\" + s + \")\").getOrElse(\"\")\n        logInfo(\"Executor updated: %s is now %s%s\".format(fullId, state, messageText))\n        if (ExecutorState.isFinished(state)) {\n          listener.executorRemoved(fullId, message.getOrElse(\"\"), exitStatus, workerLost)\n        }\n\n      case MasterChanged(masterRef, masterWebUiUrl) =>\n        logInfo(\"Master has changed, new master is at \" + masterRef.address.toSparkURL)\n        master = Some(masterRef)\n        alreadyDisconnected = false\n        masterRef.send(MasterChangeAcknowledged(appId.get))\n    } \n        \n   .... \n    override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n      case StopAppClient =>\n        markDead(\"Application has been stopped.\")\n        sendToMaster(UnregisterApplication(appId.get))\n        context.reply(true)\n        stop()      \n  }\n private def registerWithMaster(nthRetry: Int) {  //向Master 注册\n      registerMasterFutures.set(tryRegisterAllMasters())  //查看这个方法\n      registrationRetryTimer.set(registrationRetryThread.schedule(new Runnable {\n        override def run(): Unit = {\n          if (registered.get) {\n            registerMasterFutures.get.foreach(_.cancel(true))\n            registerMasterThreadPool.shutdownNow()\n          } else if (nthRetry >= REGISTRATION_RETRIES) {\n            markDead(\"All masters are unresponsive! Giving up.\")\n          } else {\n            registerMasterFutures.get.foreach(_.cancel(true))\n            registerWithMaster(nthRetry + 1)  //重试\n          }\n        }\n      }, REGISTRATION_TIMEOUT_SECONDS, TimeUnit.SECONDS))\n    }        \n```\n\n### tryRegisterAllMasters\n\n```scala\n   private def tryRegisterAllMasters(): Array[JFuture[_]] = {\n      for (masterAddress <- masterRpcAddresses) yield {\n        registerMasterThreadPool.submit(new Runnable {\n          override def run(): Unit = try {\n            if (registered.get) {\n              return\n            }\n            logInfo(\"Connecting to master \" + masterAddress.toSparkURL + \"...\")\n            val masterRef = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)\n               //给master发送RegisterApplication\n            masterRef.send(RegisterApplication(appDescription, self)) //控制权走到Master\n          } catch {\n            case ie: InterruptedException => // Cancelled\n            case NonFatal(e) => logWarning(s\"Failed to connect to master $masterAddress\", e)\n          }\n        })\n      }\n    }\n```\n\n### 回到Master\n\n### RegisterApplication\n\n```scala\n    case RegisterApplication(description, driver) =>\n      // TODO Prevent repeated registrations from some driver\n      if (state == RecoveryState.STANDBY) {\n        // ignore, don't send response\n      } else {\n        logInfo(\"Registering app \" + description.name)\n        val app = createApplication(description, driver)\n        registerApplication(app)\n        logInfo(\"Registered app \" + description.name + \" with ID \" + app.id)\n        persistenceEngine.addApplication(app)\n        driver.send(RegisteredApplication(app.id, self)) //给driver 送法消息 \n        // 启动分配Executor\n        schedule()\n      }\n```\n\n### schedule\n\n```scala\nprivate def schedule(): Unit = {\n    if (state != RecoveryState.ALIVE) {\n      return\n    }\n    // Drivers take strict precedence over executors\n    val shuffledAliveWorkers = Random.shuffle(workers.toSeq.filter(_.state == WorkerState.ALIVE))\n    val numWorkersAlive = shuffledAliveWorkers.size\n    var curPos = 0\n    for (driver <- waitingDrivers.toList) { // iterate over a copy of waitingDrivers\n      // We assign workers to each waiting driver in a round-robin fashion. For each driver, we\n      // start from the last worker that was assigned a driver, and continue onwards until we have\n      // explored all alive workers.\n      var launched = false\n      var numWorkersVisited = 0\n      while (numWorkersVisited < numWorkersAlive && !launched) {\n        val worker = shuffledAliveWorkers(curPos)\n        numWorkersVisited += 1\n        if (worker.memoryFree >= driver.desc.mem && worker.coresFree >= driver.desc.cores) {\n          launchDriver(worker, driver)\n          waitingDrivers -= driver\n          launched = true\n        }\n        curPos = (curPos + 1) % numWorkersAlive\n      }\n    }\n    startExecutorsOnWorkers()  //Worker已经部署 我们需要看这个\n  }\n```\n\n### startExecutorsOnWorkers\n\n```scala\n  private def startExecutorsOnWorkers(): Unit = {\n    // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app\n    // in the queue, then the second app, etc.\n      //这里的应用并不能并行执行,因为 再for循环里面 的任务智能一个一个分配\n    for (app <- waitingApps if app.coresLeft > 0) { //这里waitingApps保存所有需要分配资源的Driver\n      val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor\n      // Filter out workers that don't have enough resources to launch an executor\n      val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE) //调出满足应用的Exector需要的Worker \n        .filter(worker => worker.memoryFree >= app.desc.memoryPerExecutorMB && \n          worker.coresFree >= coresPerExecutor.getOrElse(1))\n        .sortBy(_.coresFree).reverse\n\t \n        //scheduleExecutorsOnWorkers是资源分配的核心算法我们来看一下\n      val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)\n\n      // Now that we've decided how many cores to allocate on each worker, let's allocate them\n      for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {\n        allocateWorkerResourceToExecutors( \n          app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))\n      }\n    }\n  }\n```\n\n### scheduleExecutorsOnWorkers\n\n资源分配的核心代码\n\n```scala\n  private def scheduleExecutorsOnWorkers(\n      app: ApplicationInfo,\n      usableWorkers: Array[WorkerInfo],\n      spreadOutApps: Boolean): Array[Int] = { //如果参数为True 仅量吧你需要的Executor分配到不同的Worker上\n    // 每一个Executor有多少Core\n    val coresPerExecutor = app.desc.coresPerExecutor\n    // minCoresPerExecutor 如果你没定义那么就是1，如果定义了就是coresPerExecutor\n    val minCoresPerExecutor = coresPerExecutor.getOrElse(1)\n\n    // 是否允许一个worker上有多个Executor\n    val oneExecutorPerWorker = coresPerExecutor.isEmpty\n\n    // 指定Memory 1G\n    val memoryPerExecutor = app.desc.memoryPerExecutorMB\n\n    // 获得usableWorkers 长度\n    val numUsable = usableWorkers.length\n\n    val assignedCores = new Array[Int](numUsable) // Number of cores to give to each worker\n    val assignedExecutors = new Array[Int](numUsable) // Number of new executors on each worker\n\n    // coresToAssign 保险\n    var coresToAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum)\n\n    /** Return whether the specified worker can launch an executor for this app. */\n    def canLaunchExecutor(pos: Int): Boolean = {\n      val keepScheduling = coresToAssign >= minCoresPerExecutor\n      val enoughCores = usableWorkers(pos).coresFree - assignedCores(pos) >= minCoresPerExecutor\n\n      // If we allow multiple executors per worker, then we can always launch new executors.\n      // Otherwise, if there is already an executor on this worker, just give it more cores.\n      val launchingNewExecutor = !oneExecutorPerWorker || assignedExecutors(pos) == 0\n      if (launchingNewExecutor) {\n        val assignedMemory = assignedExecutors(pos) * memoryPerExecutor\n        val enoughMemory = usableWorkers(pos).memoryFree - assignedMemory >= memoryPerExecutor\n        val underLimit = assignedExecutors.sum + app.executors.size < app.executorLimit\n        keepScheduling && enoughCores && enoughMemory && underLimit\n      } else {\n        // We're adding cores to an existing executor, so no need\n        // to check memory and executor limits\n        keepScheduling && enoughCores\n      }\n    }\n\n    // Keep launching executors until no more workers can accommodate any\n    // more executors, or if we have reached this application's limits\n    var freeWorkers = (0 until numUsable).filter(canLaunchExecutor)\n    while (freeWorkers.nonEmpty) {\n      freeWorkers.foreach { pos =>\n        var keepScheduling = true\n        while (keepScheduling && canLaunchExecutor(pos)) {\n          coresToAssign -= minCoresPerExecutor\n          assignedCores(pos) += minCoresPerExecutor\n\n          // If we are launching one executor per worker, then every iteration assigns 1 core\n          // to the executor. Otherwise, every iteration assigns cores to a new executor.\n          if (oneExecutorPerWorker) {\n            assignedExecutors(pos) = 1\n          } else {\n            assignedExecutors(pos) += 1\n          }\n\n          // Spreading out an application means spreading out its executors across as\n          // many workers as possible. If we are not spreading out, then we should keep\n          // scheduling executors on this worker until we use all of its resources.\n          // Otherwise, just move on to the next worker.\n          if (spreadOutApps) {\n            keepScheduling = false\n          }\n        }\n      }\n      freeWorkers = freeWorkers.filter(canLaunchExecutor)\n    }\n    assignedCores\n  }\n```\n\n### startExecutorsOnWorkers\n\n```scala\n\n  /**\n   * Schedule and launch executors on workers\n   */\n  private def startExecutorsOnWorkers(): Unit = {\n    // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app\n    // in the queue, then the second app, etc.\n    for (app <- waitingApps if app.coresLeft > 0) {\n      val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor\n      // Filter out workers that don't have enough resources to launch an executor\n      val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE)\n        .filter(worker => worker.memoryFree >= app.desc.memoryPerExecutorMB &&\n          worker.coresFree >= coresPerExecutor.getOrElse(1))\n        .sortBy(_.coresFree).reverse\n\n      val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps)\n\n      // Now that we've decided how many cores to allocate on each worker, let's allocate them\n      for (pos <- 0 until usableWorkers.length if assignedCores(pos) > 0) {\n        allocateWorkerResourceToExecutors(  //控制权 回到allocateWorkerResourceToExecutors\n          app, assignedCores(pos), coresPerExecutor, usableWorkers(pos))\n      }\n    }\n  }\n```\n\n### allocateWorkerResourceToExecutors\n\n```scala\n  private def allocateWorkerResourceToExecutors(\n      app: ApplicationInfo,\n      assignedCores: Int,\n      coresPerExecutor: Option[Int],\n      worker: WorkerInfo): Unit = {\n    // If the number of cores per executor is specified, we divide the cores assigned\n    // to this worker evenly among the executors with no remainder.\n    // Otherwise, we launch a single executor that grabs all the assignedCores on this worker.\n    val numExecutors = coresPerExecutor.map { assignedCores / _ }.getOrElse(1)\n    val coresToAssign = coresPerExecutor.getOrElse(assignedCores)\n    for (i <- 1 to numExecutors) {\n      val exec = app.addExecutor(worker, coresToAssign)\n      launchExecutor(worker, exec) //启动Executor  给 Worker\n      app.state = ApplicationState.RUNNING\n    }\n  }\n```\n\n### launchExecutor\n\n```scala\n private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = {\n    logInfo(\"Launching executor \" + exec.fullId + \" on worker \" + worker.id)\n    worker.addExecutor(exec)\n    worker.endpoint.send(LaunchExecutor(masterUrl, //给worker发送 LaunchExecutor\n      exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory))\n    exec.application.driver.send(\n      ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))\n  }\n```\n\n### 回到Worker\n\n```scala\ncase LaunchExecutor(masterUrl, appId, execId, appDesc, cores_, memory_) =>\n      if (masterUrl != activeMasterUrl) {\n        logWarning(\"Invalid Master (\" + masterUrl + \") attempted to launch executor.\")\n      } else {\n        try {\n          logInfo(\"Asked to launch executor %s/%d for %s\".format(appId, execId, appDesc.name))\n\n          // Create the executor's working directory\n          val executorDir = new File(workDir, appId + \"/\" + execId) //创建工作目录\n          if (!executorDir.mkdirs()) { //如果没有创建\n            throw new IOException(\"Failed to create directory \" + executorDir)\n          }\n\n          // Create local dirs for the executor. These are passed to the executor via the\n          // SPARK_EXECUTOR_DIRS environment variable, and deleted by the Worker when the\n          // application finishes.\n          val appLocalDirs = appDirectories.getOrElse(appId,\n            Utils.getOrCreateLocalRootDirs(conf).map { dir =>\n              val appDir = Utils.createDirectory(dir, namePrefix = \"executor\")\n              Utils.chmod700(appDir)\n              appDir.getAbsolutePath()\n            }.toSeq)\n          appDirectories(appId) = appLocalDirs \n          val manager = new ExecutorRunner( //如果可以\n            appId,\n            execId,\n            appDesc.copy(command = Worker.maybeUpdateSSLSettings(appDesc.command, conf)),\n            cores_,\n            memory_,\n            self,\n            workerId,\n            host,\n            webUi.boundPort,\n            publicAddress,\n            sparkHome,\n            executorDir,\n            workerUri,\n            conf,\n            appLocalDirs, ExecutorState.RUNNING)\n          executors(appId + \"/\" + execId) = manager\n          manager.start() //控制权到了这里\n          coresUsed += cores_\n          memoryUsed += memory_\n          sendToMaster(ExecutorStateChanged(appId, execId, manager.state, None, None)) //给Master发送Executor的资源分配已经好了 \n        } catch {\n          case e: Exception =>\n            logError(s\"Failed to launch executor $appId/$execId for ${appDesc.name}.\", e)\n            if (executors.contains(appId + \"/\" + execId)) {\n              executors(appId + \"/\" + execId).kill()\n              executors -= appId + \"/\" + execId\n            }\n            sendToMaster(ExecutorStateChanged(appId, execId, ExecutorState.FAILED,\n              Some(e.toString), None))\n        }\n      }\n```\n\n### 跳转到ExecutorRunner\n\n```scala\n  private[worker] def start() {\n    workerThread = new Thread(\"ExecutorRunner for \" + fullId) {\n      override def run() { fetchAndRunExecutor() } //启动Executro我们点进来看一下\n    }\n    workerThread.start()\n    // Shutdown hook that kills actors on shutdown.\n    shutdownHook = ShutdownHookManager.addShutdownHook { () =>\n      // It's possible that we arrive here before calling `fetchAndRunExecutor`, then `state` will\n      // be `ExecutorState.RUNNING`. In this case, we should set `state` to `FAILED`.\n      if (state == ExecutorState.RUNNING) {\n        state = ExecutorState.FAILED\n      }\n      killProcess(Some(\"Worker shutting down\")) }\n  }\n```\n\n### fetchAndRunExecutor\n\n没有特别指出\n\n```scala\n  private def fetchAndRunExecutor() {\n    try {\n      // Launch the process\n      val builder = CommandUtils.buildProcessBuilder(appDesc.command, new SecurityManager(conf),\n        memory, sparkHome.getAbsolutePath, substituteVariables)\n      val command = builder.command() //这里的builder应该是processbuilder 运行一个本地命令\n      val formattedCommand = command.asScala.mkString(\"\\\"\", \"\\\" \\\"\", \"\\\"\")\n      logInfo(s\"Launch command: $formattedCommand\")\n\n      builder.directory(executorDir)\n      builder.environment.put(\"SPARK_EXECUTOR_DIRS\", appLocalDirs.mkString(File.pathSeparator))\n      // In case we are running this from within the Spark Shell, avoid creating a \"scala\"\n      // parent process for the executor command\n      builder.environment.put(\"SPARK_LAUNCH_WITH_SCALA\", \"0\")\n\n      // Add webUI log urls\n      val baseUrl =\n        if (conf.getBoolean(\"spark.ui.reverseProxy\", false)) {\n          s\"/proxy/$workerId/logPage/?appId=$appId&executorId=$execId&logType=\"\n        } else {\n          s\"http://$publicAddress:$webUiPort/logPage/?appId=$appId&executorId=$execId&logType=\"\n        }\n      builder.environment.put(\"SPARK_LOG_URL_STDERR\", s\"${baseUrl}stderr\")\n      builder.environment.put(\"SPARK_LOG_URL_STDOUT\", s\"${baseUrl}stdout\")\n\n      process = builder.start() // 根据流程图 会创建一个 CoarseGrainedSchedulerBackend 我们去哪里看下\n      val header = \"Spark Executor Command: %s\\n%s\\n\\n\".format(\n        formattedCommand, \"=\" * 40)\n\n      // Redirect its stdout and stderr to files\n      val stdout = new File(executorDir, \"stdout\")\n      stdoutAppender = FileAppender(process.getInputStream, stdout, conf)\n\n      val stderr = new File(executorDir, \"stderr\")\n      Files.write(header, stderr, StandardCharsets.UTF_8)\n      stderrAppender = FileAppender(process.getErrorStream, stderr, conf)\n\n      // Wait for it to exit; executor may exit with code 0 (when driver instructs it to shutdown)\n      // or with nonzero exit code\n      val exitCode = process.waitFor()\n      state = ExecutorState.EXITED\n      val message = \"Command exited with code \" + exitCode\n      worker.send(ExecutorStateChanged(appId, execId, state, Some(message), Some(exitCode)))\n    } catch {\n      case interrupted: InterruptedException =>\n        logInfo(\"Runner thread for executor \" + fullId + \" interrupted\")\n        state = ExecutorState.KILLED\n        killProcess(None)\n      case e: Exception =>\n        logError(\"Error running executor\", e)\n        state = ExecutorState.FAILED\n        killProcess(Some(e.toString))\n    }\n  }\n```\n\n### 回到Master\n\n因为` sendToMaster(ExecutorStateChanged(appId, execId, manager.state, None, None))`给Master发送了一条消息\n\n```scala\n    case ExecutorStateChanged(appId, execId, state, message, exitStatus) =>\n      val execOption = idToApp.get(appId).flatMap(app => app.executors.get(execId))\n      execOption match {\n        case Some(exec) =>\n          val appInfo = idToApp(appId)\n          val oldState = exec.state\n          exec.state = state\n\n          if (state == ExecutorState.RUNNING) {\n            assert(oldState == ExecutorState.LAUNCHING,\n              s\"executor $execId state transfer from $oldState to RUNNING is illegal\")\n            appInfo.resetRetryCount()\n          }\n\t\t\t//查看一下ExecutorUpdated 再StandaloneAppClient中\n          exec.application.driver.send(ExecutorUpdated(execId, state, message, exitStatus, false))\n      }\n```\n\n### 跳转到StandaloneAppClient\n\n### ExecutorUpdated\n\n```scala\n   case ExecutorAdded(id: Int, workerId: String, hostPort: String, cores: Int, memory: Int) =>\n        val fullId = appId + \"/\" + id\n        logInfo((\"Ex\" +\n          \"ecutor added: %s on %s (%s) with %d cores\").format(fullId, workerId, hostPort,\n          cores))\n        listener.executorAdded(fullId, workerId, hostPort, cores, memory) //这里的listener\n\n      case ExecutorUpdated(id, state, message, exitStatus, workerLost) =>\n        val fullId = appId + \"/\" + id\n        val messageText = message.map(s => \" (\" + s + \")\").getOrElse(\"\")\n        logInfo(\"Executor updated: %s is now %s%s\".format(fullId, state, messageText))\n        if (ExecutorState.isFinished(state)) {\n          listener.executorRemoved(fullId, message.getOrElse(\"\"), exitStatus, workerLost)\n        }  //Executor资源基本完成\n\n      case MasterChanged(masterRef, masterWebUiUrl) =>\n        logInfo(\"Master has changed, new master is at \" + masterRef.address.toSparkURL)\n        master = Some(masterRef)\n        alreadyDisconnected = false\n        masterRef.send(MasterChangeAcknowledged(appId.get)) \n    }\n```\n\n### listener\n\n```scala\nprivate[spark] class StandaloneAppClient(\n    rpcEnv: RpcEnv,\n    masterUrls: Array[String],\n    appDescription: ApplicationDescription,\n    listener: StandaloneAppClientListener, //这就是Executor中的listener\n    conf: SparkConf)\n```\n\n### 回到StandaloneSchedulerBackend\n\n```scala\nprivate[spark] class StandaloneSchedulerBackend(\n    scheduler: TaskSchedulerImpl,\n    sc: SparkContext,\n    masters: Array[String])\n  extends CoarseGrainedSchedulerBackend(scheduler, sc.env.rpcEnv) \n  with StandaloneAppClientListener  //继承了那个listner;\n  with Logging {\n      ....\n  }\n```\n\n### StandaloneAppClientListener\n\n```scala\nprivate[spark] trait StandaloneAppClientListener {\n  def connected(appId: String): Unit\n\n  /** Disconnection may be a temporary state, as we fail over to a new Master. */\n  def disconnected(): Unit\n\n  /** An application death is an unrecoverable failure condition. */\n  def dead(reason: String): Unit\n\n  def executorAdded(\n      fullId: String, workerId: String, hostPort: String, cores: Int, memory: Int): Unit\n\n  def executorRemoved(\n      fullId: String, message: String, exitStatus: Option[Int], workerLost: Boolean): Unit\n}\n```\n\n\n\n### 查看executorAdded\n\n```scala\noverride def executorAdded(fullId: String, workerId: String, hostPort: String, cores: Int,\n    memory: Int) {\n    logInfo(\"Granted executor ID %s on hostPort %s with %d cores, %s RAM\".format(\n      fullId, hostPort, cores, Utils.megabytesToString(memory)))\n  }\n```\n\n到此为止步,Executor进程启动完成.\n\n## 任务提交和反馈\n\n通过对RDD的观察行动算子最终调用的方法runJob\n\n### SparkContext\n\n```scala\n  /**\n   * Run a function on a given set of partitions in an RDD and pass the results to the given\n   * handler function. This is the main entry point for all actions in Spark.\n   */\n  def runJob[T, U: ClassTag](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int],\n      resultHandler: (Int, U) => Unit): Unit = {\n    if (stopped.get()) {\n      throw new IllegalStateException(\"SparkContext has been shutdown\")\n    }\n    val callSite = getCallSite\n    val cleanedFunc = clean(func)\n    logInfo(\"Starting job: \" + callSite.shortForm)\n    if (conf.getBoolean(\"spark.logLineage\", false)) {\n      logInfo(\"RDD's recursive dependencies:\\n\" + rdd.toDebugString)\n    }  //我们查看一下dagScheduler\n    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)\n    progressBar.foreach(_.finishAll())\n    rdd.doCheckpoint()\n  }\n\n  /**\n   * Run a function on a given set of partitions in an RDD and return the results as an array.\n   */\n  def runJob[T, U: ClassTag](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int]): Array[U] = {\n    val results = new Array[U](partitions.size)\n    runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res)\n    results\n  }\n\n```\n\n### DAGScheduler\n\n### runrunJob\n\n```scala\n  /**\n   * Run an action job on the given RDD and pass all the results to the resultHandler function as\n   * they arrive.\n   *\n   * @param rdd target RDD to run tasks on\n   * @param func a function to run on each partition of the RDD\n   * @param partitions set of partitions to run on; some jobs may not want to compute on all\n   *   partitions of the target RDD, e.g. for operations like first()\n   * @param callSite where in the user program this job was called\n   * @param resultHandler callback to pass each result to\n   * @param properties scheduler properties to attach to this job, e.g. fair scheduler pool name\n   *\n   * @throws Exception when the job fails\n   */\n  def runJob[T, U](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int],\n      callSite: CallSite,\n      resultHandler: (Int, U) => Unit,\n      properties: Properties): Unit = {\n    val start = System.nanoTime    /**提交Job**/\n    val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) \n    // Note: Do not call Await.ready(future) because that calls `scala.concurrent.blocking`,\n    // which causes concurrent SQL executions to fail if a fork-join pool is used. Note that\n    // due to idiosyncrasies in Scala, `awaitPermission` is not actually used anywhere so it's\n    // safe to pass in null here. For more detail, see SPARK-13747.\n    val awaitPermission = null.asInstanceOf[scala.concurrent.CanAwait]\n    waiter.completionFuture.ready(Duration.Inf)(awaitPermission)\n    waiter.completionFuture.value.get match {\n      case scala.util.Success(_) =>\n        logInfo(\"Job %d finished: %s, took %f s\".format\n          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))\n      case scala.util.Failure(exception) =>\n        logInfo(\"Job %d failed: %s, took %f s\".format\n          (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9))\n        // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.\n        val callerStackTrace = Thread.currentThread().getStackTrace.tail\n        exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)\n        throw exception\n    }\n  }\n```\n\n### submitJob\n\n```scala\n  def submitJob[T, U](\n      rdd: RDD[T],\n      func: (TaskContext, Iterator[T]) => U,\n      partitions: Seq[Int],\n      callSite: CallSite,\n      resultHandler: (Int, U) => Unit,\n      properties: Properties): JobWaiter[U] = {\n    // Check to make sure we are not launching a task on a partition that does not exist.\n    val maxPartitions = rdd.partitions.length\n    partitions.find(p => p >= maxPartitions || p < 0).foreach { p =>\n      throw new IllegalArgumentException(\n        \"Attempting to access a non-existent partition: \" + p + \". \" +\n          \"Total number of partitions: \" + maxPartitions)\n    }\n\n    val jobId = nextJobId.getAndIncrement()\n    if (partitions.size == 0) {\n      // Return immediately if the job is running 0 tasks\n      return new JobWaiter[U](this, jobId, 0, resultHandler)\n    }\n\n    assert(partitions.size > 0)\n    val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) => _]\n    val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)\n    eventProcessLoop.post(JobSubmitted( //post一个JobSubmitted事件\n      jobId, rdd, func2, partitions.toArray, callSite, waiter,\n      SerializationUtils.clone(properties)))\n    waiter\n  }\n```\n\n### eventProcessLoop\n\n```scala\n  private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) \n  taskScheduler.setDAGScheduler(this)\n```\n\n### DAGSchedulerEventProcessLoop\n\n```scala\nprivate[scheduler] class DAGSchedulerEventProcessLoop(dagScheduler: DAGScheduler)\n  extends EventLoop[DAGSchedulerEvent](\"dag-scheduler-event-loop\") with Logging { //点击EventLoop\n\n....\n  }\n}\n```\n\n### EventLoop\n\n```scala\nprivate[spark] abstract class EventLoop[E](name: String) extends Logging {\n\n  private val eventQueue: BlockingQueue[E] = new LinkedBlockingDeque[E]() //阻塞队列\n\n  private val stopped = new AtomicBoolean(false)\n\n  private val eventThread = new Thread(name) {\n    setDaemon(true)\n\n    override def run(): Unit = { //run方法\n      try {\n        while (!stopped.get) {\n          val event = eventQueue.take() //循环读取队列\n          try { //读到序列\n            onReceive(event)  //开启\n          } catch {\n            case NonFatal(e) =>\n              try {\n                onError(e)\n              } catch {\n                case NonFatal(e) => logError(\"Unexpected error in \" + name, e)\n              }\n          }\n        }\n      } catch {\n        case ie: InterruptedException => // exit even if eventQueue is not empty\n        case NonFatal(e) => logError(\"Unexpected error in \" + name, e)\n      }\n    }\n  }\n....\nprotected def onReceive(event: E): Unit  //抽象方法\n```\n\n### 回到DAGSchedulerEventProcessLoop\n\n```scala\n  override def onReceive(event: DAGSchedulerEvent): Unit = {\n    val timerContext = timer.time()\n    try {\n      doOnReceive(event)\n    } finally {\n      timerContext.stop()\n    }\n  }  \nprivate def doOnReceive(event: DAGSchedulerEvent): Unit = event match {\n    case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =>\n      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)  //我们关住一下handleJobSubmitted\n\n    case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =>\n      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)\n\n    case StageCancelled(stageId) =>\n      dagScheduler.handleStageCancellation(stageId)\n\n    case JobCancelled(jobId) =>\n      dagScheduler.handleJobCancellation(jobId)\n\n    case JobGroupCancelled(groupId) =>\n      dagScheduler.handleJobGroupCancelled(groupId)\n\n    case AllJobsCancelled =>\n      dagScheduler.doCancelAllJobs()\n\n    case ExecutorAdded(execId, host) =>\n      dagScheduler.handleExecutorAdded(execId, host)\n\n    case ExecutorLost(execId, reason) =>\n      val filesLost = reason match {\n        case SlaveLost(_, true) => true\n        case _ => false\n      }\n      dagScheduler.handleExecutorLost(execId, filesLost)\n\n    case BeginEvent(task, taskInfo) =>\n      dagScheduler.handleBeginEvent(task, taskInfo)\n\n    case GettingResultEvent(taskInfo) =>\n      dagScheduler.handleGetTaskResult(taskInfo)\n\n    case completion: CompletionEvent =>\n      dagScheduler.handleTaskCompletion(completion)\n\n    case TaskSetFailed(taskSet, reason, exception) =>\n      dagScheduler.handleTaskSetFailed(taskSet, reason, exception)\n\n    case ResubmitFailedStages =>\n      dagScheduler.resubmitFailedStages()\n  }\n```\n\n### handleJobSubmitted\n\n```scala\n  private[scheduler] def handleJobSubmitted(jobId: Int,\n      finalRDD: RDD[_],\n      func: (TaskContext, Iterator[_]) => _,\n      partitions: Array[Int],\n      callSite: CallSite,\n      listener: JobListener,\n      properties: Properties) {\n      //一下代码是如何将代码拆分成一个个stage 再把\n    var finalStage: ResultStage = null\n    try {\n      // New stage creation may throw an exception if, for example, jobs are run on a\n      // HadoopRDD whose underlying HDFS files have been deleted.\n\n      finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)\n    } catch {\n      case e: Exception =>\n        logWarning(\"Creating new stage failed due to exception - job: \" + jobId, e)\n        listener.jobFailed(e)\n        return\n    }\n\n    val job = new ActiveJob(jobId, finalStage, callSite, listener, properties)\n    clearCacheLocs()\n    logInfo(\"Got job %s (%s) with %d output partitions\".format(\n      job.jobId, callSite.shortForm, partitions.length))\n    logInfo(\"Final stage: \" + finalStage + \" (\" + finalStage.name + \")\")\n    logInfo(\"Parents of final stage: \" + finalStage.parents)\n    logInfo(\"Missing parents: \" + getMissingParentStages(finalStage))\n\n    val jobSubmissionTime = clock.getTimeMillis()\n    jobIdToActiveJob(jobId) = job\n    activeJobs += job\n    finalStage.setActiveJob(job)\n    val stageIds = jobIdToStageIds(jobId).toArray\n    val stageInfos = stageIds.flatMap(id => stageIdToStage.get(id).map(_.latestInfo))\n    listenerBus.post(\n      SparkListenerJobStart(job.jobId, jobSubmissionTime, stageInfos, properties))\n      //最后提交了submitStage\n    submitStage(finalStage)\n  }\t\n```\n\n###  submitStage\n\n```scala\n  /** Submits stage, but first recursively submits any missing parents. */\n  private def submitStage(stage: Stage) {\n    val jobId = activeJobForStage(stage)\n    if (jobId.isDefined) {\n      logDebug(\"submitStage(\" + stage + \")\")\n      if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {\n          //stage是从后往前划分\n        val missing = getMissingParentStages(stage).sortBy(_.id)\n        logDebug(\"missing: \" + missing)\n        if (missing.isEmpty) { \n          logInfo(\"Submitting \" + stage + \" (\" + stage.rdd + \"), which has no missing parents\")\n          submitMissingTasks(stage, jobId.get)\n        } else { //如果获得了missing 我们卡产一下是怎么处理的\n          for (parent <- missing) {\n            submitStage(parent)\n          }\n          waitingStages += stage\n        }\n      }\n    } else {\n      abortStage(stage, \"No active job for stage \" + stage.id, None)\n    }\n  }\n```\n\n### getMissingParentStages\n\n```scala\n  private def getMissingParentStages(stage: Stage): List[Stage] = {\n    val missing = new HashSet[Stage]\n    val visited = new HashSet[RDD[_]]\n    // We are manually maintaining a stack here to prevent StackOverflowError\n    // caused by recursively visiting\n    val waitingForVisit = new Stack[RDD[_]] //栈结构的 这是为什么Stage划分从后往前 压栈执行\n    def visit(rdd: RDD[_]) {\n      if (!visited(rdd)) {\n        visited += rdd\n        val rddHasUncachedPartitions = getCacheLocs(rdd).contains(Nil)\n        if (rddHasUncachedPartitions) {\n          for (dep <- rdd.dependencies) {\n            dep match {\n              case shufDep: ShuffleDependency[_, _, _] => //如果是 宽依赖\n                val mapStage = getOrCreateShuffleMapStage(shufDep, stage.firstJobId)\n                if (!mapStage.isAvailable) { \n                  missing += mapStage\n                }\n              case narrowDep: NarrowDependency[_] => //如果是窄依赖\n                waitingForVisit.push(narrowDep.rdd)  //主要是填充stage\n            }\n          }\n        }\n      }\n    }\n    waitingForVisit.push(stage.rdd)\n    while (waitingForVisit.nonEmpty) {\n      visit(waitingForVisit.pop())\n    }\n    missing.toList\n  }\n```\n\n### submitStage\n\n```scala\n/** Submits stage, but first recursively submits any missing parents. */\nprivate def submitStage(stage: Stage) {\n  val jobId = activeJobForStage(stage)\n  if (jobId.isDefined) {\n    logDebug(\"submitStage(\" + stage + \")\") \n    if (!waitingStages(stage) && !runningStages(stage) && !failedStages(stage)) {\n      val missing = getMissingParentStages(stage).sortBy(_.id)\n      logDebug(\"missing: \" + missing) \n      if (missing.isEmpty) { \n        logInfo(\"Submitting \" + stage + \" (\" + stage.rdd + \"), which has no missing parents\")\n        submitMissingTasks(stage, jobId.get)//递归调用\n      } else {\n        for (parent <- missing) {\n          submitStage(parent) //递归\n        }\n        waitingStages += stage\n      }\n    }\n  } else {\n    abortStage(stage, \"No active job for stage \" + stage.id, None)\n  }\n}\n```\n\n### submitMissingTasks\n\n```scala\n  /** Called when stage's parents are available and we can now do its task. */\n  private def submitMissingTasks(stage: Stage, jobId: Int) {\n    logDebug(\"submitMissingTasks(\" + stage + \")\")\n    // Get our pending tasks and remember them in our pendingTasks entry\n    stage.pendingPartitions.clear()\n\n    // First figure out the indexes of partition ids to compute.\n    val partitionsToCompute: Seq[Int] = stage.findMissingPartitions()\n\n    // Use the scheduling pool, job group, description, etc. from an ActiveJob associated\n    // with this Stage\n    val properties = jobIdToActiveJob(jobId).properties\n\n    runningStages += stage\n    // SparkListenerStageSubmitted should be posted before testing whether tasks are\n    // serializable. If tasks are not serializable, a SparkListenerStageCompleted event\n    // will be posted, which should always come after a corresponding SparkListenerStageSubmitted\n    // event.\n    stage match {\n      case s: ShuffleMapStage =>\n        outputCommitCoordinator.stageStart(stage = s.id, maxPartitionId = s.numPartitions - 1)\n      case s: ResultStage =>\n        outputCommitCoordinator.stageStart(\n          stage = s.id, maxPartitionId = s.rdd.partitions.length - 1)\n    }\n    val taskIdToLocations: Map[Int, Seq[TaskLocation]] = try {\n      stage match {\n        case s: ShuffleMapStage =>\n          partitionsToCompute.map { id => (id, getPreferredLocs(stage.rdd, id))}.toMap\n        case s: ResultStage =>\n          partitionsToCompute.map { id =>\n            val p = s.partitions(id)\n            (id, getPreferredLocs(stage.rdd, p))\n          }.toMap\n      }\n    } catch {\n      case NonFatal(e) =>\n        stage.makeNewStageAttempt(partitionsToCompute.size)\n        listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))\n        abortStage(stage, s\"Task creation failed: $e\\n${Utils.exceptionString(e)}\", Some(e))\n        runningStages -= stage\n        return\n    }\n\n    stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)\n    listenerBus.post(SparkListenerStageSubmitted(stage.latestInfo, properties))\n\n    // TODO: Maybe we can keep the taskBinary in Stage to avoid serializing it multiple times.\n    // Broadcasted binary for the task, used to dispatch tasks to executors. Note that we broadcast\n    // the serialized copy of the RDD and for each task we will deserialize it, which means each\n    // task gets a different copy of the RDD. This provides stronger isolation between tasks that\n    // might modify state of objects referenced in their closures. This is necessary in Hadoop\n    // where the JobConf/Configuration object is not thread-safe.\n    var taskBinary: Broadcast[Array[Byte]] = null\n    try {\n      // For ShuffleMapTask, serialize and broadcast (rdd, shuffleDep).\n      // For ResultTask, serialize and broadcast (rdd, func).\n      val taskBinaryBytes: Array[Byte] = stage match {\n        case stage: ShuffleMapStage =>\n          JavaUtils.bufferToArray(\n            closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef))\n        case stage: ResultStage =>\n          JavaUtils.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): AnyRef))\n      }\n\n      taskBinary = sc.broadcast(taskBinaryBytes)\n    } catch {\n      // In the case of a failure during serialization, abort the stage.\n      case e: NotSerializableException =>\n        abortStage(stage, \"Task not serializable: \" + e.toString, Some(e))\n        runningStages -= stage\n\n        // Abort execution\n        return\n      case NonFatal(e) =>\n        abortStage(stage, s\"Task serialization failed: $e\\n${Utils.exceptionString(e)}\", Some(e))\n        runningStages -= stage\n        return\n    }\n\n    val tasks: Seq[Task[_]] = try {  //转换成一系列Task任务\n      stage match {\n        case stage: ShuffleMapStage =>\n          partitionsToCompute.map { id =>\n            val locs = taskIdToLocations(id)\n            val part = stage.rdd.partitions(id)\n            new ShuffleMapTask(stage.id, stage.latestInfo.attemptId,\n              taskBinary, part, locs, stage.latestInfo.taskMetrics, properties, Option(jobId),\n              Option(sc.applicationId), sc.applicationAttemptId)\n          }\n\n        case stage: ResultStage =>\n          partitionsToCompute.map { id =>\n            val p: Int = stage.partitions(id)\n            val part = stage.rdd.partitions(p)\n            val locs = taskIdToLocations(id)\n            new ResultTask(stage.id, stage.latestInfo.attemptId,\n              taskBinary, part, locs, id, properties, stage.latestInfo.taskMetrics,\n              Option(jobId), Option(sc.applicationId), sc.applicationAttemptId)\n          }\n      }\n    } catch {\n      case NonFatal(e) =>\n        abortStage(stage, s\"Task creation failed: $e\\n${Utils.exceptionString(e)}\", Some(e))\n        runningStages -= stage\n        return\n    }\n\n    if (tasks.size > 0) {\n      logInfo(\"Submitting \" + tasks.size + \" missing tasks from \" + stage + \" (\" + stage.rdd + \")\")\n      stage.pendingPartitions ++= tasks.map(_.partitionId)\n      logDebug(\"New pending partitions: \" + stage.pendingPartitions)\n      taskScheduler.submitTasks(new TaskSet( //提交我们的任务,把我们所有的的任务封装成了一个对象\n        tasks.toArray, stage.id, stage.latestInfo.attemptId, jobId, properties))\n      stage.latestInfo.submissionTime = Some(clock.getTimeMillis())\n    } else {\n      // Because we posted SparkListenerStageSubmitted earlier, we should mark\n      // the stage as completed here in case there are no tasks to run\n      markStageAsFinished(stage, None)\n\n      val debugString = stage match {\n        case stage: ShuffleMapStage =>\n          s\"Stage ${stage} is actually done; \" +\n            s\"(available: ${stage.isAvailable},\" +\n            s\"available outputs: ${stage.numAvailableOutputs},\" +\n            s\"partitions: ${stage.numPartitions})\"\n        case stage : ResultStage =>\n          s\"Stage ${stage} is actually done; (partitions: ${stage.numPartitions})\"\n      }\n      logDebug(debugString)\n\n      submitWaitingChildStages(stage)\n    }\n  }\n```\n\n### submitTasks\n\n```scala\n  def submitTasks(taskSet: TaskSet): Unit\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190611230725.png)\n\n找到实现类\n\n### 跳转TaskSchedulerImpl\n\n### submitTasks\n\n```scala\n  override def submitTasks(taskSet: TaskSet) {\n    val tasks = taskSet.tasks\n    logInfo(\"Adding task set \" + taskSet.id + \" with \" + tasks.length + \" tasks\")\n    this.synchronized {  //同步\n      val manager = createTaskSetManager(taskSet, maxTaskFailures)\n      val stage = taskSet.stageId\n      val stageTaskSets =\n        taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, new HashMap[Int, TaskSetManager])\n      stageTaskSets(taskSet.stageAttemptId) = manager\n      val conflictingTaskSet = stageTaskSets.exists { case (_, ts) =>\n        ts.taskSet != taskSet && !ts.isZombie\n      }\n      if (conflictingTaskSet) {\n        throw new IllegalStateException(s\"more than one active taskSet for stage $stage:\" +\n          s\" ${stageTaskSets.toSeq.map{_._2.taskSet.id}.mkString(\",\")}\")\n      }\n      schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)\n\n      if (!isLocal && !hasReceivedTask) {\n        starvationTimer.scheduleAtFixedRate(new TimerTask() {\n          override def run() {\n            if (!hasLaunchedTask) {\n              logWarning(\"Initial job has not accepted any resources; \" +\n                \"check your cluster UI to ensure that workers are registered \" +\n                \"and have sufficient resources\")\n            } else {\n              this.cancel()\n            }\n          }\n        }, STARVATION_TIMEOUT_MS, STARVATION_TIMEOUT_MS)\n      }\n      hasReceivedTask = true\n    }\n    backend.reviveOffers() //这个backend再StandaloneSchedulerBackend出现过\n  }\n```\n\n在tandaloneSchedulerBackend没有找到.我们去`CoarseGrainedSchedulerBackend`中寻找一下\n\n### 回到CoarseGrainedSchedulerBackend\n\n```scala\ncase ReviveOffers =>\n        makeOffers() //跳转到\n\n...\n\noverride def reviveOffers() {\n    driverEndpoint.send(ReviveOffers) //\n  }\n```\n\n### makeOffers\n\n```scala\n  // Make fake resource offers on all executors\n    private def makeOffers() {\n      // Filter out executors under killing\n      val activeExecutors = executorDataMap.filterKeys(executorIsAlive)\n      val workOffers = activeExecutors.map { case (id, executorData) =>\n        new WorkerOffer(id, executorData.executorHost, executorData.freeCores)\n      }.toIndexedSeq\n      launchTasks(scheduler.resourceOffers(workOffers //运行任务 申请资源  resourceOffers 具体做关键资源\n    }\n```\n\n### 跳转TaskSchedulerImpl\n\n### resourceOffers\n\n```scala\n /**\n   * Called by cluster manager to offer resources on slaves. We respond by asking our active task\n   * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so\n   * that tasks are balanced across the cluster.\n   */  \n//  该类具体做具体的资源分配\n  def resourceOffers(offers: IndexedSeq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {\n    // Mark each slave as alive and remember its hostname\n    // Also track if new executor is added\n    var newExecAvail = false\n    for (o <- offers) {\n      if (!hostToExecutors.contains(o.host)) {\n        hostToExecutors(o.host) = new HashSet[String]()\n      }\n      if (!executorIdToRunningTaskIds.contains(o.executorId)) {\n        hostToExecutors(o.host) += o.executorId\n        executorAdded(o.executorId, o.host)\n        executorIdToHost(o.executorId) = o.host\n        executorIdToRunningTaskIds(o.executorId) = HashSet[Long]()\n        newExecAvail = true\n      }\n      for (rack <- getRackForHost(o.host)) {\n        hostsByRack.getOrElseUpdate(rack, new HashSet[String]()) += o.host\n      }\n    }\n\n    // Randomly shuffle offers to avoid always placing tasks on the same set of workers.\n    val shuffledOffers = Random.shuffle(offers)\n    // Build a list of tasks to assign to each worker.\n    val tasks = shuffledOffers.map(o => new ArrayBuffer[TaskDescription](o.cores))\n    val availableCpus = shuffledOffers.map(o => o.cores).toArray\n    val sortedTaskSets = rootPool.getSortedTaskSetQueue\n    for (taskSet <- sortedTaskSets) {\n      logDebug(\"parentName: %s, name: %s, runningTasks: %s\".format(\n        taskSet.parent.name, taskSet.name, taskSet.runningTasks))\n      if (newExecAvail) {\n        taskSet.executorAdded()\n      }\n    }\n\n    // Take each TaskSet in our scheduling order, and then offer it each node in increasing order\n    // of locality levels so that it gets a chance to launch local tasks on all of them.\n    // NOTE: the preferredLocality order: PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY\n    for (taskSet <- sortedTaskSets) {\n      var launchedAnyTask = false\n      var launchedTaskAtCurrentMaxLocality = false\n      for (currentMaxLocality <- taskSet.myLocalityLevels) {\n        do {\n          launchedTaskAtCurrentMaxLocality = resourceOfferSingleTaskSet(\n            taskSet, currentMaxLocality, shuffledOffers, availableCpus, tasks)\n          launchedAnyTask |= launchedTaskAtCurrentMaxLocality\n        } while (launchedTaskAtCurrentMaxLocality)\n      }\n      if (!launchedAnyTask) {\n        taskSet.abortIfCompletelyBlacklisted(hostToExecutors)\n      }\n    }\n\n    if (tasks.size > 0) {\n      hasLaunchedTask = true\n    }\n    return tasks\n  }\n```\n\n### 回到CoarseGrainedSchedulerBackend\n\n```scala\n    // Launch tasks returned by a set of resource offers\n    private def launchTasks(tasks: Seq[Seq[TaskDescription]]) {\n      for (task <- tasks.flatten) {\n        val serializedTask = ser.serialize(task)\n        if (serializedTask.limit >= maxRpcMessageSize) {\n          scheduler.taskIdToTaskSetManager.get(task.taskId).foreach { taskSetMgr =>\n            try {\n              var msg = \"Serialized task %s:%d was %d bytes, which exceeds max allowed: \" +\n                \"spark.rpc.message.maxSize (%d bytes). Consider increasing \" +\n                \"spark.rpc.message.maxSize or using broadcast variables for large values.\"\n              msg = msg.format(task.taskId, task.index, serializedTask.limit, maxRpcMessageSize)\n              taskSetMgr.abort(msg)\n            } catch {\n              case e: Exception => logError(\"Exception in error callback\", e)\n            }\n          }\n        }\n        else {\n          val executorData = executorDataMap(task.executorId)\n          executorData.freeCores -= scheduler.CPUS_PER_TASK\n\n          logDebug(s\"Launching task ${task.taskId} on executor id: ${task.executorId} hostname: \" +\n            s\"${executorData.executorHost}.\")\n\t\t//控制权转到CoarseGrainedExecutorBackend的LaunchTask\n          executorData.executorEndpoint.send(LaunchTask(new SerializableBuffer(serializedTask)))\n        }\n      }\n    }\n```\n\n### 跳转CoarseGrainedExecutorBackend\n\n### LaunchTask\n\n```scala\n   case LaunchTask(data) =>\n      if (executor == null) {\n        exitExecutor(1, \"Received LaunchTask command but executor was null\")\n      } else {\n        val taskDesc = ser.deserialize[TaskDescription](data.value)\n        logInfo(\"Got assigned task \" + taskDesc.taskId) //调用executor的launchTask\n        executor.launchTask(this, taskId = taskDesc.taskId, attemptNumber = taskDesc.attemptNumber,\n          taskDesc.name, taskDesc.serializedTask)\n      }\n```\n\n### 跳转到Executor\n\n### launchTask\n\n```scala\n  def launchTask(\n      context: ExecutorBackend,\n      taskId: Long,\n      attemptNumber: Int,\n      taskName: String,\n      serializedTask: ByteBuffer): Unit = {\n      \t\t//创建一个TaskRunner\n    val tr = new TaskRunner(context, taskId = taskId, attemptNumber = attemptNumber, taskName,\n      serializedTask)\n    runningTasks.put(taskId, tr)\n    threadPool.execute(tr)  //线程池 \n  }\n```\n\n### TaskRunner\n\n我们关注一下run方法 上面的西线程池运行的是此处的润方法\n\n```scala\n     override def run(): Unit = {\n      threadId = Thread.currentThread.getId\n      Thread.currentThread.setName(threadName)\n      val threadMXBean = ManagementFactory.getThreadMXBean\n      val taskMemoryManager = new TaskMemoryManager(env.memoryManager, taskId)\n      val deserializeStartTime = System.currentTimeMillis()\n      val deserializeStartCpuTime = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n        threadMXBean.getCurrentThreadCpuTime\n      } else 0L\n      Thread.currentThread.setContextClassLoader(replClassLoader)\n      val ser = env.closureSerializer.newInstance()\n      logInfo(s\"Running $taskName (TID $taskId)\")\n      execBackend.statusUpdate(taskId, TaskState.RUNNING, EMPTY_BYTE_BUFFER)\n      var taskStart: Long = 0\n      var taskStartCpu: Long = 0\n      startGCTime = computeTotalGcTime()\n\n      try {\n        val (taskFiles, taskJars, taskProps, taskBytes) =\n          Task.deserializeWithDependencies(serializedTask)\n\n        // Must be set before updateDependencies() is called, in case fetching dependencies\n        // requires access to properties contained within (e.g. for access control).\n        Executor.taskDeserializationProps.set(taskProps)\n\n        updateDependencies(taskFiles, taskJars)\n        task = ser.deserialize[Task[Any]](taskBytes, Thread.currentThread.getContextClassLoader)\n        task.localProperties = taskProps\n        task.setTaskMemoryManager(taskMemoryManager)\n\n        // If this task has been killed before we deserialized it, let's quit now. Otherwise,\n        // continue executing the task.\n        if (killed) {\n          // Throw an exception rather than returning, because returning within a try{} block\n          // causes a NonLocalReturnControl exception to be thrown. The NonLocalReturnControl\n          // exception will be caught by the catch block, leading to an incorrect ExceptionFailure\n          // for the task.\n          throw new TaskKilledException\n        }\n\n        logDebug(\"Task \" + taskId + \"'s epoch is \" + task.epoch)\n        env.mapOutputTracker.updateEpoch(task.epoch)\n\n        // Run the actual task and measure its runtime.\n        taskStart = System.currentTimeMillis()\n        taskStartCpu = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n          threadMXBean.getCurrentThreadCpuTime\n        } else 0L\n        var threwException = true\n        val value = try {\n          val res = task.run(\n            taskAttemptId = taskId,\n            attemptNumber = attemptNumber,\n            metricsSystem = env.metricsSystem)\n          threwException = false\n          res\n        } finally {\n          val releasedLocks = env.blockManager.releaseAllLocksForTask(taskId)\n          val freedMemory = taskMemoryManager.cleanUpAllAllocatedMemory()\n\n          if (freedMemory > 0 && !threwException) {\n            val errMsg = s\"Managed memory leak detected; size = $freedMemory bytes, TID = $taskId\"\n            if (conf.getBoolean(\"spark.unsafe.exceptionOnMemoryLeak\", false)) {\n              throw new SparkException(errMsg)\n            } else {\n              logWarning(errMsg)\n            }\n          }\n\n          if (releasedLocks.nonEmpty && !threwException) {\n            val errMsg =\n              s\"${releasedLocks.size} block locks were not released by TID = $taskId:\\n\" +\n                releasedLocks.mkString(\"[\", \", \", \"]\")\n            if (conf.getBoolean(\"spark.storage.exceptionOnPinLeak\", false)) {\n              throw new SparkException(errMsg)\n            } else {\n              logWarning(errMsg)\n            }\n          }\n        }\n        val taskFinish = System.currentTimeMillis()\n        val taskFinishCpu = if (threadMXBean.isCurrentThreadCpuTimeSupported) {\n          threadMXBean.getCurrentThreadCpuTime\n        } else 0L\n\n        // If the task has been killed, let's fail it.\n        if (task.killed) {\n          throw new TaskKilledException\n        }\n\n        val resultSer = env.serializer.newInstance()\n        val beforeSerialization = System.currentTimeMillis()\n        val valueBytes = resultSer.serialize(value)\n        val afterSerialization = System.currentTimeMillis()\n\n        // Deserialization happens in two parts: first, we deserialize a Task object, which\n        // includes the Partition. Second, Task.run() deserializes the RDD and function to be run.\n        task.metrics.setExecutorDeserializeTime(\n          (taskStart - deserializeStartTime) + task.executorDeserializeTime)\n        task.metrics.setExecutorDeserializeCpuTime(\n          (taskStartCpu - deserializeStartCpuTime) + task.executorDeserializeCpuTime)\n        // We need to subtract Task.run()'s deserialization time to avoid double-counting\n        task.metrics.setExecutorRunTime((taskFinish - taskStart) - task.executorDeserializeTime)\n        task.metrics.setExecutorCpuTime(\n          (taskFinishCpu - taskStartCpu) - task.executorDeserializeCpuTime)\n        task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)\n        task.metrics.setResultSerializationTime(afterSerialization - beforeSerialization)\n\n        // Note: accumulator updates must be collected after TaskMetrics is updated\n        val accumUpdates = task.collectAccumulatorUpdates()\n        // TODO: do not serialize value twice\n        val directResult = new DirectTaskResult(valueBytes, accumUpdates)\n        val serializedDirectResult = ser.serialize(directResult)\n        val resultSize = serializedDirectResult.limit\n\n        // directSend = sending directly back to the driver\n        val serializedResult: ByteBuffer = {\n          if (maxResultSize > 0 && resultSize > maxResultSize) {\n            logWarning(s\"Finished $taskName (TID $taskId). Result is larger than maxResultSize \" +\n              s\"(${Utils.bytesToString(resultSize)} > ${Utils.bytesToString(maxResultSize)}), \" +\n              s\"dropping it.\")\n            ser.serialize(new IndirectTaskResult[Any](TaskResultBlockId(taskId), resultSize))\n          } else if (resultSize > maxDirectResultSize) {\n            val blockId = TaskResultBlockId(taskId)\n            env.blockManager.putBytes(\n              blockId,\n              new ChunkedByteBuffer(serializedDirectResult.duplicate()),\n              StorageLevel.MEMORY_AND_DISK_SER)\n            logInfo(\n              s\"Finished $taskName (TID $taskId). $resultSize bytes result sent via BlockManager)\")\n            ser.serialize(new IndirectTaskResult[Any](blockId, resultSize))\n          } else {\n            logInfo(s\"Finished $taskName (TID $taskId). $resultSize bytes result sent to driver\")\n            serializedDirectResult\n          }\n        }\n\t\t//无论失败与否发送一个状态statusUpdate  点击statusUpdate 我们查看一下\n        execBackend.statusUpdate(taskId, TaskState.FINISHED, serializedResult)\n\n      } catch {\n        case ffe: FetchFailedException =>\n          val reason = ffe.toTaskFailedReason\n          setTaskFinishedAndClearInterruptStatus()\n          execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason))\n\n        case _: TaskKilledException =>\n          logInfo(s\"Executor killed $taskName (TID $taskId)\")\n          setTaskFinishedAndClearInterruptStatus()\n          execBackend.statusUpdate(taskId, TaskState.KILLED, ser.serialize(TaskKilled))\n\n        case _: InterruptedException if task.killed =>\n          logInfo(s\"Executor interrupted and killed $taskName (TID $taskId)\")\n          setTaskFinishedAndClearInterruptStatus()\n          execBackend.statusUpdate(taskId, TaskState.KILLED, ser.serialize(TaskKilled))\n\n        case CausedBy(cDE: CommitDeniedException) =>\n          val reason = cDE.toTaskFailedReason\n          setTaskFinishedAndClearInterruptStatus()\n          execBackend.statusUpdate(taskId, TaskState.FAILED, ser.serialize(reason))\n\n        case t: Throwable =>\n          // Attempt to exit cleanly by informing the driver of our failure.\n          // If anything goes wrong (or this was a fatal exception), we will delegate to\n          // the default uncaught exception handler, which will terminate the Executor.\n          logError(s\"Exception in $taskName (TID $taskId)\", t)\n\n          // Collect latest accumulator values to report back to the driver\n          val accums: Seq[AccumulatorV2[_, _]] =\n            if (task != null) {\n              task.metrics.setExecutorRunTime(System.currentTimeMillis() - taskStart)\n              task.metrics.setJvmGCTime(computeTotalGcTime() - startGCTime)\n              task.collectAccumulatorUpdates(taskFailed = true)\n            } else {\n              Seq.empty\n            }\n\n          val accUpdates = accums.map(acc => acc.toInfo(Some(acc.value), None))\n\n          val serializedTaskEndReason = {\n            try {\n              ser.serialize(new ExceptionFailure(t, accUpdates).withAccums(accums))\n            } catch {\n              case _: NotSerializableException =>\n                // t is not serializable so just send the stacktrace\n                ser.serialize(new ExceptionFailure(t, accUpdates, false).withAccums(accums))\n            }\n          }\n          setTaskFinishedAndClearInterruptStatus()\n          execBackend.statusUpdate(taskId, TaskState.FAILED, serializedTaskEndReason)\n\n          // Don't forcibly exit unless the exception was inherently fatal, to avoid\n          // stopping other tasks unnecessarily.\n          if (Utils.isFatalError(t)) {\n            SparkUncaughtExceptionHandler.uncaughtException(t)\n          }\n\n      } finally {\n        runningTasks.remove(taskId)\n      }\n    }\n  }\n\n```\n\n### statusUpdate\n\n```scala\nprivate[spark] trait ExecutorBackend {\n  def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer): Unit\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190611233512.png)\n\n```scala\n  override def statusUpdate(taskId: Long, state: TaskState, data: ByteBuffer) {\n    val msg = StatusUpdate(executorId, taskId, state, data)\n    driver match {\n      case Some(driverRef) => driverRef.send(msg)  //给driver发送了一条消息 控制权回到Driver\n      case None => logWarning(s\"Drop $msg because has not yet connected to driver\")\n    }\n  }\n```\n\n### 回到CoarseGrainedSchedulerBackend\n\n```scala\n   override def receive: PartialFunction[Any, Unit] = {\n      case StatusUpdate(executorId, taskId, state, data) =>\n        scheduler.statusUpdate(taskId, state, data.value) //调用了scheduler(TaskSchedulerImpl)的statusUpdate方法\n        if (TaskState.isFinished(state)) { //如果任务结束了\n          executorDataMap.get(executorId) match {\n            case Some(executorInfo) => //如果还有executor的资源\n              executorInfo.freeCores += scheduler.CPUS_PER_TASK\n              makeOffers(executorId) //继续调度\n            case None =>\n              // Ignoring the update since we don't know about the executor.\n              logWarning(s\"Ignored task status update ($taskId state $state) \" +\n                s\"from unknown executor with ID $executorId\")\n          }\n        }\n```\n\n### 回到TaskSchedulerImpl\n\n### statusUpdate\n\n```scala\n  def statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer) {\n    var failedExecutor: Option[String] = None\n    var reason: Option[ExecutorLossReason] = None\n    synchronized {\n      try {\n        taskIdToTaskSetManager.get(tid) match {\n          case Some(taskSet) =>\n            if (state == TaskState.LOST) { //如果所有的任务状态都完成了\n              // TaskState.LOST is only used by the deprecated Mesos fine-grained scheduling mode,\n              // where each executor corresponds to a single task, so mark the executor as failed.\n              val execId = taskIdToExecutorId.getOrElse(tid, throw new IllegalStateException(\n                \"taskIdToTaskSetManager.contains(tid) <=> taskIdToExecutorId.contains(tid)\"))\n              if (executorIdToRunningTaskIds.contains(execId)) {\n                reason = Some(\n                  SlaveLost(s\"Task $tid was lost, so marking the executor as lost as well.\"))\n                removeExecutor(execId, reason.get)  //删除Executor\n                failedExecutor = Some(execId)\n              }\n            }\n            if (TaskState.isFinished(state)) {\n              cleanupTaskState(tid)\n              taskSet.removeRunningTask(tid)\n              if (state == TaskState.FINISHED) {\n                taskResultGetter.enqueueSuccessfulTask(taskSet, tid, serializedData)\n              } else if (Set(TaskState.FAILED, TaskState.KILLED, TaskState.LOST).contains(state)) {\n                taskResultGetter.enqueueFailedTask(taskSet, tid, state, serializedData)\n              }\n            }\n          case None =>\n            logError(\n              (\"Ignoring update with state %s for TID %s because its task set is gone (this is \" +\n                \"likely the result of receiving duplicate task finished status updates) or its \" +\n                \"executor has been marked as failed.\")\n                .format(state, tid))\n        }\n      } catch {\n        case e: Exception => logError(\"Exception in statusUpdate\", e)\n      }\n    }\n    // Update the DAGScheduler without holding a lock on this, since that can deadlock\n    if (failedExecutor.isDefined) {\n      assert(reason.isDefined)\n      dagScheduler.executorLost(failedExecutor.get, reason.get)\n      backend.reviveOffers()\n    }\n  }\n```\n\n至此任务提交已经基本完成。\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark"],"categories":["大数据"]},{"title":"Spark内核解析2","url":"/2019/06/09/Spark内核解析2/","content":"\n {{ \"Spark的交互流程- - -节点启动\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609154253.png)\n\nMaster启动时首先创一个RpcEnv对象，负责管理所有通信逻辑\nMaster通过RpcEnv对象创建一个Endpoint，Master就是一个Endpoint，Worker可以与其进行通信\nWorker启动时也是创一个RpcEnv对象\nWorker通过RpcEnv对象创建一个Endpoint\nWorker通过RpcEnv对，建立到Master的连接，获取到一个RpcEndpointRef对象，通过该对象可以与Master通信\nWorker向Master注册，注册内容包括主机名、端口、CPU Core数量、内存数量\nMaster接收到Worker的注册，将注册信息维护在内存中的Table中，其中还包含了一个到Worker的RpcEndpointRef对象引用\nMaster回复Worker已经接收到注册，告知Worker已经注册成功\n此时如果有用户提交Spark程序，Master需要协调启动Driver；而Worker端收到成功注册响应后，开始周期性向Master发送心跳\n\n## Mster\n\nMaster的main方法启动创建了一个startRpcEnvAndEndpoint\n\n```scala\ndef main(argStrings: Array[String]) {\n    // 1、初始化log对象\n    Utils.initDaemon(log)\n    // 2、加载SparkConf\n    val conf = new SparkConf\n    // 3、解析Master启动参数\n    val args = new MasterArguments(argStrings, conf)  \n    // 4、启动RPC框架端点\n    val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)\n    rpcEnv.awaitTermination()\n  }\n```\n\n我们点击`startRpcEnvAndEndpoint`查看中创建了一个Master\n\n```scala\n  def startRpcEnvAndEndpoint(\n      host: String,  \n      port: Int,\n      webUiPort: Int,\n      conf: SparkConf): (RpcEnv, Int, Option[Int]) = {\n    val securityMgr = new SecurityManager(conf)\n    val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr)\n    val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME,\n      new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf))  //创建一个Master\n    val portsResponse = masterEndpoint.askWithRetry[BoundPortsResponse](BoundPortsRequest)\n    (rpcEnv, portsResponse.webUIPort, portsResponse.restPort)\n  }\n}\n```\n\n在Master中我们重点关注一下onStart()方法,绑定WebUI,和定期心跳检测\n\n```scala\noverride def onStart(): Unit = {\n    logInfo(\"Starting Spark master at \" + masterUrl)\n    logInfo(s\"Running Spark version ${org.apache.spark.SPARK_VERSION}\")\n    webUi = new MasterWebUI(this, webUiPort)\n    webUi.bind()\n    masterWebUiUrl = \"http://\" + masterPublicAddress + \":\" + webUi.boundPort\n    if (reverseProxy) {\n      masterWebUiUrl = conf.get(\"spark.ui.reverseProxyUrl\", masterWebUiUrl)\n      logInfo(s\"Spark Master is acting as a reverse proxy. Master, Workers and \" +\n       s\"Applications UIs are available at $masterWebUiUrl\")\n    }\n    checkForWorkerTimeOutTask = forwardMessageThread.scheduleAtFixedRate(new Runnable {\n      override def run(): Unit = Utils.tryLogNonFatalError {\n        self.send(CheckForWorkerTimeOut)  //周期性的自己给自己发送一条消息\n      }\n    }, 0, WORKER_TIMEOUT_MS, TimeUnit.MILLISECONDS)  //WORKER_TIMEOUT_MS为60*1000 也就是一分钟超时时间为1分钟\n\n    if (restServerEnabled) {\n      val port = conf.getInt(\"spark.master.rest.port\", 6066)\n      restServer = Some(new StandaloneRestServer(address.host, port, conf, self, masterUrl))\n    }\n    restServerBoundPort = restServer.map(_.start())\n\n    masterMetricsSystem.registerSource(masterSource)\n    masterMetricsSystem.start()\n    applicationMetricsSystem.start()\n    // Attach the master and app metrics servlet handler to the web ui after the metrics systems are\n    // started.\n    masterMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)\n    applicationMetricsSystem.getServletHandlers.foreach(webUi.attachHandler)\n \n    val serializer = new JavaSerializer(conf)\n    val (persistenceEngine_, leaderElectionAgent_) = RECOVERY_MODE match { //RECOVERY_MODE是否设置\n      case \"ZOOKEEPER\" =>\n        logInfo(\"Persisting recovery state to ZooKeeper\")\n        val zkFactory =\n          new ZooKeeperRecoveryModeFactory(conf, serializer)\n        (zkFactory.createPersistenceEngine(), zkFactory.createLeaderElectionAgent(this))\n      case \"FILESYSTEM\" =>\n        val fsFactory =\n          new FileSystemRecoveryModeFactory(conf, serializer)\n        (fsFactory.createPersistenceEngine(), fsFactory.createLeaderElectionAgent(this))\n      case \"CUSTOM\" =>\n        val clazz = Utils.classForName(conf.get(\"spark.deploy.recoveryMode.factory\"))\n        val factory = clazz.getConstructor(classOf[SparkConf], classOf[Serializer])\n          .newInstance(conf, serializer)\n          .asInstanceOf[StandaloneRecoveryModeFactory]\n        (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(this))\n      case _ =>\n        (new BlackHolePersistenceEngine(), new MonarchyLeaderAgent(this))\n    }\n    persistenceEngine = persistenceEngine_\n    leaderElectionAgent = leaderElectionAgent_\n  }\n```\n\n我们在查看一下receive 这些只是受到消息,然而并没有恢复\n\n```scala\noverride def receive: PartialFunction[Any, Unit] = {\n    case ElectedLeader => // Leader选举,用在SparkHA中\n      val (storedApps, storedDrivers, storedWorkers) = persistenceEngine.readPersistedData(rpcEnv)\n      state = if (storedApps.isEmpty && storedDrivers.isEmpty && storedWorkers.isEmpty) {\n        RecoveryState.ALIVE\n      } else {\n        RecoveryState.RECOVERING\n      }\n      logInfo(\"I have been elected leader! New state: \" + state)\n      if (state == RecoveryState.RECOVERING) {\n        beginRecovery(storedApps, storedDrivers, storedWorkers)\n        recoveryCompletionTask = forwardMessageThread.schedule(new Runnable {\n          override def run(): Unit = Utils.tryLogNonFatalError {\n            self.send(CompleteRecovery)\n          }\n        }, WORKER_TIMEOUT_MS, TimeUnit.MILLISECONDS)\n      }\n\n    case CompleteRecovery => completeRecovery() //成功地恢复\n\n    case RevokedLeadership => //\n      logError(\"Leadership has been revoked -- master shutting down.\")\n      System.exit(0)\n\t//注册应用\n    case RegisterApplication(description, driver) =>\n      // TODO Prevent repeated registrations from some driver\n      if (state == RecoveryState.STANDBY) {\n        // ignore, don't send response\n      } else {\n        logInfo(\"Registering app \" + description.name)\n        val app = createApplication(description, driver)\n        registerApplication(app)\n        logInfo(\"Registered app \" + description.name + \" with ID \" + app.id)\n        persistenceEngine.addApplication(app)\n        driver.send(RegisteredApplication(app.id, self))\n        // 启动分配Executor\n        schedule()\n      }\n    // 检测 Executor\n    case ExecutorStateChanged(appId, execId, state, message, exitStatus) =>\n      val execOption = idToApp.get(appId).flatMap(app => app.executors.get(execId))\n      execOption match {\n        case Some(exec) =>\n          val appInfo = idToApp(appId)\n          val oldState = exec.state\n          exec.state = state\n\n          if (state == ExecutorState.RUNNING) {\n            assert(oldState == ExecutorState.LAUNCHING,\n              s\"executor $execId state transfer from $oldState to RUNNING is illegal\")\n            appInfo.resetRetryCount()\n          }\n\n          exec.application.driver.send(ExecutorUpdated(execId, state, message, exitStatus, false))\n\n          if (ExecutorState.isFinished(state)) {\n            // Remove this executor from the worker and app\n            logInfo(s\"Removing executor ${exec.fullId} because it is $state\")\n            // If an application has already finished, preserve its\n            // state to display its information properly on the UI\n            if (!appInfo.isFinished) {\n              appInfo.removeExecutor(exec)\n            }\n            exec.worker.removeExecutor(exec)\n\n            val normalExit = exitStatus == Some(0)\n            // Only retry certain number of times so we don't go into an infinite loop.\n            // Important note: this code path is not exercised by tests, so be very careful when\n            // changing this `if` condition.\n            if (!normalExit\n                && appInfo.incrementRetryCount() >= MAX_EXECUTOR_RETRIES\n                && MAX_EXECUTOR_RETRIES >= 0) { // < 0 disables this application-killing path\n              val execs = appInfo.executors.values\n              if (!execs.exists(_.state == ExecutorState.RUNNING)) {\n                logError(s\"Application ${appInfo.desc.name} with ID ${appInfo.id} failed \" +\n                  s\"${appInfo.retryCount} times; removing it\")\n                removeApplication(appInfo, ApplicationState.FAILED)\n              }\n            }\n          }\n          schedule()\n        case None =>\n          logWarning(s\"Got status update for unknown executor $appId/$execId\")\n      }\n\n    case DriverStateChanged(driverId, state, exception) =>\n      state match {\n        case DriverState.ERROR | DriverState.FINISHED | DriverState.KILLED | DriverState.FAILED =>\n          removeDriver(driverId, state, exception)\n        case _ =>\n          throw new Exception(s\"Received unexpected state update for driver $driverId: $state\")\n      }\n\t//心跳机制\n    case Heartbeat(workerId, worker) =>\n      idToWorker.get(workerId) match {\n        case Some(workerInfo) =>\n          workerInfo.lastHeartbeat = System.currentTimeMillis()\n        case None =>\n          if (workers.map(_.id).contains(workerId)) {\n            logWarning(s\"Got heartbeat from unregistered worker $workerId.\" +\n              \" Asking it to re-register.\")\n            worker.send(ReconnectWorker(masterUrl))\n          } else {\n            logWarning(s\"Got heartbeat from unregistered worker $workerId.\" +\n              \" This worker was never registered, so ignoring the heartbeat.\")\n          }\n      }\n\n    case MasterChangeAcknowledged(appId) =>\n      idToApp.get(appId) match {\n        case Some(app) =>\n          logInfo(\"Application has been re-registered: \" + appId)\n          app.state = ApplicationState.WAITING\n        case None =>\n          logWarning(\"Master change ack from unknown app: \" + appId)\n      }\n\n      if (canCompleteRecovery) { completeRecovery() }\n\t//反馈Worker的运行状态\n    case WorkerSchedulerStateResponse(workerId, executors, driverIds) =>\n      idToWorker.get(workerId) match {\n        case Some(worker) =>\n          logInfo(\"Worker has been re-registered: \" + workerId)\n          worker.state = WorkerState.ALIVE\n\n          val validExecutors = executors.filter(exec => idToApp.get(exec.appId).isDefined)\n          for (exec <- validExecutors) {\n            val app = idToApp.get(exec.appId).get\n            val execInfo = app.addExecutor(worker, exec.cores, Some(exec.execId))\n            worker.addExecutor(execInfo)\n            execInfo.copyState(exec)\n          }\n\n          for (driverId <- driverIds) {\n            drivers.find(_.id == driverId).foreach { driver =>\n              driver.worker = Some(worker)\n              driver.state = DriverState.RUNNING\n              worker.drivers(driverId) = driver\n            }\n          }\n        case None =>\n          logWarning(\"Scheduler state from unknown worker: \" + workerId)\n      }\n\n      if (canCompleteRecovery) { completeRecovery() }\n\n    case WorkerLatestState(workerId, executors, driverIds) =>\n      idToWorker.get(workerId) match {\n        case Some(worker) =>\n          for (exec <- executors) {\n            val executorMatches = worker.executors.exists {\n              case (_, e) => e.application.id == exec.appId && e.id == exec.execId\n            }\n            if (!executorMatches) {\n              // master doesn't recognize this executor. So just tell worker to kill it.\n              worker.endpoint.send(KillExecutor(masterUrl, exec.appId, exec.execId))\n            }\n          }\n\n          for (driverId <- driverIds) {\n            val driverMatches = worker.drivers.exists { case (id, _) => id == driverId }\n            if (!driverMatches) {\n              // master doesn't recognize this driver. So just tell worker to kill it.\n              worker.endpoint.send(KillDriver(driverId))\n            }\n          }\n        case None =>\n          logWarning(\"Worker state from unknown worker: \" + workerId)\n      }\n\n    case UnregisterApplication(applicationId) =>\n      logInfo(s\"Received unregister request from application $applicationId\")\n      idToApp.get(applicationId).foreach(finishApplication)\n\t//给积极扫那个一个消息,检测自己是否挂掉\n    case CheckForWorkerTimeOut =>\n      timeOutDeadWorkers()\n\n  }\n    \n```\n\n注意:上述都是没有回复的,我们需要查看一下`receiveAndReply`\n\n```scala\n    case RequestSubmitDriver(description) =>\n      if (state != RecoveryState.ALIVE) {\n        val msg = s\"${Utils.BACKUP_STANDALONE_MASTER_PREFIX}: $state. \" +\n          \"Can only accept driver submissions in ALIVE state.\"\n        context.reply(SubmitDriverResponse(self, false, None, msg))\n      } else {\n        logInfo(\"Driver submitted \" + description.command.mainClass)\n        val driver = createDriver(description)\n        persistenceEngine.addDriver(driver)\n        waitingDrivers += driver\n        drivers.add(driver)\n        schedule()\n\n        // TODO: It might be good to instead have the submission client poll the master to determine\n        //       the current status of the driver. For now it's simply \"fire and forget\".\n\n        context.reply(SubmitDriverResponse(self, true, Some(driver.id),\n          s\"Driver successfully submitted as ${driver.id}\"))\n      }\n\n    case RequestKillDriver(driverId) =>\n      if (state != RecoveryState.ALIVE) {\n        val msg = s\"${Utils.BACKUP_STANDALONE_MASTER_PREFIX}: $state. \" +\n          s\"Can only kill drivers in ALIVE state.\"\n        context.reply(KillDriverResponse(self, driverId, success = false, msg))\n      } else {\n        logInfo(\"Asked to kill driver \" + driverId)\n        val driver = drivers.find(_.id == driverId)\n        driver match {\n          case Some(d) =>\n            if (waitingDrivers.contains(d)) {\n              waitingDrivers -= d\n              self.send(DriverStateChanged(driverId, DriverState.KILLED, None))\n            } else {\n              // We just notify the worker to kill the driver here. The final bookkeeping occurs\n              // on the return path when the worker submits a state change back to the master\n              // to notify it that the driver was successfully killed.\n              d.worker.foreach { w =>\n                w.endpoint.send(KillDriver(driverId))\n              }\n            }\n            // TODO: It would be nice for this to be a synchronous response\n            val msg = s\"Kill request for $driverId submitted\"\n            logInfo(msg)\n            //回复KillDriverResponse\n            context.reply(KillDriverResponse(self, driverId, success = true, msg))\n          case None =>\n            val msg = s\"Driver $driverId has already finished or does not exist\"\n            logWarning(msg)\n            context.reply(KillDriverResponse(self, driverId, success = false, msg))\n        }\n      }\n\n    case RequestDriverStatus(driverId) =>\n      if (state != RecoveryState.ALIVE) {\n        val msg = s\"${Utils.BACKUP_STANDALONE_MASTER_PREFIX}: $state. \" +\n          \"Can only request driver status in ALIVE state.\"\n        context.reply(\n          DriverStatusResponse(found = false, None, None, None, Some(new Exception(msg))))\n      } else {\n        (drivers ++ completedDrivers).find(_.id == driverId) match {\n          case Some(driver) =>\n            context.reply(DriverStatusResponse(found = true, Some(driver.state),\n              driver.worker.map(_.id), driver.worker.map(_.hostPort), driver.exception))\n          case None =>\n            context.reply(DriverStatusResponse(found = false, None, None, None, None))\n        }\n      }\n\n    case RequestMasterState =>\n      context.reply(MasterStateResponse(\n        address.host, address.port, restServerBoundPort,\n        workers.toArray, apps.toArray, completedApps.toArray,\n        drivers.toArray, completedDrivers.toArray, state))\n\n    case BoundPortsRequest =>\n      context.reply(BoundPortsResponse(address.port, webUi.boundPort, restServerBoundPort))\n\n    case RequestExecutors(appId, requestedTotal) =>\n      context.reply(handleRequestExecutors(appId, requestedTotal))\n\n    case KillExecutors(appId, executorIds) =>\n      val formattedExecutorIds = formatExecutorIds(executorIds)\n      context.reply(handleKillExecutors(appId, formattedExecutorIds))\n  }\n```\n\n## Worker\n\n```scala\nprivate[deploy] class Worker(\n    override val rpcEnv: RpcEnv,\n    webUiPort: Int,\n    cores: Int,\n    memory: Int,\n    masterRpcAddresses: Array[RpcAddress],\n    endpointName: String,\n    workDirPath: String = null,\n    val conf: SparkConf,\n    val securityMgr: SecurityManager)\n  extends ThreadSafeRpcEndpoint with Logging { //Worker继承了ThreadSafeRpcEndpoint,它也是一个通信端点\n```\n\n我们查看一下Worker的main方法\n\n```scala\n  def main(argStrings: Array[String]) {\n    Utils.initDaemon(log)\n    val conf = new SparkConf  //创建SparkConf\n    val args = new WorkerArguments(argStrings, conf) //解析参数\n    val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores, //创建rpcEnv\n      args.memory, args.masters, args.workDir, conf = conf)\n    rpcEnv.awaitTermination()\n  }\n```\n\n我们在查看一下调用的`startRpcEnvAndEndpoint`\n\n```scala\n def startRpcEnvAndEndpoint(\n      host: String,\n      port: Int,\n      webUiPort: Int,\n      cores: Int,\n      memory: Int,\n      masterUrls: Array[String],\n      workDir: String,\n      workerNumber: Option[Int] = None,\n      conf: SparkConf = new SparkConf): RpcEnv = {\n\n    // The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments\n    val systemName = SYSTEM_NAME + workerNumber.map(_.toString).getOrElse(\"\")\n    val securityMgr = new SecurityManager(conf)\n    val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr)\n    val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_))\n    rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory, //生成Worker会有很多属性.\n      masterAddresses, ENDPOINT_NAME, workDir, conf, securityMgr))\n    rpcEnv\n  }\n```\n\n我们在查看一下`onStar`t方法\n\n```scala\noverride def onStart() {\n    assert(!registered)\n    logInfo(\"Starting Spark worker %s:%d with %d cores, %s RAM\".format(\n      host, port, cores, Utils.megabytesToString(memory)))\n    logInfo(s\"Running Spark version ${org.apache.spark.SPARK_VERSION}\")\n    logInfo(\"Spark home: \" + sparkHome)  //打印日志\n    createWorkDir()\n    shuffleService.startIfEnabled()\n    webUi = new WorkerWebUI(this, workDir, webUiPort) //创建WorkerWebUI\n    webUi.bind()\n\n    workerWebUiUrl = s\"http://$publicAddress:${webUi.boundPort}\" //8081端\n    registerWithMaster() //注册点击查看\n\t//测量系统启动\n    metricsSystem.registerSource(workerSource)\n    metricsSystem.start()\n    // Attach the worker metrics servlet handler to the web ui after the metrics system is started.\n    metricsSystem.getServletHandlers.foreach(webUi.attachHandler)\n  }\n```\n\n我们查看一下`registerWithMaster`\n\n```scala\n private def registerWithMaster() {\n    // onDisconnected may be triggered multiple times, so don't attempt registration\n    // if there are outstanding registration attempts scheduled.\n    registrationRetryTimer match {  //重试机制 \n      case None =>\n        registered = false //未注册的\n        registerMasterFutures = tryRegisterAllMasters()\n        connectionAttemptCount = 0\n        registrationRetryTimer = Some(forwordMessageScheduler.scheduleAtFixedRate( //\n          new Runnable {\n            override def run(): Unit = Utils.tryLogNonFatalError {\n              Option(self).foreach(_.send(ReregisterWithMaster)) //发送\n            }\n          },\n          INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS,\n          INITIAL_REGISTRATION_RETRY_INTERVAL_SECONDS,\n          TimeUnit.SECONDS))\n      case Some(_) =>\n        logInfo(\"Not spawning another attempt to register with the master, since there is an\" +\n          \" attempt scheduled already.\")\n    }\n  }\n```\n\n点击`ReregisterWithMaster`\n\n```scala\ncase ReregisterWithMaster =>\n  reregisterWithMaster()  // 调用方法\n```\n\n```scala\nprivate def reregisterWithMaster(): Unit = {\n    Utils.tryOrExit {\n      connectionAttemptCount += 1\n      if (registered) {\n        cancelLastRegistrationRetry()\n      } else if (connectionAttemptCount <= TOTAL_REGISTRATION_RETRIES) {\n        logInfo(s\"Retrying connection to master (attempt # $connectionAttemptCount)\")\n      //\n        master match {\n          case Some(masterRef) => //获得masterRef\n            // registered == false && master != None means we lost the connection to master, so\n            // masterRef cannot be used and we need to recreate it again. Note: we must not set\n            // master to None due to the above comments.\n            if (registerMasterFutures != null) {\n              registerMasterFutures.foreach(_.cancel(true))\n            }\n            val masterAddress = masterRef.address\n            registerMasterFutures = Array(registerMasterThreadPool.submit(new Runnable {\n              override def run(): Unit = {\n                try {\n                  logInfo(\"Connecting to master \" + masterAddress + \"...\")\n                  val masterEndpoint = rpcEnv.setupEndpointRef(masterAddress, Master.ENDPOINT_NAME)\n                  registerWithMaster(masterEndpoint) //重试机制\n                } catch {\n                  case ie: InterruptedException => // Cancelled\n                  case NonFatal(e) => logWarning(s\"Failed to connect to master $masterAddress\", e)\n                }\n              }\n            }))\n          case None =>\n            if (registerMasterFutures != null) {\n              registerMasterFutures.foreach(_.cancel(true))\n            }\n            // We are retrying the initial registration\n            registerMasterFutures = tryRegisterAllMasters()\n        }\n        // We have exceeded the initial registration retry threshold\n        // All retries from now on should use a higher interval\n        if (connectionAttemptCount == INITIAL_REGISTRATION_RETRIES) {\n          registrationRetryTimer.foreach(_.cancel(true))\n          registrationRetryTimer = Some(\n            forwordMessageScheduler.scheduleAtFixedRate(new Runnable {\n              override def run(): Unit = Utils.tryLogNonFatalError {\n                self.send(ReregisterWithMaster) //重试机制\n              }\n            }, PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS,\n              PROLONGED_REGISTRATION_RETRY_INTERVAL_SECONDS,\n              TimeUnit.SECONDS))\n        }\n      } else {\n        logError(\"All masters are unresponsive! Giving up.\")\n        System.exit(1)\n      }\n    }\n  }\n```\n\n## Master\n\n在Master中我们查看一下`receiveAndReply`\n\n```scala\n  override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n    case RegisterWorker(\n        id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =>\n      logInfo(\"Registering worker %s:%d with %d cores, %s RAM\".format( //正在注册\n        workerHost, workerPort, cores, Utils.megabytesToString(memory)))\n      if (state == RecoveryState.STANDBY) { //如果state不是主节点\n        context.reply(MasterInStandby) //回复MasterInStandby\n      } else if (idToWorker.contains(id)) {\n        context.reply(RegisterWorkerFailed(\"Duplicate worker ID\"))\n      } else {\n        val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory, //否则new 一个WorkerInfo 将接收到的消息传入WorkerInfo 注册信息包含有主机名端口 CPU数量 内存数量\n          workerRef, workerWebUiUrl)\n        if (registerWorker(worker)) { //点击查看registerWorker\n          persistenceEngine.addWorker(worker)\n          context.reply(RegisteredWorker(self, masterWebUiUrl)) //成功放入\n          schedule()\n        } else { //放入失败\n          val workerAddress = worker.endpoint.address\n          logWarning(\"Worker registration failed. Attempted to re-register worker at same \" +\n            \"address: \" + workerAddress)\n          context.reply(RegisterWorkerFailed(\"Attempted to re-register worker at same address: \"\n            + workerAddress))\n        }\n      }\n```\n\n点击查看`registerWorker`\n\n```scala\n  private def registerWorker(worker: WorkerInfo): Boolean = {\n    // There may be one or more refs to dead workers on this same node (w/ different ID's),\n    // remove them.\n    workers.filter { w =>\n      (w.host == worker.host && w.port == worker.port) && (w.state == WorkerState.DEAD)\n    }.foreach { w =>\n      workers -= w\n    }\n\n    val workerAddress = worker.endpoint.address\n    if (addressToWorker.contains(workerAddress)) {\n      val oldWorker = addressToWorker(workerAddress)\n      if (oldWorker.state == WorkerState.UNKNOWN) {\n        // A worker registering from UNKNOWN implies that the worker was restarted during recovery.\n        // The old worker must thus be dead, so we will remove it and accept the new worker.\n        removeWorker(oldWorker)\n      } else {\n        logInfo(\"Attempted to re-register worker at same address: \" + workerAddress)\n        return false\n      }\n    }\n\n    workers += worker  //这是一个HashSet[WorkerInfo]   维护的是Worker的注册信息\n    idToWorker(worker.id) = worker\n    addressToWorker(workerAddress) = worker\n    if (reverseProxy) {\n       webUi.addProxyTargets(worker.id, worker.webUiAddress)\n    }\n    true\n  }\n```\n\n在Master上如果我们成功了则调用`context.reply(RegisteredWorker(self, masterWebUiUrl))` 告知Worker注册成功\n\n```scala\n  private def handleRegisterResponse(msg: RegisterWorkerResponse): Unit = synchronized {\n    msg match {\n      case RegisteredWorker(masterRef, masterWebUiUrl) =>  //注册成功打印日志\n        logInfo(\"Successfully registered with master \" +   masterRef.address.toSparkURL)\n        registered = true  //改变registered的状态\n        changeMaster(masterRef, masterWebUiUrl)\n        forwordMessageScheduler.scheduleAtFixedRate(new Runnable {\n          override def run(): Unit = Utils.tryLogNonFatalError {\n            self.send(SendHeartbeat) //发送心跳\n          }\n        }, 0, HEARTBEAT_MILLIS, TimeUnit.MILLISECONDS)\n        if (CLEANUP_ENABLED) {\n          logInfo(\n            s\"Worker cleanup enabled; old application directories will be deleted in: $workDir\")\n          forwordMessageScheduler.scheduleAtFixedRate(new Runnable {\n            override def run(): Unit = Utils.tryLogNonFatalError {\n              self.send(WorkDirCleanup)\n            }\n          }, CLEANUP_INTERVAL_MILLIS, CLEANUP_INTERVAL_MILLIS, TimeUnit.MILLISECONDS)\n        }\n\n        val execs = executors.values.map { e =>\n          new ExecutorDescription(e.appId, e.execId, e.cores, e.state)\n        }\n        masterRef.send(WorkerLatestState(workerId, execs.toList, drivers.keys.toSeq))\n\n      case RegisterWorkerFailed(message) =>\n        if (!registered) {\n          logError(\"Worker registration failed: \" + message)\n          System.exit(1)\n        }\n\n      case MasterInStandby =>\n        // Ignore. Master not yet ready.\n    }\n  }\n```\n\n```scala\n\n  override def receive: PartialFunction[Any, Unit] = synchronized {\n    case SendHeartbeat => \t\t\t\t\t//发送的消息为workerId 和 它自身\n      if (connected) { sendToMaster(Heartbeat(workerId, self)) } //sendToMaster给Master发送心跳\n\n   ...\n  }\n```\n\n点击进入`sendToMaster`\n\n```scala\n  private def sendToMaster(message: Any): Unit = {\n    master match { \n      case Some(masterRef) => masterRef.send(message) //把消息发送给了Master\n      case None =>\n        logWarning(\n          s\"Dropping $message because the connection to master has not yet been established\")\n    }\n  }\n```\n\nMaster中收到发送的消息\n\n```scala\n    case Heartbeat(workerId, worker) =>\n      idToWorker.get(workerId) match { //Worker中是否有\n        case Some(workerInfo) =>  //如果有\n          workerInfo.lastHeartbeat = System.currentTimeMillis() //更新lastHeartbeat的时间\n        case None => //如果没有发现\n          if (workers.map(_.id).contains(workerId)) {\n            logWarning(s\"Got heartbeat from unregistered worker $workerId.\" +\n              \" Asking it to re-register.\")\n            worker.send(ReconnectWorker(masterUrl))//需要重新链接masterUrl\n          } else { \n            logWarning(s\"Got heartbeat from unregistered worker $workerId.\" +\n              \" This worker was never registered, so ignoring the heartbeat.\")\n          }\n      }\n```\n\n我们回过头看一下Master中的`Onstart`调用的`self.send(CheckForWorkerTimeOut)` \n\n```scala\n    case CheckForWorkerTimeOut =>\n      timeOutDeadWorkers() //调用\n```\n\n```scala\n private def timeOutDeadWorkers() {\n    // Copy the workers into an array so we don't modify the hashset while iterating through it\n    val currentTime = System.currentTimeMillis()  //获取当前时间\n    val toRemove = workers.filter(_.lastHeartbeat < currentTime - WORKER_TIMEOUT_MS).toArray //\n    for (worker <- toRemove) { //如果被filter出来\n      if (worker.state != WorkerState.DEAD) {\n        logWarning(\"Removing %s because we got no heartbeat in %d seconds\".format(\n          worker.id, WORKER_TIMEOUT_MS / 1000))\n        removeWorker(worker) //worker 被移除\n      } else {\n        if (worker.lastHeartbeat < currentTime - ((REAPER_ITERATIONS + 1) * WORKER_TIMEOUT_MS)) {\n          workers -= worker // we've seen this DEAD worker in the UI, etc. for long enough; cull it\n        }\n      }\n    }\n  }\n```\n\n","tags":["Spark"],"categories":["大数据"]},{"title":"Spark内核解析1","url":"/2019/06/09/Spark内核解析/","content":"\n {{ \"Spark通讯架构 脚本探究\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 概述\n\nSpark 内核泛指 Spark 的核心运行机制，包括 Spark 核心组件的运行机制、Spark 任务调度机制、Spark 内存管理机制、Spark 核心功能的运行原理。\n\n## 核心组件\n\n### Driver\n\nSpark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：\n\n1. 将用户程序转化为任务（job）；\n\n2. 在 Executor 之间调度任务（task）；\n\n3. 跟踪 Executor 的执行情况；\n\n4. 通过 UI 展示查询运行情况；\n\n### Executor\n\nSpark Executor 节点是一个 JVM 进程，负责在 Spark 作业中运行具体任务，任务彼此之间相互独立。Spark 应用启动时， Executor  节点被同时启动， 并且始终伴随着整个 Spark  应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他 Executor 节点上继续运行。\n\nExecutor 有两个核心功能：\n\n1. 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程； \n\n2. 它们通过自身的块管理器（ Block Manager）为用户程序中要求缓存的 RDD提供内存式存储。RDD  是直接缓存在 Executor  进程内的，因此任务可以在运行时充分利用缓存数据加速运算。\n\n### 运行流程\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190609103825.png)\n\nSpark 通用运行流程， 不论 Spark 以何种模式进行部署， 任务提交后， 都会先启动 Driver 进程，随后 Driver 进程向集群管理器注册应用程序，之后集群管理器根据此任务的配置文件分配 Executor 并启动，当 Driver 所需的资源全部满足后， \n\nDriver 开始执行 main 函数， Spark 查询为懒执行， 当执行到 action 算子时开始`反向推算`，根据宽依赖进行 stage 的划分，随后每一个 stage 对应一个 taskset，taskset 中有多个 task，根据本地化原则， task 会被分发到指定的 Executor 去执行，在任务执行的过程中， Executor 也会不断与 Driver 进行通信，报告任务运行情况。\n\n## 部署模式\t\n\n###  Standalone\n\nStandalone 集群有四个重要组成部分， 分别是：\n\nDriver： 是一个进程，我们编写的 Spark  应用程序就运行在 Driver  上， 由Driver 进程执行； \n\nMaster：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责； \n\nWorker：是一个进程，一个 Worker 运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储 RDD 的某个或某些 partition；另一个是启动其他进程和线程（Executor） ，对 RDD 上的 partition 进行并行的处理和计算。\n\nExecutor：是一个进程， 一个 Worker 上可以运行多个 Executor， Executor 通过启动多个线程（ task）来执行对 RDD 的 partition 进行并行计算，也就是执行我们对 RDD 定义的例如 map、flatMap、reduce 等算子操作。\n\n#### Standalone Client\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609114709.png)\n\nStandalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。\n\n#### Standalone Cluster\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609115516.png)\n\n Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker 启动 Driver进程， Driver 启动后向 Master 注册应用程序， Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。\n\n注意， Standalone  的两种模式下（ client/Cluster） ， Master  在接到 Driver  注册\n\nSpark 应用程序的请求后，会获取其所管理的剩余资源能够启动一个 Executor 的所有 Worker， 然后在这些 Worker 之间分发 Executor， 此时的分发只考虑 Worker 上的资源是否足够使用，直到当前应用程序所需的所有 Executor 都分配完毕， Executor 反向注册完毕后，Driver 开始执行 main 程序。\n\n### YARN Client\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609115616.png)\n\n在 YARN Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster， 随后 ResourceManager 分配 container ， 在 合 适 的 NodeManager   上启动 ApplicationMaster ，此时的 \n\nApplicationMaster  的功能相当于一个 ExecutorLaucher， 只负责向 ResourceManager 申请 Executor 内存。\n\nResourceManager  接到 ApplicationMaster  的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。\n\n### YARN Cluster \n\n在 YARN  Cluster  模式下， 任务提交后会和 ResourceManager  通讯申请启动ApplicationMaster， 随后 ResourceManager  分配 container，在合适的 NodeManager上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。\n\nDriver 启动后向 ResourceManager 申请 Executor 内存， ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动 Executor 进程，Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage  生成对应的 taskSet，之后将 task  分发到各个Executor 上执行。\n\n## 通讯架构\n\nSpark2.x 版本使用 Netty 通讯框架作为内部通讯组件。spark  基于 netty 新的 rpc框架借鉴了 Akka 的中的设计， 它是基于Actor 模型。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609120140.png)\n\nScala里面处理通信采用Actor架构，Actor架构其实就是一个邮局模型， AKKA为给予Actor模型的工程实现。Akka不同版本之间无法通信，存在兼容性问题。用户使用Akka与Spark中的Akka存在冲突。Spark对Akka没有自身维护，需要新功能时只能等待新版本，比较牵制Spark发展。因此在Spark2中已经抛弃了Akka。\n\nSpark早期版本中采用Akka作为内部通信部件。\nSpark1.3中引入Netty通信框架，为了解决Shuffle的大数据传输问题使用\nSpark1.6中Akka和Netty可以配置使用。Netty完全实现了Akka在Spark中的功能。\nSpark2系列中，Spark抛弃Akka，使用Netty。\n\nSpark 通讯框架中各个组件（ Client/Master/Worker）可以认为是一个个独立的实体，各个实体之间通过消息来进行通信。具体各个组件之间的关系图如下： \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609120949.png)\n\nEndpoint（ Client/Master/Worker）有 1 个 InBox 和 N 个 OutBox（ N>=1，N 取决于当前 Endpoint 与多少其他的 Endpoint 进行通信， 一个与其通讯的其他 Endpoint 对应一个 OutBox）， Endpoint  接收到的消息被写入 InBox， 发送出去的消息写入OutBox 并被发送到其他 Endpoint 的 InBox 中。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609121127.png)\n\n1) RpcEndpoint：RPC 端点，Spark 针对每个节点（ Client/Master/Worker）都称之为一个 Rpc 端点，且都实现 RpcEndpoint 接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用 Dispatcher；\n\n2) RpcEnv： RPC  上下文环境， 每个 RPC  端点运行时依赖的上下文环境称为RpcEnv；\n\n3) Dispatcher：消息分发器，针对于 RPC 端点需要发送消息或者从远程 RPC 接收到的消息，分发至对应的指令收件箱/发件箱。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱； \n\n4)  Inbox：指令消息收件箱，一个本地 RpcEndpoint 对应一个收件箱，Dispatcher 在每次向 Inbox 存入消息时， 都将对应 EndpointData 加入内部 ReceiverQueue 中， 另外 Dispatcher 创建时会启动一个单独线程进行轮询 ReceiverQueue，进行收件箱消息消费； \n\n5)  RpcEndpointRef：RpcEndpointRef 是对远程 RpcEndpoint 的一个引用。当我们需要向一个具体的 RpcEndpoint 发送消息时，一般我们需要获取到该 RpcEndpoint 的引用，然后通过该应用发送消息。\n\n6)  OutBox ： 指令消息发件箱 ， 对于当前 RpcEndpoint 来说 ， 一个目标RpcEndpoint  对应一个发件箱， 如果向多个目标 RpcEndpoint 发送信息， 则有多个OutBox。当消息放入 Outbox 后，紧接着通过 TransportClient 将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行； \n\n7)  RpcAddress： 表示远程的 RpcEndpointRef 的地址， Host + Port。\n\n8)  TransportClient：Netty 通信客户端，一个 OutBox 对应一个 TransportClient，TransportClient 不断轮询 OutBox，根据 OutBox 消息的 receiver 信息，请求对应的远程 TransportServer；\n\n9)  TransportServer ： Netty通信服务端 ， 一 个 RpcEndpoint 对应一个TransportServer，接受远程消息后调用Dispatcher 分发消息至对应收发件箱； \n\nRpcEndPoint就代表一个通信端点， 一个端点就有一个inbox，  一个 transportServer  一个 Dispatcher，  根据你通信的其他端点的数目，就有多个Outbox， 一个outbox有一个 transportClient，   transportClient主要负责和 transportServer来通信。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609122558.png)\n\n在我们的传统认知中，多个端点要通信，中间要有一个节点类似于总的路由，节点之间的通信靠中间的“路由”，而 Spark没有中间的这个“路由”，如果中间的“路由”存在一定会存在瓶颈问题。Spark很巧妙的把中间的“路由”拆分到各个节点上。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609123350.png)\n\n### 高层视图\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609121509.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/内核/20190609123756.png)\n\n```scala\nprivate[spark] trait RpcEndpoint {\n\n  /**\n   * The [[RpcEnv]] that this [[RpcEndpoint]] is registered to.\n   */\n  val rpcEnv: RpcEnv\n   ....\n}\n\n /**\n   * Process messages from [[RpcEndpointRef.send]] or [[RpcCallContext.reply)]]. If receiving a\n   * unmatched message, [[SparkException]] will be thrown and sent to `onError`.\n   */\n  def receive: PartialFunction[Any, Unit] = {\n    case _ => throw new SparkException(self + \" does not implement 'receive'\")\n  }\n\n /**\n   * Process messages from [[RpcEndpointRef.ask]]. If receiving a unmatched message,\n   * [[SparkException]] will be thrown and sent to `onError`.\n   */\n  def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = {\n    case _ => context.sendFailure(new SparkException(self + \" won't reply anything\"))\n  }\n\n  /**\n   * Invoked before [[RpcEndpoint]] starts to handle any message.\n   */\n  def onStart(): Unit = {\n    // By default, do nothing.\n  }\n```\n\n   RpcEndpoint  注意三个方法，\n\n1、receive   改方法被子类实现，用于接收其他节点发送的消息。\n2、receiveAndReply    该方法被子类实现，用于接收并回复其他节点发送的消息。\n3、onStart    该方法被子类实现，该方法在端口启动的时候自动调用。\n\n我们查看以下RpcEnv的实现发现实现是NettyRpcEnv\n\n```scala\nprivate[netty] class NettyRpcEnv(\n    val conf: SparkConf,\n    javaSerializerInstance: JavaSerializerInstance,\n    host: String,\n    securityManager: SecurityManager) extends RpcEnv(conf) with Logging {\n\n  private[netty] val transportConf = SparkTransportConf.fromSparkConf(\n    conf.clone.set(\"spark.rpc.io.numConnectionsPerPeer\", \"1\"),\n    \"rpc\",\n    conf.getInt(\"spark.rpc.io.threads\", 0))\n\n  // 设置一个消息分发器\n  private val dispatcher: Dispatcher = new Dispatcher(this)\n\n  private val streamManager = new NettyStreamManager(this)\n\n  private val transportContext = new TransportContext(transportConf,\n    new NettyRpcHandler(dispatcher, this, streamManager))\n\n  private def createClientBootstraps(): java.util.List[TransportClientBootstrap] = {\n    if (securityManager.isAuthenticationEnabled()) {\n      java.util.Arrays.asList(new SaslClientBootstrap(transportConf, \"\", securityManager,\n        securityManager.isSaslEncryptionEnabled()))\n    } else {\n      java.util.Collections.emptyList[TransportClientBootstrap]\n    }\n  }\n\n  private val clientFactory = transportContext.createClientFactory(createClientBootstraps())\n\n  /**\n   * A separate client factory for file downloads. This avoids using the same RPC handler as\n   * the main RPC context, so that events caused by these clients are kept isolated from the\n   * main RPC traffic.\n   *\n   * It also allows for different configuration of certain properties, such as the number of\n   * connections per peer.\n   */\n  @volatile private var fileDownloadFactory: TransportClientFactory = _\n\n  val timeoutScheduler = ThreadUtils.newDaemonSingleThreadScheduledExecutor(\"netty-rpc-env-timeout\")\n\n  // Because TransportClientFactory.createClient is blocking, we need to run it in this thread pool\n  // to implement non-blocking send/ask.\n  // TODO: a non-blocking TransportClientFactory.createClient in future\n  private[netty] val clientConnectionExecutor = ThreadUtils.newDaemonCachedThreadPool(\n    \"netty-rpc-connection\",\n    conf.getInt(\"spark.rpc.connect.threads\", 64))\n\n  @volatile private var server: TransportServer = _\n\n  private val stopped = new AtomicBoolean(false)\n\n  /**\n   * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],\n   * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.\n   */\n    // 多个地址对应的发件箱\n  private val outboxes = new ConcurrentHashMap[RpcAddress, Outbox]()\n\n  /**\n   * Remove the address's Outbox and stop it.\n   */\n  private[netty] def removeOutbox(address: RpcAddress): Unit = {\n    val outbox = outboxes.remove(address)\n    if (outbox != null) {\n      outbox.stop()\n    }\n  }\n  // 启动TransportServer来接收远程消息\n  def startServer(bindAddress: String, port: Int): Unit = {\n    val bootstraps: java.util.List[TransportServerBootstrap] =\n      if (securityManager.isAuthenticationEnabled()) {\n        java.util.Arrays.asList(new SaslServerBootstrap(transportConf, securityManager))\n      } else {\n        java.util.Collections.emptyList()\n      }\n    server = transportContext.createServer(bindAddress, port, bootstraps)\n    dispatcher.registerRpcEndpoint(\n      RpcEndpointVerifier.NAME, new RpcEndpointVerifier(this, dispatcher))\n  }\n\n  @Nullable\n  override lazy val address: RpcAddress = {\n    if (server != null) RpcAddress(host, server.getPort()) else null\n  }\n\n  // 注册当前端点\n  override def setupEndpoint(name: String, endpoint: RpcEndpoint): RpcEndpointRef = {\n    dispatcher.registerRpcEndpoint(name, endpoint)\n  }\n.... \n```\n\n我们似乎没有看到Inbox在哪里点击Dispatcher\n\n```scala\n  private class EndpointData(\n      val name: String,\n      val endpoint: RpcEndpoint,\n      val ref: NettyRpcEndpointRef) {\n    val inbox = new Inbox(ref, endpoint)\n  }\n```\n\n## 启动脚本\n\n### start-all.sh\n\n```shell\n# Start all spark daemons.\n# Starts the master on this node.\n# Starts a worker on each node specified in conf/slaves\n\nif [ -z \"${SPARK_HOME}\" ]; then  #如果没有发现Spark环境变量\n  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\" # 获得当前的目录把当前目录设置为SPARK_HOME\nfi\n\n# Load the Spark configuration\n. \"${SPARK_HOME}/sbin/spark-config.sh\" #加载 spark-config.sh配置\n\n# Start Master\n\"${SPARK_HOME}/sbin\"/start-master.sh\n\n# Start Workers\n\"${SPARK_HOME}/sbin\"/start-slaves.sh\n```\n\n### spark-config.sh\n\n````shell\n# included in all the spark scripts with source command\n# should not be executable directly\n# also should not be passed any arguments, since we need original $*\n\n# symlink and absolute path should rely on SPARK_HOME to resolve\nif [ -z \"${SPARK_HOME}\" ]; then\n  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\nfi\n\nexport SPARK_CONF_DIR=\"${SPARK_CONF_DIR:-\"${SPARK_HOME}/conf\"}\" #设置 SPARK_CONF_DIR 目录\n# Add the PySpark classes to the PYTHONPATH:\nif [ -z \"${PYSPARK_PYTHONPATH_SET}\" ]; then\n  export PYTHONPATH=\"${SPARK_HOME}/python:${PYTHONPATH}\"\n  export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.4-src.zip:${PYTHONPATH}\"\n  export PYSPARK_PYTHONPATH_SET=1\nfi\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n\n````\n\n### start-master.sh\n\n```shell\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\n# Starts the master on the machine this script is executed on.\n\nif [ -z \"${SPARK_HOME}\" ]; then\n  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\nfi\n\n# NOTE: This exact class name is matched downstream by SparkSubmit.\n# Any changes need to be reflected there.\nCLASS=\"org.apache.spark.deploy.master.Master\"  #调用Master\n\nif [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\n  echo \"Usage: ./sbin/start-master.sh [options]\"\n  pattern=\"Usage:\"\n  pattern+=\"\\|Using Spark's default log4j profile:\"\n  pattern+=\"\\|Registered signal handlers for\"\n\n  \"${SPARK_HOME}\"/bin/spark-class $CLASS --help 2>&1 | grep -v \"$pattern\" 1>&2\n  exit 1\nfi\n\nORIGINAL_ARGS=\"$@\"\n\n. \"${SPARK_HOME}/sbin/spark-config.sh\"  \n\n. \"${SPARK_HOME}/bin/load-spark-env.sh\" #加载环境变量\n\nif [ \"$SPARK_MASTER_PORT\" = \"\" ]; then # 如果没有端口 默认7077\n  SPARK_MASTER_PORT=7077\nfi\n\nif [ \"$SPARK_MASTER_HOST\" = \"\" ]; then\n  case `uname` in\n      (SunOS)           # 如果没有设置HOST 则把/usr/sbin/check-hostname作为主机名\n          SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '{print $NF}'`\"\n          ;;\n      (*)\n          SPARK_MASTER_HOST=\"`hostname -f`\"\n          ;;\n  esac\nfi\n\nif [ \"$SPARK_MASTER_WEBUI_PORT\" = \"\" ]; then\n  SPARK_MASTER_WEBUI_PORT=8080\nfi\n\n\"${SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS 1 \\\n  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \\\n  $ORIGINAL_ARGS\n```\n\n```scala\n  def main(argStrings: Array[String]) {\n    // 1、初始化log对象\n    Utils.initDaemon(log)\n    // 2、加载SparkConf\n    val conf = new SparkConf\n    // 3、解析Master启动参数\n    val args = new MasterArguments(argStrings, conf)\n    // 4、启动RPC框架端点\n    val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)\n    rpcEnv.awaitTermination()\n  }\n```\n\n### start-slaves.sh\n\n```shell\n# Starts a slave instance on each machine specified in the conf/slaves file.\n\nif [ -z \"${SPARK_HOME}\" ]; then\n  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\" #获取当前的目录\nfi\n\n. \"${SPARK_HOME}/sbin/spark-config.sh\"\n. \"${SPARK_HOME}/bin/load-spark-env.sh\" #加载配置\n\n# Find the port number for the master\nif [ \"$SPARK_MASTER_PORT\" = \"\" ]; then\n  SPARK_MASTER_PORT=7077\nfi\n\nif [ \"$SPARK_MASTER_HOST\" = \"\" ]; then\n  case `uname` in\n      (SunOS)\n          SPARK_MASTER_HOST=\"`/usr/sbin/check-hostname | awk '{print $NF}'`\"\n          ;;\n      (*)\n          SPARK_MASTER_HOST=\"`hostname -f`\"\n          ;;\n  esac\nfi\n\n# Launch the slaves 调用了start-slave.sh\n\"${SPARK_HOME}/sbin/slaves.sh\" cd \"${SPARK_HOME}\" \\; \"${SPARK_HOME}/sbin/start-slave.sh\" \"spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT\"\n\n```\n\n### start-slave.sh\n\n```shell\n# Starts a slave on the machine this script is executed on.\n#\n# Environment Variables\n#\n#   SPARK_WORKER_INSTANCES  The number of worker instances to run on this\n#                           slave.  Default is 1.\n#   SPARK_WORKER_PORT       The base port number for the first worker. If set,\n#                           subsequent workers will increment this number.  If\n#                           unset, Spark will find a valid port number, but\n#                           with no guarantee of a predictable pattern.\n#   SPARK_WORKER_WEBUI_PORT The base port for the web interface of the first\n#                           worker.  Subsequent workers will increment this\n#                           number.  Default is 8081.\n\nif [ -z \"${SPARK_HOME}\" ]; then\n  export SPARK_HOME=\"$(cd \"`dirname \"$0\"`\"/..; pwd)\"\nfi\n\n# NOTE: This exact class name is matched downstream by SparkSubmit.\n# Any changes need to be reflected there.\nCLASS=\"org.apache.spark.deploy.worker.Worker\"\n\nif [[ $# -lt 1 ]] || [[ \"$@\" = *--help ]] || [[ \"$@\" = *-h ]]; then\n  echo \"Usage: ./sbin/start-slave.sh [options] <master>\"\n  pattern=\"Usage:\"\n  pattern+=\"\\|Using Spark's default log4j profile:\"\n  pattern+=\"\\|Registered signal handlers for\"\n\n  \"${SPARK_HOME}\"/bin/spark-class $CLASS --help 2>&1 | grep -v \"$pattern\" 1>&2\n  exit 1\nfi\n\n. \"${SPARK_HOME}/sbin/spark-config.sh\"\n\n. \"${SPARK_HOME}/bin/load-spark-env.sh\"\n\n# First argument should be the master; we need to store it aside because we may\n# need to insert arguments between it and the other arguments\nMASTER=$1\nshift\n\n# Determine desired worker port\nif [ \"$SPARK_WORKER_WEBUI_PORT\" = \"\" ]; then\n  SPARK_WORKER_WEBUI_PORT=8081\nfi\n\n# Start up the appropriate number of workers on this machine.\n# quick local function to start a worker\nfunction start_instance {\n  WORKER_NUM=$1\n  shift\n\n  if [ \"$SPARK_WORKER_PORT\" = \"\" ]; then\n    PORT_FLAG=\n    PORT_NUM=\n  else\n    PORT_FLAG=\"--port\"\n    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))\n  fi\n  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))\n   #调用org.apache.spark.deploy.worker.Worker\n  \"${SPARK_HOME}/sbin\"/spark-daemon.sh start $CLASS $WORKER_NUM \\\n     --webui-port \"$WEBUI_PORT\" $PORT_FLAG $PORT_NUM $MASTER \"$@\" \n}\n\nif [ \"$SPARK_WORKER_INSTANCES\" = \"\" ]; then\n  start_instance 1 \"$@\"\nelse\n  for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do\n    start_instance $(( 1 + $i )) \"$@\"\n  done\nfi\n```\n\nworkerMain方法\n\n```scala\ndef main(argStrings: Array[String]) {\n    Utils.initDaemon(log)\n    val conf = new SparkConf\n    val args = new WorkerArguments(argStrings, conf)\n    val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,\n      args.memory, args.masters, args.workDir, conf = conf)\n    rpcEnv.awaitTermination()\n  }\n```\n\n ## 任务提交\n\n### spark-submit\n\n```shell\nif [ -z \"${SPARK_HOME}\" ]; then\n  source \"$(dirname \"$0\")\"/find-spark-home\nfi\n\n# disable randomized hash for string in Python 3.3+\nexport PYTHONHASHSEED=0\n\nexec \"${SPARK_HOME}\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\"\n```\n\n### spark-class\n\n```shell\nif [ -z \"${SPARK_HOME}\" ]; then\n  source \"$(dirname \"$0\")\"/find-spark-home\nfi\n\n. \"${SPARK_HOME}\"/bin/load-spark-env.sh\n\n# Find the java binary\nif [ -n \"${JAVA_HOME}\" ]; then\n  RUNNER=\"${JAVA_HOME}/bin/java\"\nelse\n  if [ \"$(command -v java)\" ]; then\n    RUNNER=\"java\"\n  else\n    echo \"JAVA_HOME is not set\" >&2\n    exit 1\n  fi\nfi\n\n# Find Spark jars.\nif [ -d \"${SPARK_HOME}/jars\" ]; then\n  SPARK_JARS_DIR=\"${SPARK_HOME}/jars\"\nelse\n  SPARK_JARS_DIR=\"${SPARK_HOME}/assembly/target/scala-$SPARK_SCALA_VERSION/jars\"\nfi\n\nif [ ! -d \"$SPARK_JARS_DIR\" ] && [ -z \"$SPARK_TESTING$SPARK_SQL_TESTING\" ]; then\n  echo \"Failed to find Spark jars directory ($SPARK_JARS_DIR).\" 1>&2\n  echo \"You need to build Spark with the target \\\"package\\\" before running this program.\" 1>&2\n  exit 1\nelse\n  LAUNCH_CLASSPATH=\"$SPARK_JARS_DIR/*\"\nfi\n\n# Add the launcher build dir to the classpath if requested.\nif [ -n \"$SPARK_PREPEND_CLASSES\" ]; then\n  LAUNCH_CLASSPATH=\"${SPARK_HOME}/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH\"\nfi\n\n# For tests\nif [[ -n \"$SPARK_TESTING\" ]]; then\n  unset YARN_CONF_DIR\n  unset HADOOP_CONF_DIR\nfi\n\n# The launcher library will print arguments separated by a NULL character, to allow arguments with\n# characters that would be otherwise interpreted by the shell. Read that in a while loop, populating\n# an array that will be used to exec the final command.\n#\n# The exit code of the launcher is appended to the output, so the parent shell removes it from the\n# command array and checks the value to see if the launcher succeeded.\nbuild_command() {\n  \"$RUNNER\" -Xmx128m -cp \"$LAUNCH_CLASSPATH\" org.apache.spark.launcher.Main \"$@\"\n  printf \"%d\\0\" $?\n}\n\nCMD=()\nwhile IFS= read -d '' -r ARG; do\n  CMD+=(\"$ARG\")\ndone < <(build_command \"$@\")\n\nCOUNT=${#CMD[@]}\nLAST=$((COUNT - 1))\nLAUNCHER_EXIT_CODE=${CMD[$LAST]}\n\n# Certain JVM failures result in errors being printed to stdout (instead of stderr), which causes\n# the code that parses the output of the launcher to get confused. In those cases, check if the\n# exit code is an integer, and if it's not, handle it as a special error case.\nif ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then\n  echo \"${CMD[@]}\" | head -n-1 1>&2\n  exit 1\nfi\n\nif [ $LAUNCHER_EXIT_CODE != 0 ]; then\n  exit $LAUNCHER_EXIT_CODE\nfi\n\nCMD=(\"${CMD[@]:0:$LAST}\")\nexec \"${CMD[@]}\"\n```\n\n查看SparkSubmit\n\n```scala\nobject SparkSubmit {\n\n  // Cluster managers\n  private val YARN = 1\n  private val STANDALONE = 2\n  private val MESOS = 4\n  private val LOCAL = 8\n  private val ALL_CLUSTER_MGRS = YARN | STANDALONE | MESOS | LOCAL\n\n  // Deploy modes\n  private val CLIENT = 1\n  private val CLUSTER = 2\n  private val ALL_DEPLOY_MODES = CLIENT | CLUSTER\n\n  // Special primary resource names that represent shells rather than application jars.\n  private val SPARK_SHELL = \"spark-shell\"\n  private val PYSPARK_SHELL = \"pyspark-shell\"\n  private val SPARKR_SHELL = \"sparkr-shell\"\n  private val SPARKR_PACKAGE_ARCHIVE = \"sparkr.zip\"\n  private val R_PACKAGE_ARCHIVE = \"rpkg.zip\"\n\n  private val CLASS_NOT_FOUND_EXIT_STATUS = 101\n\n  // scalastyle:off println\n  // Exposed for testing\n  private[spark] var exitFn: Int => Unit = (exitCode: Int) => System.exit(exitCode)\n  private[spark] var printStream: PrintStream = System.err\n  private[spark] def printWarning(str: String): Unit = printStream.println(\"Warning: \" + str)\n  private[spark] def printErrorAndExit(str: String): Unit = {\n    printStream.println(\"Error: \" + str)\n    printStream.println(\"Run with --help for usage help or --verbose for debug output\")\n    exitFn(1)\n  }\n  private[spark] def printVersionAndExit(): Unit = {\n    printStream.println(\"\"\"Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version %s\n      /_/\n                        \"\"\".format(SPARK_VERSION))  \n    printStream.println(\"Using Scala %s, %s, %s\".format(\n      Properties.versionString, Properties.javaVmName, Properties.javaVersion))\n    printStream.println(\"Branch %s\".format(SPARK_BRANCH))\n    printStream.println(\"Compiled by user %s on %s\".format(SPARK_BUILD_USER, SPARK_BUILD_DATE))\n    printStream.println(\"Revision %s\".format(SPARK_REVISION))\n    printStream.println(\"Url %s\".format(SPARK_REPO_URL))\n    printStream.println(\"Type --help for more information.\")\n    exitFn(0)\n  }\n  // scalastyle:on println\n\n  def main(args: Array[String]): Unit = {\n    val appArgs = new SparkSubmitArguments(args)\n    if (appArgs.verbose) {\n      // scalastyle:off println\n      printStream.println(appArgs)\n      // scalastyle:on println\n    }\n    appArgs.action match {\n      case SparkSubmitAction.SUBMIT => submit(appArgs)\n      case SparkSubmitAction.KILL => kill(appArgs)\n      case SparkSubmitAction.REQUEST_STATUS => requestStatus(appArgs)\n    }\n  }\n```\n\n### spark-shell\n\n```shell\n# Shell script for starting the Spark Shell REPL\n\ncygwin=false\ncase \"$(uname)\" in\n  CYGWIN*) cygwin=true;;\nesac\n\n# Enter posix mode for bash\nset -o posix\n\nif [ -z \"${SPARK_HOME}\" ]; then\n  source \"$(dirname \"$0\")\"/find-spark-home\nfi\n\nexport _SPARK_CMD_USAGE=\"Usage: ./bin/spark-shell [options]\"\n\n# SPARK-4161: scala does not assume use of the java classpath,\n# so we need to add the \"-Dscala.usejavacp=true\" flag manually. We\n# do this specifically for the Spark shell because the scala REPL\n# has its own class loader, and any additional classpath specified\n# through spark.driver.extraClassPath is not automatically propagated.\nSPARK_SUBMIT_OPTS=\"$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true\"\n\nfunction main() {\n  if $cygwin; then\n    # Workaround for issue involving JLine and Cygwin\n    # (see http://sourceforge.net/p/jline/bugs/40/).\n    # If you're using the Mintty terminal emulator in Cygwin, may need to set the\n    # \"Backspace sends ^H\" setting in \"Keys\" section of the Mintty options\n    # (see https://github.com/sbt/sbt/issues/562).\n    stty -icanon min 1 -echo > /dev/null 2>&1\n    export SPARK_SUBMIT_OPTS=\"$SPARK_SUBMIT_OPTS -Djline.terminal=unix\"\n    \"${SPARK_HOME}\"/bin/spark-submit --class org.apache.spark.repl.Main --name \"Spark shell\" \"$@\"\n    stty icanon echo > /dev/null 2>&1\n  else\n    export SPARK_SUBMIT_OPTS\n    \"${SPARK_HOME}\"/bin/spark-submit --class org.apache.spark.repl.Main --name \"Spark shell\" \"$@\"\n  fi  \n}\n\n# Copy restore-TTY-on-exit functions from Scala script so spark-shell exits properly even in\n# binary distribution of Spark where Scala is not installed\nexit_status=127\nsaved_stty=\"\"\n\n# restore stty settings (echo in particular)\nfunction restoreSttySettings() {\n  stty $saved_stty\n  saved_stty=\"\"\n}\n\nfunction onExit() {\n  if [[ \"$saved_stty\" != \"\" ]]; then\n    restoreSttySettings\n  fi\n  exit $exit_status\n}\n\n# to reenable echo if we are interrupted before completing.\ntrap onExit INT\n\n# save terminal settings\nsaved_stty=$(stty -g 2>/dev/null)\n# clear on error so we don't later try to restore them\nif [[ ! $? ]]; then\n  saved_stty=\"\"\nfi\n\nmain \"$@\" #调用的main函数 最终执行的依旧是spark-submi\n\n# record the exit status lest it be overwritten:\n# then reenable echo and propagate the code.\nexit_status=$?\nonExit\n\n```\n\n```scala\npackage org.apache.spark.repl\n\nobject Main extends Logging {\n\n  initializeLogIfNecessary(true)\n  Signaling.cancelOnInterrupt()\n\n  val conf = new SparkConf()\n  val rootDir = conf.getOption(\"spark.repl.classdir\").getOrElse(Utils.getLocalDir(conf))\n  val outputDir = Utils.createTempDir(root = rootDir, namePrefix = \"repl\")\n\n  var sparkContext: SparkContext = _\n  var sparkSession: SparkSession = _\n  // this is a public var because tests reset it.\n  var interp: SparkILoop = _\n\n  private var hasErrors = false\n\n  private def scalaOptionError(msg: String): Unit = {\n    hasErrors = true\n    Console.err.println(msg)\n  }\n\n  def main(args: Array[String]) {\n    doMain(args, new SparkILoop)\n  }\n\n  // Visible for testing\n  private[repl] def doMain(args: Array[String], _interp: SparkILoop): Unit = {\n    interp = _interp\n    val jars = Utils.getUserJars(conf, isShell = true).mkString(File.pathSeparator)\n    val interpArguments = List(\n      \"-Yrepl-class-based\",\n      \"-Yrepl-outdir\", s\"${outputDir.getAbsolutePath}\",\n      \"-classpath\", jars\n    ) ++ args.toList\n\n    val settings = new GenericRunnerSettings(scalaOptionError)\n    settings.processArguments(interpArguments, true)\n\n    if (!hasErrors) {\n      interp.process(settings) // Repl starts and goes in loop of R.E.P.L\n      Option(sparkContext).map(_.stop)\n    }\n  }\n\n  def createSparkSession(): SparkSession = {\n    val execUri = System.getenv(\"SPARK_EXECUTOR_URI\")\n    conf.setIfMissing(\"spark.app.name\", \"Spark shell\")\n    // SparkContext will detect this configuration and register it with the RpcEnv's\n    // file server, setting spark.repl.class.uri to the actual URI for executors to\n    // use. This is sort of ugly but since executors are started as part of SparkContext\n    // initialization in certain cases, there's an initialization order issue that prevents\n    // this from being set after SparkContext is instantiated.\n    conf.set(\"spark.repl.class.outputDir\", outputDir.getAbsolutePath())\n    if (execUri != null) {\n      conf.set(\"spark.executor.uri\", execUri)\n    }\n    if (System.getenv(\"SPARK_HOME\") != null) {\n      conf.setSparkHome(System.getenv(\"SPARK_HOME\"))\n    }\n\n    val builder = SparkSession.builder.config(conf)\n    if (conf.get(CATALOG_IMPLEMENTATION.key, \"hive\").toLowerCase == \"hive\") {\n      if (SparkSession.hiveClassesArePresent) {\n        // In the case that the property is not set at all, builder's config\n        // does not have this value set to 'hive' yet. The original default\n        // behavior is that when there are hive classes, we use hive catalog.\n        sparkSession = builder.enableHiveSupport().getOrCreate()\n        logInfo(\"Created Spark session with Hive support\")\n      } else {\n        // Need to change it back to 'in-memory' if no hive classes are found\n        // in the case that the property is set to hive in spark-defaults.conf\n        builder.config(CATALOG_IMPLEMENTATION.key, \"in-memory\")\n        sparkSession = builder.getOrCreate()\n        logInfo(\"Created Spark session\")\n      }\n    } else {\n      // In the case that the property is set but not to 'hive', the internal\n      // default is 'in-memory'. So the sparkSession will use in-memory catalog.\n      sparkSession = builder.getOrCreate()\n      logInfo(\"Created Spark session\")\n    }\n    sparkContext = sparkSession.sparkContext\n    sparkSession\n  }\n\n}\n```\n\n```scala\nprivate[repl] trait SparkILoopInit {\n  self: SparkILoop =>\n\n  /** Print a welcome message */\n  def printWelcome() {\n    echo(\"\"\"Welcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version %s\n      /_/\n\"\"\".format(SPARK_VERSION))\n    import Properties._\n    val welcomeMsg = \"Using Scala %s (%s, Java %s)\".format(\n      versionString, javaVmName, javaVersion)\n    echo(welcomeMsg)\n    echo(\"Type in expressions to have them evaluated.\")\n    echo(\"Type :help for more information.\")\n   }\n\n  protected def asyncMessage(msg: String) {\n    if (isReplInfo || isReplPower)\n      echoAndRefresh(msg)\n  }\n\n  private val initLock = new java.util.concurrent.locks.ReentrantLock()\n  private val initCompilerCondition = initLock.newCondition() // signal the compiler is initialized\n  private val initLoopCondition = initLock.newCondition()     // signal the whole repl is initialized\n  private val initStart = System.nanoTime\n\n  private def withLock[T](body: => T): T = {\n    initLock.lock()\n    try body\n    finally initLock.unlock()\n  }\n  // a condition used to ensure serial access to the compiler.\n  @volatile private var initIsComplete = false\n  @volatile private var initError: String = null\n  private def elapsed() = \"%.3f\".format((System.nanoTime - initStart).toDouble / 1000000000L)\n\n  // the method to be called when the interpreter is initialized.\n  // Very important this method does nothing synchronous (i.e. do\n  // not try to use the interpreter) because until it returns, the\n  // repl's lazy val `global` is still locked.\n  protected def initializedCallback() = withLock(initCompilerCondition.signal())\n\n  // Spins off a thread which awaits a single message once the interpreter\n  // has been initialized.\n  protected def createAsyncListener() = {\n    io.spawn {\n      withLock(initCompilerCondition.await())\n      asyncMessage(\"[info] compiler init time: \" + elapsed() + \" s.\")\n      postInitialization()\n    }\n  }\n\n  // called from main repl loop\n  protected def awaitInitialized(): Boolean = {\n    if (!initIsComplete)\n      withLock { while (!initIsComplete) initLoopCondition.await() }\n    if (initError != null) {\n      // scalastyle:off println\n      println(\"\"\"\n        |Failed to initialize the REPL due to an unexpected error.\n        |This is a bug, please, report it along with the error diagnostics printed below.\n        |%s.\"\"\".stripMargin.format(initError)\n      )\n      // scalastyle:on println\n      false\n    } else true\n  }\n  // private def warningsThunks = List(\n  //   () => intp.bind(\"lastWarnings\", \"\" + typeTag[List[(Position, String)]], intp.lastWarnings _),\n  // )\n\n  protected def postInitThunks = List[Option[() => Unit]](\n    Some(intp.setContextClassLoader _),\n    if (isReplPower) Some(() => enablePowerMode(true)) else None\n  ).flatten\n  // ++ (\n  //   warningsThunks\n  // )\n  // called once after init condition is signalled\n  protected def postInitialization() {\n    try {\n      postInitThunks foreach (f => addThunk(f()))\n      runThunks()\n    } catch {\n      case ex: Throwable =>\n        initError = stackTraceString(ex)\n        throw ex\n    } finally {\n      initIsComplete = true\n\n      if (isAsync) {\n        asyncMessage(\"[info] total init time: \" + elapsed() + \" s.\")\n        withLock(initLoopCondition.signal())\n      }\n    }\n  }\n\n  def initializeSpark() {\n    intp.beQuietDuring {\n      command(\"\"\"\n        @transient val spark = org.apache.spark.repl.Main.interp.createSparkSession()\n        @transient val sc = {\n          val _sc = spark.sparkContext\n          if (_sc.getConf.getBoolean(\"spark.ui.reverseProxy\", false)) {\n            val proxyUrl = _sc.getConf.get(\"spark.ui.reverseProxyUrl\", null)\n            if (proxyUrl != null) {\n              println(s\"Spark Context Web UI is available at ${proxyUrl}/proxy/${_sc.applicationId}\")\n            } else {\n              println(s\"Spark Context Web UI is available at Spark Master Public URL\")\n            }\n          } else {\n            _sc.uiWebUrl.foreach {\n              webUrl => println(s\"Spark context Web UI available at ${webUrl}\")\n            }\n          }\n          println(\"Spark context available as 'sc' \" +\n            s\"(master = ${_sc.master}, app id = ${_sc.applicationId}).\")\n          println(\"Spark session available as 'spark'.\")\n          _sc\n        }\n        \"\"\")\n      command(\"import org.apache.spark.SparkContext._\")\n      command(\"import spark.implicits._\")\n      command(\"import spark.sql\")\n      command(\"import org.apache.spark.sql.functions._\")\n    }\n  }\n\n  // code to be executed only after the interpreter is initialized\n  // and the lazy val `global` can be accessed without risk of deadlock.\n  private var pendingThunks: List[() => Unit] = Nil\n  protected def addThunk(body: => Unit) = synchronized {\n    pendingThunks :+= (() => body)\n  }\n  protected def runThunks(): Unit = synchronized {\n    if (pendingThunks.nonEmpty)\n      logDebug(\"Clearing \" + pendingThunks.size + \" thunks.\")\n\n    while (pendingThunks.nonEmpty) {\n      val thunk = pendingThunks.head\n      pendingThunks = pendingThunks.tail\n      thunk()\n    }\n  }\n}\n```\n\n\n\n\n\n\n","tags":["Spark"],"categories":["大数据"]},{"title":"Spark之GraphX","url":"/2019/06/08/Spark之GraphX/","content":"\n {{ \"GraphX相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\nGraphX 是 Spark 图表和图形并行计算的新组件。GraphX 延伸 Spark [RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) 通过引入新的[图形](http://spark.apache.org/docs/latest/graphx-programming-guide.html#property_graph)的抽象：计算与连接到每个顶点和边缘性的向量。以支持图形计算，GraphX 公开了一组基本的操作符（例如  [subgraph](http://spark.apache.org/docs/latest/graphx-programming-guide.html#structural_operators), [joinVertices](http://spark.apache.org/docs/latest/graphx-programming-guide.html#join_operators)和 [aggregateMessages](http://spark.apache.org/docs/latest/graphx-programming-guide.html#aggregateMessages)）以及一个优化高阶API。此外，GraphX 包括的图形越来越多的收集 [algorithms](http://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_algorithms) 和 [builders](http://spark.apache.org/docs/latest/graphx-programming-guide.html#graph_builders) ，以简化图形分析任务。\n\n## 概念\n\n### 顶点\n\n`RDD[(VertexId, VD)]  `表示顶点。  VertexId 就是Long类型，表示顶点的ID【主键】。 VD表示类型参数，可以是任意类型, 表示的是该顶点的属性。\nVertexRDD[VD] 继承了RDD[(VertexId, VD)]， 他是顶点的另外一种表示方式， 在内部的计算上提供了很多的优化还有一些更高级的API。\n\n### 边\n\nRDD[Edge[VD]]  表示边，  Edge中有三个东西： srcId表示 源顶点的ID， dstId表示的是目标顶点的ID， attr表示表的属性，属性的类型是VD类型，VD是一个类型参数，可以是任意类型。\n\nEdgeRDD[ED] 继承了 RDD[Edge[ED]] ,他是边的另外一种表示方式，在内部的计算上提供您改了很多的优化还有一些更高级的API。\n\n### 三元组\n\n EdgeTriplet[VD, ED] extends Edge[ED]   他表示一个三元组， 比边多了两个顶点的属性\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608150952.png)\n\n### 图\n\n Graph[VD: ClassTag, ED: ClassTag]  VD 是顶点的属性、  ED是边的属性\n\n## 思路\n\n1、直接创建 sparkConf  -》  sparkContext\n2、创建顶点的RDD  RDD[(VertexId, VD)]\n3、创建边的RDD  RDD[Edge[ED]] \n4、根据边和顶点创建 Graph\n5、对图进行计算\n6、关闭 SparkContext\n\n```scala\nimport org.apache.spark.graphx.{Edge, Graph, VertexId}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject GraphxHelloWorld extends App {\n  //创建sparkConf\n  val sparkConf = new SparkConf().setAppName(\"graphx\").setMaster(\"local[*]\")\n\n  //创建SparkContext\n  val sc = new SparkContext(sparkConf)\n\n  //业务逻辑\n  val users: RDD[(VertexId, (String, String))] =\n    sc.parallelize(Array(\n      (3L, (\"rxin\", \"student\")),\n      (7L, (\"jgonzal\", \"postdoc\")),\n      (5L, (\"franklin\", \"prof\")),\n      (2L, (\"istoica\", \"prof\")))\n    )\n\n\n  val relationships: RDD[Edge[String]] =\n    sc.parallelize(Array(\n      Edge(3L, 7L, \"collab\"),\n      Edge(5L, 3L, \"advisor\"),\n      Edge(2L, 5L, \"colleague\"),\n      Edge(5L, 7L, \"pi\"))\n    )\n\n  val defaultUser = (\"John Doe\", \"Missing\")\n  val graph = Graph(users, relationships, defaultUser)\n  val facts: RDD[String] =\n    graph.triplets.map(triplet =>\n      triplet.srcAttr._1 + \" is the \" + triplet.attr + \" of \" + triplet.dstAttr._1)\n  facts.collect.foreach(println(_))\n\n  //关闭\n  sc.stop()\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608153045.png)\n\n## 操作\n\n### 创建操作\n\n 根据边和顶点的数据来创建。\n\n```scala\ndef apply[VD: ClassTag, ED: ClassTag](\n    vertices: RDD[(VertexId, VD)],\n    edges: RDD[Edge[ED]],\n    defaultVertexAttr: VD = null.asInstanceOf[VD],\n    edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,\n    vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]  \n```\n\n根据边直接创建， 所有顶点的属性都一样为 defaultValue\n\n```scala\ndef fromEdges[VD: ClassTag, ED: ClassTag](\n    edges: RDD[Edge[ED]],\n    defaultValue: VD,\n    edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,\n    vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, ED]\n```\n\n根据裸边来进行创建，顶点的属性是 defaultValue  ，边的属性为1\n\n```scala\ndef fromEdgeTuples[VD: ClassTag](\n    rawEdges: RDD[(VertexId, VertexId)],\n    defaultValue: VD,\n    uniqueEdges: Option[PartitionStrategy] = None,\n    edgeStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY,\n    vertexStorageLevel: StorageLevel = StorageLevel.MEMORY_ONLY): Graph[VD, Int] \n```\n\n### 转换操作\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608153513.png)\n\n`numEdges`  返回边的个数\n\n`numVertices`  顶点的个数\n\n`inDegrees: VertexRDD[Int] `  返回顶点的入度， 返回类型为 `RDD[(VertexId, Int)] Int`就是入度的具体值\n\n`outDegrees: VertexRDD[Int]`  返回顶点的出度， 返回类型为 `RDD[(VertexId, Int)]` Int就是出度的具体值\n `degrees: VertexRDD[Int]`  返回顶点的入度和出度之和。 返回类型为 `RDD[(VertexId, Int)]` Int就是出度的具体值\n\n### 结构操作\n\n` def reverse: Graph[VD, ED]`   反转整个图  ，将边的方向调头\n\n`def subgraph( epred: EdgeTriplet[VD, ED] => Boolean = (x => true), vpred: (VertexId, VD) => Boolean = ((v, d) => true)) : Graph[VD, ED]  `可以通过参数名来指定传参，  如果``subGraph`中有的边没有顶点对应，那么会自动将该边去除  。   没有边的顶点不会自动被删除\n\t\n`def mask[VD2: ClassTag, ED2: ClassTag](other:Graph[VD2, ED2]): Graph[VD, ED]   `将当前图和Other图做交集，返回一个新图，如果other中的属性和原图的属性不同，那么保留原图的属性\n\t\n`def groupEdges(merge: (ED, ED) => ED): Graph[VD, ED]` 合并两条边，通过函数合并边的属性。\n\n### 聚合操作\n\n`def collectNeighbors(edgeDirection: EdgeDirection): VertexRDD[Array[(VertexId, VD)]]` 收集邻居节点的数据，根据指定的方向。返回的数据为RDD[(VertexId,  Array[(VertexId, VD)] )]   顶点的属性是 一个数组。数组中包含邻居节点的顶点\n\n`def collectNeighborIds(edgeDirection: EdgeDirection): VertexRDD[Array[VertexId]] ` 跟上一个相同，只不过只收集ID\n\n`def aggregateMessages[A: ClassTag]( sendMsg: EdgeContext[VD, ED, A] => Unit, mergeMsg: (A, A) => A, tripletFields: TripletFields = TripletFields.All) : VertexRDD[A] `  每一个边都会通过`sendMsg` 发送一个消息， 每一个顶点都会通过`mergeMsg` 来处理所有他收到的消息。  `TripletFields`存在主要用于定制 `EdgeContext `对象中的属性的值是否存在， 为了减少数据通信量。\n\n### 关联操作\n\n` def joinVertices[U: ClassTag](table: RDD[(VertexId, U)])(mapFunc: (VertexId, VD, U) => VD) : Graph[VD, ED] ` 将相同顶点ID的数据进行加权，  将U这种类型的数据加入到 VD这种类型的数据上，但是不能修改VD的类型。\n\n`def outerJoinVertices[U: ClassTag, VD2: ClassTag](other: RDD[(VertexId, U)]) (mapFunc: (VertexId, VD, Option[U]) => VD2)(implicit eq: VD =:= VD2 = null) : Graph[VD2, ED]`   和`joinVertices`类似。，只不是如果没有相对应的节点，那么join的值默认为None。\n\n### Pregel\n\n节点：  有两种状态：\n\n1、钝化态【类似于休眠，不做任何事】 \n\n 2、激活态【干活】\n2、节点能够处于激活态需要有条件：\n\n（1）、节点收到消息  \n\n （2）、成功发送了任何一条消息\n\n```scala\ndef pregel[A: ClassTag](\n    initialMsg: A,        //     图初始化的时候，开始模型计算的时候，所有节点都会先收到一个消息。\n    maxIterations: Int = Int.MaxValue,     //最大迭代次数  \n    activeDirection: EdgeDirection = EdgeDirection.Either)   //规定了发送消息的方向\n   (\n    vprog: (VertexId, VD, A) => VD,  //节点调用该消息将聚合后的数据和本节点进行属性的合并。  \n    sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],   //激活态的节点调用该方法发送消息\n    mergeMsg: (A, A) => A)\t//如果一个节点接收到多条消息，先用mergeMsg 来将多条消息聚合成为一条消息，如果节点只收到一条消息，则不调用该函数\n  : Graph[VD, ED]\n\n```\n\n## 案例\n\n```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.graphx.{Edge, _}\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by wuyufei on 2017/9/22.\n  *\n  */\nobject Practice extends App {\n\n  //屏蔽日志\n  Logger.getLogger(\"org.apache.spark\").setLevel(Level.ERROR)\n  Logger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF)\n\n\n  //设定一个SparkConf\n  val conf = new SparkConf().setAppName(\"SimpleGraphX\").setMaster(\"local[4]\")\n  val sc = new SparkContext(conf)\n\n  //初始化顶点集合\n  val vertexArray = Array(\n    (1L, (\"Alice\", 28)),\n    (2L, (\"Bob\", 27)),\n    (3L, (\"Charlie\", 65)),\n    (4L, (\"David\", 42)),\n    (5L, (\"Ed\", 55)),\n    (6L, (\"Fran\", 50))\n  )\n  //创建顶点的RDD表示\n  val vertexRDD: RDD[(Long, (String, Int))] = sc.parallelize(vertexArray)\n\n  //初始化边的集合\n  val edgeArray = Array(\n    Edge(2L, 1L, 7),\n    Edge(2L, 4L, 2),\n    Edge(3L, 2L, 4),\n    Edge(3L, 6L, 3),\n    Edge(4L, 1L, 1),\n    Edge(2L, 5L, 2),\n    Edge(5L, 3L, 8),\n    Edge(5L, 6L, 3)\n  )\n\n  //创建边的RDD表示\n  val edgeRDD: RDD[Edge[Int]] = sc.parallelize(edgeArray)\n\n  //创建一个图\n  val graph: Graph[(String, Int), Int] = Graph(vertexRDD, edgeRDD)\n\n\n  //***************************  图的属性    ****************************************\n\n  println(\"属性演示\")\n  println(\"**********************************************************\")\n  println(\"找出图中年龄大于30的顶点：\")\n  graph.vertices.filter { case (id, (name, age)) => age > 30 }.collect.foreach {\n    case (id, (name, age)) => println(s\"$name is $age\")\n  }\n\n  println\n  //\n  println(\"找出图中属性大于5的边：\")\n  graph.edges.filter(e => e.attr > 5).collect.foreach(e => println(s\"${e.srcId} to ${e.dstId} att ${e.attr}\"))\n  println\n\n  //triplets操作，((srcId, srcAttr), (dstId, dstAttr), attr)\n  println(\"列出边属性>5的tripltes：\")\n  for (triplet <- graph.triplets.filter(t => t.attr > 5).collect) {\n    println(s\"${triplet.srcAttr._1} likes ${triplet.dstAttr._1}\")\n  }\n  println\n\n  //Degrees操作\n  println(\"找出图中最大的出度、入度、度数：\")\n\n  def max(a: (VertexId, Int), b: (VertexId, Int)): (VertexId, Int) = {\n    if (a._2 > b._2) a else b\n  }\n\n  println(\"max of outDegrees:\" + graph.outDegrees.reduce(max) + \" max of inDegrees:\" + graph.inDegrees.reduce(max) + \" max of Degrees:\" + graph.degrees.reduce(max))\n  println\n\n  //***************************  转换操作    ****************************************\n  println(\"转换操作\")\n  println(\"**********************************************************\")\n  println(\"顶点的转换操作，顶点age + 10：\")\n  graph.mapVertices { case (id, (name, age)) => (id, (name, age + 10)) }.vertices.collect.foreach(v => println(s\"${v._2._1} is ${v._2._2}\"))\n  println\n  println(\"边的转换操作，边的属性*2：\")\n  graph.mapEdges(e => e.attr * 2).edges.collect.foreach(e => println(s\"${e.srcId} to ${e.dstId} att ${e.attr}\"))\n  println\n  println(\"三元组的转换操作，边的属性为端点的age相加：\")\n  graph.mapTriplets(tri => tri.srcAttr._2 * tri.dstAttr._2).triplets.collect.foreach(e => println(s\"${e.srcId} to ${e.dstId} att ${e.attr}\"))\n  println\n\n  //***************************  结构操作    ****************************************\n  println(\"结构操作\")\n  println(\"**********************************************************\")\n  println(\"顶点年纪>30的子图：\")\n  val subGraph = graph.subgraph(vpred = (id, vd) => vd._2 >= 30)\n  println(\"子图所有顶点：\")\n  subGraph.vertices.collect.foreach(v => println(s\"${v._2._1} is ${v._2._2}\"))\n  println\n  println(\"子图所有边：\")\n  subGraph.edges.collect.foreach(e => println(s\"${e.srcId} to ${e.dstId} att ${e.attr}\"))\n  println\n  println(\"反转整个图：\")\n  val reverseGraph = graph.reverse\n  println(\"子图所有顶点：\")\n  reverseGraph.vertices.collect.foreach(v => println(s\"${v._2._1} is ${v._2._2}\"))\n  println\n  println(\"子图所有边：\")\n  reverseGraph.edges.collect.foreach(e => println(s\"${e.srcId} to ${e.dstId} att ${e.attr}\"))\n  println\n\n  //***************************  连接操作    ****************************************\n  println(\"连接操作\")\n  println(\"**********************************************************\")\n  val inDegrees: VertexRDD[Int] = graph.inDegrees\n\n  case class User(name: String, age: Int, inDeg: Int, outDeg: Int)\n\n  //创建一个新图，顶类点VD的数据型为User，并从graph做类型转换\n  val initialUserGraph: Graph[User, Int] = graph.mapVertices { case (id, (name, age)) => User(name, age, 0, 0) }\n\n  //initialUserGraph与inDegrees、outDegrees（RDD）进行连接，并修改initialUserGraph中inDeg值、outDeg值\n  val userGraph = initialUserGraph.outerJoinVertices(initialUserGraph.inDegrees) {\n    case (id, u, inDegOpt) => User(u.name, u.age, inDegOpt.getOrElse(0), u.outDeg)\n  }.outerJoinVertices(initialUserGraph.outDegrees) {\n    case (id, u, outDegOpt) => User(u.name, u.age, u.inDeg, outDegOpt.getOrElse(0))\n  }\n\n  println(\"连接图的属性：\")\n  userGraph.vertices.collect.foreach(v => println(s\"${v._2.name} inDeg: ${v._2.inDeg}  outDeg: ${v._2.outDeg}\"))\n  println\n\n  println(\"出度和入读相同的人员：\")\n  userGraph.vertices.filter {\n    case (id, u) => u.inDeg == u.outDeg\n  }.collect.foreach {\n    case (id, property) => println(property.name)\n  }\n  println\n\n  //***************************  聚合操作    ****************************************\n  println(\"聚合操作\")\n  println(\"**********************************************************\")\n  println(\"collectNeighbors：获取当前节点source节点的id和属性\")\n  graph.collectNeighbors(EdgeDirection.In).collect.foreach(v => {\n    println(s\"id: ${v._1}\"); for (arr <- v._2) {\n      println(s\"      ${arr._1} (name: ${arr._2._1}  age: ${arr._2._2})\")\n    }\n  })\n\n  println(\"aggregateMessages版本：\")\n  graph.aggregateMessages[Array[(VertexId, (String, Int))]](ctx => ctx.sendToDst(Array((ctx.srcId.toLong, (ctx.srcAttr._1, ctx.srcAttr._2)))), _ ++ _).collect.foreach(v => {\n    println(s\"id: ${v._1}\"); for (arr <- v._2) {\n      println(s\"    ${arr._1} (name: ${arr._2._1}  age: ${arr._2._2})\")\n    }\n  })\n\n  println(\"聚合操作\")\n  println(\"**********************************************************\")\n  println(\"找出年纪最大的追求者：\")\n\n\n  val oldestFollower: VertexRDD[(String, Int)] = userGraph.aggregateMessages[(String, Int)](\n    // 将源顶点的属性发送给目标顶点，map过程\n    ctx => ctx.sendToDst((ctx.srcAttr.name, ctx.srcAttr.age)),\n    // 得到最大追求者，reduce过程\n    (a, b) => if (a._2 > b._2) a else b\n  )\n\n  userGraph.vertices.leftJoin(oldestFollower) { (id, user, optOldestFollower) =>\n    optOldestFollower match {\n      case None => s\"${user.name} does not have any followers.\"\n      case Some((name, age)) => s\"${name} is the oldest follower of ${user.name}.\"\n    }\n  }.collect.foreach { case (id, str) => println(str) }\n  println\n\n  //***************************  实用操作    ****************************************\n  println(\"聚合操作\")\n  println(\"**********************************************************\")\n\n  val sourceId: VertexId = 5L // 定义源点\n  val initialGraph = graph.mapVertices((id, _) => if (id == sourceId) 0.0 else Double.PositiveInfinity)\n\n  initialGraph.triplets.collect().foreach(println)\n\n  println(\"找出5到各顶点的最短距离：\")\n  val sssp = initialGraph.pregel(Double.PositiveInfinity, Int.MaxValue, EdgeDirection.Out)(\n    (id, dist, newDist) => {\n      println(\"||||\" + id); math.min(dist, newDist)\n    },\n    triplet => { // 计算权重\n      println(\">>>>\" + triplet.srcId)\n      if (triplet.srcAttr + triplet.attr < triplet.dstAttr) {\n        //发送成功\n        Iterator((triplet.dstId, triplet.srcAttr + triplet.attr))\n      } else {\n        //发送不成功\n        Iterator.empty\n      }\n    },\n    (a, b) => math.min(a, b) // 当前节点所有输入的最短距离\n  )\n  sssp.triplets.collect().foreach(println)\n\n  println(sssp.vertices.collect.mkString(\"\\n\"))\n\n  sc.stop()\n}\n\n```\n\n## PageRank\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608155611.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608155624.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608155637.png)\n\n```scala\nimport org.apache.spark.graphx.{Graph, VertexId}\nimport org.apache.spark.{SparkConf, SparkContext, graphx}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/8.\n  */\nobject PageRank {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName(\"Spark Graphx PageRank\").setMaster(\"local[*]\")\n    val sc = new SparkContext(sparkConf)\n    val erdd = sc.textFile(\"D:\\\\input\\\\graphx-wiki-edges.txt\")\n    val edges = erdd.map(x => {\n      val para = x.split(\"\\t\"); graphx.Edge(para(0).trim.toLong, para(1).trim.toLong, 0)\n    })\n    val vrdd = sc.textFile(\"D:\\\\input\\\\graphx-wiki-vertices.txt\")\n    val vertices = vrdd.map(x =>{val para =x.split(\"\\t\");(para(0).trim.toLong,para(1).trim)})\n    val graph =Graph(vertices,edges)\n    println(\"*****************************************************\")\n    println(\"PageRank计算,获取最有价值的数据\")\n    println(\"*****************************************************\")\n\n    val prGraph = graph.pageRank(0.001).cache()\n\n    val titleAndPrGraph = graph.outerJoinVertices(prGraph.vertices) {\n      (v, title, rank) => (rank.getOrElse(0.0), title)\n    }\n\n   titleAndPrGraph.vertices.top(10) {\n      Ordering.by((entry: (VertexId, (Double, String))) => entry._2._1)\n    }.foreach(t => println(t._2._2 + \": \" + t._2._1))\n    \n    sc.stop()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/Graphx/20190608162612.png)\n\n\n\n\n\n\n","tags":["Spark","GraphX"],"categories":["大数据"]},{"title":"Spark之StructuredStreaming","url":"/2019/06/07/Spark之StructuredStreaming/","content":"\n {{ \"Structured Streaming相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\n Structured Streaming是Spark2.0版本提出的新的实时流框架，是一种基于Spark SQL引擎的可扩展且容错的流处理引擎。在内部，默认情况下，结构化流式查询使用微批处理引擎进行处理，该引擎将数据流作为一系列小批量作业处理，从而实现低至100毫秒的端到端延迟和完全一次的容错保证。自Spark 2.3以来，引入了一种称为连续处理的新型低延迟处理模式，它可以实现低至1毫秒的端到端延迟，并且具有至少一次保证。\n\n相比于Spark Streaming，优点如下：\n\n支持多种数据源的输入和输出\n以结构化的方式操作流式数据，能够像使用Spark SQL处理离线的批处理一样，处理流数据，代码更简洁，写法更简单\n基于Event-Time，相比于Spark Streaming的Processing-Time更精确，更符合业务场景\n解决了Spark Streaming存在的代码升级，DAG图变化引起的任务失败，无法断点续传的问题。\n\n## WordCount\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/7.\n  */\nobject WordCount {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder()\n      .appName(\"StructuredNetworkWordCount\")\n      .master(\"local[*]\")\n      .getOrCreate()\n\n    val lines = spark.readStream\n      .format(\"socket\")\n      .option(\"host\", \"datanode1\")\n      .option(\"port\", 9999)\n      .load()\n\n    import spark.implicits._\n    val words = lines.as[String].flatMap(_.split(\" \"))\n    val wordCounts = words.groupBy(\"value\").count()\n\n    val query = wordCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .start()\n\n    query.awaitTermination()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/StructuredStreaming.gif)\n\n## 编程模型\n\n结构化流的关键思想是将活生生的数据流看作一张正在被连续追加数据的表。产生了一个与批处理模型非常相似的新的流处理模型。可以像在静态表之上的标准批处理查询一样，Spark是使用在一张无界的输入表之上的增量式查询来执行流计算的。\n\n![Stream as a Table](http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png)\n\n数据流Data Stream看成了表的行数据，连续地往表中追加。结构化流查询将会产生一张结果表（Result Table）:\n\n![Model](http://spark.apache.org/docs/latest/img/structured-streaming-model.png)\n\n第一行是Time，每秒有个触发器，第二行是输入流，对输入流执行查询后产生的结果最终会被更新到第三行的结果表中。第四行驶输出，图中显示的输出模式是完全模式（Complete Mode）。图中显示的是无论结果表何时得到更新，我们将希望将改变的结果行写入到外部存储。输出有三种不同的模式：\n\n（1）完全模式（Complete Mode）\n\n整个更新的结果表（Result Table）将被写入到外部存储。这取决于外部连接决定如何操作整个表的写入。\n\n（2）追加模式（Append Mode）\n\n只有从上一次触发后追加到结果表中新行会被写入到外部存储。适用于已经存在结果表中的行不期望被改变的查询。\n\n（3）更新模式（Update Mode）\n\n只有从上一次触发后在结果表中更新的行将会写入外部存储（Spark 2.1.1之后才可用）。这种模式不同于之前的完全模式，它仅仅输出上一次触发后改变的行。如果查询中不包含聚合，这种模式与追加模式等价的。每种模式适用于特定类型的查询。下面以单词计数的例子说明三种模式的区别（单词计数中使用了聚合）\n\n![Model](http://spark.apache.org/docs/latest/img/structured-streaming-example-model.png)\n\n### Event-time Late Data\n\n Event-time是嵌入到数据本身的基于事件的时间。对于许多的应用来说，你可能希望操作这个事件-时间。例如，如果你想获得每分钟物联网设备产生的事件数量，然后想使用数据产生时的时间（也就是数据的event-time），而不是Spark接收他们的时间。每个设备中的事件是表中的一行，而事件-时间是行中的一个列值。这就允许将基于窗口的聚合（比如每分钟的事件数）看成是事件-时间列的分组和聚合的特殊类型——每个时间窗口是一个组，每行可以属于多个窗口/组。\n\n 进一步，这个模型自然处理那些比期望延迟到达的事件-时间数据。当Spark正在更新结果表时，当有延迟数据，它就会完全控制更新旧的聚合，而且清理旧的聚合去限制中间状态数据的大小。从Spark 2.1开始，我们已经开始支持水印（watermarking ），它允许用户确定延迟的阈值，允许引擎相应地删除旧的状态。\n\n### 窗口操作\n\n在滑动的事件-时间窗口上的聚合对于结构化流是简单的，非常类似于分组聚合。在分组聚合中，聚合的值对用户确定分组的列保持唯一的。在基于窗口的聚合中，聚合的值对每个窗口的事件-时间保持唯一的。\n\n 修改我们前面的单词计数的例子，现在当产生一行句子时，附件一个时间戳。我们想每5分钟统计一次10分钟内的单词数。例如，12:00 - 12:10, 12:05 - 12:15, 12:10 - 12:20等。注意到12:00 - 12:10是一个窗口，表示数据12:00之后12:10之前到达。比如12:07到达的单词，这个单词应该在12:00 - 12:10和12:05 - 12:15两个窗口中都要被统计。如图：\n\n![img](http://spark.apache.org/docs/latest/img/structured-streaming-window.png)\n\n```scala\nimport java.sql.Timestamp\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/7.\n  */\nobject WindowOnEventTime {\n\n  case class TimeWord(word: String, timestamp: Timestamp)\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession.builder()\n      .appName(\"Structrued-Streaming\")\n      .master(\"local[*]\")\n      .getOrCreate()\n      \n    val lines = spark.readStream\n      .format(\"socket\")\n      .option(\"host\", \"datanode1\")\n      .option(\"port\", 9999)\n      .option(\"includeTimestamp\", true) //添加时间戳\n      .load()\n    import spark.implicits._\n\n    val words = lines.as[(String, Timestamp)]\n      .flatMap(line => line._1.split(\" \")\n        .map(word => TimeWord(word, line._2))).toDF()\n\n    // 计数\n    val windowedCounts = words.groupBy(\n      window($\"timestamp\",\"10 seconds\" ,\"5 seconds\"), $\"word\"\n    ).count().orderBy(\"window\")\n\n    // 查询\n    val query = windowedCounts.writeStream\n      .outputMode(\"complete\")\n      .format(\"console\")\n      .option(\"truncate\", \"false\")\n      .start()\n    query.awaitTermination()\n\n  }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/StructuredStreaming_WindowandEvent.gif)\n\n### 容错语义\n\n提供end-to-end exactly-once语义是structured streaming设计背后的关键目标之一。 为了实现这一点，Spark设计了structured streaming的sources，sinks和执行引擎，可靠地跟踪处理进程的准确进度，以便它可以通过重新启动和/或重新处理来解决任何类型的故障。 假设每个Streaming源具有跟踪流中读取位置的偏移（类似于Kafka偏移或Kinesis序列号）。 引擎使用检查点和WAL（write ahead logs）记录每个触发器中正在处理的数据的偏移范围。 Streaming sinks为了解决重复计算被设计为幂等。 一起使用可重放sources和幂等sinks，Structured Streaming可以在任何故障下确保end-to-end exactly-once的语义。\n\n###  Watermarking\n\n现在考虑如果其中一个事件延迟到达应用程序会发生什么。 例如，说在12:04（即事件时间）生成的一个单词可以在12:11被应用程序接收。 应用程序\n\n应该使用时间12:04而不是12:11更新12:00 - 12:10的窗口的较旧计数。 这在我们基于窗口的分组中自然发生 - Structured Streaming可以长时间维持部分聚合的中间状态，以便迟到的数据可以正确地更新旧窗口的聚合，如下所示。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190608100551.png)\n\n为了持续几天运行这个查询，系统必须限制其累积的内存中间状态的数量。这意味着系统需要知道什么时候可以从内存状态中删除旧的聚合，因为应用程序不会再为该集合接收到较晚的数据。为了实现这一点，在Spark 2.1中引入了watermarking，让引擎自动跟踪数据中的当前event-time，并尝试相应地清理旧状态。您可以通过指定事件时间列来定义查询的watermarking，并根据事件时间指定数据的延迟时间的阈值。对于从时间T开始的特定窗口，引擎将保持状态，并允许延迟数据更新状态，直到引擎看到最大事件时间-迟到的最大阈值。换句话说，阈值内的迟到数据将被聚合，但是比阈值晚的数据将被丢弃。让我们以一个例子来理解这一点。我们可以使用Watermark（）轻松定义上一个例子中的watermarking ，\n\n```scala\nval windowedCounts = words\n    .withWatermark(\"timestamp\", \"10 minutes\")\n    .groupBy(\n        window($\"timestamp\", \"10 minutes\", \"5 minutes\"),\n        $\"word\")\n    .count()\n```\n\n在这个例子中，我们正在定义“timestamp”列的查询的watermark ，并将“10分钟”定义为允许数据延迟的阈值。 如果此查询在更新输出模式下运行（稍后在“输出模式”部分中讨论），则引擎将继续更新Resule表中窗口的计数，直到窗口比watermark 旧，滞后于当前事件时间列“ timestamp“10分钟。 \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190608101146.png)\n\n与之前的更新模式类似，引擎维护每个窗口的中间计数。 但是，部分计数不会更新到结果表，也不写入sink。 引擎等待“10分钟”接收迟到数据，然后丢弃窗口（watermark）的中间状态，并将最终计数附加到结果表sink。 例如，窗口12:00 - 12:10的最终计数仅在watermark更新到12:11之后才附加到结果表中。 \nwatermarking 清理聚合状态的条件重要的是要注意，为了清理聚合查询中的状态，必须满足以下条件（从Spark 2.1.1开始，以后再进行更改）。 \n\n- 输出模式必须是追加或更新。 完整模式要求保留所有聚合数据，因此不能使用watermarking 去掉中间状态。 有关每种输出模式的语义的详细说明，请参见“输出模式”部分。 \n- 聚合必须具有事件时间列或事件时间列上的窗口。 \n- 必须在与聚合中使用的时间戳列相同的列上使用withWatermark 。 例如，df.withWatermark（“time”，“1 min”）.groupBy（“time2”）.count（）在附加输出模式中无效，因为watermark 在不同的列上定义为聚合列。 \n- 必须在聚合之前调用withWatermark才能使用watermark 细节。 例如，在附加输出模式下，``df.groupBy（“time”）.count（）.withWatermark（“time”，“1 min”）``无效。\n\n### Join操作\n\nStreaming DataFrames可以与静态 DataFrames连接，以创建新的Streaming DataFrames。 例如下面的例子。\n\n```scala\nval staticDf = spark.read. ...\nval streamingDf = spark.readStream. ... \n\nstreamingDf.join(staticDf, \"type\")          // inner equi-join with a static DF\nstreamingDf.join(staticDf, \"type\", \"right_join\")  // right outer join with a static DF\n```\n\n\n在Spark 2.3中，Spark添加了对流 - 流 Join的支持，也就是说，您可以加入两个 streaming Datasets/DataFrames。 在两个数据流之间生成连接结果的挑战是，在任何时间点，dataset的view对于连接的两侧都是不完整的，这使得在输入之间找到匹配更加困难。 从一个输入流接收的任何行的数据都可以与来自另一个输入流的未来输入的任何一条数据匹配，尚未接收的行匹配。 因此，对于两个输入流，我们将过去的输入缓冲为流状态，以便我们可以将每个未来输入与过去的输入相匹配，从而生成Join结果。 此外，类似于流聚合，Spark自动处理迟到的无序数据，并可以使用水印限制状态。 \n\n```scala\nmport org.apache.spark.sql.functions.expr\n\nval impressions = spark.readStream. ...\nval clicks = spark.readStream. ...\n\n// Apply watermarks on event-time columns\nval impressionsWithWatermark = impressions.withWatermark(\"impressionTime\", \"2 hours\")\nval clicksWithWatermark = clicks.withWatermark(\"clickTime\", \"3 hours\")\n\n// Join with event-time constraints\nimpressionsWithWatermark.join(\n  clicksWithWatermark,\n  expr(\"\"\"\n    clickAdId = impressionAdId AND\n    clickTime >= impressionTime AND\n    clickTime <= impressionTime + interval 1 hour\n    \"\"\")\n)\n```\n\n## 不支持的操作\n\n有几个DataFrame / Dataset操作不支持streaming DataFrames / Datasets。 其中一些如下。 \n- streaming Datasets不支持多个streaming聚合（即streaming DF上的聚合链）。 \n- 流数据集不支持limit和取前N行。 \n- Streaming Datasets不支持Distinct 操作。 \n- 只有在在完全输出模式的聚合之后，streaming Datasets才支持排序操作。 \n- 有条件地支持Streaming和静态Datasets之间的外连接。 \n不支持与 streaming Dataset的Full outer join \n不支持streaming Dataset 在右侧的Left outer join \n不支持streaming Dataset在左侧的Right outer join\n\n- 两个streaming Datasets之间的任何种类型的join都不受支持\n\n此外，还有一些Dataset方法将不适用于streaming Datasets。 它们是立即运行查询并返回结果的操作，这在streaming Datasets上没有意义。 相反，这些功能可以通过显式启动streaming查询来完成（参见下一节）。 \n\\- count() - 无法从流数据集返回单个计数。 而是使用ds.group By.count（）返回一个包含running count的streaming Dataset 。 \n\\- foreach() - 而是使用ds.writeStream.foreach（…）（见下一节）。 \n\\- show() - Instead use the console sink (see next section).\n\n## 流式查询\n\n### 输出模式\n\n- Append mode (default) - 这是默认模式，其中只有从上次触发后添加到结果表的新行将被输出到sink。 只有那些添加到“结果表”中并且从不会更改的行的查询才支持这一点。 因此，该模式保证每行只能输出一次（假定容错sink）。 例如，只有select，where，map，flatMap，filter，join等的查询将支持Append模式。 \n- Complete mode -每个触发后，整个结果表将被输出到sink。 聚合查询支持这一点。 \n- Update mode - （自Spark 2.1.1以来可用）只有结果表中自上次触发后更新的行才会被输出到sink。 更多信息将在以后的版本中添加。\n\n| Type                                            | Supported Output Modes   | 备注                                                         |\n| ----------------------------------------------- | ------------------------ | ------------------------------------------------------------ |\n| 没有聚合的查询                                  | Append, Update           | 不支持完整模式，因为将所有数据保存在结果表中是不可行的。     |\n| 有聚合的查询：使用watermark对event-time进行聚合 | Append, Update, Complete | 附加模式使用watermark 来降低旧聚合状态。 但是，窗口化聚合的输出会延迟“withWatermark（）”中指定的晚期阈值，因为模式语义可以在结果表中定义后才能将结果表添加到结果表中（即在watermark 被交叉之后）。 有关详细信息，请参阅后期数据部分。更新模式使用水印去掉旧的聚合状态。完全模式不会丢弃旧的聚合状态，因为根据定义，此模式保留结果表中的所有数据。 |\n| 有聚合的查询：其他聚合                          | Complete, Update         | 由于没有定义watermark （仅在其他类别中定义），旧的聚合状态不会被丢弃。不支持附加模式，因为聚合可以更新，从而违反了此模式的语义。 |\n\n### Output Sinks\n\nFile sink-将输出存储到目录\n\n```scala\nwriteStream\n    .format(\"parquet\")        // 也可以是 \"orc\", \"json\", \"csv\", 等等.\n    .option(\"path\", \"path/to/destination/dir\")\n    .start()\n```\n\nForeach sink - 对输出中的记录运行任意计算。 \n\n```scala\nwriteStream\n    .foreach(...)\n    .start()\n```\n\nConsole sink (for debugging) \n\n每次触发时将输出打印到控制台/ stdout。 都支持“Append ”和“Complete ”输出模式。 这应该用于低数据量的调试目的，因为在每次触发后，整个输出被收集并存储在驱动程序的内存中。\n\n```scala\nwriteStream\n    .format(\"console\")\n    .start()\n```\n\nMemory sink (for debugging) \n\n输出作为内存表存储在内存中。 都支持“Append ”和“Complete ”输出模式。 由于整个输出被收集并存储在驱动程序的内存中，所以应用于低数据量的调试目的。 因此，请谨慎使用。\n\n```scala\nwriteStream\n    .format(\"memory\")\n    .queryName(\"tableName\")\n    .start()\n```\n\n| sink         | Supported Output Modes    | Options                                                      | Fault-tolerant                                           | Notes                                     |\n| ------------ | ------------------------- | ------------------------------------------------------------ | -------------------------------------------------------- | ----------------------------------------- |\n| File Sink    | Append                    | path：输出目录的路径，必须指定。 maxFilesPerTrigger：每个触发器中要考虑的最大新文件数（默认值：无最大值） latestFirst：是否首先处理最新的新文件，当有大量的文件积压（default：false）时很有用 有关特定于文件格式的选项，请参阅DataFrameWriter（Scala / Java / Python）中的相关方法。 例如。 对于“parquet”格式选项请参阅DataFrameWriter.parquet（） | yes                                                      | 支持对分区表的写入。 按时间划分可能有用。 |\n| Foreach Sink | Append, Update, Compelete | None                                                         | 取决于ForeachWriter的实现                                | 更多细节在下一节                          |\n| Console Sink | Append, Update, Complete  | numRows：每次触发打印的行数（默认值：20）truncate：输出太长是否截断（默认值：true） | no                                                       |                                           |\n| Memory Sink  | Append, Complete          | None                                                         | 否。但在Complete模式下，重新启动的查询将重新创建整个表。 | 查询名就是表名                            |\n\n您必须调用start（）来实际启动查询的执行。 这将返回一个StreamingQuery对象，它是连续运行执行的句柄。 您可以使用此对象来管理查询\n\n```scala\nval noAggDF = deviceDataDf.select(\"device\").where(\"signal > 10\")   \n\n\n// Print new data to console\nnoAggDF\n  .writeStream\n  .format(\"console\")\n  .start()\n\n\n// Write new data to Parquet files\nnoAggDF\n  .writeStream\n  .format(\"parquet\")\n  .option(\"checkpointLocation\", \"path/to/checkpoint/dir\")\n  .option(\"path\", \"path/to/destination/dir\")\n  .start()\n\n// ========== DF with aggregation ==========\nval aggDF = df.groupBy(\"device\").count()\n\n\n// Print updated aggregations to console\naggDF\n  .writeStream\n  .outputMode(\"complete\")\n  .format(\"console\")\n  .start()\n\n\n// Have all the aggregates in an in-memory table \naggDF\n  .writeStream\n  .queryName(\"aggregates\")    // this query name will be the table name\n  .outputMode(\"complete\")\n  .format(\"memory\")\n  .start()\n\n\nspark.sql(\"select * from aggregates\").show()   // interactively query in-memory table\n```\n\n### Foreach和ForeachBatch\n\nforeach和foreachBatch操作允许您在流式查询的输出上应用任意操作和编写逻辑。 它们的用例略有不同 - 虽然foreach允许在每一行上自定义写入逻辑，foreachBatch允许在每个微批量的输出上进行任意操作和自定义逻辑。\n\nforeachBatch（）允许您指定在流式查询的每个微批次的输出数据上执行的函数。 从Spark 2.4开始，Scala，Java和Python都支持它。 它需要两个参数：DataFrame或Dataset，它具有微批次的输出数据和微批次的唯一ID。\n\n```scala\nstreamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) =>\n  // Transform and write batchDF \n}.start()\n```\n\n#### foreachBatch\n\n重用现有的批处理数据源 - 对于许多存储系统，可能还没有可用的流式接收器，但可能已经存在用于批量查询的数据写入器。使用foreachBatch，您可以在每个微批次的输出上使用批处理数据编写器。\n\n写入多个位置 - 如果要将流式查询的输出写入多个位置，则可以简单地多次写入输出DataFrame / Dataset。但是，每次写入尝试都会导致重新计算输出数据（包括可能重新读取输入数据）。要避免重新计算，您应该缓存输出DataFrame / Dataset，将其写入多个位置，然后将其解除。这是一个大纲。\n\n```scala\nstreamingDF.writeStream.foreachBatch { (batchDF: DataFrame, batchId: Long) => batchDF.persist() batchDF.write.format(…).save(…) // location 1 batchDF.write.format(…).save(…) // location 2 batchDF.unpersist() }\n```\n\n应用其他DataFrame操作 - 流式DataFrame中不支持许多DataFrame和Dataset操作，因为Spark不支持在这些情况下生成增量计划。使用foreachBatch，您可以在每个微批输出上应用其中一些操作。但是，您必须自己解释执行该操作的端到端语义。注意：默认情况下，foreachBatch仅提供至少一次写保证。但是，您可以使用提供给该函数的batchId作为重复数据删除输出并获得一次性保证的方法。 foreachBatch不适用于连续处理模式，因为它从根本上依赖于流式查询的微批量执行。如果以连续模式写入数据，请改用foreach。\n\n注意：默认情况下，foreachBatch仅提供至少一次写保证。 但是，您可以使用提供给该函数的batchId作为重复数据删除输出并获得一次性保证的方法。 foreachBatch不适用于连续处理模式，因为它从根本上依赖于流式查询的微批量执行。 如果以连续模式写入数据，请改用foreach。\n\n#### Foreach\n\n如果foreachBatch不是一个选项（例如，相应的批处理数据写入器不存在，或连续处理模式），那么您可以使用foreach表达自定义编写器逻辑。 具体来说，您可以通过将数据划分为三种方法来表达数据写入逻辑：打开，处理和关闭。 从Spark 2.4开始，foreach可用于Scala，Java和Python。\n\n```scala\ntreamingDatasetOfString.writeStream.foreach(\n  new ForeachWriter[String] {\n\n    def open(partitionId: Long, version: Long): Boolean = {\n      // Open connection\n    }\n\n    def process(record: String): Unit = {\n      // Write string to connection\n    }\n\n    def close(errorOrNull: Throwable): Unit = {\n      // Close the connection\n    }\n  }\n).start()\n```\n\n执行语义启动流式查询时，Spark以下列方式调用函数或对象的方法：\n\n此对象的单个副本负责查询中单个任务生成的所有数据。换句话说，一个实例负责处理以分布式方式生成的数据的一个分区。\n\n此对象必须是可序列化的，因为每个任务都将获得所提供对象的新的序列化反序列化副本。 因此，强烈建议在调用open（）方法之后完成用于写入数据的任何初始化（例如，打开连接或启动事务），这表示任务已准备好生成数据。\n\n```scala\n- 方法的生命周期如下：\n-   For each partition with partition_id:\n    - For each batch/epoch of streaming data with epoch_id:\n        - open(partitionId, epochId) 被调用\n        - 如果open（...）返回true，则对于partition和 batch/epoch中的每一行，将调用方法process(row)\n        - 调用方法close（错误），在处理行时看到错误（如果有的话话）。\n```\n\n当失败导致某些输入数据的重新处理时，open（）方法中的partitionId和epochId可用于对生成的数据进行重复数据删除。 这取决于查询的执行模式。 如果以微批处理模式执行流式查询，则保证由唯一元组（partition_id，epoch_id）表示的每个分区具有相同的数据。 因此，（partition_id，epoch_id）可用于对数据进行重复数据删除和/或事务提交，并实现一次性保证。 但是，如果正在以连续模式执行流式查询，则此保证不成立，因此不应用于重复数据删除。\n\n### 触发器\n\n流式查询的触发器设置定义了流式数据处理的时间，查询是作为具有固定批处理间隔的微批量查询还是作为连续处理查询来执行。 以下是支持的各种触发器。\n\n| Trigger Type                                           | Description                                                  |\n| :----------------------------------------------------- | :----------------------------------------------------------- |\n| 未指定（默认）                                         | 如果未明确指定触发设置，则默认情况下，查询将以微批处理模式执行，一旦前一个微批处理完成处理，将立即生成微批处理。 |\n| **Fixed interval micro-batches**                       | 查询将以微批处理模式执行，其中微批处理将以用户指定的间隔启动。<br/>如果先前的微批次在该间隔内完成，则引擎将等待该间隔结束，然后开始下一个微批次。<br/>如果前一个微批次需要的时间长于完成的间隔（即如果错过了间隔边界），则下一个微批次将在前一个完成后立即开始（即，它不会等待下一个间隔边界） ）。<br/>如果没有可用的新数据，则不会启动微批次。 |\n| One-time micro-batch                                   | 查询将执行*仅一个*微批处理所有可用数据，然后自行停止。 这在您希望定期启动集群，处理自上一个时间段以来可用的所有内容，然后关闭集群的方案中非常有用。 在某些情况下，这可能会显着节省成本。 |\n| **Continuous with fixed checkpoint interval** *(实验)* | 查询将以新的低延迟，连续处理模式执行。在下面的连续处理部分中阅读更多相关信息。 [Continuous Processing section](http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#continuous-processing-experimental) |\n\n```scala\nimport org.apache.spark.sql.streaming.Trigger\n\n// Default trigger (runs micro-batch as soon as it can)\ndf.writeStream\n  .format(\"console\")\n  .start()\n\n// ProcessingTime trigger with two-seconds micro-batch interval\ndf.writeStream\n  .format(\"console\")\n  .trigger(Trigger.ProcessingTime(\"2 seconds\"))\n  .start()\n\n// One-time trigger\ndf.writeStream\n  .format(\"console\")\n  .trigger(Trigger.Once())\n  .start()\n\n// Continuous trigger with one-second checkpointing interval\ndf.writeStream\n  .format(\"console\")\n  .trigger(Trigger.Continuous(\"1 second\"))\n  .start()\n```\n\n## 管理流式查询\n\n启动查询时创建的StreamingQuery对象可用于监视和管理查询。\n\n```scala\nval query = df.writeStream.format(\"console\").start()   // get the query object\n\nquery.id          // get the unique identifier of the running query that persists across restarts from checkpoint data\n\nquery.runId       // get the unique id of this run of the query, which will be generated at every start/restart\n\nquery.name        // get the name of the auto-generated or user-specified name\n\nquery.explain()   // print detailed explanations of the query\n\nquery.stop()      // stop the query\n\nquery.awaitTermination()   // block until query is terminated, with stop() or with error\n\nquery.exception       // the exception if the query has been terminated with error\n\nquery.recentProgress  // an array of the most recent progress updates for this query\n\nquery.lastProgress    // the most recent progress update of this streaming query\n```\n\n您可以在单个SparkSession中启动任意数量的查询。 它们将同时运行，共享群集资源,您可以使用`sparkSession.streams()`来获取可用于管理当前活动的查询的StreamingQueryManager\n\n```scala\nval spark: SparkSession = ...\n\nspark.streams.active    // get the list of currently active streaming queries\n\nspark.streams.get(id)   // get a query object by its unique id\n\nspark.streams.awaitAnyTermination()   // block until any one of them terminates\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190608115959.png)\n\n更多请参考Spark官方网站\n\n## 参考资料\n\nSpark官方网站\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","Structured Streaming"],"categories":["大数据"]},{"title":"Spark之SparkStreaming的DStream操作","url":"/2019/06/06/Spark之SparkStreaming的DStream操作/","content":"\n {{ \"DStream的转换操作和输出、累加器等\"}}：<Excerpt in index | 首页摘要><!-- more --> \n \n## 转换\n\nDStream上的原语分为Transformations（转换）和Output Operations（输出）两种，此外转换操作中还有一些比较特殊的原语，如：updateStateByKey()、transform()以及各种Window相关的原语。\n\n| **Transformation**               | **Meaning**                                                  |\n| -------------------------------- | ------------------------------------------------------------ |\n| map(func)                        | 将源DStream中的每个元素通过一个函数func从而得到新的DStreams。 |\n| flatMap(func)                    | 和map类似，但是每个输入的项可以被映射为0或更多项。           |\n| filter(func)                     | 选择源DStream中函数func判为true的记录作为新DStreams          |\n| repartition(numPartitions)       | 通过创建更多或者更少的partition来改变此DStream的并行级别。   |\n| union(otherStream)               | 联合源DStreams和其他DStreams来得到新DStream                  |\n| count()                          | 统计源DStreams中每个RDD所含元素的个数得到单元素RDD的新DStreams。 |\n| reduce(func)                     | 通过函数func(两个参数一个输出)来整合源DStreams中每个RDD元素得到单元素RDD的DStreams。这个函数需要关联从而可以被并行计算。 |\n| countByValue()                   | 对于DStreams中元素类型为K调用此函数，得到包含(K,Long)对的新DStream，其中Long值表明相应的K在源DStream中每个RDD出现的频率。 |\n| reduceByKey(func, [numTasks])    | 对(K,V)对的DStream调用此函数，返回同样（K,V)对的新DStream，但是新DStream中的对应V为使用reduce函数整合而来。*Note*：默认情况下，这个操作使用Spark默认数量的并行任务（本地模式为2，集群模式中的数量取决于配置参数spark.default.parallelism）。你也可以传入可选的参数numTaska来设置不同数量的任务。 |\n| join(otherStream, [numTasks])    | 两DStream分别为(K,V)和(K,W)对，返回(K,(V,W))对的新DStream。  |\n| cogroup(otherStream, [numTasks]) | 两DStream分别为(K,V)和(K,W)对，返回(K,(Seq[V],Seq[W])对新DStreams |\n| transform(func)                  | 将RDD到RDD映射的函数func作用于源DStream中每个RDD上得到新DStream。这个可用于在DStream的RDD上做任意操作。 |\n| updateStateByKey(func)           | 得到”状态”DStream，其中每个key状态的更新是通过将给定函数用于此key的上一个状态和新值而得到。这个可用于保存每个key值的任意状态数据。 |\n\nDStream 的转化操作可以分为无状态(stateless)和有状态(stateful)两种。 \n\n在无状态转化操作中，每个批次的处理不依赖于之前批次的数据。常见的 RDD 转化操作，例如 map()、filter()、reduceByKey() 等，都是无状态转化操作。 \n\n相对地，有状态转化操作需要使用之前批次的数据或者是中间结果来计算当前批次的数据。有状态转化操作包括基于滑动窗口的转化操作和追踪状态变化的转化操作。\n\n### 无状态转化\n\n无状态转化操作就是把简单的 RDD 转化操作应用到每个批次上，也就是转化 DStream 中的每一个 RDD。部分无状态转化操作列在了下表中。注意，针对键值对的 DStream 转化操作(比如reduceByKey())要添加import StreamingContext._ 才能在Scala中使用。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190606111311.png)\n\n尽管这些函数看起来像作用在整个流上一样，但事实上每个 DStream 在内部是由许多 RDD(批次)组成，且无状态转化操作是分别应用到每个 RDD 上的。例如， reduceByKey() 会归约每个时间区间中的数据，但不会归约不同区间之间的数据。 \n\n举个例子，在之前的wordcount程序中，我们只会统计1秒内接收到的数据的单词个数，而不会累加。 \n\n无状态转化操作也能在多个 DStream 间整合数据，不过也是在各个时间区间内。例如，键 值对 DStream 拥有和 RDD 一样的与连接相关的转化操作，也就是 cogroup()、join()、 leftOuterJoin() 等。我们可以在 DStream 上使用这些操作，这样就对每个批次分别执行了对应的 RDD 操作。\n\n我们还可以像在常规的 Spark 中一样使用 DStream 的 union() 操作将它和另一个 DStream 的内容合并起来，也可以使用 StreamingContext.union() 来合并多个流。 \n\n### 有状态转化\n\nUpdateStateByKey原语用于记录历史记录，有时，我们需要在 DStream 中跨批次维护状态(例如流计算中累加wordcount)。针对这种情况，updateStateByKey() 为我们提供了对一个状态变量的访问，用于键值对形式的 DStream。给定一个由(键，事件)对构成的 DStream，并传递一个指定如何根据新的事件 更新每个键对应状态的函数，它可以构建出一个新的 DStream，其内部数据为(键，状态) 对。 \n\nupdateStateByKey() 的结果会是一个新的 DStream，其内部的 RDD 序列是由每个时间区间对应的(键，状态)对组成的。\n\nupdateStateByKey操作使得我们可以在用新信息进行更新时保持任意的状态。\n\n```scala\n  def updateStateByKey[S: ClassTag](   \n      updateFunc: (Seq[V], Option[S]) => Option[S]\n    ): DStream[(K, S)] = ssc.withScope {\n    updateStateByKey(updateFunc, defaultPartitioner())\n  }\n```\n\n状态存储在CheckPoint中，类似于一个HashMap，key就是KV结构的Key，updateFunc中的seq[V]是Rdd中所有Value的集合，第一个Option[s]是上一次的状态，第二个Option[S]是新产生的状态。\n\n```scala\n\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/6.\n  */\nobject StateFulWordCount extends App {\n  val sparkConf = new SparkConf().setAppName(\"Stateful WordCount\").setMaster(\"local[*]\")\n  val ssc = new StreamingContext(sparkConf, Seconds(5))\n  ssc.sparkContext.setCheckpointDir(\"./checkpoint\")\n  val line = ssc.socketTextStream(\"datanode1\", 9999)\n  val words = line.flatMap(_.split(\" \"))\n  val word2Count = words.map((_, 1))\n\n  val state = word2Count.updateStateByKey[Int] { (values: Seq[Int], state: Option[Int]) =>\n    state match {\n      case None => Some(values.sum)\n      case Some(pre) => Some(values.sum + pre)\n    }\n  }\n  state.print()\n  ssc.start()\n  ssc.awaitTermination()\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/StateFUlSparkStreaming.gif)\n\n## Window Operations\n\nWindow Operations 以设置窗口的大小和滑动窗口的间隔来动态的获取当前Steaming的允许状态。\n\n基于窗口的操作会在一个比 StreamingContext 的批次间隔更长的时间范围内，通过整合多个批次的结果，计算出整个窗口的结果。 \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190606195914.png)\n\n所有基于窗口的操作都需要两个参数，分别为窗口时长以及滑动步长，两者都必须是 StreamContext 的批次间隔的整数倍。窗口时长控制每次计算最近的多少个批次的数据，其实就是最近的 windowDuration/batchInterval 个批次。如果有一个以 10 秒为批次间隔的源 DStream，要创建一个最近 30 秒的时间窗口(即最近 3 个批次)，就应当把 windowDuration 设为 30 秒。而滑动步长的默认值与批次间隔相等，用来控制对新的 DStream 进行计算的间隔。如果源 DStream 批次间隔为 10 秒，并且我们只希望每两个批次计算一次窗口结果， 就应该把滑动步长设置为 20 秒。 \n\n假设，你想拓展前例从而每隔十秒对持续30秒的数据生成word count。为做到这个，我们需要在持续30秒数据的(word,1)对DStream上应用reduceByKey。使用操作reduceByKeyAndWindow.\n\n````scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/6.\n  */\nobject PairDstreamFunctions extends App {\n  val sparkConf = new SparkConf().setAppName(\"stateful\").setMaster(\"local[*]\")\n  val ssc = new StreamingContext(sparkConf, Seconds(5))\n  ssc.sparkContext.setCheckpointDir(\"./checkpoint\")\n\n  val lines = ssc.socketTextStream(\"datanode1\", 9999)\n  val words = lines.flatMap(_.split(\" \"))\n  val word2Count = words.map((_, 1))\n  val state = word2Count.reduceByKeyAndWindow((a: Int, b: Int) => a + b, Seconds(15), Seconds(5))\n  state.print()\n\n  ssc.start()\n  ssc.awaitTermination()\n}\n````\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/SparkStreamingWindow.gif)\n\n## DStreams输出\n\n输出操作指定了对流数据经转化操作得到的数据所要执行的操作(例如把结果推入外部数据库或输出到屏幕上)。与 RDD 中的惰性求值类似，如果一个 DStream 及其派生出的 DStream 都没有被执行输出操作，那么这些 DStream 就都不会被求值。如果 StreamingContext 中没有设定输出操作，整个 context 就都不会启动。 \n\n| **Output Operation**                        | **Meaning**                                                  |\n| ------------------------------------------- | ------------------------------------------------------------ |\n| **print**()                                 | 在运行流程序的驱动结点上打印DStream中每一批次数据的最开始10个元素。这用于开发和调试。在Python API中，同样的操作叫pprint()。 |\n| **saveAsTextFiles**(*prefix*, [*suffix*])   | 以text文件形式存储这个DStream的内容。每一批次的存储文件名基于参数中的prefix和suffix。”prefix-Time_IN_MS[.suffix]”. |\n| **saveAsObjectFiles**(*prefix*, [*suffix*]) | 以Java对象序列化的方式将Stream中的数据保存为 SequenceFiles . 每一批次的存储文件名基于参数中的为\"prefix-TIME_IN_MS[.suffix]\". Python中目前不可用。 |\n| **saveAsHadoopFiles**(*prefix*, [*suffix*]) | 将Stream中的数据保存为 Hadoop files. 每一批次的存储文件名基于参数中的为\"prefix-TIME_IN_MS[.suffix]\".     Python API Python中目前不可用。 |\n| **foreachRDD**(*func*)                      | 这是最通用的输出操作，即将函数func用于产生于stream的每一个RDD。其中参数传入的函数func应该实现将每一个RDD中数据推送到外部系统，如将RDD存入文件或者通过网络将其写入数据库。注意：函数func在运行流应用的驱动中被执行，同时其中一般函数RDD操作从而强制其对于流RDD的运算。 |\n\n通用的输出操作 foreachRDD()，它用来对 DStream 中的 RDD 运行任意计算。这和transform() 有些类似，都可以让我们访问任意 RDD。在 foreachRDD() 中，可以重用我们在 Spark 中实现的所有行动操作。比如，常见的用例之一是把数据写到诸如 MySQL 的外部数据库中。 \n\n需要注意的：\n\n连接不能写在driver层面\n\n如果写在foreach则每个RDD都创建，得不偿失\n\n增加foreachPartition，在分区创建\n\n可以考虑使用连接池优化\n\n我们写一个Mysql连接池\n\n```scala\nimport java.sql.Connection;\nimport java.sql.DriverManager;\nimport java.util.LinkedList;\n\npublic class ConnectionPool {\n\n    private static LinkedList<Connection> connectionQueue;\n\n    static {\n        try {\n            Class.forName(\"com.mysql.jdbc.Driver\");\n        } catch (ClassNotFoundException e) {\n            e.printStackTrace();\n        }\n    }\n\n    public synchronized static Connection getConnection() {\n        try {\n            if (connectionQueue == null) {\n                connectionQueue = new LinkedList<Connection>();\n                for (int i = 0; i < 5; i++) {\n                    Connection conn = DriverManager.getConnection(\n                            \"jdbc:mysql://192.168.1.101:3306/sparkstreaming\",\n                            \"root\",\n                            \"123456\");\n                    connectionQueue.push(conn);\n                }\n            }\n        } catch (Exception e) {\n            e.printStackTrace();\n        }\n        return connectionQueue.poll();\n    }\n\n    public static void returnConnection(Connection conn) {\n        connectionQueue.push(conn);\n    }\n}\n```\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/6.\n  */\nobject DStream2Mysql extends App {\n  val sparkConf = new SparkConf().setAppName(\"stateful\").setMaster(\"local[*]\")\n  val ssc = new StreamingContext(sparkConf, Seconds(5))\n  val lines = ssc.socketTextStream(\"datanode1\", 9999)\n  val words = lines.flatMap(_.split(\" \"))\n  val word2Count = words.map((_, 1)).reduceByKey(_ + _)\n  word2Count.print()\n  word2Count.foreachRDD { rdd =>\n    rdd.foreachPartition { partitionOfRecords => {\n      // ConnectionPool is a static, lazily initialized pool of connections\n      val connection = ConnectionPool.getConnection()\n      partitionOfRecords.foreach(record => {\n        val sql = \"insert into streaming_wordCount(item,count) values('\" + record._1 + \"',\" + record._2 + \")\"\n        val stmt = connection.createStatement();\n        stmt.executeUpdate(sql);\n      })\n      ConnectionPool.returnConnection(connection) // return to the pool for future reuse\n    }\n    }\n  }\n  ssc.start()\n  ssc.awaitTermination()\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/SparkStreaming2ysql.gif)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190606234958.png)\n\n## 累加器广播变量\n\n累加器(Accumulators)和广播变量(Broadcast variables)不能从Spark Streaming的检查点中恢复。如果你启用检查并也使用了累加器和广播变量，那么你必须创建累加器和广播变量的延迟单实例从而在驱动因失效重启后他们可以被重新实例化。如下例述：\n\n```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.broadcast.Broadcast\nimport org.apache.spark.util.LongAccumulator\n\nobject WordBlacklist {\n\n  @volatile private var instance: Broadcast[Seq[String]] = null\n\n  def getInstance(sc: SparkContext): Broadcast[Seq[String]] = {\n    if (instance == null) {\n      synchronized {\n        if (instance == null) {\n          val wordBlacklist = Seq(\"flink\", \"spark\", \"hadoop\")\n          instance = sc.broadcast(wordBlacklist)\n        }\n      }\n    }\n    instance\n  }\n}\n\nobject DroppedWordsCounter {\n\n  @volatile private var instance: LongAccumulator = null\n\n  def getInstance(sc: SparkContext): LongAccumulator = {\n    if (instance == null) {\n      synchronized {\n        if (instance == null) {\n          instance = sc.longAccumulator(\"WordsInBlacklistCounter\")\n        }\n      }\n    }\n    instance\n  }\n}\n```\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.{Seconds, StreamingContext, Time}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/7.\n  */\nobject Accumulators {\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setAppName(\"Streaming_AccumulatorAndBroadcast\").setMaster(\"local[*]\")\n    val ssc = new StreamingContext(sparkConf, Seconds(5))\n    val lines = ssc.socketTextStream(\"datanode1\", 9999)\n    val words = lines.flatMap(_.split(\",\"))\n    val wordCounts = words.map((_, 1)).reduceByKey(_ + _) //转换为二元组进行累加操作\n\n    wordCounts.foreachRDD { (rdd: RDD[(String, Int)], time: Time) =>\n      // Get or register the blacklist Broadcast\n      val blacklist = WordBlacklist.getInstance(rdd.sparkContext)\n      // Get or register the droppedWordsCounter Accumulator\n      val droppedWordsCounter = DroppedWordsCounter.getInstance(rdd.sparkContext)\n      // Use blacklist to drop words and use droppedWordsCounter to count them\n      val counts = rdd.filter { case (word, count) =>\n        if (blacklist.value.contains(word)) {\n          droppedWordsCounter.add(count)\n          false\n        } else {\n          true\n        }\n      }.collect().mkString(\"[\", \", \", \"]\")\n      val output = \"Counts at time \" + time + \" \" + counts\n      println(output)\n    }\n\n    wordCounts.print()\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/SparkStreamingAccumlators.gif)\n\n## DataFrame ans SQL Operations\n\n你可以很容易地在流数据上使用DataFrames和SQL。你必须使用SparkContext来创建StreamingContext要用的SQLContext。此外，这一过程可以在驱动失效后重启。我们通过创建一个实例化的SQLContext单实例来实现这个工作。如下例所示。我们对前例word count进行修改从而使用DataFrames和SQL来产生word counts。每个RDD被转换为DataFrame，以临时表格配置并用SQL进行查询\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.streaming.{Minutes, Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/7.\n  */\nobject StreamingSQL {\n  def main(args: Array[String]): Unit = {\n    val conf = new SparkConf().setAppName(\"SparkStreaming SQL\").setMaster(\"local[*]\")\n    val ssc = new StreamingContext(conf, Seconds(5))\n\n    val linesDStream = ssc.socketTextStream(\"datanode1\", 9999)\n\n    linesDStream.foreachRDD { rdd =>\n\n      // Get the singleton instance of SparkSession\n      val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()\n      import spark.implicits._\n\n      // Convert RDD[String] to DataFrame\n      val wordsDataFrame = rdd.toDF(\"word\")\n\n      // Create a temporary view\n      wordsDataFrame.createOrReplaceTempView(\"words\")\n\n      // Do word count on DataFrame using SQL and print it\n      val wordCountsDataFrame =\n        spark.sql(\"select word, count(*) as total from words group by word\")\n      wordCountsDataFrame.show()\n    }\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/SparkStreamingSQL.gif)\n\n## Caching / Persistence\n\nDStreams允许开发者将流数据保存在内存中。也就是说，在DStream上使用persist()方法将会自动把DStreams中的每个RDD保存在内存中。当DStream中的数据要被多次计算时，这个非常有用（如在同样数据上的多次操作）。对于像reduceByWindow和reduceByKeyAndWindow以及基于状态的(updateStateByKey)这种操作，保存是隐含默认的。因此，即使开发者没有调用persist()，由基于窗操作产生的DStreams会自动保存在内存中。 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","SparkStreaming"],"categories":["大数据"]},{"title":"Spark之SparkStreaming数据源","url":"/2019/06/05/Spark之SparkStreaming数据源/","content":"\n {{ \"SparkStreaming的数据源 文件 Flume Kafka\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## DStreams输入\n\nSpark Streaming原生支持一些不同的数据源。一些“核心”数据源已经被打包到Spark Streaming 的 Maven 工件中，而其他的一些则可以通过 `spark-streaming-kafka` 等附加工件获取。每个接收器都以 Spark 执行器程序中一个长期运行的任务的形式运行，因此会占据分配给应用的 CPU 核心。此外，我们还需要有可用的 CPU 核心来处理数据。这意味着如果要运行多个接收器，就必须至少有和接收器数目相同的核心数，还要加上用来完成计算所需要的核心数。例如，如果我们想要在流计算应用中运行 10 个接收器，那么至少需要为应用分配 11 个 CPU 核心。所以如果在本地模式运行，不要使用local或者local[1]。\n\n### 文件数据源\n\n文件数据流：能够读取所有HDFS API兼容的文件系统文件，通过fileStream方法进行读取。\n\nSpark Streaming 将会监控 dataDirectory 目录并不断处理移动进来的文件，目前不支持嵌套目录。\n\n文件需要有相同的数据格式。\n\n文件进入 dataDirectory的方式需要通过移动或者重命名来实现。\n\n一旦文件移动进目录，则不能再修改，即便修改了也不会读取新数据。\n\n如果文件比较简单，则可以使用 streamingContext.textFileStream(dataDirectory)方法来读取文件。文件流不需要接收器，不需要单独分配CPU核。\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nobject StreamingHDFS {\n  def main(args: Array[String]): Unit = {\n    //创建配置\n    val sparkConf = new SparkConf().setAppName(\"streaming data from HDFS\").setMaster(\"local[*]\")\n    //创建StreamingContext\n    val ssc = new StreamingContext(sparkConf, Seconds(5))\n    //从HDFS接口数据\n    val lines = ssc.textFileStream(\"hdfs://datanode1:9000/input/streaming/\")\n\n    val words = lines.flatMap(_.split(\" \"))\n\n    val wordCounts = words.map((_, 1)).reduceByKey(_ + _)\n\n    wordCounts.print()\n    ssc.start()\n    ssc.awaitTermination()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/HDFSStreaming.gif)\n\n### 自定义配置\n\n通过继承Receiver，并实现onStart、onStop方法来自定义数据源采集。\n\n```scala\nimport java.io.{BufferedReader, InputStreamReader}\nimport java.net.Socket\n\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.receiver.Receiver\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/3.\n  */\nclass CustomerRecevicer(host: String, port: Int) extends Receiver[String](StorageLevel.MEMORY_ONLY) {\n  //接收器启动的时候子自动调用\n  override def onStart(): Unit = {\n    //创建线程\n    new Thread(\"receiver\") {\n      override def run(): Unit = {\n        //接受数据并提交给框架\n        receive()\n      }\n    }.start()\n  }\n\n  def receive(): Unit = {\n\n    var socket: Socket = null\n    var input: String = null\n    try {\n      socket = new Socket(host, port)\n      //生成输入流\n      val reader = new BufferedReader(new InputStreamReader(socket.getInputStream))\n\n      //接收数据\n      //            input = reader.readLine()\n      while (!isStopped() && (input = reader.readLine()) != null) {\n        store(input)\n      }\n      restart(\"restart\")\n\n    } catch {\n      case e: java.net.ConnectException => restart(\"restart\")\n      case t:Throwable => restart(\"restart\")\n    }\n  }\n\n  //接收器关闭的时候调用\n  override def onStop(): Unit = {\n  }\n}\n```\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n\nobject CustomerStreamingWordCount extends App {\n  //创建配置\n  val sparkConf = new SparkConf().setAppName(\"streaming word count\").setMaster(\"local[*]\")\n  //创建StreamingContext\n  val ssc = new StreamingContext(sparkConf, Seconds(5))\n  //从socket接口数据   \n  val lineDStream = ssc.receiverStream(new CustomerRecevicer(\"datanode1\", 9999))  //自定义的使用的是receiverStream\n\n  val wordDStream = lineDStream.flatMap(_.split(\" \"))\n  val word2CountDStream = wordDStream.map((_, 1))\n  val result = word2CountDStream.reduceByKey(_ + _)\n\n\n  result.print()\n  //启动\n  ssc.start()\n  ssc.awaitTermination()\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/CustomersparkStreamingWordCount.gif)\n\n### RDD队列\n\nSpark Streaming也可以使用 streamingContext.queueStream(queueOfRDDs)来创建DStream，每一个推送到这个队列中的RDD，都会作为一个DStream处理。\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\nimport scala.collection.mutable\n\nobject QueueRdd {\n\n  def main(args: Array[String]) {\n\n    val conf = new SparkConf().setMaster(\"local[2]\").setAppName(\"QueueRdd\")\n    val ssc = new StreamingContext(conf, Seconds(1))\n\n    //创建RDD队列\n    val rddQueue = new mutable.SynchronizedQueue[RDD[Int]]()\n\n    // 创建QueueInputDStream\n    val inputStream = ssc.queueStream(rddQueue)\n\n    //处理队列中的RDD数据\n    val mappedStream = inputStream.map(x => (x % 10, 1))\n    val reducedStream = mappedStream.reduceByKey(_ + _)\n\n    //打印结果\n    reducedStream.print()\n\n    //启动计算\n    ssc.start()\n\n    // Create and push some RDDs into\n    for (i <- 1 to 30) {\n      rddQueue += ssc.sparkContext.makeRDD(1 to 300, 10)\n      Thread.sleep(5000)\n\n      //通过程序停止StreamingContext的运行\n      //ssc.stop()\n    }\n  }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/QueueRdd.gif)\n\n### 高级数据源(Kafka等)\n\n这一类的来源需要外部接口，其中一些有复杂的依赖关系（如Kafka和Flume),因此通过这些来源创建DStreams需要明确其依赖。在工程中需要引入 Maven 工件 spark- streaming-kafka_2.10 来使用它。包内提供的 KafkaUtils 对象可以在 StreamingContext 和 JavaStreamingContext 中以你的 Kafka 消息创建出 DStream。由于 KafkaUtils 可以订阅多个主题，因此它创建出的 DStream 由成对的主题和消息组成。要创建出一个流数据，需 要使用 StreamingContext 实例、一个由逗号隔开的 ZooKeeper 主机列表字符串、消费者组的名字(唯一名字)，以及一个从主题到针对这个主题的接收器线程数的映射表来调用 createStream() 方法\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190604111236.png)\n\n```scala\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.kafka.KafkaUtils\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/4.\n  */\nobject KafkaStreming {\n  def main(args: Array[String]): Unit = {\n\n    //配置\n    val conf = new SparkConf().setAppName(\"KafkaStreaming\") setMaster (\"local[*]\")\n\n    val ssc = new StreamingContext(conf, Seconds(5))\n\n    //kafka的参数\n    val brokers = \"datanode1:9092,datanode2:9092,datanode3:9092\"\n    val zookeeper = \"datanode1:2181,datanode2:2181,datanode3:2181\"\n    val sourceTopic = \"source\"\n    val targetTopic = \"target\"\n    val consumerGroup = \"consumer\"\n\n    //封装kafka参数\n    val kafkaParams = Map[String, String] {\n      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokers\n      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringDeserializer\"\n      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> \"org.apache.kafka.common.serialization.StringDeserializer\"\n      ConsumerConfig.GROUP_ID_CONFIG -> consumerGroup\n\n\n    }\n\n    val kafkaDStream = KafkaUtils.x(ssc, kafkaParams, Set(sourceTopic))\n    kafkaDStream.print()\n\n    ssc.start()\n    ssc.awaitTermination()\n\n\n  }\n}\n\n```\n\n```shell\nnohup /opt/module/kafka/bin/kafka-server-start.sh /opt/module/kafka/config/server.properties & #启动kafka\n[hadoop@datanode1 bin]$ nohup ./kafka-manager  -java-home /opt/module/jdk1.8.0_162/  -Dconfig.file=../conf/application.conf >/dev/null 2>&1 &  #启动kafkamanager\n /opt/module/kafka/bin/kafka-topics.sh --zookeeper datanode1:2181 --create --replication-factor 2 --partitions 2 --topic source #创建一个topic\n  /opt/module/kafka/bin/kafka-console-producer.sh --broker-list datanode1:9092 --topic source #启动生产者\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/KafkaSpark.gif)\n\n```scala\nimport org.apache.kafka.clients.consumer.ConsumerConfig\nimport org.apache.kafka.common.serialization.StringDeserializer\nimport org.apache.spark.{SparkConf, TaskContext}\nimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\nimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\nimport org.apache.spark.streaming.kafka010.{HasOffsetRanges, KafkaUtils, OffsetRange}\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/5.\n  */\nobject StreamingWithKafka {\n\n  private val brokeList = \"datanode1:9092,datanode2:9092,datanode2:9092\"\n  private val topic = \"topic-spark\"\n  private val group = \"group-spark\"\n  private val checkpointDir = \"/opt/kafka/checkpoint\"\n\n  def main(args: Array[String]): Unit = {\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"StreamingWithKafka\")\n\n    val ssc = new StreamingContext(sparkConf, Seconds(5))\n    ssc.checkpoint(checkpointDir)\n\n    //创建kafka的连接对象\n    val kafkaParams = Map[String, Object](\n      ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG -> brokeList, //Kafka集群\n      ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer], //序列化\n      ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG -> classOf[StringDeserializer], //序列化\n      ConsumerConfig.GROUP_ID_CONFIG -> group, //消费者组\n      ConsumerConfig.AUTO_OFFSET_RESET_CONFIG -> \"latest\", //latest自动重置偏移量为最新的偏移量\n      ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG -> (false: java.lang.Boolean) //是否自动提交\n    )\n    //创建DStream,发挥接受的消息\n    val stream = KafkaUtils.createDirectStream[String, String](\n      ssc,\n      PreferConsistent,\n      Subscribe[String, String](List(topic), kafkaParams))\n\n    val value = stream.map(record => {\n      val intVal = Integer.valueOf(record.value())\n      println(intVal)        // 打印输入数字\n      intVal\n    }).reduce(_ + _)   //相加\n    value.print()   //输出\n\n    stream.foreachRDD(rdd => {\n      val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges\n      rdd.foreachPartition { iter =>\n        val o: OffsetRange = offsetRanges(TaskContext.getPartitionId())\n        print(s\"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}\")\n\n      }\n    }\n    )\n    \n    ssc.start\n    ssc.awaitTermination\n  }\n\n}\n```\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --create --replication-factor 3 --partitions 1 --topic-spark\nbin/kafka-console-producer.sh --broker-list datanode1:9092 --topic topic-spark\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/KafkaSparkStreaming.gif)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190605200456.png)\n\n因为在本地运行E盘放了我们程序相当于E盘就是根目录了,可以指定HDFS.\n\n#### 两种连接方式\n\nSpark对于Kafka的连接主要有两种方式，一种是DirectKafkaInputDStream，另外一种是KafkaInputDStream。DirectKafkaInputDStream 只在 driver 端接收数据，所以继承了 InputDStream，是没有 receivers 的。\n\n主要通过KafkaUtils.createDirectStream以及KafkaUtils.createStream这两个 API 来创建，除了要传入的参数不同外，接收 kafka 数据的节点、拉取数据的时机也完全不同。\n\n##### createStream[Receiver-based]\n\n这种方法使用一个 Receiver 来接收数据。在该 Receiver 的实现中使用了 Kafka high-level consumer API。Receiver 从 kafka 接收的数据将被存储到 Spark executor 中，随后启动的 job 将处理这些数据。\n\n在默认配置下，该方法失败后会丢失数据（保存在 executor 内存里的数据在 application 失败后就没了），若要保证数据不丢失，需要启用 WAL（即预写日志至 HDFS、S3等），这样再失败后可以从日志文件中恢复数据。\n\n在该函数中，会新建一个 KafkaInputDStream对象，KafkaInputDStream继承于 ReceiverInputDStream。KafkaInputDStream实现了getReceiver方法，返回接收器的实例：\n\n```scala\n  def getReceiver(): Receiver[(K, V)] = {\n    if (!useReliableReceiver) {\n      //< 不启用 WAL\n      new KafkaReceiver[K, V, U, T](kafkaParams, topics, storageLevel)\n    } else {\n      //< 启用 WAL\n      new ReliableKafkaReceiver[K, V, U, T](kafkaParams, topics, storageLevel)\n    }\n  }\n```\n\n根据是否启用 WAL，receiver 分为KafkaReceiver 和 ReliableKafkaReceiver。下图描述了 KafkaReceiver 接收数据的具体流程：\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190605201219.png)\n\nKafka Topic 的 partitions 与RDD 的 partitions 没有直接关系，不能一一对应。如果增加 topic 的 partition 个数的话仅仅会增加单个 Receiver 接收数据的线程数。事实上，使用这种方法只会在一个 executor 上启用一个 Receiver，该 Receiver 包含一个线程池，线程池的线程个数与所有 topics 的 partitions 个数总和一致，每条线程接收一个 topic 的一个 partition 的数据。而并不会增加处理数据时的并行度。\n\n对于一个 topic，可以使用多个 groupid 相同的 input DStream 来使用多个 Receivers 来增加并行度，然后 union 他们；对于多个 topics，除了可以用上个办法增加并行度外，还可以对不同的 topic 使用不同的 input DStream 然后 union 他们来增加并行度\n\n如果你启用了 WAL，为能将接收到的数据将以 log 的方式在指定的存储系统备份一份，需要指定输入数据的存储等级为 StorageLevel.MEMORY_AND_DISK_SER 或 StorageLevel.MEMORY_AND_DISK_SER_2\n\n##### createDirectStream[WithOut Receiver]\n\n自 Spark-1.3.0 起，提供了不需要 Receiver 的方法。替代了使用 receivers 来接收数据，该方法定期查询每个 topic+partition 的 lastest offset，并据此决定每个 batch 要接收的 offsets 范围。\n\ncreateDirectStream调用中，会新建DirectKafkaInputDStream，DirectKafkaInputDStream#compute(validTime: Time)会从 kafka 拉取数据并生成 RDD，流程如下：\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190605201632.png)\n\n该函数主要做了以下三个事情：\n\n确定要接收的 partitions 的 offsetRange，以作为第2步创建的 RDD 的数据来源\n\n创建 RDD 并执行 count 操作，使 RDD 真实具有数据\n\n以 streamId、数据条数，offsetRanges 信息初始化 inputInfo 并添加到 JobScheduler 中\n\n```scala\n进一步看 KafkaRDD 的 getPartitions 实现：\n\n  override def getPartitions: Array[Partition] = {\n\n    offsetRanges.zipWithIndex.map { case (o, i) =>\n\n        val (host, port) = leaders(TopicAndPartition(o.topic, o.partition))\n\n        new KafkaRDDPartition(i, o.topic, o.partition, o.fromOffset, o.untilOffset, host, port)\n\n    }.toArray\n\n  }\n```\n\n\n\n从上面的代码可以很明显看到，KafkaRDD 的 partition 数据与 Kafka topic 的某个 partition 的 o.fromOffset 至 o.untilOffset 数据是相对应的，也就是说 KafkaRDD 的 partition 与 Kafka partition 是一一对应的\n\n该方式相比使用 Receiver 的方式有以下好处：\n\n简化并行：不再需要创建多个 kafka input DStream 然后再 union 这些 input DStream。使用 directStream，Spark Streaming会创建与 Kafka partitions 相同数量的 paritions 的 RDD，RDD 的 partition与 Kafka 的 partition 一一对应，这样更易于理解及调优\n\n高效：在方式一中要保证数据零丢失需要启用 WAL（预写日志），这会占用更多空间。而在方式二中，可以直接从 Kafka 指定的 topic 的指定 offsets 处恢复数据，不需要使用 WAL\n\n恰好一次语义保证：基于Receiver方式使用了 Kafka 的 high level API 来在 Zookeeper 中存储已消费的 offsets。这在某些情况下会导致一些数据被消费两次，比如 streaming app 在处理某个 batch  内已接受到的数据的过程中挂掉，但是数据已经处理了一部分，但这种情况下无法将已处理数据的 offsets 更新到 Zookeeper 中，下次重启时，这批数据将再次被消费且处理。基于direct的方式，使用kafka的简单api，Spark Streaming自己就负责追踪消费的offset，并保存在checkpoint中。Spark自己一定是同步的，因此可以保证数据是消费一次且仅消费一次。这种方式中，只要将 output 操作和保存 offsets 操作封装成一个原子操作就能避免失败后的重复消费和处理，从而达到恰好一次的语义（Exactly-once）\n\n通过以上分析，我们可以对这两种方式的区别做一个总结：\n\ncreateStream会使用 Receiver；而createDirectStream不会\n\ncreateStream使用的 Receiver 会分发到某个 executor 上去启动并接受数据；而createDirectStream直接在 driver 上接收数据\n\ncreateStream使用 Receiver 源源不断的接收数据并把数据交给 ReceiverSupervisor 处理最终存储为 blocks 作为 RDD 的输入，从 kafka 拉取数据与计算消费数据相互独立；而createDirectStream会在每个 batch 拉取数据并就地消费，到下个 batch 再次拉取消费，周而复始，从 kafka 拉取数据与计算消费数据是连续的，没有独立开\n\ncreateStream中创建的KafkaInputDStream 每个 batch 所对应的 RDD 的 partition 不与 Kafka partition 一 一对应；而createDirectStream中创建的 DirectKafkaInputDStream 每个 batch 所对应的 RDD 的 partition 与 Kafka partition 一 一对应\n\n#### Flume\n\nSpark提供两个不同的接收器来使用Apache Flum 两个接收器简介如下。 \n\n推式接收器该接收器以 Avro 数据池的方式工作，由 Flume 向其中推数据。 \n\n拉式接收器该接收器可以从自定义的中间数据池中拉数据，而其他进程可以使用 Flume 把数据推进 该中间数据池。 \n\n两种方式都需要重新配置 Flume，并在某个节点配置的端口上运行接收器(不是已有的 Spark 或者 Flume 使用的端口)。要使用其中任何一种方法，都需要在工程中引入 Maven 工件 spark-streaming-flume_2.10。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190605202028.png)\n\nAvro 数据池的方式工作，我们需要配置 Flume 来把数据发到 Avro 数据池。我们提供的 FlumeUtils 对象会把接收器配置在一个特定的工作节点的主机名及端口号上。这些设置必须和 Flume 配置相匹配。 \n\n虽然这种方式很简洁，但缺点是没有事务支持。这会增加运行接收器的工作节点发生错误 时丢失少量数据的几率。不仅如此，如果运行接收器的工作节点发生故障，系统会尝试从 另一个位置启动接收器，这时需要重新配置 Flume 才能将数据发给新的工作节点。这样配 置会比较麻烦。 \n\n较新的方式是拉式接收器(在Spark 1.1中引入)，它设置了一个专用的Flume数据池供 Spark Streaming读取，并让接收器主动从数据池中拉取数据。这种方式的优点在于弹性较好，Spark Streaming通过事务从数据池中读取并复制数据。在收到事务完成的通知前，这些数据还保留在数据池中。 \n\n我们需要先把自定义数据池配置为 Flume 的第三方插件。安装插件的最新方法请参考 Flume 文档的相关部分([链接](https://flume.apache.org/FlumeUserGuide.html#installing-third-party- plugins))。由于插件是用 Scala 写的，因此需要把插件本身以及 Scala 库都添加到 Flume 插件 中。\n\n```xml\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-streaming-flume-sink_2.11</artifactId>\n    <version>1.2.0</version>\n</dependency>\n<dependency>\n    <groupId>org.scala-lang</groupId>\n    <artifactId>scala-library</artifactId>\n    <version>2.11.11</version>\n</dependency>\n```\n\n当你把自定义 Flume 数据池添加到一个节点上之后，就需要配置 Flume 来把数据推送到这个数据池中， \n\n```properties\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# source\na1.sources.r1.type = spooldir\na1.sources.r1.spoolDir = /home/hadoop/flumedata\na1.sources.r1.fileHeader = true\n\n# Describe the sink\na1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSink\n#表示从这里拉数据\na1.sinks.k1.hostname = 192.168.1.101\na1.sinks.k1.port = 4444\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n```\n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.dstream.{DStream, ReceiverInputDStream}\nimport org.apache.spark.streaming.flume.{FlumeUtils, SparkFlumeEvent}\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/6/5.\n  */\nobject FlumeStream {\n  def main(args: Array[String]): Unit = {\n    //spark配置\n    val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"Flume Spark Streaming\")\n\n    val ssc = new StreamingContext(conf, Seconds(5))\n\n    val inputDstream:ReceiverInputDStream[SparkFlumeEvent]\n    = FlumeUtils.createPollingStream(ssc, \"192.168.1.101\", 4444, StorageLevel.MEMORY_AND_DISK)\n    val words = inputDstream.flatMap(x => new String(x.event.getBody.array()).split(\" \"))\n    val wordAndOne : DStream[(String,Int)] = words.map((_,1))\n    val result : DStream[(String,Int)] = wordAndOne.reduceByKey(_+_)\n\n    result.print()\n\n    // 启动流\n    ssc.start()\n    ssc.awaitTermination()\n  }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/flume_Spark.gif)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","SparkStreaming"],"categories":["大数据"]},{"title":"Spark之SparkStreaming理论篇","url":"/2019/06/03/Spark之SparkStreaming理论篇/","content":"\n {{ \"SparkStreaming的相关理论学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\nSpark Streaming用于流式数据的处理。Spark Streaming有高吞吐量和容错能力强等特点。Spark Streaming支持的数据输入源很多，例如：Kafka、Flume、Twitter、ZeroMQ和简单的TCP套接字等等。数据输入后可以用Spark的高度抽象原语如：map、reduce、join、window等进行运算。结果也能保存在很多地方，如HDFS，数据库等。Spark Streaming也能和MLlib（机器学习）以及Graphx完美融合。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603142207.png)\n\n和Spark基于RDD的概念很相似，Spark Streaming使用离散化流(discretized stream)作为抽象表示，叫作DStream。DStream 是随时间推移而收到的数据的序列。在内部，每个时间区间收到的数据都作为 RDD 存在，而 DStream 是由这些 RDD 所组成的序列(因此 得名“离散化”)。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603142349.png)\n\nDStream 可以从各种输入源创建，比如 Flume、Kafka 或者 HDFS。创建出来的DStream。支持两种操作，一种是转化操作(transformation)，会生成一个新的DStream，另一种是输出操作(output operation)，可以把数据写入外部系统中。DStream 提供了许多与 RDD 所支持的操作相类似的操作支持，还增加了与时间相关的新操作，比如滑动窗口。\n\n## 对比\n\n### 批处理比较\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603142911.png)\n\n### 流处理比较\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603143002.png)\n\n后续会更新Flink的学习笔记。\n\n## HelloWorld\n\n### pom\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>sparkstreaming</artifactId>\n        <groupId>com.hph</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>helloworld</artifactId>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-streaming_2.11</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n```\n\n### \n\n```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n\n\nobject StreamingWordCount extends App {\n  //创建配置\n  val sparkConf = new SparkConf().setAppName(\"streaming word count\").setMaster(\"local[*]\")\n  //创建StreamingContext\n  val ssc = new StreamingContext(sparkConf, Seconds(5))\n  //从socket接口数据\n  val lineDStream = ssc.socketTextStream(\"datanode1\", 9999)\n\n  val wordDStream = lineDStream.flatMap(_.split(\" \"))\n  val word2CountDStream = wordDStream.map((_, 1))\n  val result = word2CountDStream.reduceByKey(_ + _)\n\n\n  result.print()\n\n  //启动\n  ssc.start()\n  ssc.awaitTermination()\n\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/sparkStreamingWordCount.gif)\n\n### 模式\n\nSpark Streaming使用“微批次”的架构，把流式计算看作一系列连续的小规模批处理来对待。Spark Streaming从各种输入源中读取数据，并把数据分组为小的批次。新的批次按均匀的时间间隔创建出来。在每个时间区间开始的时候，一个新的批次就创建出来，在该区间内收到的数据都会被添加到这个批次中。在时间区间结束时，批次停止增长。时间区间的大小是由批次间隔这个参数决定的。批次间隔一般设在500毫秒到几秒之间，由应用开发者配置。每个输入批次都形成一个RDD，以 Spark 作业的方式处理并生成其他的 RDD。 处理的结果可以以批处理的方式传给外部系统。因此严格意义上来说Spark Streaming并不是一个真正的实时计算框架。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603160340.png)\n\nSpark Streaming的编程抽象是离散化流，也就是DStream。它是一个 RDD 序列，每个RDD代表数据流中一个时间片内的数据。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603161713.png)\n\nSpark Streaming 在 Spark 的驱动器程序 工作节点的结构的执行过程如下图所示。Spark Streaming 为每个输入源启动对应的接收器。接收器以任务的形式运行在应用的执行器进程中，从输入源收集数据并保存为 RDD。它们收集到输入数据后会把数据复制到另一个执行器进程来保障容错性(默认行为)。数据保存在执行器进程的内存中，和缓存 RDD 的方式一样。驱动器程序中的 StreamingContext 会周期性地运行 Spark 作业来处理这些数据，把数据与之前时间区间中的 RDD 进行整合。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603164340.png)\n\n### 注意\n\n1. StreamingContext一旦启动，对DStreams的操作就不能修改了。\n\n2. 在同一时间一个JVM中只有一个StreamingContext可以启动\n\n3. stop() 方法将同时停止SparkContext，可以传入参数stopSparkContext用于只停止StreamingContext\n\n4. 在Spark1.4版本后，如何优雅的停止SparkStreaming而不丢失数据，通过设置sparkConf.set(\"spark.streaming.stopGracefullyOnShutdown\",\"true\") 即可。在StreamingContext的start方法中已经注册了Hook方法。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603162825.png)\n\n```scala def start(): Unit = synchronized {\n def start(): Unit = synchronized {\n    state match {\n      case INITIALIZED =>\n        startSite.set(DStream.getCreationSite())\n        StreamingContext.ACTIVATION_LOCK.synchronized {\n          StreamingContext.assertNoOtherContextIsActive()\n          try {\n            validate()\n\n            // Start the streaming scheduler in a new thread, so that thread local properties\n            // like call sites and job groups can be reset without affecting those of the\n            // current thread.\n            ThreadUtils.runInNewThread(\"streaming-start\") {\n              sparkContext.setCallSite(startSite.get)\n              sparkContext.clearJobGroup()\n              sparkContext.setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, \"false\")\n              savedProperties.set(SerializationUtils.clone(sparkContext.localProperties.get()))\n              scheduler.start()\n            }\n            state = StreamingContextState.ACTIVE\n          } catch {\n            case NonFatal(e) =>\n              logError(\"Error starting the context, marking it as stopped\", e)\n              scheduler.stop(false)\n              state = StreamingContextState.STOPPED\n              throw e\n          }\n          StreamingContext.setActiveContext(this)\n        }\n        logDebug(\"Adding shutdown hook\") // force eager creation of logger\n        shutdownHookRef = ShutdownHookManager.addShutdownHook(\n          StreamingContext.SHUTDOWN_HOOK_PRIORITY)(stopOnShutdown)\n        // Registering Streaming Metrics at the start of the StreamingContext\n        assert(env.metricsSystem != null)\n        env.metricsSystem.registerSource(streamingSource)\n        uiTab.foreach(_.attach())\n        logInfo(\"StreamingContext started\")\n      case ACTIVE =>\n        logWarning(\"StreamingContext has already been started\")\n      case STOPPED =>\n        throw new IllegalStateException(\"StreamingContext has already been stopped\")\n    }\n  }\n```\n\n## DStreams\n\nDiscretized Stream是Spark Streaming的基础抽象，代表持续性的数据流和经过各种Spark原语操作后的结果数据流。在内部实现上，DStream是一系列连续的RDD来表示。每个RDD含有一段时间间隔内的数据。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603163741.png)\n\n对数据的操作也是按照RDD为单位来进行的\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603163808.png)\n\n计算过程由Spark engine来完成\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SparkStreaming/20190603163827.png)\n\n\n\n","tags":["Spark","SparkStreaming"],"categories":["大数据"]},{"title":"Spark之SparkSQL数据源","url":"/2019/06/01/Spark之SparkSQL数据源/","content":"\n {{ \"SparkSQL数据源:parquet Json Mysql  Hive\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## SparkSQL数据源\n\n### 手动指定选项\n\nSpark SQL的DataFrame接口支持多种数据源的操作。一个DataFrame可以进行RDD的方式的操作，也可以被注册为临时表。把DataFrame注册为临时表之后，就可以对该DataFrame执行SQL查询。\n\nSpark SQL的默认数据源为Parquet格式。数据源为Parquet文件时，Spark SQL可以方便的执行所有的操作。修改配置项spark.sql.sources.default，可修改默认数据源格式。\n\n```scala\nval df = spark.read.load(\"/input/sparksql/users.parquet\") \ndf.show()\ndf.select(\"name\",\"favorite_color\").write.save(\"/output/sparksql_out/namesAndFavColors.parquet\")\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601204039.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601204231.png)\n\n当数据源格式不是parquet格式文件时，需要手动指定数据源的格式。数据源格式需要指定全名（例如：`org.apache.spark.sql.parquet`），如果数据源格式为内置格式，则只需要指定简称定`json, parquet, jdbc, orc, libsvm, csv,` text来指定数据的格式。可以通过SparkSession提供的read.load方法用于通用加载数据，使用write和save保存数据。\n\n```scala\nval peopleDF = spark.read.format(\"json\").load(\"/input/sparksql/people.json\")\npeopleDF.show()\npeopleDF.write.format(\"parquet\").save(\"/output/sparksql_out/namesAndAges.parquet\")\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601204334.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601204445.png)\n\n同时也可以直接运行SQL在文件上：\n\n```scala\nval sqlDF = spark.sql(\"SELECT * FROM parquet.`/output/sparksql_out/namesAndAges.parquet`\")\nsqlDF.show()\nval sqlDF = spark.sql(\"SELECT * FROM parquet.`/output/sparksql_out/namesAndAges.parquet`\")\nsqlDF.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601204928.png)\n\n### 文件保存选项\n\nSaveMode定义了对数据的处理模式，这些保存模式不使用任何锁定，f非原子操作。当使用Overwrite方式执行时，在输出新数据之前原数据就已经被删除。\n\n| Scala/Java                      | Any Language     | Meaning              |\n| ------------------------------- | ---------------- | -------------------- |\n| SaveMode.ErrorIfExists(default) | \"error\"(default) | 如果文件存在，则报错 |\n| SaveMode.Append                 | \"append\"         | 追加                 |\n| SaveMode.Overwrite              | \"overwrite\"      | 覆写                 |\n| SaveMode.Ignore                 | \"ignore\"         | 数据存在，则忽略     |\n\n### Parquet格式\n\nParquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。\n\nParquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n\n通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。\n\n[![k9A97d.png](https://s2.ax1x.com/2019/01/18/k9A97d.png)](https://s2.ax1x.com/2019/01/18/k9A97d.png)\n\nParquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。\n\n####  Parquet读写\n\nParquet格式经常在Hadoop生态圈中被使用，它也支持Spark SQL的全部数据类型。Spark SQL 提供了直接读取和存储 Parquet 格式文件的方法。 \n\n```scala\nval peopleDF = spark.read.json(\"/input/sparksql/people.json\")\npeopleDF.collect\npeopleDF.write.parquet(\"/output/sparksql_out/people.parquet\")\nval parquetFileDF = spark.read.parquet(\"/output/sparksql_out/people.parquet\")\nparquetFileDF.createOrReplaceTempView(\"parquetFile\")\nval namesDF = spark.sql(\"SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19\")\nnamesDF.show()\nnamesDF.map(attributes => \"Name: \" + attributes(0)).show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602131629.png)\n\n#### 解析分区信息\n\n对表进行分区是对数据进行优化的方式之一。在分区的表内，数据通过分区列将数据存储在不同的目录下。Parquet数据源现在能够自动发现并解析分区信息。例如，对人口数据进行分区存储，分区列为gender和country，使用下面的目录结构：\n\n```scala\npath\n└── to\n    └── table\n        ├── gender=male\n        │   ├── ...\n        │   │\n        │   ├── country=US\n        │   │   └── data.parquet\n        │   ├── country=CN\n        │   │   └── data.parquet\n        │   └── ...\n        └── gender=female\n            ├── ...\n            │\n            ├── country=US\n            │   └── data.parquet\n            ├── country=CN\n            │   └── data.parquet\n            └── ...\n```\n\n通过传递path/to/table给 SQLContext.read.parquet或SQLContext.read.load，Spark SQL将自动解析分区信息。返回的DataFrame的Schema如下：\n\n```sca\nroot\n|-- name: string (nullable = true)\n|-- age: long (nullable = true)\n|-- gender: string (nullable = true)\n|-- country: string (nullable = true)\n```\n\n数据的分区列的数据类型是自动解析的。当前，支持数值类型和字符串类型。自动解析分区类型的参数为：`spark.sql.sources.partitionColumnTypeInference.enabled`，默认值为true。如果想关闭该功能，直接将该参数设置为disabled。此时，分区列数据格式将被默认设置为string类型，不再进行类型解析。\n\n#### Schema合并\n\nParquet也支持Schema evolution（Schema演变）。用户先定义一个简单的Schema，然后逐渐的向Schema中增加列描述。通过这种方式，用户可以获取多个有不同Schema但相互兼容的Parquet文件。现在Parquet数据源能自动检测这种情况，并合并这些文件的schemas。\n\n因为Schema合并是一个高消耗的操作，在大多数情况下并不需要，所以Spark SQL从1.5.0开始默认关闭了该功能。可以通过下面两种方式开启该功能：\n\n当数据源为Parquet文件时，将数据源选项mergeSchema设置为true\n\n设置全局SQL选项spark.sql.parquet.mergeSchema为true\n\n```scala\nval df1 = sc.makeRDD(1 to 5).map(i => (i, i * 2)).toDF(\"single\", \"double\")\ndf1.write.parquet(\"/data/test_table/key=1\")\nval df2 = sc.makeRDD(6 to 10).map(i => (i, i * 3)).toDF(\"single\", \"triple\")\ndf2.write.parquet(\"/data/test_table/key=2\")\nval df3 = spark.read.option(\"mergeSchema\", \"true\").parquet(\"/data/test_table\")\ndf3.printSchema()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602132456.png).\n\n### Hive数据库\n\nApache Hive是Hadoop上的SQL引擎，Spark SQL编译时可以包含Hive支持，也可以不包含。包含Hive支持的Spark SQL可以支持Hive表访问、UDF(用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的 一点是，如果要在Spark SQL中包含Hive的库，并不需要事先安装Hive。一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。 \n\n若要把Spark SQL连接到一个部署好的Hive上，你必须把hive-site.xml复制到 Spark的配置文件目录中($SPARK_HOME/conf)。即使没有部署好Hive，Spark SQL也可以运行。 需要注意的是，如果你没有部署好Hive，Spark SQL会在当前的工作目录中创建出自己的Hive 元数据仓库，叫作 metastore_db。此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。\n\n#### 内嵌Hive\n\n内嵌的Hive可以直接使用,为了方便演示我们尽量使用指定master为lcoal\n\n```scala\nspark.sql(\"show tables\").show()\nspark.sql(\"CREATE TABLE IF NOT EXISTS test (key INT, value STRING)\")\nspark.sql(\"LOAD DATA LOCAL INPATH '/home/hadoop/data/kv1.txt' INTO TABLE test\")\nspark.sql(\"SELECT * FROM test\").show()\nspark.sql(\"SELECT COUNT(*) FROM test\").show()\nval sqlDF = sql(\"SELECT key, value FROM test WHERE key < 10 ORDER BY key\")\n\nimport org.apache.spark.sql._\n\nval stringsDS = sqlDF.map {\ncase Row(key: Int, value: String) => s\"Key: $key, Value: $value\"\n}\nstringsDS.show()\n\ncase class Record(key: Int, value: String)\nval recordsDF = spark.createDataFrame((1 to 100).map(i => Record(i, s\"val_$i\")))\nrecordsDF.createOrReplaceTempView(\"records\")\nspark.sql(\"SELECT * FROM records r JOIN test s ON r.key = s.key\").show()\n```\n\n<font color='red'> 注意:</font>如果使用的是内部的Hive，在Spark2.0之后，spark.sql.warehouse.dir用于指定数据仓库的地址，如果使用HDFS作为路径，那么需要将core-site.xml和hdfs-site.xml 加入到`Spark conf`目录，否则只会创建master节点上的warehouse目录，查询时会出现文件找不到的问题，这是需要向使用HDFS，则需要将metastore删除，重启集群。\n\n#### 外部Hive\n\n1)  将Hive中的hive-site.xml拷贝或者软连接到Spark安装目录下的conf目录下。\n\n```scala\ncp /opt/module/hadoop/etc/hadoop/core-site.xml /opt/module/spark/conf/\ncp /opt/module/hadoop/etc/hadoop/hdfs-site.xml /opt/module/spark/conf/\ncd /opt/module/spark/conf\nln -s /opt/module/hive/conf/hive-site.xml\n```\n\n2) 打开spark shell，注意带上访问Hive元数据库的JDBC客户端\n\n```scala\nspark-shell --master spark://datanode1:7077 --jars mysql-connector-java-5.1.27-bin.jar\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602152130.png)\n\n配置比较简单。\n\n#### Windos开发\n\n 在本地windos上需要将`core-site.xml`, `hive-site.xml`,`hdfs-site.xml的`配置拷贝到resources中去\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602155649.png)\n\n同时pom文件配置如下\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>spark</artifactId>\n        <groupId>com.hph</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>sparksql</artifactId>\n    <packaging>pom</packaging>\n    <modules>\n        <module>sparksql-helloword</module>\n    </modules>\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-sql_2.11</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-hive_2.11</artifactId>\n            <version>2.1.0</version>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.27</version>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-hive_2.11</artifactId>\n            <version>${spark.version}</version>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\nobject LocalHive {\n\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder()\n      .enableHiveSupport()\n      .appName(\"Spark Hive\")\n      .master(\"local[2]\")\n      .getOrCreate()\n\n    spark.sql(\"show tables\").limit(5).show()\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602160113.png)\n\n### JSON数据集\n\nSpark SQL 能够自动推测 JSON数据集的结构，并将它加载为一个Dataset[Row]. 可以通过SparkSession.read.json()去加载一个 Dataset[String]或者一个JSON 文件.注意，这个JSON文件不是一个传统的JSON文件，每一行都得是一个JSON串。\n\n```json\n{\"name\":\"Michael\"}\n{\"name\":\"Andy\", \"age\":30}\n{\"name\":\"Justin\", \"age\":19}\n```\n\n```scala\n//读取数据\nval peopleDF  = spark.read.json(\"/input/sparksql/people.json\")\npeopleDF.printSchema()\n//创建临时表\npeopleDF.createOrReplaceTempView(\"people\")\n//查询\nval teenagerNamesDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\")\nteenagerNamesDF.show()\n//隐式转换\nimport spark.implicits._\n//创建Dataset\nval otherPeopleDataset = spark.createDataset(\"\"\"{\"name\":\"Yin\",\"address\":{\"city\":\"Columbus\",\"state\":\"Ohio\"}}\"\"\" :: Nil)\n//官方网站案例 可以直接将读取otherPeopleDataset,然而spark2.1需要转化一下成为javaRDD\nval otherPeopleRdd = otherPeopleDataset.toJavaRDD\nval otherPeople = spark.read.json(otherPeopleRdd)\notherPeople.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602193248.png)\n\n### JDBC\n\nSpark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。\n\n```scala\nimport java.util.Properties\n\nimport org.apache.spark.sql.SparkSession\n\nobject Test {\n  def main(args: Array[String]): Unit = {\n\n    //创建SparkConf()并设置App名称\n    val spark = SparkSession.builder()\n      .appName(\"Spark SQL Strong Type UDF example\")\n      .master(\"local[*]\")\n      .getOrCreate()\n\n    //配置JDBC\n    //配置JDBC\n    val jdbcDF = spark.read.format(\"jdbc\")\n      .option(\"url\", \"jdbc:mysql://datanode1:3306/rdd\")\n      .option(\"dbtable\", \" rddtable\")\n      .option(\"user\", \"root\")\n      .option(\"password\", \"123456\")\n      .load()\n\n    val connectionProperties = new Properties()\n    connectionProperties.put(\"user\", \"root\")\n    connectionProperties.put(\"password\", \"123456\")\n    val jdbcDF2 = spark.read.jdbc(\"jdbc:mysql://datanode1:3306/rdd\", \"rddtable\", connectionProperties)\n\n    // 写入数据库\n    jdbcDF.write\n      .format(\"jdbc\")\n      .option(\"url\", \"jdbc:mysql://datanode1:3306/rdd\")\n      .option(\"dbtable\", \"Spark_2_Mysql\")\n      .option(\"user\", \"root\")\n      .option(\"password\", \"123456\")\n      .save()\n\n\n    jdbcDF2.write.jdbc(\"jdbc:mysql://datanode1:3306/rdd\", \"Spark_2_Mysql_1\", connectionProperties)\n\n    // 创建表时指定数据类别\n    jdbcDF.write.\n      option(\"createTableColumnTypes\", \"name CHAR(64), comments VARCHAR(1024)\")\n      .jdbc(\"jdbc:mysql://datanode1:3306/rdd\", \"SparkRDD2Mysql_Type\", connectionProperties)\n  }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602194541.png)\n\n原始数据\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602200638.png)\n\n### JDBC/ODBC服务器\n\nSpark SQL也提供JDBC连接支持，这对于让商业智能(BI)工具连接到Spark集群上以 及在多用户间共享一个集群的场景都非常有用。JDBC 服务器作为一个独立的 Spark 驱动 器程序运行，可以在多用户之间共享。任意一个客户端都可以在内存中缓存数据表，对表 进行查询。集群的资源以及缓存数据都在所有用户之间共享。 \n\nSpark SQL的JDBC服务器与Hive中的HiveServer2相一致。由于使用了Thrift通信协议，它也被称为“Thrift server”。 \n\n服务器可以通过 Spark 目录中的 `sbin/start-thriftserver.sh` 启动。这个 脚本接受的参数选项大多与 spark-submit 相同。默认情况下，服务器会在 localhost:10000 上进行监听，我们可以通过环境变量(HIVE_SERVER2_THRIFT_PORT 和 HIVE_SERVER2_THRIFT_BIND_HOST)修改这些设置，也可以通过 Hive配置选项(hive. server2.thrift.port 和 hive.server2.thrift.bind.host)来修改。你也可以通过命令行参 数--hiveconf property=value来设置Hive选项。\n\n```shell\n./sbin/start-thriftserver.sh \\\n--hiveconf hive.server2.thrift.port=<listening-port> \\\n--hiveconf hive.server2.thrift.bind.host=<listening-host> \\\n--master <master-uri>\n...\n./bin/beeline\n\n```\n\n\n\n```shell\nsbin/start-thriftserver.sh\n./bin/beeline\n!connect jdbc:hive2://datanode1:10000\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602201059.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602201658.png)\n\n### Spark SQL CLI\n\nSpark SQL CLI可以很方便的在本地运行Hive元数据服务以及从命令行执行查询任务。需要注意的是，Spark SQL CLI不能与Thrift JDBC服务交互。\n 在Spark目录下执行如下命令启动Spark SQL CLI：``spark-sql `` 配置Hive需要替换 conf/ 下的 hive-site.xml 。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190602202234.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","SparKSQL"],"categories":["大数据"]},{"title":"Spark之SparkSQL实战","url":"/2019/05/30/Spark之SparkSQL实战/","content":"\n {{ \"DataFrames 基本操作和 DSL SQL风格 UDF函数 以及数据源\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## SparkSQL查询\n\nJson数据准备\n\n```json\n{\"name\":\"Michael\"}\n{\"name\":\"Andy\", \"age\":30}\n{\"name\":\"Justin\", \"age\":19}\n```\n\n```scala\nval df =spark.read.json(\"/input/sparksql/json/people.json\")\ndf.show()\ndf.filter($\"age\">21).show();\ndf.createOrReplaceTempView(\"person\")\nspark.sql(\"SELECT * FROM person\").show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530221923.png)\n\n## IDEA创建SparkSQL程序\n\n```xml\n<dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.11</artifactId>\n    <version>2.1.1</version>\n    <scope>provided</scope>\n</dependency>\n```\n\n```scala\npackage com.hph.sql\n\nimport org.apache.spark.sql.SparkSession\n\n\nobject HelloWorld {\n\n  def main(args: Array[String]) {\n    //创建SparkConf()并设置App名称\n\n    val spark = SparkSession\n      .builder()\n      .appName(\"Spark SQL basic example\")\n      .config(\"spark.some.config.option\", \"some-value\")\n      .master(\"local[*]\")\n      .getOrCreate()\n\n    // For implicit conversions like converting RDDs to DataFrames\n    import spark.implicits._\n\n    val df = spark.read.json(\"F:\\\\spark\\\\examples\\\\src\\\\main\\\\resources\\\\people.json\")\n\n    // Displays the content of the DataFrame to stdout\n    df.show()\n\n    df.filter($\"age\" > 21).show()\n\n    df.createOrReplaceTempView(\"persons\")\n\n    spark.sql(\"SELECT * FROM persons where age > 21\").show()\n\n    spark.stop()\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190531121301.png)\n\n## SparkSession\n\n老的版本中，SparkSQL提供两种SQL查询起始点，一个叫SQLContext，用于Spark自己提供的SQL查询，一个叫HiveContext，用于连接Hive的查询，SparkSession是Spark最新的SQL查询起始点，实质上是SQLContext和HiveContext的组合，所以在SQLContext和HiveContext上可用的API在SparkSession上同样是可以使用的。SparkSession内部封装了sparkContext，所以计算实际上是由sparkContext完成的。\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\n\nobject HelloWorld {\n\n  def main(args: Array[String]) {\n    //创建SparkConf()并设置App名称\n\n    val spark = SparkSession\n      .builder()\n      .appName(\"Spark SQL basic example\")\n      .config(\"spark.some.config.option\", \"some-value\")\n      .master(\"local[*]\")\n      .getOrCreate()\n\n    // 用于将DataFrames隐式转换成RDD，使df能够使用RDD中的方法。\n    import spark.implicits._\n\n    val df = spark.read.json(\"hdfs://datanode1:9000/input/sparksql/people.json\")\n\n    // Displays the content of the DataFrame to stdout\n    df.show()\n\n    df.filter($\"age\" > 21).show()\n\n    df.createOrReplaceTempView(\"persons\")\n\n    spark.sql(\"SELECT * FROM persons where age > 21\").show()\n\n    spark.stop()\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601104448.png)\n\n\n\n## 创建DataFrames\n\nSparkSession是创建DataFrames和执行SQL的入口，创建DataFrames有三种方式，一种是可以从一个存在的RDD进行转换，还可以从Hive Table进行查询返回，或者通过Spark的数据源进行创建。\n\n```scala\nval df = spark.read.json(\"/input/sparksql/people.json\")\t\ndf.show()\nval peopleRdd = sc.textFile(\"/input/sparksql/people.txt\")\nval peopleDF = peopleRdd.map(_.split(\",\")).map(paras => (paras(0),paras(1).trim().toInt)).toDF(\"name\",\"age\")\npeopleDF.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601123919.png)\n\n### 常用操作\n\n###  DSL风格语法\n\n```scala\ndf.printSchema() \ndf.select(\"name\").show()\ndf.select($\"name\", $\"age\" + 1).show()  \ndf.filter($\"age\" > 21).show()\ndf.groupBy(\"age\").count().show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601124441.png)\n\n###  SQL风格语法\n\n```scala\ndf.createOrReplaceTempView(\"people\")\nval sqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()\ndf.createGlobalTempView(\"people\")\nspark.sql(\"SELECT * FROM global_temp.people\").show()\nspark.newSession().sql(\"SELECT * FROM global_temp.people\").show()\n```\n\n临时表是Session范围内的，Session退出后，表就失效了。如果想应用范围内有效，可以使用全局表。注意使用全局表时需要全路径访问，如：global_temp.people\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601124700.png)\n\n## 创建DataSet\n\n注意: Case classes in Scala 2.10只支持 22 字段. 你可以使用自定义的 classes 来实现对字段的映射\n`case class Person(name: String, age: Long)`\n\n```scala\ncase class Person(name: String, age: Long)\nval caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\ncaseClassDS.show()\nimport spark.implicits._ \nval primitiveDS = Seq(1, 2, 3).toDS() \nprimitiveDS.map(_ + 1).collect()\n\nval path = \"/input/sparksql/people.json\"\nval peopleDS = spark.read.json(path).as[Person]\npeopleDS.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601151413.png)\n\n## 相互转化\n\n具体的转换可以参考: [三者共性](https://www.hphblog.cn/2019/05/30/Spark之SparkSQL/#三者共性)\n\n## UDF函数\n\n```scala\nimport org.apache.spark.sql.SparkSession\n\nobject UDF {\n  def main(args: Array[String]): Unit = {\n    //创建SparkConf()并设置App名称\n    val spark = SparkSession.builder()\n      .appName(\"Spark SQL UDF example\")\n      .master(\"local[*]\")\n      .getOrCreate()\n    //读取数据\n    val df = spark.read.json(\"hdfs://datanode1:9000/input/sparksql/people.json\")\n    df.show()\n\n    //注册UDF函数\n    spark.udf.register(\"AddOne\", (age: Int) => age + 1)\n\n    df.createOrReplaceTempView(\"people\")\n    //SQL语句\n    spark.sql(\"Select name,AddOne(age), age from people\").show()\n\n    spark.stop()\n\n  }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601162725.png)\n\n## 自定义聚合函数\n\n强类型的Dataset和弱类型的DataFrame都提供了相关的聚合函数， 如 count()，countDistinct()，avg()，max()，min()。除此之外，用户可以设定自己的自定义聚合函数。\n\n弱类型用户自定义聚合函数：通过继承`UserDefinedAggregateFunction`来实现用户自定义聚\n\n```scala\npackage com.hph.sql\n\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}\nimport org.apache.spark.sql.types._\n\n//自定义UDAF函数需要继承UserDefinedAggregateFunction\nclass AverageSal extends UserDefinedAggregateFunction {\n  //输入数据\n  override def inputSchema: StructType = StructType(StructField(\"salary\", LongType) :: Nil)\n\n  //定义每一个分区中的共享变量\n  override def bufferSchema: StructType = StructType(StructField(\"sum\", LongType) :: StructField(\"count\", IntegerType) :: Nil)\n\n  //表示UDAF函数的输出类型\n  override def dataType: DataType = DoubleType\n\n  //如果有相同的输入是否会存在相同的输出,如果是则为True\n  override def deterministic: Boolean = true\n\n  //初始化 每一个分区中的共享变量\n  override def initialize(buffer: MutableAggregationBuffer): Unit = {\n    buffer(0) = 0L; //  sum\n    buffer(1) = 0 //   sum\n  }\n\n  //每一个分区中的每一条数据聚合的时候需要调用该方法\n  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = {\n    //获取这一行中的工资,然后将工资加入到sum里\n    buffer(0) = buffer.getLong(0) + input.getLong(0)\n    //需要将工资的个数加1\n    buffer(1) = buffer.getInt(1) + 1\n  }\n\n  //将每一个没去的输出合并形成最后的数据\n  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {\n    //合并总的工资\n    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)\n    //合并总的工资个数\n    buffer1(1) = buffer1.getInt(1) + buffer2.getInt(1)\n  }\n\n  //给出计算结果\n  override def evaluate(buffer: Row): Any = {\n    //取出总的工资  / 总的工资个数\n    buffer.getLong(0).toDouble / buffer.getInt(1)\n  }\n}\n\nobject AverageSal {\n  def main(args: Array[String]): Unit = {\n\n    //创建SparkConf()并设置App名称\n    val spark = SparkSession.builder()\n      .appName(\"Spark SQL UDF example\")\n      .master(\"local[*]\")\n      .getOrCreate()\n\n    val employee = spark.read.json(\"hdfs://datanode1:9000/input/sparksql/employees.json\")\n    employee.createOrReplaceTempView(\"employee\")\n      \n    spark.udf.register(\"average\", new AverageSal)\n    spark.sql(\"select average(salary)  from employee\").show()\n    spark.stop()\n    \n  }\n}\n\n```\n\n### 强类型函数\n\n```scala\npackage com.hph.sql\n\nimport org.apache.spark.sql.{Encoder, Encoders, SparkSession}\nimport org.apache.spark.sql.expressions.Aggregator\n\n\ncase class Employee(name: String, salary: Long)\n\ncase class Aver(var sum: Long, var count: Int)\n\nclass Average extends Aggregator[Employee, Aver, Double] {\n  //初始化方法:初始化每一个分区的共享变量\n  override def zero: Aver = Aver(0L, 0)\n\n  //每一个分区的每一条数据聚合的时候需要回调该方法\n  override def reduce(b: Aver, a: Employee): Aver = {\n    b.sum = b.sum + a.salary\n    b.count = b.count + 1\n    b\n  }\n\n  //将每一个分区的输出 合并 形成最后的数据\n  override def merge(b1: Aver, b2: Aver): Aver = {\n    b1.sum = b1.sum + b2.sum\n    b1.count = b1.count + b2.count\n    b1\n  }\n\n  //输出计算结果\n  override def finish(reduction: Aver): Double = {\n    reduction.sum.toDouble / reduction.count\n  }\n\n  override def bufferEncoder: Encoder[Aver] = Encoders.product\n\n  override def outputEncoder: Encoder[Double] = Encoders.scalaDouble\n}\n\nobject Average {\n  def main(args: Array[String]): Unit = {\n    //创建SparkConf()并设置App名称\n    val spark = SparkSession.builder()\n      .appName(\"Spark SQL Strong Type UDF example\")\n      .master(\"local[*]\")\n      .getOrCreate()\n    import spark.implicits._\n    val employee = spark.read.json(\"hdfs://datanode1:9000/input/sparksql/employees.json\").as[Employee]\n    val aver = new Average().toColumn.name(\"average\")\n    employee.select(aver).show()\n\n    spark.close()\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601200128.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","SparKSQL"],"categories":["大数据"]},{"title":"Spark之SparkSQL理论篇","url":"/2019/05/30/Spark之SparkSQL/","content":"\n {{ \"Spark SQL 理论学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n \n## 简介\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191139.png)\n\nSpark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。\n\n## 特点\n\n1）易整合\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191724.png)\n\n2) 统一的数据访问方式\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191703.png)\n\n3）兼容Hive\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191746.png)\n\n4）标准的数据连接\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191824.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530191859.png)\n\n SparkSQL可以看做是一个转换层，向下对接各种不同的结构化数据源，向上提供不同的数据访问方式。\n\n 在SparkSQL中Spark为我们提供了两个新的抽象，分别是DataFrame和DataSet\n\n`RDD (Spark1.0)` —>` Dataframe(Spark1.3)` —>` Dataset(Spark1.6)`\n\n同样的数据都给到这三个数据结构有相同的结果。不同是的他们的执行效率和执行方式。在后期的Spark版本中，DataSet会逐步取代RDD和DataFrame成为唯一的API接口。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530192332.png)\n\n## RDD\n\nRDD是一个懒执行的不可变的可以支持Lambda表达式的并行数据集合。简单，API设计友好。但它是一个JVM驻内存对象，受GC的限制和数据增加时Java序列化成本的升高。\n\n```scala\n val rdd = sc.textFile(\"file:///opt/module/spark/README.md\")\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530193912.png)\n\n```scala\nrdd.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530194214.png)\n\n## Dataframe\n\n在Spark中DataFrame与RDD类似，也是一个分布式数据容器。但是DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息（schema），与Hive类似，DataFrame也支持嵌套数据类型``（struct、array和map）``。DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。与R和Pandas的DataFrame类似。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530194628.png)\n\nRDD[Person]虽然以Person为类型参数，但Spark框架本身不了解Person类的内部结构。右侧的DataFrame却提供了详细的结构信息，使得Spark SQL可以清楚地知道该数据集中包含哪些列，每列的名称和类型各是什么。DataFrame多了数据的结构信息，即schema。RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化，比如filter下推、裁剪等。\n\n性能上比RDD要高，主要有两方面原因： \n\nDataFrame是为数据提供了Schema的视图。可以把它当做数据库中的一张表来对待是同时DataFrame也是懒执行的。\n\n定制化内存管理数据以二进制的方式存在于``非堆内存``，节省了大量空间之外，还摆脱了GC的限制。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530194934.png)\n\n优化的执行计划查询计划通过Spark catalyst optimiser进行优化. \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530195026.png)\n\n举个例子\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530195702.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530195713.png)\n\n上图展示的人口数据分析的示例，构造了两个DataFrame，join之后做了filter操作。直接地执行这个执行计划，执行效率很差。join是代价较大的操作，如果能将filter下推到 join下方，先对DataFrame进行过滤，再join过滤后的较小的结果集，可以缩短执行时间。Spark SQL的查询优化器是这样做的：逻辑查询计划优化就是一个利用基于关系代数的等价变换，将高成本的操作替换为低成本操作的过程。 \n\n得到的优化执行计划在转换成物理执行计划的过程中，还可以根据具体的数据源的特性将过滤条件下推至数据源内。最右侧的物理执行计划中Filter之所以消失不见，就是因为溶入了用于执行最终的读取操作的表扫描节点内。 \n\n对于普通开发者而言：即便是经验并不丰富的程序员写出的次优的查询，也可以被尽量转换为高效的形式予以执行。但是由于在编译期缺少类型安全检查，导致运行时容易出错。\n\n##   Dataset\n\n1）是Dataframe API的一个扩展，是Spark最新的数据抽象\n\n2）用户友好的API风格，既具有类型安全检查也具有Dataframe的查询优化特性。\n\n3）Dataset支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。\n\n4）样例类被用来在Dataset中定义数据的结构信息，样例类中每个属性的名称直接映射到DataSet中的字段名称。\n\n5） Dataframe是Dataset的特列，DataFrame=Dataset[Row] ，所以可以通过as方法将Dataframe转换为Dataset。Row是一个类型，跟Car、Person这些的类型一样，所有的表结构信息我都用Row来表示。\n\n6）DataSet是强类型的。比如可以有Dataset[Car]，Dataset[Person].\n\n7）DataFrame只是知道字段，但是不知道字段的类型，所以在执行这些操作的时候是没办法在编译的时候检查是否类型失败的，比如你可以对一个String进行减法操作，在执行的时候才报错，而DataSet不仅仅知道字段，而且知道字段类型，所以有更严格的错误检查。就跟JSON对象和类对象之间的类比。\n\n数据准备\n\n```json\n{\"name\":\"Hadoop\"}\n{\"name\":\"Spark\", \"Year\":2015}\n{\"name\":\"Flink\", \"Year\":2018}\n```\n\n```scala\ncase class  Bigdata(name:String,Year:Int) \nval ds = spark.sqlContext.read.json(\"file:///opt/module/spark/json/bigdata.json\")\nds.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530203550.png)\n\nRDD让我们能够决定怎么做，而DataFrame和DataSet让我们决定做什么,控制的粒度不一样。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530203640.png)\n\n\n\n## 三者共性\n\n1）都是spark平台下的分布式弹性数据集。\n\n2）三者都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过\n\n3）都有惰性机制，在进行创建、转换，如map方法时，不会立即执行，只有在遇到Action如foreach时，三者才会开始遍历运算，极端情况下，如果代码里面有创建、转换，但是后面没有在Action中使用对应的结果，在执行时会被直接跳过。\n\n```scala\nval rdd=spark.sparkContext.parallelize(Seq((\"a\", 1), (\"b\", 1), (\"a\", 1)))\n// map不运行\n// map不运行\nrdd.map{line=>\n  println(\"运行\")\n  line._1\n}.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530204902.png)+\n\n4)都会根据spark的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出\n\n5)都有partition的概念\n\n6)都有partition的概念\n\n7)对DataFrame和Dataset进行操作许多操作都需要这个包进行支持 `import spark.implicits._`\n\n8)DataFrame和Dataset均可使用模式匹配获取各个字段的值和类型\n\n## 三者区别\n\n1) RDD一般和spark mlib同时使用，但是不支持sparksql操作\n\n2) DataFrame:与RDD和Dataset不同，DataFrame每一行的类型固定为Row，只有通过解析才能获取各个字段的值，每一列的值没法直接访问\n\n```scala\nval testDF = spark.read.json(\"file:///opt/module/spark/json/bigdata.json\")\ntestDF.foreach{\n  line =>\n    val col1=line.getAs[String](\"name\")\n    val col2=line.getAs[Long](\"Year\")\n}\n```\n\n3) DataFrame与Dataset一般不与spark ml同时使用\n\n4) DataFrame与Dataset均支持sparksql的操作，比如select，groupby之类，还能注册临时表/视窗，进行sql语句操作：\n\n```scala\nval testDF = spark.read.json(\"file:///opt/module/spark/json/bigdata.json\")\ntestDF.createOrReplaceTempView(\"tmp\")\nspark.sql(\"select * from tmp\").show(100,false)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530211720.png)\n\n5) DataFrame与Dataset支持一些特别方便的保存方式，比如保存成csv，可以带上表头，这样每一列的字段名一目了然\n\n```scala\n\"\",\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"\n\"1\",5.1,3.5,1.4,0.2,\"setosa\"\n\"2\",4.9,3,1.4,0.2,\"setosa\"\n\"3\",4.7,3.2,1.3,0.2,\"setosa\"\n\"4\",4.6,3.1,1.5,0.2,\"setosa\"\n\"5\",5,3.6,1.4,0.2,\"setosa\"\n\"6\",5.4,3.9,1.7,0.4,\"setosa\"\n\"7\",4.6,3.4,1.4,0.3,\"setosa\"\n\"8\",5,3.4,1.5,0.2,\"setosa\"\n\"9\",4.4,2.9,1.4,0.2,\"setosa\"\n\"10\",4.9,3.1,1.5,0.1,\"setosa\"\n```\n\n```scala\n//读取\nval options = Map(\"header\" -> \"true\", \"delimiter\" -> \"\\t\", \"path\" -> \"file:///opt/module/spark/csv/iris.csv\")\nval datarDF= spark.read.options(options).format(\"csv\").load()\ndatarDF.show()\n//保存\nval saveoptions = Map(\"header\" -> \"true\", \"delimiter\" -> \"\\t\", \"path\" -> \"hdfs://datanode1:9000/test/saveToCSV\")\ndatarDF.write.format(\"csv\").mode(org.apache.spark.sql.SaveMode.Overwrite).options(saveoptions).save()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530214629.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190530214810.png)\n\n### 三者转换\n\n### RDD => DataFrame\n\n#### 手动确定\n\n```scala\nval peopleRDD = sc.textFile(\"/input/sparksql/people.txt\")\nval name2AgeRDD = peopleRDD.map{x => val para = x.split(\",\");(para(0).trim, para(1).trim.toInt) }\nname2AgeRDD.collect\nimport spark.implicits._\nval df = name2AgeRDD.toDF(\"name\",\"age\")\ndf.show\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190531202646.png)\n\n#### 反射确定\n\n利用case class\n\n```scala\nval peopleRDD = sc.textFile(\"/input/sparksql/people.txt\")\nclass classPeople(name:String,age:Int)\nval df = peopleRDD.map{x => val para = x.split(\",\");People(para(0).trim, para(1).trim.toInt) }.toDS\ndf.show()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601095101.png)\n\n####   编程方式\n\n1)准备Scheam\n\n```scala\nimport org.apache.spark.sql.types._\nval schema = StructType( StructField(\"name\",StringType)::StructField(\"age\",org.apache.spark.sql.types.IntegerType)::Nil)\n```\n\n2)准备Data   【需要Row类型】\n\n```scala\nval peopleRDD = sc.textFile(\"/input/sparksql/people.txt\")\nimport org.apache.spark.sql._\nval data = peopleRDD.map{ x => val para = x.split(\",\");Row(para(0),para(1).trim.toInt)}\n```\n\n3)生成DataFrame\n\n```scala\nval dataFrame = spark.createDataFrame(data, schema)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601101135.png)\n\n### DataFrame => RDD\n\n直接DataFrame.rdd即可\n\n```scala\ndataFrame.rdd\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601101408.png)\n\n### RDD  <==>DataSet\n\n#### RDD -》  DataSet\n\n```scala\nval peopleRDD = sc.textFile(\"/input/sparksql/people.txt\")\ncase class People(name:String, age:Int)  //case class 确定schema\nval ds = peopleRDD.map{x => val para = x.split(\",\");People(para(0), para(1).trim.toInt)}.toDS\nds.show()\n```\n\n![1559355499746](C:\\Users\\Schindler\\AppData\\Roaming\\Typora\\typora-user-images\\1559355499746.png)\n\n#### DataSet -》 RDD\n\n```scala\nval dsRDD = ds.rdd\ndsRDD.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601102125.png)\n\n### DataFrame  <==> DataSet\n\n#### DataSet  => DataFrame\n\n```scala\nval ds2df = ds.totoDF\nds2df.show\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601102502.png)\n\n#### DataFrame  =>DataSet\n\n```scala\ncase class People(name:String, age:Int)\nval df2ds = ds2df.as[People]\ndf2ds.show\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601102723.png)\n\n## 参考资料\n\n关于SparkSQL原理深入的学习可以参考《图解Spark》\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","SparKSQL"],"categories":["大数据"]},{"title":"Spark之RDD实战篇3","url":"/2019/05/29/Spark之RDD实战篇3/","content":"\n {{ \"键值对RDD、数据读取与保存、累加器、广播变量\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n##  键值对RDD\n\nSpark 为包含键值对类型的 RDD 提供了一些专有的操作 在PairRDDFunctions专门进行了定义。这些 RDD 被称为 pair RDD。有很多种方式创建`pair RDD`，在输入输出章节会讲解。一般如果从一个普通的RDD转 为pair RDD时，可以调用map()函数来实现，传递的函数需要返回键值对。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529191639.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529191720.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529191855.png)\n\n果从一个普通的RDD转为`pair RDD`时，可以调用map()函数来实现，传递的函数需要返回键值对。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529192923.png)\n\n### 转化操作列表\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529193023.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529193034.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529193100.png)\n\n### 聚合操作\n\n当数据集以键值对形式组织的时候，聚合具有相同键的元素进行一些统计是很常见的操作。之前讲解过基础RDD上的fold()、combine()、reduce()等行动操作，pair RDD上则 有相应的针对键的转化操作。Spark 有一组类似的操作，可以组合具有相同键的值。这些 操作返回 RDD，因此它们是转化操作而不是行动操作。 \n\nreduceByKey() 与 reduce() 相当类似;它们都接收一个函数，并使用该函数对值进行合并。 reduceByKey() 会为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合 并起来。因为数据集中可能有大量的键，所以 reduceByKey() 没有被实现为向用户程序返回一 个值的行动操作。实际上，它会返回一个由各键和对应键归约出来的结果值组成的新的 RDD。 \n\nfoldByKey() 则与 fold() 相当类似;它们都使用一个与 RDD 和合并函数中的数据类型相 同的零值作为初始值。与 fold() 一样，foldByKey() 操作所使用的合并函数对零值与另一 个元素进行合并，结果仍为该元素。 \n\n```scala\nval rdd =sc.parallelize(Array((\"panda\",0),(\"pink\",3),(\"pirate\",3),(\"panda\",1),(\"pink\",4)),2)\nval result =rdd.mapValues(x => (x, 1)).reduceByKey((x, y) => (x._1 + y._1, x._2 + y._2))\nresult.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529195651.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529195711.png)\n\ncombineByKey() 是常用的基于键进行聚合的函数。大多数基于键聚合的函数都是用它实现的。和 aggregate() 一样，combineByKey() 可以让用户返回与输入数据的类型不同的 返回值。 \n\n由于 combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。 \n\n如果这是一个新的元素，combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 那个键对应的累加器的初始值。需要注意的是，这一过程会在每个分区中第一次出现各个键时发生，而不是在整个 RDD 中第一次出现一个键时发生。 \n\n如果这是一个在处理当前分区之前已经遇到的键，它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并。 \n\n由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更 多的分区都有对应同一个键的累加器，就需要使用用户提供的 mergeCombiners() 方法将各 个分区的结果进行合并。 \n\n```scala\nval rdd =sc.parallelize(Array((\"panda\",0),(\"pink\",3),(\"pirate\",3),(\"panda\",1),(\"pink\",4)),2)\nval result = rdd.combineByKey(\n  (v) => (v, 1),\n  (acc: (Int, Int), v) => (acc._1 + v, acc._2 + 1),\n  (acc1: (Int, Int), acc2: (Int, Int)) => (acc1._1 + acc2._1, acc1._2 + acc2._2)\n)\nresult.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529200208.png)\n\n### 数据分组\n\n如果数据已经以预期的方式提取了键，groupByKey() 就会使用 RDD 中的键来对数据进行 分组。对于一个由类型 K 的键和类型 V 的值组成的 RDD，所得到的结果 RDD 类型会是 [K, Iterable[V]]。 \n\n多个RDD分组，可以使用cogroup函数，cogroup() 的函数对多个共享同 一个键的 RDD 进行分组。对两个键的类型均为 K 而值的类型分别为 V 和 W 的 RDD 进行 cogroup() 时，得到的结果 RDD 类型为 [(K, (Iterable[V], Iterable[W]))]。如果其中的 一个 RDD 对于另一个 RDD 中存在的某个键没有对应的记录，那么对应的迭代器则为空。 cogroup() 提供了为多个 RDD 进行数据分组的方法。 \n\n```scala\nvar rddl = sc.makeRDD(Array((\"A\",0), (\"A\",2), (\"B\",1), (\"B\",2), (\"Cn\",1)))\nrdd1.groupByKey().collect\n//使用reduceByKey操作将RDD[K,V]中每个K对应的V值根据映射函数来运算                 \nvar rdd2 = rdd1.reduceByKey((x,y) => x + y)\n//对rddl使用reduceByKey操作进行重新分区\nvar rdd2 = rdd1.reduceByKey (new org.apache.spark.HashPartitioner(2),(x, y) => x + y)\nrdd2.collect\n\nvar rdd2 = sc.makeRDD(Array((\"A\",\"a\"),(\"C\",\"c\"),(\"D\",\"d\")),2)\nvar rdd3 = sc.makeRDD(Array((\"A\",\"A\"),(\"E\",\"E\")),2)\nvar rdd4 = rddl.cogroup(rdd2,rdd3)\nrdd4.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529203008.png)\n\n### 连接\n\n连接主要用于多个Pair RDD的操作，连接方式多种多样:右外连接、左外连接、交 叉连接以及内连接。 \n\n普通的 join 操作符表示内连接 2。只有在两个 pair RDD 中都存在的键才叫输出。当一个输 入对应的某个键有多个值时，生成的pair RDD会包括来自两个输入RDD的每一组相对应 的记录。 \n\nleftOuterJoin()产生的pair RDD中，源RDD的每一个键都有对应的记录。每个 键相应的值是由一个源 RDD 中的值与一个包含第二个 RDD 的值的 Option(在 Java 中为 Optional)对象组成的二元组。 \n\nrightOuterJoin() 几乎与 leftOuterJoin() 完全一样，只不过预期结果中的键必须出现在第二个 RDD 中，而二元组中的可缺失的部分则来自于源 RDD 而非第二个 RDD。这些连接操作都是继承了cgroup\n\n```scala\nvar rdd1 = sc.makeRDD (Array((\"A\", \"1\"), (\"B\",\"2\") , (\"C\",\"3\")), 2)\nvar rdd2 = sc.makeRDD (Array((\"A\", \"a\"), (\"C\",\"c\") , (\"D\",\"d\")), 2)\n//进行内连接操作\nrdd1.join(rdd2).collect\n//进行左连接操作\nrdd1.leftOuterJoin(rdd2).collect\nrdd1.leftOuterJoin(rdd2).collect                      \n//进行右连接操作\nrdd1.rightOuterJoin(rdd2).collect                                      \n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529204438.png)\n\n### 数据排序\n\nsortByKey() 函数接收一个叫作 ascending 的参数，表示我们是否想要让结果按升序排序(默认值为 true)。 \n\n```scala\n··val rdd = sc.parallelize(Array((3,\"hadoop\"),(6,\"hohblog\"),(2,\"flink\"),(1,\"spark\")))\nrdd.sortByKey(true).collect()\nrdd.sortByKey(false).collect()\n```\n\n![img](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559015691057.png)\n\n### 行动操作\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529204942.png)\n\n### 数据分区\n\nSpark目前支持Hash分区和Range分区，用户也可以自定义分区，Hash分区为当前的默认分区，Spark中分区器直接决定了RDD中分区的个数、RDD中每条数据经过Shuffle过程属于哪个分区和Reduce的个数，注意：\n\n(1)只有Key-Value类型的RDD才有分区的，非Key-Value类型的RDD分区的值是None。\n(2)每个RDD的分区ID范围：0~numPartitions-1，决定这个值是属于那个分区的。\n\n```scala\nval pairs =sc.parallelize(List((1,1),(2,2),(3,3)))\npairs.partitioner\nval partitioned = pairs.partitionBy(new org.apache.spark.HashPartitioner(2))\npartitioned.partitioner\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529210421.png)\n\n#### Hash分区方式\n\n对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID。\n\n```scala\nval nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)\nnopar.partitioner\nnopar.mapPartitionsWithIndex((index,iter)=>{ Iterator(index.toString+\" : \"+iter.mkString(\"|\")) }).collect\nval hashpar = nopar.partitionBy(new org.apache.spark.HashPartitioner(7))\nhashpar.count\nhashpar.partitioner\nhashpar.mapPartitions(iter => Iterator(iter.length)).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529210826.png)\n\n#### Ranger分区方式\n\nHashPartitioner分区弊端：可能导致每个分区中数据量的不均匀，极端情况下会导致某些分区拥有RDD的全部数据。\n\nRangePartitioner分区优势：尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；\n\n但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。\n\nRangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法用到了[水塘抽样算法](<https://www.cnblogs.com/krcys/p/9121487.html>)。\n\n```scala\nval nopar = sc.parallelize(List((1,3),(1,2),(2,4),(2,3),(3,6),(3,8)),8)\nnopar.partitioner\nnopar.mapPartitionsWithIndex((index,iter)=>{ Iterator(index.toString+\" : \"+iter.mkString(\"|\")) }).collect\nval rangepar = nopar.partitionBy(new org.apache.spark.RangePartitioner(2,nopar))\nrangepar.count\nrangepar.partitioner\nrangepar.mapPartitions(iter => Iterator(iter.length)).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529211534.png)\n\n#### 自定义分区\n\n要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法。 \n\nnumPartitions: Int:返回创建出来的分区数。\n\ngetPartition(key: Any): Int:返回给定键的分区编号(0到numPartitions-1)。 \n\nequals():Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个 RDD 的分区方式是否相同。  \n\n假设我们需要将相同后缀的数据写入相同的文件，我们通过将相同后缀的数据分区到相同的分区并保存输出来实现。\n\n```scala\nval data=sc.parallelize(List(\"aa.2\",\"bb.2\",\"cc.3\",\"dd.3\",\"ee.5\").zipWithIndex,2)\ndata.collect\ndata.mapPartitionsWithIndex((index,iter)=>Iterator(index.toString +\" : \"+ iter.mkString(\"|\"))).collect\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529215147.png)\n\n```scala\nimport org.apache.spark.{Partitioner, SparkConf, SparkContext}\nclass CustomerPartitioner(numParts:Int) extends org.apache.spark.Partitioner{\n\n  //覆盖分区数\n  override def numPartitions: Int = numParts\n\n  //覆盖分区号获取函数\n  override def getPartition(key: Any): Int = {\n    val ckey: String = key.toString\n    ckey.substring(ckey.length-1).toInt%numParts\n  }\n}\n\nval result = data.partitionBy(new CustomerPartitioner(4))\nresult.mapPartitionsWithIndex((index,iter)=>Iterator(index.toString +\" : \"+ iter.mkString(\"|\"))).collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529215215.png)\n\n使用自定义的 Partitioner 是很容易的:只要把它传给 partitionBy() 方法即可。Spark 中有许多依赖于数据混洗的方法，比如 join() 和 groupByKey()，它们也可以接收一个可选的 Partitioner 对象来控制输出数据的分区方式。\n\n#### 分区shuffle优化\n\n在分布式程序中， 通信的代价是很大的，因此控制数据分布以获得最少的网络传输可以极大地提升整体性能。 \n\nSpark 中所有的键值对 RDD 都可以进行分区。系统会根据一个针对键的函数对元素进行分组。 主要有哈希分区和范围分区，当然用户也可以自定义分区函数。\n\n通过分区可以有效提升程序性能。如下例子：\n\n它在内存中保存着一张很大的用户信息表—— 也就是一个由 (UserID, UserInfo) 对组成的 RDD，其中 UserInfo 包含一个该用户所订阅 的主题的列表。该应用会周期性地将这张表与一个小文件进行组合，这个小文件中存着过 去五分钟内发生的事件——其实就是一个由 (UserID, LinkInfo) 对组成的表，存放着过去五分钟内某网站各用户的访问情况。例如，我们可能需要对用户访问其未订阅主题的页面 的情况进行统计。 \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529215423.png)\n\n代码可以正确运行，但是不够高效。这是因为在每次调用 processNewLogs() 时都会用到 join() 操作，而我们对数据集是如何分区的却一无所知。默认情况下，连接操作会将两个数据集中的所有键的哈希值都求出来，将该哈希值相同的记录通过网络传到同一台机器上，然后在那台机器上对所有键相同的记录进行连接操作。因为 userData 表比每五分钟出现的访问日志表 events 要大得多，所以要浪费时间做很多额外工作:在每次调用时都对 userData 表进行哈希值计算和跨节点数据混洗，降低了程序的执行效率。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529215459.png)\n\n我们在构建 userData 时调用了 partitionBy()，Spark 就知道了该 RDD 是根据键的哈希值来分区的，这样在调用 join() 时，Spark 就会利用到这一点。具体来说，当调用 userData. join(events) 时，Spark 只会对 events 进行数据混洗操作，将 events 中特定 UserID 的记 录发送到 userData 的对应分区所在的那台机器上。这样，需要通过网络传输的 数据就大大减少了，程序运行速度也可以显著提升了。 \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529215606.png)\n\n#### 基于分区进行操作\n\n基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开 数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的 工作。Spark 提供基于分区的 mapPartition 和 foreachPartition，让你的部分代码只对 RDD 的每个分区运行 一次，这样可以帮助降低这些操作的代价。\n\n#### 从分区中获益的操作\n\n能够从数据分区中获得性能提升的操作有cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等。\n\n### 数据读取与保存\n\n#### 文本文件\n\n当我们将一个文本文件读取为RDD 时，输入的每一行都会成为RDD的一个元素。也可以将多个完整的文本文件一次性读取为一个pair RDD，其中键是文件名，值是文件内容。\n\n如果传递目录，则将目录下的所有文件读取作为RDD。\n\n文件路径支持通配符。通过wholeTextFiles()对于大量的小文件读取效率比较高，大文件效果没有那么高。\n\nSpark通过saveAsTextFile() 进行文本文件的输出，该方法接收一个路径，并将 RDD 中的内容都输入到路径对应的文件中。Spark 将传入的路径作为目录对待，会在那个 目录下输出多个文件。这样，Spark 就可以从多个节点上并行输出了。 \n\n```scala\nval rdd = sc.textFile(\"/input/test.txt\")\nrdd.collect\nval rdd = sc.textFile(\"/input/*\")\nrdd.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529220140.png)\n\n#### JSON文件\n\nJSON文件中每一行就是一个JSON记录，可以通过将JSON文件当做文本文件来读取，然后利用相关的JSON库对每一条数据进行JSON解析。\n\n```json\n{\"name\":\"Hadoop\",\"age\":13}\n{\"name\":\"Spark\", \"age\":11}\n{\"name\":\"Flink\", \"age\":3}\n```\n\n```scala\npackage com.hph\n\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.json4s.ShortTypeHints\nimport org.json4s.jackson.JsonMethods._\nimport org.json4s.jackson.Serialization\n\nobject TestJson {\n\n  case class BigData(name:String,year:Int)\n\n  def main(args: Array[String]): Unit = {\n\n    val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"JSON\")\n    val sc = new SparkContext(conf)\n    sc.setLogLevel(\"ERROR\")\n    implicit val formats = Serialization.formats(ShortTypeHints(List()))\n    val input = sc.textFile(\"D:\\\\input\\\\people.json\")\n\n    input.collect().foreach(x => {var c = parse(x).extract[BigData];println(c.name + \",\" + c.year)})\n\n  }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529225006.png)\n\n####  CSV文件\n\n读取 CSV/TSV 数据和读取 JSON 数据相似，都需要先把文件当作普通文本文件来读取数据，然后通过将每一行进行解析实现对CSV的读取。CSV/TSV数据的输出也是需要将结构化RDD通过相关的库转换成字符串RDD，然后使用 Spark 的文本文件 API 写出去。\n\n#### Sequence文件\n\n SequenceFile文件是[Hadoop](http://lib.csdn.net/base/hadoop)用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。\n\n Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用 sequenceFile[ keyClass, valueClass](path)。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529225226.png)\n\n#### 对象文件\n\n对象文件是将对象序列化后保存的文件，采用Java的序列化机制。可以通过objectFile[k,v](path) 函数接收一个路径，读取对象文件，返回对应的 RDD，也可以通过调用saveAsObjectFile() 实现对对象文件的输出。因为是序列化所以要指定类型。\n\n```scala\nval data=sc.parallelize(List((1,\"hphblog\"),(2,\"Spark\"),(3,\"Flink\"),(4,\"SpringBoot\"),(5,\"SpringCloud\")))\ndata.saveAsObjectFile(\"hdfs://datanode1:9000/objfile\")\nval objrdd:org.apache.spark.rdd.RDD[(Int,String)] = sc.objectFile[(Int,String)](\"hdfs://datanode1:9000/objfile/p*\")\nobjrdd.collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529231917.png)\n\n#### HDFS\n\nSpark的整个生态系统与Hadoop是完全兼容的,所以对于Hadoop所支持的文件类型或者数据库类型,Spark也同样支持.另外,由于Hadoop的API有新旧两个版本,所以Spark为了能够兼容Hadoop所有的版本,也提供了两套创建操作接口.对于外部存储创建操作而言,hadoopRDD和newHadoopRDD是最为抽象的两个函数接口,主要包含以下四个参数.\n\n1）输入格式(InputFormat): 制定数据输入的类型,如TextInputFormat等,新旧两个版本所引用的版本分别是org.apache.hadoop.mapred.InputFormat和org.apache.hadoop.mapreduce.InputFormat(NewInputFormat)\n\n2）键类型: 指定[K,V]键值对中K的类型\n\n3）值类型: 指定[K,V]键值对中V的类型\n\n4）分区值: 指定由外部存储生成的RDD的partition数量的最小值,如果没有指定,系统会使用默认值defaultMinSplits\n\n其他创建操作的API接口都是为了方便最终的Spark程序开发者而设置的,是这两个接口的高效实现版本.例如,对于textFile而言,只有path这个指定文件路径的参数,其他参数在系统内部指定了默认值\n\n分为新旧API，**注意:**\n\n1.在Hadoop中以压缩形式存储的数据,不需要指定解压方式就能够进行读取,因为Hadoop本身有一个解压器会根据压缩文件的后缀推断解压算法进行解压.\n\n2.如果用Spark从Hadoop中读取某种类型的数据不知道怎么读取的时候,上网查找一个使用map-reduce的时候是怎么读取这种这种数据的,然后再将对应的读取方式改写成上面的hadoopRDD和newAPIHadoopRDD两个类就行了\n\n```scala\nval data = sc.parallelize(Array((1,\"Hadoop\"), (2,\"Spark\"), (3,\"Flink\")))\ndata.saveAsHadoopFile(\"hdfs://datanode1:9000/output/hdfs_spark\",classOf[Text],classOf[IntWritable],classOf[TextOutputFormat[Text,IntWritable]])\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529233324.png)\n\n```scala\nval data = sc.parallelize(Array((\"Hadoop\",1), (\"Spark\",2), (\"Flink\",3)))\ndata.saveAsNewAPIHadoopFile(\"hdfs://datanod1:9000/output/NewAPI/\",classOf[Text],classOf[IntWritable] , classOf[org.apache.hadoop.mapreduce.OutputFormat[Text,IntWritable]])\n```\n\n#### 文件系统\n\nSpark 支持读写很多种文件系统， 像本地文件系统、Amazon S3、HDFS等甚至是腾讯和阿里的COS等。\n\n#### 数据库\n\n支持通过Java JDBC访问关系型数据库。需要通过JdbcRDD进行，不过需要我们把驱动包放入Spark的\n\n```scala\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/5/30.\n  */\nobject JDBCRdd {\n  def main (args: Array[String] ) {\n    val sparkConf = new SparkConf ().setMaster (\"local[*]\").setAppName (\"JdbcApp\")\n    val sc = new SparkContext (sparkConf)\n    val rdd = new org.apache.spark.rdd.JdbcRDD (\n      sc,\n      () => {\n        Class.forName (\"com.mysql.jdbc.Driver\").newInstance()\n        java.sql.DriverManager.getConnection (\"jdbc:mysql://datanode1:3306/rdd\", \"root\", \"123456\")\n      },\n      \"select * from rddtable where id >= ? and id <= ?;\",  //SQL\n      1,   // 下界\n      10, //上界\n      1, //分区数\n      r => (r.getInt(1), r.getString(2)))\n\n    println (rdd.count () )\n    rdd.foreach (println (_) )\n    sc.stop ()\n  }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530094256.png)![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530094311.png)\n\nMysql写入：\n\n```scala\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/5/30.\n  */\nobject JDBCRDD2MySQL {\n  def main(args: Array[String]) {\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"HBaseApp\")\n    val sc = new SparkContext(sparkConf)\n    val data = sc.parallelize(List(\"JDBC2Mysql\", \"JDBCSaveToMysql\",\"RDD2Mysql\"))\n\n    data.foreachPartition(insertData)\n  }\n\n  def insertData(iterator: Iterator[String]): Unit = {\n    Class.forName (\"com.mysql.jdbc.Driver\").newInstance()\n    val conn = java.sql.DriverManager.getConnection(\"jdbc:mysql://datanode1:3306/rdd\", \"root\", \"hive\")\n    iterator.foreach(data => {\n      val ps = conn.prepareStatement(\"insert into rddtable(name) values (?)\")\n      ps.setString(1, data)\n      ps.executeUpdate()\n    })\n  }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530095104.png)\n\nJdbcRDD 接收这样几个参数。 \n\n• 首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配置后创建自己读取数据的连接。 \n\n• 接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中`lowerBound`和 `upperBound` 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。\n\n• 这个函数的最后一个参数是一个可以将输出结果从转为对操作数据有用的格式的函数。如果这个参数空缺，Spark会自动将每行结果转为一个对象数组。 \n\n```scala\ncreate 'fruit','info'\nput 'fruit','1001','info:name','Apple'\nput 'fruit','1001','info:color','Read'\nput 'fruit','1002','info:name','Banana'\nput 'fruit','1002','info:color','Yelow'\n```\n\n````scala\nimport org.apache.hadoop.hbase.HBaseConfiguration\nimport org.apache.hadoop.hbase.mapreduce.TableInputFormat\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/5/30.\n  */\nobject ReadHBase {\n  def main(args: Array[String]) {\n\n\n    val sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"HBaseApp\")\n    val sc = new SparkContext(sparkConf)\n\n    sc.setLogLevel(\"ERROR\")\n    val conf = HBaseConfiguration.create()\n    conf.set(\"hbase.zookeeper.quorum\", \"192.168.1.101\");\n    conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\n    //HBase中的表名\n    conf.set(TableInputFormat.INPUT_TABLE, \"fruit\")\n\n    val hBaseRDD = sc.newAPIHadoopRDD(conf, classOf[TableInputFormat],\n      classOf[org.apache.hadoop.hbase.io.ImmutableBytesWritable],\n      classOf[org.apache.hadoop.hbase.client.Result])\n\n    val count = hBaseRDD.count()\n    println(\"hBaseRDD RDD Count:\" + count)\n    hBaseRDD.cache()\n    hBaseRDD.foreach {\n      case (_, result) =>\n        val key = Bytes.toString(result.getRow)\n        val name = Bytes.toString(result.getValue(\"info\".getBytes, \"name\".getBytes))\n        val color = Bytes.toString(result.getValue(\"info\".getBytes, \"color\".getBytes))\n        println(\"Row key:\" + key + \" Name:\" + name + \" Color:\" + color)\n    }\n    sc.stop()\n  }\n}\n````\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530104837.png)\n\n```scala\nimport org.apache.hadoop.hbase.client.{HBaseAdmin, Put}\nimport org.apache.hadoop.hbase.{HBaseConfiguration, HColumnDescriptor, HTableDescriptor, TableName}\nimport org.apache.hadoop.hbase.io.ImmutableBytesWritable\nimport org.apache.hadoop.hbase.mapred.TableOutputFormat\nimport org.apache.hadoop.hbase.util.Bytes\nimport org.apache.hadoop.mapred.JobConf\nimport org.apache.spark.{SparkConf, SparkContext}\n\n/**\n  * Created by 清风笑丶 Cotter on 2019/5/30.\n  */\nobject Write2Hbase {\n  def main(args: Array[String]) {\n    val sparkConf = new SparkConf().setMaster(\"local[2]\").setAppName(\"HBaseApp\")\n    val sc = new SparkContext(sparkConf)\n\n    sc.setLogLevel(\"ERROR\")\n    val conf = HBaseConfiguration.create()\n    conf.set(\"hbase.zookeeper.quorum\", \"192.168.1.101\");\n    conf.set(\"hbase.zookeeper.property.clientPort\", \"2181\")\n\n    val jobConf = new JobConf(conf)\n    jobConf.setOutputFormat(classOf[TableOutputFormat])\n    jobConf.set(TableOutputFormat.OUTPUT_TABLE, \"fruit_spark\")\n\n    val fruitTable = TableName.valueOf(\"fruit_spark\")\n    val tableDescr = new HTableDescriptor(fruitTable)\n    tableDescr.addFamily(new HColumnDescriptor(\"info\".getBytes))\n\n    val admin = new HBaseAdmin(conf)\n    if (admin.tableExists(fruitTable)) {\n      admin.disableTable(fruitTable)\n      admin.deleteTable(fruitTable)\n    }\n    admin.createTable(tableDescr)\n\n    def convert(triple: (Int, String, Int)) = {\n      val put = new Put(Bytes.toBytes(triple._1))\n      put.addImmutable(Bytes.toBytes(\"info\"), Bytes.toBytes(\"name\"), Bytes.toBytes(triple._2))\n      put.addImmutable(Bytes.toBytes(\"info\"), Bytes.toBytes(\"price\"), Bytes.toBytes(triple._3))\n      (new ImmutableBytesWritable, put)\n    }\n    val initialRDD = sc.parallelize(List((1,\"apple\",11), (2,\"banana\",12), (3,\"pear\",13)))\n    val localData = initialRDD.map(convert)\n\n    localData.saveAsHadoopDataset(jobConf)\n  }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530105359.png)\n\n### 共享变量\n\n####  累加器\n\n一个全局共享变量,可以完成对信息进行操作,相当于MapReduce中的计数器, Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本， 更新这些副本的值也不会影响驱动器中的对应变量。 如果我们想实现所有分片处理时更新共享变量的功能，那么累加器可以实现我们想要的效果。\n\n```scala\nval notice = sc.textFile(\"file:///opt/module/spark/README.md\")\n val blanklines = sc.accumulator(0)\nval tmp = notice.flatMap(line => {\n         if (line == \"\") {\n            blanklines += 1\n         }\n         line.split(\" \")\n      })\ntmp.count()\nblanklines.value\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530111120.png)\n\n```scala\n  @deprecated(\"use AccumulatorV2\", \"2.0.0\")\n  def accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T] = {\n    val acc = new Accumulator(initialValue, param)\n    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))\n    acc\n  }\n```\n\n通过在驱动器中调用`SparkContext.accumulator(initialValue)`方法，创建出存有初始值的累加器。返回值为\n`org.apache.spark.Accumulator[T]`对象，T 是初始值 initialValue 的类型。Spark闭包里的执行器代码可以使用累加器的 += 方法(在Java中是 add)增加累加器的值。 驱动器程序可以调用累加器的value属性(在Java中使用value()或setValue())来访问累加器的值。 \n\n为什么有了reduce()这样的聚合操作了,还要累加器呢?因为RDD本身提供的同步机制力度太粗,尤其是在转换操作中变量状态不能同步,累加器可以对那些与RDD本身的范围和粒度不一样的值进行聚合,只不过它是一个只写变量,无法读取这个值,只能在驱动程序中读取累加器的值。\n\n#### 自定义累加器\n\n在2.0版本后，累加器的易用性有了较大的改进，而且官方还提供了一个新的抽象类：AccumulatorV2来提供更加友好的自定义类型累加器的实现方式。实现自定义类型累加器需要继承AccumulatorV2并至少覆写下例中出现的方法，下面这个累加器可以用于在程序运行过程中收集一些文本类信息，最终以Set[String]的形式返回。\n\n```scala\nimport org.apache.spark.{SparkConf, SparkContext}\n\nimport scala.collection.JavaConversions._\n\nclass LogAccumulator extends org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] {\n  private val _logArray: java.util.Set[String] = new java.util.HashSet[String]()\n\n  override def isZero: Boolean = {\n    _logArray.isEmpty\n  }\n\n  override def reset(): Unit = {\n    _logArray.clear()\n  }\n\n  override def add(v: String): Unit = {\n    _logArray.add(v)\n  }\n\n  override def merge(other: org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]]): Unit = {\n    other match {\n      case o: LogAccumulator => _logArray.addAll(o.value)\n    }\n\n  }\n\n  override def value: java.util.Set[String] = {\n    java.util.Collections.unmodifiableSet(_logArray)\n  }\n\n  override def copy():org.apache.spark.util.AccumulatorV2[String, java.util.Set[String]] = {\n    val newAcc = new LogAccumulator()\n    _logArray.synchronized{\n      newAcc._logArray.addAll(_logArray)\n    }\n    newAcc\n  }\n}\n\n// 过滤掉带字母的\nobject LogAccumulator {\n  def main(args: Array[String]) {\n    val conf=new SparkConf().setAppName(\"LogAccumulator\").setMaster(\"local[*]\")\n    val sc=new SparkContext(conf)\n\n    val accum = new LogAccumulator\n    sc.register(accum, \"logAccum\")\n    val sum = sc.parallelize(Array(\"1\", \"2a\", \"3\", \"4b\", \"5\", \"6\", \"7cd\", \"8\", \"9\"), 2).filter(line => {\n      val pattern = \"\"\"^-?(\\d+)\"\"\"\n      val flag = line.matches(pattern)\n      if (!flag) {\n        accum.add(line)\n      }\n      flag\n    }).map(_.toInt).reduce(_ + _)  //1+3+5+6+7+8+9 =32\n\n    println(\"sum: \" + sum)\n    for (v <- accum.value) print(v + \"\")\n    println()\n    sc.stop()\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530124720.png)\n\n#### 广播变量\n\nSpark的算子逻辑是发送到Executor中运行的，数据是分区的，因此当Executor中需要引用外部变量的时候，就需要我们用到广播变量(Broadcast)\n\n累加器相当于统筹大变量，通常用于计数，统计广播变量允许程序员缓存一个只读的变量在每一台机器上（worker）上，而不是每一个任务保存一份备份。利用广播变量可以以更有效的方式将大数据量输入集合的副本分配到每一个节点。\n\n广播变量通过两方面提高数据共享效率：\n\n1)集群重的每一个节点(物理机器)只有一个副本，默认的闭包是每一个任务一个副本；\n\n2)广播传输时通过BT下载模式实现的，也就是P2P下载的，在集群很多的情况下可以极大地提高数据传输速率。广播变量修改后，不会反馈到其他节点。\n\n在Spark中，它会自动把所有音容变量发送到工作节点是，虽然很方便，但是效率比较低：\n\n1)默认地任务发射机制时专门为小任务进行优化的。\n\n2)实际过程中可能会在多个并行操作中使用同一个变量，而Spark会分别为每个操作发送这个变量。\n\n```scala\nval broadcastVar = sc.broadcast(Array(1,2,3,4,5))\nbroadcastVar.value\nsc.parallelize(Array(1,2,3,4,5,6,7,8)).flatMap(x => (broadcastVar.value)).collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530182120.png)\n\n广播变量内部存储地数据量较小地时候可以进行高效地广播，当这个变量变得非常大地时候，例如:在广播规则库的时候，规则库比较大，从主节点发送这样的一个规则数组非常消耗内存，如果之后还需要用到规则库这个变量，则需要再向每个节点发送一遍，同时如果一个节点的Executor中多个Task都用到这个变量，那么每个Task中都需要从driver端发送一份规则库的变量，最终导致占用的内存空间很大，如果变量为外部变量，进行广播前要进行collect操作。\n\n```scala\nval broadcas = sc.textFile(\"file:///opt/module/spark/README.md\")\nval broadcasRDD = broadcas.collect\nval c = sc.broadcast(broadcasRDD)\nc.value\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190530184944.png)\n\n我们通过调用一个对象SparkContext.broadcast创建一个Broadcast对象，任何可以序列化对象都可以这样实现。需要注意的是，如果变量是从外部读取的，需要先进行collect操作，再进行广播，给如果广播的值比较大，可以选择即快又好的序列化格式。在Scala和Java API中默认使用Java序列化库，对于除基本的数组以外的任何对象都比较低效，我们可以使用`spark.serialler`属性选择另外一种序列化库来优化序列化的过程(也可以使用reduce()方法为Python的pickle库自定义序列化)\n\n\n\n\n\n\n\n","tags":["Spark","RDD"],"categories":["大数据"]},{"title":"Spark之RDD实战2","url":"/2019/05/28/Spark之RDD实战篇2/","content":"\n {{ \"宽窄依赖、DAG RDD相关概念\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 依赖\n\nRDD和它依赖的父RDD（s）的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528225959.png)\n\n### 窄依赖\n\n窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用。\n\n### 宽依赖\n\n窄依赖指的是每一个父RDD的Partition最多被子RDD的一个Partition使用,窄依赖我们形象的比喻为独生子女\n\n### Lineage\n\nRDD只支持粗粒度转换，即在大量记录上执行的单个操作。将创建RDD的一系列Lineage（即血统）记录下来，以便恢复丢失的分区。RDD的Lineage会记录RDD的元数据信息和转换行为，当该RDD的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528230120.png)\n\n```scala\nval text = sc.textFile(\"/input/test.txt\")\nval words = text.flatMap(_.split(\" \"))\nval date = words.map((_,1))\nval result = date.reduceByKey(_+_)\ndate.dependencies\nresult.dependencies\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528231029.png)\n\n## DAG的生成\n\nDAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528231121.png)\n\n## RDD相关概念关系\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528231142.png)\n\n输入可能以多个文件的形式存储在HDFS上，每个File都包含了很多块，称为Block。当Spark读取这些文件作为输入时，会根据具体数据格式对应的InputFormat进行解析，一般是将若干个Block合并成一个输入分片，称为InputSplit，注意InputSplit不能跨越文件。随后将为这些输入分片生成具体的Task。InputSplit与Task是一一对应的关系。随后这些具体的Task每个都会被分配到集群上的某个节点的某个Executor去执行。\n\n1)      每个节点可以起一个或多个Executor。\n\n2)      每个Executor由若干core组成，每个Executor的每个core一次只能执行一个Task。\n\n3)      每个Task执行的结果就是生成了目标RDD的一个partiton。\n\n注意: 这里的core是虚拟的core而不是机器的物理CPU核，可以理解为就是Executor的一个工作线程。而 Task被执行的并发度 = Executor数目 * 每个Executor核数。至于partition的数目：\n\n1)      对于数据读入阶段，例如sc.textFile，输入文件被划分为多少InputSplit就会需要多少初始Task。\n\n2)      在Map阶段partition数目保持不变。\n\n3)      在Reduce阶段，RDD的聚合会触发shuffle操作，聚合后的RDD的partition数目跟具体操作有关，例如repartition操作会聚合成指定分区数，还有一些算子是可配置的。\n\nRDD在计算的时候，每个分区都会起一个task，所以rdd的分区数目决定了总的的task数目。申请的计算节点（Executor）数目和每个计算节点核数，决定了你同一时刻可以并行执行的task。\n\n比如的RDD有100个分区，那么计算的时候就会生成100个task，你的资源配置为10个计算节点，每个两2个核，同一时刻可以并行的task数目为20，计算这个RDD就需要5个轮次。如果计算资源不变，你有101个task的话，就需要6个轮次，在最后一轮中，只有一个task在执行，其余核都在空转。如果资源不变，你的RDD只有2个分区，那么同一时刻只有2个task运行，其余18个核空转，造成资源浪费。这就是在spark调优中，增大RDD分区数目，增大任务并行度的做法。","tags":["Spark","RDD"],"categories":["大数据"]},{"title":"Spark之RDD实战篇","url":"/2019/05/27/Spark之RDD实战篇/","content":"\n {{ \"Spark RDD创建、转换、行动算子、RDD的持久化\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## RDD编程\n\n在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用action触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。\n\n要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527214645.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527215048.png)\n\n## 解析器集成\n\n与Ruby和Python类似，Scala提供了一个交互式Shell (解析器)，借助内存数据带来的低延迟特性，可以让用户通过解析器对大数据进行交互式查询。Spark解析器将用户输入的多行命令解析为相应Java对象的示例如图所示\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527215246.png)\n\nScala解析器处理过程一般为：\n\n①将用户输入的每一行编译成一个类；\n\n②将该类载入到JVM 中；\n\n③调用该类的某个函数。在该类中包含一个单利对象，对象中包含当前行的变量或函数，在初始化方法中包含处理该行的代码。例如，如果用户输入“varx=5”，在换行输入primln(x),那解析器会定义一个叫Linel的类，该类包含X，第二行编译成println (Linel.getlnstance().x)。\n\nSpark中做了以下两个改变。\n(1)类传输：为了让工作节点能够从各行生成的类中获取到字节码，通过HTTP传输。\n(2)代码生成器的改动：通常各种代码生成的单例对象是由类的静态方法来提供的。也就是说，当序列化一个引用上一行定义变量的闭包（例如上面例子的Linel.x), Java不会通过检索对象树的方式去传输包含x的Linel实例。因此工作节点不能够得到x,在Spark中修改了代码生成器的逻辑，让各行对象的实例可以被字节应用。在图中显示了 Spark修改之后解析器是如何把用户输入的每一行变成Java对象的。\n\n## 内存管理\n\nSpark提供了 3种持久化RDD的存储策略：\n\n1.未序列化Java对象存在内存中、\n\n2.序列化的数据存于内存中\n\n3.存储在磁盘中\n\n第一个选项的性能是最优的，因为可以直接访问在Java虚拟机内存里的RDD对象；在空间有限的情况下，第二种方式可以让用户釆用比Java对象更有效的内存组织方式，但代价是降低了性能；第三种策略使用于RDD太大的情形，每次重新计算该RDD会带来额外的资源开销（如I/O等)。对于内存使用LRU回收算法来进行管理，当计算得到一个新的RDD分区，但没有足够空间来存储时，系统会从最近最少使用的RDD回收其一个分区的空间。除非该RDD是新分区对应的RDD，这种情况下Spark会将旧的分区继续保留在内存中，防止同一个RDD的分区被循环调入/调出。这点很关键，因为大部分的操作会在一个RDD的所有分区上进行，那么很有可能己经存在内存中的分区将再次被使用。\n\n## 多用户管理\n\nRDD模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群能够支持多种资源共享算法。特别地，每个RDD应用可以在执行过程中动态调整访问资源。\n在每个应用程序中，Spark运行多线程同时提交作业，并通过一种等级公平调度器来实现多个作业对集群资源的共享，这种调度器和Hadoop Fair Scheduler类似。该算法主 要用于创建基于针对相同内存数据的多用户应用，例如：Spark SQL引擎有一个服务 模式支持多用户并行查询。公平调度算法确保短的作业能够在即使长作业占满集群资源的情况下尽早完成。\n\nSpark的公平调度也使用延迟调度，通过轮询每台机器的数据，在保持公平的情况下给予作业高的本地性。Spark支持多级本地化访问策略（本地化)，包括内存、磁盘和机 架。\n\n由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。Spark中可以使用Mesos来实现细粒度的资源共享，这使得Spark应用能相互之间或在不同的计算框架之间实现资源的动态共享。Spark使用Sparrow系统扩展支持分布式调度，该调度允许多个Spark应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。\n\n## RDD创建\n\n### 集合中创建RDD\n\n从已有的集合中创建RDD\n\n```scala\nval rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8,9,10))\n```\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527222920.png)\n\n```scala\n//并行化操作  \ndef parallelize[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope { //默认是多少呢\n    assertNotStopped()\n    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())\n  }\n//本地模式下  \noverride def defaultParallelism(): Int =\n    scheduler.conf.getInt(\"spark.default.parallelism\", totalCores)  \n//CoarseGrainedSchedulerBackend\n override def defaultParallelism(): Int = {\n    conf.getInt(\"spark.default.parallelism\", math.max(totalCoreCount.get(), 2))\n  }\n//stanlone继承了CoarseGrainedSchedulerBackend 因此绝大部分的情况下并行化处理数据的并行度为CPU的核数\n\n//makeRDD本质上还是调用了parallelize\n  def makeRDD[T: ClassTag](\n      seq: Seq[T],\n      numSlices: Int = defaultParallelism): RDD[T] = withScope {\n    parallelize(seq, numSlices)\n  }\n```\n\n```scala\n/**\n * Distribute a local Scala collection to form an RDD, with one or more\n * location preferences (hostnames of Spark nodes) for each object.\n * Create a new partition for each collection item.\n */\n//\ndef makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {\n  assertNotStopped()\n  val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap\n  new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs)\n}\n```\n\n```scala\nval test1 = sc.parallelize(List(1,2,3,4))\nval seq = List((1,List(\"datanode1\")),(2,List(\"datanode2\")))  //可以提供位置信息\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527230912.png)\n\n`def parallelize[T: ClassTag]` 和`def makeRDD[T: ClassTag]`返回的都是ParallelCollectionRDD,而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。\n\n### 外部存储系统的数据集创建\n\n包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等\n\n```scala\nval datasets =sc.textFile(\"hdfs://datanode1:9000/input/test.txt\")\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527232248.png)\n\n## RDD转换\n\n### map()\n\nmap操作时对RDD中的每一个数都执行一个指定的函数来产生一个新的RDD,任何元RDD中的元素在新RDD中都有且只有一个元素与之对应。\n\n```scala\nval data = sc.parallelize(1 to 10).collect()\nval map = data.map(_ * 2)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527232843.png)\n\n### mapPartitions()\n\n类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是`Iterator[T] => Iterator[U]`。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。其中`preservesPartitioning`表示是否保留父RDD的partitiones分区信息，如果在映射过程中需要频繁创建对象，使用mapPartitions操作要比map操作高 效得多。比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection,这样开销很大。如果使用mapPartitions，那么只需要针对每一个分区建立一个connectiono mapPartitionsWithlndex操作作用类似于mapPartitions,只是输入参数多了一个分区索引。W\n\n```scala\n def mapPartitions[U: ClassTag](\n      f: Iterator[T] => Iterator[U],\n      preservesPartitioning: Boolean = false): RDD[U] = withScope {\n    val cleanedF = sc.clean(f)\n    new MapPartitionsRDD(\n      this,\n      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(iter),\n      preservesPartitioning)\n  }\n```\n\n```scala\n//创建RDD使RDD有两个分区\n var rdd1= sc.makeRDD(1 to 10,2)\n///使用mapPartitions对rddl进行重新分区\n  var rdd2 = rdd1.mapPartitions{ x => {\n      var result = List[Int]()\n      var i = 0\n      while(x.hasNext){\n          i += x.next()\n    }\n     result.::(i).iterator\n   }}\n  //rdd2将rddl中每个分区中的数值累加\n  rdd2.collect\n\n//重新对rdd1分区\nvar rdd3 = rdd1.mapPartitionsWithIndex{\n     (x,iter) => {\n       var result = List[String]()\n       var i = 0\n       while(iter.hasNext){\n          i += iter.next()\n         }\n        result.::(x + \"|\" + i).iterator\n   }\n}\nrdd3.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190528083325.png)\n\n### glom()\n\nRDD中每一个分区所有类型为T的数据转变成元素类型为T的数组[Array[T]].\n\n```scala\nvar rdd = sc.parallelize(1 to 16,4)\nrdd.glom().collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190528083705.png)\n\n### flatMap()\n\nflatMap操作原RDD中的每一个元素生成一个或多个元素来构建新的RDD\n\n```scala\nval rdd1 = sc.parallelize(1 to 5)\nval flatMap = rdd1.flatMap(1 to _)\nflatMap.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528084629.png)\n\n### filter()\n\n返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成.\n\n```scala\nvar sourceFilter = sc.parallelize(Array(\"hadoop\",\"spark\",\"flink\",\"hphblog\"))\nval filter = sourceFilter.filter(_.contains(\"h\"))\nfilter.collect\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528084943.png)\n\n### mapPartitionsWithIndex()\n\n类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是`(Int, Interator[T]) =>Iterator[U]`\n\n```scala\n//创建RDD使RDD有两个分区\n var rdd1= sc.makeRDD(1 to 10,2)\n///使用mapPartitions对rddl进行重新分区\n  var rdd2 = rdd1.mapPartitions{ x => {\n      var result = List[Int]()\n      var i = 0\n      while(x.hasNext){\n          i += x.next()\n    }\n     result.::(i).iterator\n   }}\n  //rdd2将rddl中每个分区中的数值累加\n  rdd2.collect\n\n//重新对rdd1分区\nvar rdd3 = rdd1.mapPartitionsWithIndex{\n     (x,iter) => {\n       var result = List[String]()\n       var i = 0\n       while(iter.hasNext){\n          i += iter.next()\n         }\n        result.::(x + \"|\" + i).iterator\n   }\n}\nrdd3.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190528083325.png)\n\n###  sample(withReplacement, fraction, seed)\n\n以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为2（即可能以1 2 3的其中一个起始值）\n\n```scala\nval rdd = sc.parallelize(1 to 10)\nrdd.collect\nvar sample1 = rdd.sample(true,0.5,2)\nsample1.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528091220.png)\n\n### distinct([numTasks]))\n\n对原来RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。\n\n```scala\n val rdd = sc.parallelize(List(1,2,2,3,3,4,4,5,5,5,6,6,7,7,8))\n val rdd1 = rdd.distinct()\n rdd1.collect\n\n val rdd3 = rdd1.distinct(10)\n rdd3.collect\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528093127.png)\n\n### partitionBy\n\n对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。\n\n```scala\nval rdd = sc.parallelize(Array((1,\"hadoop\"),(2,\"spark\"),(3,\"flink\"),(4,\"hphblog\")),4)\nrdd.partitions.size\nvar rdd2 = rdd.partitionBy(new org.apache.spark.HashPartitioner(2))\nrdd.collect\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528093534.png)\n\n###  coalesce((numPartitions, shuffle)\n\n缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。shuffle默认关闭.\n\n```scala\nval rdd = sc.parallelize(1 to 10000,4)\nval coalesceRDD = rdd.coalesce(2)\nval shuffleRDD = rdd.coalesce(2,true)\nshuffleRDD.collect\nrdd.collect\ncoalesceRDD.partitions.size\nshuffleRDD.partitions.size\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528094743.png)\n\n### repartition(numPartitions)\n\n根据分区数，从新通过网络随机洗牌所有数据。底层调用的是`coalesce(numPartitions, shuffle = true)`\n\n```scala\nval rdd = sc.parallelize(1 to 10000,4)\nrdd.partitions.size\nval rerdd = rdd.repartition(2)\nrerdd.partitions.size\nval rerdd = rdd.repartition(4)\nrerdd.partitions.size\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528100745.png)\n\n### repartitionAndSortWithinPartitions\n\nrepartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。 \n\n```scala\ndef repartitionAndSortWithinPartitions(partitioner: Partitioner): RDD[(K, V)] = self.withScope {\n  new ShuffledRDD[K, V, V](self, partitioner).setKeyOrdering(ordering)\n}\n```\n\n###  sortBy([ascending], [numTasks])\n\n```scala\ndef sortBy[K](\n    f: (T) => K,\n    ascending: Boolean = true,\n    numPartitions: Int = this.partitions.length)\n```\n\n```scala\nval rdd =sc.parallelize(List(1,2,3,4,5,6,7,8,9))\nrdd.sortBy(x => x ,ascending=false).collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528102152.png)\n\n###  union(otherDataset)\n\n对源RDD和参数RDD求并集后返回一个新的RDD `不去重`\n\n```scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(5 to 15)\nval rdd3 = rdd1.union(rdd2)\nrdd3.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528102902.png)\n\n### subtract (otherDataset)\n\n计算差的一种函数，去除两个RDD中相同的元素，不同的RDD将保留下来 \n\n```scala\n val rdd1 = sc.parallelize(1 to 10)\n val rdd2 = sc.parallelize(5 to 15)\n val rdd3 = rdd1.subtract(rdd2)\n rdd3.collect\n val rdd4 =rdd2.subtract(rdd1)\n rdd4.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/1559010918532.png)\n\n### intersection(otherDataset)\n\n对源RDD和参数RDD求交集后返回一个新的RDD\n\n```scala\nval rdd1 = sc.parallelize(1 to 10)\nval rdd2 = sc.parallelize(5 to 15)\nval rdd3 = rdd1.intersection(rdd2)\nrdd3.collect\nval rdd4 = rdd2.intersection(rdd1)\nrdd4.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528103820.png)\n\n###  cartesian(otherDataset)\n\n笛卡尔积\n\n```scala\nval rdd1 = sc.parallelize(1 to 3)\nval rdd2 = sc.parallelize(2 to 5)\nrdd1.cartesian(rdd2).collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528104041.png)\n\n###  pipe(command, [envVars])\n\n管道，对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD\n\n```shell\n#!/bin/sh\necho \"hello Spark This is Linux bash\"\nwhile read LINE; do\n   echo \">>>\"${LINE}\ndone\n```\n\n```scala\nval rdd = sc.parallelize(List(\"hi\",\"Hello\",\"hadoop\",\"spark\",\"flink\",\"hphblog\"),1)\nrdd.pipe(\"/home/hadoop/pipe.sh\").collect()\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528104554.png)\n\n###  join(otherDataset, [numTasks])\n\n在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD\n\n```scala\nval rdd = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\")))\nval rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))\nrdd.join(rdd1).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528104857.png)\n\n###  cogroup(otherDataset,[numTasks])\n\n在类型为(K,V)和(K,W)的RDD上调用，返回一个``(K,(Iterable<V>,Iterable<W>))``类型的RDD\n\n```scala\nval rdd = sc.parallelize(Array((1,\"a\"),(2,\"b\"),(3,\"c\")))\nval rdd1 = sc.parallelize(Array((1,4),(2,5),(3,6)))\nrdd.cogroup(rdd1).collect()\nval rdd2 = sc.parallelize(Array((4,4),(2,5),(3,6)))\nrdd.cogroup(rdd2).collect()\nval rdd3 = sc.parallelize(Array((1,\"a\"),(1,\"d\"),(2,\"b\"),(3,\"c\")))\nrdd3.cogroup(rdd2).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528110138.png)\n\n### reduceByKey(func, [numTasks])\n\n在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。\n\n```scala\nval rdd = sc.parallelize(List((\"hadoop\",1),(\"spark\",5),(\"spark\",5),(\"flink\",3)))\nval reduce = rdd.reduceByKey((x,y)=>(x+y))\nreduce.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528110533.png)\n\n### groupByKey\n\ngroupByKey也是对每个key进行操作，但只生成一个sequence\n\n```scala\nval words = Array(\"hadoop\", \"spark\", \"spark\", \"flink\", \"flink\", \"flink\")\nval wordPairsRDD = sc.parallelize(words).map(word => (word, 1))\nval group = wordPairsRDD.groupByKey()\ngroup.collect()\nval result = group.map(t => (t._1, t._2.sum))\nresult.collect\nval map = group.map(t => (t._1, t._2.sum))\nmap.collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528111123.png)\n\n###  combineByKey[C]\n\n(  createCombiner: V => C,  mergeValue: (C, V) => C,  mergeCombiners: (C, C) => C) 对相同K，把V合并成一个集合。\n\ncreateCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建 \n 那个键对应的累加器的初始值\n\nmergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并\n\nmergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。\n\n```scala\nval scores = Array((\"Fred\", 88), (\"Fred\", 95), (\"Fred\", 91), (\"Wilma\", 93), (\"Wilma\", 95), (\"Wilma\", 98))\nval input = sc.parallelize(scores)\nval combine = input.combineByKey(\n          (v)=>(v,1),\n          (acc:(Int,Int),v)=>(acc._1+v,acc._2+1),\n          (acc1:(Int,Int),acc2:(Int,Int))=>(acc1._1+acc2._1,acc1._2+acc2._2)\n)\n\nval result = combine.map{\n         case (key,value) => (key,value._1/value._2.toDouble)\n}\nresult.collect()\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528111935.png)\n\n### aggregateByKey\n\n`(zeroValue:U,[partitioner: Partitioner]) (seqOp:(U, V) => U,combOp: (U, U) => U) `\n\n在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。\n\nseqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。\n\n```scala\nval rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)\nval agg = rdd.aggregateByKey(0)(math.max(_,_),_+_)\nagg.collect()\nagg.partitions.size\nval rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),1)\nval agg = rdd.aggregateByKey(0)(math.max(_,_),_+_).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528114417.png)\n\n### foldByKey\n\n`(zeroValue: V)(func: (V, V) => V): RDD[(K, V)]`aggregateByKey的简化操作，seqop和combop相同\n\n```scala\nval rdd = sc.parallelize(List((1,3),(1,2),(1,4),(2,3),(3,6),(3,8)),3)\nval agg = rdd.foldByKey(0)(_+_)\nagg.collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528115239.png)\n\n### sortByKey([ascending], [numTasks])\n\n在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD\n\n```scala\nval rdd = sc.parallelize(Array((3,\"hadoop\"),(6,\"hohblog\"),(2,\"flink\"),(1,\"spark\")))\nrdd.sortByKey(true).collect()\nrdd.sortByKey(false).collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/1559015691057.png)\n\n### mapValues\n\n针对于(K,V)形式的类型只对V进行操作 \n\n```scala\nval rdd = sc.parallelize(Array((3,\"hadoop\"),(6,\"hohblog\"),(2,\"flink\"),(1,\"spark\")))\nrdd.mapValues(_+\"==> www.hphblog.cn\").collect()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528115636.png)\n\n## RDD行动算子\n\n### reduce(func)\n\n通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.reduce(_+_)\nval rdd1 = sc.makeRDD(Array((\"a\",1),(\"a\",3),(\"c\",3),(\"d\",5)))\nrdd1.reduce((x,y)=>(x._1 + y._1,x._2 + y._2))\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528120250.png)\n\n### collect()\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.collect()\n```\n\n在驱动程序中，以数组的形式返回数据集的所有元素\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528120451.png)\n\n###  count()\n\n返回RDD的元素个数\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd. count()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528120547.png)\n\n### first()\n\n返回RDD的第一个元素（类似于take(1)）\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.first()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528120819.png)\n\n###  take(n)\n\n返回一个由数据集的前n个元素组成的数组\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.take(10)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528120952.png)\n\n### takeSample(withReplacement,num, [seed])\n\n返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.takeSample(true,10,2)\nrdd.takeSample(false,10,2)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528121504.png)\n\n### takeOrdered(n)\n\n返回前几个的排序\n\n```scala\nval rdd = sc.makeRDD(1 to 100,2)\nrdd.take(10)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528121702.png)\n\n### aggregate\n\n`(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)`aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。\n\n```scala\nval rdd = sc.makeRDD(1 to 10,2)\nrdd.aggregate(1)(\n     {(x : Int,y : Int) => x + y}, \n      {(a : Int,b : Int) => a + b}\n      )\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528183859.png)\n\n为什么是58呢:\n\n```scala\nrdd.mapPartitionsWithIndex{\n    (partid,iter)=>{\n        var part_map = scala.collection.mutable.Map[String,List[Int]]()\n        var part_name = \"part_\" + partid\n        part_map(part_name) = List[Int]()\n        while(iter.hasNext){\n            part_map(part_name) :+= iter.next()//:+= 列表尾部追加元素\n        }\n        part_map.iterator\n    }\n}.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528185402.png)\n\n遍历第一个分区的数据我们知道第一个分区的数据是(1,2,3,4,5),第二个分区的数据是(6,7,8,9,10)首先在每一个分区执行`(x : Int,y : Int) => x + y`我们传入的zeroValue的值为1,即在`part_0中zeroValue+5+4+3+2+1=19`,在`part_1中zeroValue+6+7+8+9+10=41`,在将连个分局的结果合并`(a : Int,b : Int) => a + b`,并且使用zeroValue的值1即`zeroValue+part_0+part_1=1+16+41=58`因此结果为58.\n\n```scala\nrdd.aggregate(1)(\n     {(x : Int,y : Int) => x * y},\n      {(a : Int,b : Int) => a + b}\n      )\n```\n\n相同的我们可以刻分析出来\n\n首先在每一个分区执行`(x : Int,y : Int) => x * y`我们传入的zeroValue的值为1,即在`part_0中zeroValue*5*4*3*2*1=120`,在`part_1中zeroValue*6*7*8*9*10=30240`,在将连个分局的结果合并`(a : Int,b : Int) => a + b`,并且使用zeroValue的值1即`zeroValue+part_0+part_1=1+120+30240=30361`因此结果为30361.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528192803.png)\n\n### fold(num)(func)\n\n折叠操作，aggregate的简化操作，seqop和combop一样。\n\n```scala\nval rdd = sc.makeRDD(1 to 10,2)\nrdd.aggregate(1)(\n     {(x : Int,y : Int) => x + y},\n      {(a : Int,b : Int) => a + b}\n      )\nrdd.fold(1)(_+_)\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528201005.png)\n\n### saveAsTextFile(path)\n\n将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本\n\n```scala\n val rdd = sc.makeRDD(1 to 10,2)\n rdd.saveAsTextFile(\"hdfs://datanode1:9000/spark/saveAsTextFile/\")\n```\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528201529.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528201656.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528201735.png)\n\n### saveAsSequenceFile(path) \n\n将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。\n\n### saveAsObjectFile(path) \n\n用于将RDD中的元素序列化成对象，存储到文件中。\n\n###  countByKey()\n\n针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。\n\n```scala\nval rdd = sc.parallelize(List((\"hadoop\",3),(\"spark\",2),(\"hphblog\",3),(\"flink\",9),(\"flink\",9),(\"spark\",10)),3)\nrdd.countByKey()\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528202419.png)\n\n### foreach(func)\n\n在数据集的每一个元素上，运行函数func进行更新。注意foreach遍历RDD,将函数f应用于每一个元素.要注意如果对RDD执行foreach,只会在Executor端有效,而不是Driver.比如rdd.collect().foreach(println),只会在Executor端有效,Driver端是看不到的.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528202856.png)\n\n### sortBy(funct)\n\n```scala\nvar rdd = sc.makeRDD(Array((\"A\",2),(\"D\",5), (\"A\",1), (\"B\",6), (\"B\",3), (\"E\", 7),(\"C\",4)))\nrdd.sortBy(x => x).collect\nrdd.sortBy(x => x._2,false).collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528204821.png)\n\n##  RDD持久化\n\nSpark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果 希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。\n\n### 缓存方式\n\nRDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空 间中。 \n\n但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。\n\n```scala\n/**\n * Persist this RDD with the default storage level (`MEMORY_ONLY`).\n */\ndef persist(): this.type = persist(StorageLevel.MEMORY_ONLY)  //默认的持久化是内存中\n\n/**\n * Persist this RDD with the default storage level (`MEMORY_ONLY`).\n */\ndef cache(): this.type = persist()   //cache最终也是调用了persist方法\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528224007.png)\n\n在存储级别的末尾加上“_2”来把持久化数据存为两份\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528224100.png)\n\n缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。\n\n```scala\nval rdd = sc.makeRDD(1 to 10)\nval nocache = rdd.map(_.toString+\"[\"+System.currentTimeMillis+\"]\")\nval cache =  rdd.map(_.toString+\"[\"+System.currentTimeMillis+\"]\")\ncache.cache\nnocache.collect\nnocache.collect\ncache.collect\ncache.collect\ncache.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190528224651.png)\n\n我们发现持久化的内存时间戳没有变化,未持久化的内存时间戳是有变化的\n\n##  RDD检查点机制\n\nSpark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。\n\ncache 和 checkpoint 是有显著区别的，  缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志）， 也不能丢掉， 当某个点某个 executor 宕了，上面cache 的RDD就会丢掉， 需要通过依赖链重放计算出来， 不同的是， checkpoint是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。\n\n如果存在以下场景，则比较适合使用检查点机制：\n\n1）DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。\n\n2）在宽依赖上做Checkpoint获得的收益更大。\n\n为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。\n\n```scala\nval data = sc.parallelize(1 to 1000 , 5)\nsc.setCheckpointDir(\"hdfs://datanode1:9000/checkpoint\")\ndata.checkpoint\ndata.count\nval ch1 = sc.parallelize(1 to 20)\nval ch2 = ch1.map(_.toString+\"[\"+System.currentTimeMillis+\"]\")\nval ch3 = ch1.map(_.toString+\"[\"+System.currentTimeMillis+\"]\")\nch3.checkpoint\nch2.collect\nch2.collect\nch2.collect\nch3.collect\nch3.collect\nch3.collect\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/RDD/20190529115207.png)","tags":["Spark","RDD"],"categories":["大数据"]},{"title":"Spark之RDD理论篇","url":"/2019/05/27/Spark之RDD/","content":"\n {{ \"Spark的基石RDD\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## RDD与MapReduce\n\nSpark的编程模型是弹性分布式数据集(Resilient Distributed Dataset,RDD),它是MapReduce的扩展和延申,解决了MapReduce的缺陷:在并行计算阶段高效地进行数据共享.运行高效的数据共享概念和类似于MapReduce操作方式,使并行计算高效运行。Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527204519.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527204541.png)\n\n### RDD\n\nRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。\n\nRDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 \n\nSpark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。\n\n#### 属性\n\n一组分片（Partition）：数据集的基本组成单位。对于RDD每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。\n\n一个计算每个分区的函数：Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n\nRDD之间的依赖关系：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n\n一个Partitioner：即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n 一个列表：存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。\n\n#### 弹性\n\n`自动进行内存和磁盘数据存储的切换`：  Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。\n`基于血统的高效容错机制`： 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。\n\n`Task如果失败会自动进行特定次数的重试`：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。\n\n`Stage如果失败会自动进行特定次数的重试`：  如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。\n\n`  如果Job的某个Stage阶段计算失败`，框架也会自动进行任务的重新计算，默认次数也是4次：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除\n\n`数据调度弹性`：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。\n\n`数据分片的高度弹性`： 可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。\n\n#### 特点\n\n##### 分区\n\nRDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527210835.png)\n\n##### 只读\n\nRDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211304.png)\n\n 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211347.png)\n\n RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211453.png)\n\n##### 依赖\n\nRDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。依赖包括两种，一种是窄依赖，RDDs之间分区是一 一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211742.png)\n\n通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。\n\n##### 缓存\n\n如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。\n\n#####  CheckPoint\n\n虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。\n\n给定一个RDD我们至少可以知道如下几点信息：\n\n1、分区数以及分区方式；\n\n2、由父RDDs衍生而来的相关依赖信息；\n\n3、计算每个分区的数据，计算步骤为：\n\n1）如果被缓存，则从缓存中取的分区的数据；\n\n2）如果被checkpoint，则从checkpoint处恢复数据；\n\n3）根据血缘关系计算分区的数据。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","RDD"],"categories":["大数据"]},{"title":"Spark之RDD理论篇","url":"/2019/05/27/Spark之RDD理论篇/","content":"\n {{ \"Spark的基石RDD\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## RDD与MapReduce\n\nSpark的编程模型是弹性分布式数据集(Resilient Distributed Dataset,RDD),它是MapReduce的扩展和延申,解决了MapReduce的缺陷:在并行计算阶段高效地进行数据共享.运行高效的数据共享概念和类似于MapReduce操作方式,使并行计算高效运行。Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527204519.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527204541.png)\n\n### RDD\n\nRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。在 Spark 中，对数据的所有操作不外乎创建 RDD、转化已有RDD 以及调用 RDD 操作进行求值。每个 RDD 都被分为多个分区，这些分区运行在集群中的不同节点上。RDD 可以包含 Python、Java、Scala 中任意类型的对象， 甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。\n\nRDD支持两种操作:转化操作和行动操作。RDD 的转化操作是返回一个新的 RDD的操作，比如 map()和 filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如 count() 和 first()。 \n\nSpark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark 的 RDD 会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个 RDD，可以使用 RDD.persist() 让 Spark 把这个 RDD 缓存下来。\n\n#### 属性\n\n一组分片（Partition）：数据集的基本组成单位。对于RDD每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。\n\n一个计算每个分区的函数：Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。\n\nRDD之间的依赖关系：RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。\n\n一个Partitioner：即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。\n\n 一个列表：存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。\n\n#### 弹性\n\n`自动进行内存和磁盘数据存储的切换`：  Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换。\n`基于血统的高效容错机制`： 在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。\n\n`Task如果失败会自动进行特定次数的重试`：RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。\n\n`Stage如果失败会自动进行特定次数的重试`：  如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。\n\n`  如果Job的某个Stage阶段计算失败`，框架也会自动进行任务的重新计算，默认次数也是4次：RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除\n\n`数据调度弹性`：Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。\n\n`数据分片的高度弹性`： 可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。\n\n#### 特点\n\n##### 分区\n\nRDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527210835.png)\n\n##### 只读\n\nRDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211304.png)\n\n 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211347.png)\n\n RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211453.png)\n\n##### 依赖\n\nRDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。依赖包括两种，一种是窄依赖，RDDs之间分区是一 一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527211742.png)\n\n通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。\n\n##### 缓存\n\n如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。\n\n#####  CheckPoint\n\n虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。\n\n给定一个RDD我们至少可以知道如下几点信息：\n\n1、分区数以及分区方式；\n\n2、由父RDDs衍生而来的相关依赖信息；\n\n3、计算每个分区的数据，计算步骤为：\n\n1）如果被缓存，则从缓存中取的分区的数据；\n\n2）如果被checkpoint，则从checkpoint处恢复数据；\n\n3）根据血缘关系计算分区的数据。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spark","RDD"],"categories":["大数据"]},{"title":"Spark生态圈及安装","url":"/2019/05/26/Spark生态圈及安装/","content":"\n {{ \"Spark生态圈的简单介绍和安装\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## Spark\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526214751.png)\n\n2009年由马泰·扎哈里亚在加州伯克利分校的AMPLab实现开发的子项目,经过开源捐给了Apache基金会,最后成为了我们熟悉的Apache Spark,Spark式式由Scala语言实现的专门为大规模数据处理而设计的快速通用的计算引擎,经过多年的发展势头迅猛,当然,Flink的出现,也将打破Spark在流式计算的一些短板.后续会更新FLink相关的学习记录.\n\nSpark生态系统已经发展成为一个包含多个子项目的集合，其中包含SparkSQL、Spark Streaming、GraphX、MLib、SparkR等子项目，Spark是`基于内存计算的大数据并行计算框架`。除了扩展了广泛使用的 MapReduce 计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark 适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark使我们可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析 过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分别管理的负担。 \n\n大一统的软件栈，各个组件关系密切并且可以相互调用，这种设计有几个好处：\n\n1、软件栈中所有的程序库和高级组件都可以从下层的改进中获益。\n\n2、运行整个软件栈的代价变小了。不需要运 行 5 到10 套独立的软件系统了，一个机构只需要运行一套软件系统即可。系统的部署、维护、测试、支持等大大缩减。\n\n3、能够构建出无缝整合不同处理模型的应用。\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526215518.png)\n\n### <font color='red'>Spark Core</font>\n\n实现了 Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core 中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的 API 定义。\n\n### <font color='red'>Spark SQL</font>\n\n是 Spark 用来操作结构化数据的程序包。通过 Spark SQL，我们可以使用 SQL 或者 Apache Hive 版本的 SQL 方言(HQL)来查询数据。Spark SQL 支持多种数据源，比如 Hive 表、Parquet 以及 JSON 等。\n\n### <font color='red'>Spark Streaming</font>\n\n是 Spark 提供的对实时数据进行流式计算的组件。提供了用来操作数据流的 API，并且与 Spark Core 中的 RDD API 高度对应。\n\n### <font color='red'>Spark MLlib</font>\n提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。 \n\n### <font color='red'>集群管理器</font>\nSpark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark 支持在各种集群管理器(cluster manager)上运行，包括 Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度器，叫作独立调度器。 \n\n## 特点\n\n### <font color='red'>快</font>\n与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。\n\n### <font color='red'>易用</font>\nSpark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。\n\n### <font color='red'>通用</font>\nSpark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。\n\n### <font color='red'>兼容性</font>\nSpark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管 理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。\n\n## 用户\n\n我们大致把Spark的用例分为两类：数据科学应用和数据处理应用。也就对应的有两种人群：数据科学家和工程师。\n\n### <font color='red'>数据科学任务</font>\n主要是数据分析领域，数据科学家要负责分析数据并建模，具备 SQL、统计、预测建模(机器学习)等方面的经验，以及一定的使用 Python、 Matlab 或 R 语言进行编程的能力。\n\n### <font color='red'>数据处理应用</font>\n工程师定义为使用 Spark 开发 生产环境中的数据处理应用的软件开发者，通过对接Spark的API实现对处理的处理和转换等任务。\n\n## 集群角色\n\n从物理部署层面上来看，Spark主要分为两种类型的节点，Master节点和Worker节点：Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526220332.png)\n\n从Spark程序运行的层面来看，Spark主要分为驱动器节点和执行器节点。\n\n## 运行模式\n\n### Local\n\n 所有计算都运行在一个线程当中，没有任何并行计算，测试学习练习使用。\n\nlocal[K]: 指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力;\n\n`local[*]`: 这种模式直接帮你按照cpu最多cores来设置线程数了。\n\n### Standalone\n\n构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526220740.png)\n\n### YarnSpark\n\n客户端直接连接Yarn；不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，\n\n主要区别在于：Driver程序的运行节点。\n\nyarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。\n\nyarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526220903.png)\n\n### Mesos\n\nSpark客户端直接连接Mesos；不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。\n\n## Spark2.X新特性\n\n### 精简的API\n\n1. 统一的DataFrame和DataSet接口。统\n2. 一Scala和Java的DataFrame、Dataset接口，在R和Python中缺乏安全类型,DataFrame成为主要的程序接口。\n3. 新增SparkSession入口，SparkSession替代原来的SQLContext和HiveContext作为DataFrame和Dataset的入口函数。SQLContext和HiveContext保持向后兼容。\n4. 为SparkSession通过全新的工作流式配置。\n5. 更易用、更高效的计算接口。\n6. DataSet中的聚合操作有全新的、改进的聚合接口。\n\n### Spark作为编译器\n\nSpark2.0搭载了第二代Tungsten引擎，该引擎根据现代编译器与MPP数据库的理念来构建的，它将这些理念用于数据处理中，其中的主要的思想就是在运行时使用优化的字节码，将整体查询合称为单个函数，不再使用虚拟函数调用，而是利用CPU来注册中间数据。效果得到了很大的提升\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526222559.png)\n\n### 智能化程度\n\n为了实现Spark更快、更轻松、更智能的目标、Spark2.X再许多模块上都做了更新，比如Structred Streaming 引入了低延迟的连续处理(Continuous Processing)、支持Stream-steam Joins、通过Pandas UDFs的性能提升PySpark、支持4种调度引擎：Kubernets Clusters 、Standalone、YARN、Mesos。\n\n## 安装\n\n上传并解压spark安装包\n\n```shell\ntar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/ #解压到指定目录\nmv spark-2.1.1-bin-hadoop2.7/ spark #重命名spark\n```\n\n进入spark安装目录下的conf文件夹\n\n```sh\ncd spark/conf/\n```\n\n修改slave文件，添加work节点\n\n```shell\n[hadoop@datanode1 conf]$ vim slaves\ndatanode1\ndatanode2\ndatanode3\n```\n\n修改spark-env.sh文件\n\n```shell\n[hadoop@datanode1 conf]$ cp spark-env.sh.template spark-env.sh\n[hadoop@datanode1 conf]$ vim spark-env.sh\n######################                     配置如下                     ######################\n# Options for the daemons used in the standalone deploy mode\nSPARK_MASTER_HOST=datanode1  #指定Master\nSPARK_MASTER_PORT=7077      #指定Master端口\n```\n\n分发\n\n```shell\nxsync spark/\n```\n\n如果遇到这样的问题\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526224233.png)\n\n我们需要设置一下JAVA_HOME，需要再sbin目录下的spark-config.sh 文件中加入JAVA_HOME的路径\n\n```shell\nvim /opt/module/spark/sbin/spark-config.sh \n\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526225637.png)\n\n访问datanode1:8080即可访问\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527115650.png)\n\n## 测试\n\n```shell\n bin/spark-submit \\       \n--class org.apache.spark.examples.SparkPi \\            #主类\n--master spark://datanode1:7077 \\                      #master\n--executor-memory 1G \\\t\t\t\t\t\t\t    #任务的资源指定内存为1G\n--total-executor-cores 2 \\\t\t\t\t\t\t\t#使用cpu核数\t\t\t\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\t\t  #jar包\n100\t\t\t\t\t\t\t\t\t\t\t\t  #蒙特卡罗算法迭代次数\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526230433.png)\n\n```shell\n./bin/spark-submit \\ \n--class <main-class> # 应用的启动类 (如 org.apache.spark.examples.SparkPi)\n--master <master-url> \\ #指定Master的地址\n--deploy-mode <deploy-mode> \\  #是否发布你的驱动到worker节点(cluster) 或者作为一个本地客户端 (client) (default: client)*\n--conf <key>=<value> \\ # 任意的Spark配置属性， 格式key=value. 如果值包含空格，可以加引号“key=value” \n... # other options\n<application-jar> \\ #打包好的应用jar,包含依赖. 这个URL在集群中全局可见。 比如hdfs:// 共享存储系统， 如果是 file:// path， 那么所有的节点的path都包含同样的jar\napplication-arguments: 传给main()方法的参数\n--executor-memory 1G 指定每个executor可用内存为1G\n--total-executor-cores 2 指定每个executor使用的cup核数为2个\n```\n\n可以粗略的计算出PI大致为\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526230454.png)\n\n再启动spark shell的时候我们也可以指定Spark的Master如果我们不指定的话，则使用的使local模式\n\n```java\nbin/spark-shell \\\n--master spark://datanode1:7077 \\\n--executor-memory 1g \\\n--total-executor-cores 2\n```\n\nSpark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可 \n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526231126.png)\n\n因此我们可以\n\n```scala\nsc.textFile(\"file:///opt/module/spark/word.txt\").flatMap(_.split(\" \")).map((_,1)).reduceByKey(_+_).collect\n```\n\n准备数据\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526232053.png)\n\n由于使分布式启动我们需要把数据同步一下\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526233811.png)\n\n## JobHistoryServer\n\n修改spark-default.conf.template名称\n\n```shell\n[hadoop@datanode1 conf]$  mv spark-defaults.conf.template spark-defaults.conf\n#修改下面配置 确保HDFS开启\nspark.master                     spark://datanode1:7077\nspark.eventLog.enabled           true\nspark.eventLog.dir               hdfs://datanode1:9000/sparklog\n```\n\n<font color='red'>注意：HDFS上的目录需要提前存在。</font>\n\n修改spark-env.sh文件，添加如下配置\n\n```shell\nexport SPARK_HISTORY_OPTS=\"-Dspark.history.ui.port=4000 \n-Dspark.history.retainedApplications=3 \n-Dspark.history.fs.logDirectory=hdfs://datanode1:9000/sparklog\"\n```\n\n启动历史服务器\n\n```shell\nsbin/start-history-server.sh\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190526235029.png)\n\n我们再次执行任务\n\n```shell\nbin/spark-submit \\\n--class org.apache.spark.examples.SparkPi \\\n--master spark://datanode1:7077 \\\n--executor-memory 1G \\\n--total-executor-cores 2 \\\n./examples/jars/spark-examples_2.11-2.1.1.jar \\\n100\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527000603.png)\n\n任务过程中会出现这样的界面，任务完成后我们可以查看\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527000746.png)\n\n同时在HDFS上也会生成日志\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527000818.png)\n\n## HA高可用\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527001032.png)\n\n1.首先我们要确保zookeeper正常安装并启动(具体参阅本人博客)\n\n```shell\nzkstart\n```\n\n修改spark-env.sh的配置文件\n\n```shell\n#注释以下内容\n\n#SPARK_MASTER_HOST=datanode1\n#SPARK_MASTER_PORT=7077\n\n#添加以下内容\nexport SPARK_DAEMON_JAVA_OPTS=\"\n-Dspark.deploy.recoveryMode=ZOOKEEPER\n-Dspark.deploy.zookeeper.url=datanode1,datanode2,datanode3\n-Dspark.deploy.zookeeper.dir=/spark\"\n```\n\n分发配置\n\n```shell\nxsync spark-env.sh\n```\n\ndatanode1节点上启动所有节点\n\n```shell\n[hadoop@datanode1 spark]$ sbin/start-all.sh\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527121554.png)\n\ndatanode2启动master\n\n```shell\n[hadoop@datanode2 spark]$ sbin/start-all.sh\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527121833.png)\n\nsparkHA访问集群\n\n```shell\n /opt/module/spark/bin/spark-shell --master spark://datanode1:7077,datanode2:7077 --executor-memory 1g --total-executor-cores 1\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527122708.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527122807.png)\n\n我们在datanode2节点模拟节点出现故障\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527123554.png)\n\n任务依旧可以执行。\n\n## YARN\n\n修改hadoop配置文件yarn-site.xml,添加如下内容：\n\n```xml\n <!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->\n<property>\n      <name>yarn.nodemanager.pmem-check-enabled</name>\n      <value>false</value>\n</property>\n<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true -->\n <property>\n        <name>yarn.nodemanager.vmem-check-enabled</name>\n        <value>false</value>\n</property>\n```\n\n修改spark-env.sh，添加如下配置：\n\n```shell\nYARN_CONF_DIR=/opt/module/hadoop/etc/hadoop  \nHADOOP_CONF_DIR=/opt/module/hadoop/etc/hadoop \n```\n\n同步以下配置文件(脚本参考Hadoop篇)\n\n```shell\nxsync /opt/module/hadoop/etc/hadoop/yarn-site.xml\nxsync /opt/module/spark/conf/spark-env.sh\n```\n\n启动HDFS和YARN集群\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527131327.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527131423.png)\n\n## IDEA环境配置\n\nspark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。\n\n首先我们先创建一个Maven的父项目\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.hph</groupId>\n    <artifactId>spark</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <modules>\n        <module>sparkcore</module>\n        <module>sparksql</module>\n        <module>sparkGraphx</module>\n    </modules>\n\n    <!-- 表明当前项目是一个父项目，没有具体代码，只有声明的共有信息 -->\n    <packaging>pom</packaging>\n\n    <!-- 声明公有的属性 -->\n    <properties>\n        <spark.version>2.1.1</spark.version>\n        <scala.version>2.11.8</scala.version>\n        <log4j.version>1.2.17</log4j.version>\n        <slf4j.version>1.7.22</slf4j.version>\n    </properties>\n\n    <!-- 声明并引入公有的依赖 -->\n    <dependencies>\n        <!-- Logging -->\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>jcl-over-slf4j</artifactId>\n            <version>${slf4j.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-api</artifactId>\n            <version>${slf4j.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>${slf4j.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>log4j</groupId>\n            <artifactId>log4j</artifactId>\n            <version>${log4j.version}</version>\n        </dependency>\n        <!-- Logging End -->\n        <dependency>\n            <groupId>org.scala-lang</groupId>\n            <artifactId>scala-library</artifactId>\n            <version>${scala.version}</version>\n            <!--<scope>provided</scope>-->\n        </dependency>\n\n    </dependencies>\n\n    <!-- 仅声明公有的依赖 -->\n    <dependencyManagement>\n        <dependencies>\n            <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core -->\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-core_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!-- 编译环境能用，运行环境不可用 -->\n                <!--<scope>provided</scope>-->\n            </dependency>\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-sql_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!-- 编译环境能用，运行环境不可用 -->\n                <!--<scope>provided</scope>-->\n            </dependency>\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-streaming_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!--<scope>provided</scope>-->\n            </dependency>\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-streaming_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!--<scope>provided</scope>-->\n            </dependency>\n\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-core_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!--<scope>provided</scope>-->\n            </dependency>\n\n            <dependency>\n                <groupId>org.apache.spark</groupId>\n                <artifactId>spark-graphx_2.11</artifactId>\n                <version>${spark.version}</version>\n                <!-- 编译环境能用，运行环境不可用 -->\n                <!--<scope>provided</scope>-->\n            </dependency>\n\n\n\n        </dependencies>\n    </dependencyManagement>\n\n    <!-- 配置构建信息 -->\n    <build>\n\n        <!-- 声明并引入构建的插件 -->\n        <plugins>\n            <!-- 设置项目的编译版本 -->\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-compiler-plugin</artifactId>\n                <version>3.6.1</version>\n                <configuration>\n                    <source>1.8</source>\n                    <target>1.8</target>\n                </configuration>\n            </plugin>\n\n            <!-- 用于编译Scala代码到class -->\n            <plugin>\n                <groupId>net.alchim31.maven</groupId>\n                <artifactId>scala-maven-plugin</artifactId>\n                <version>3.2.2</version>\n                <executions>\n                    <execution>\n                        <goals>\n                            <goal>compile</goal>\n                            <goal>testCompile</goal>\n                        </goals>\n                    </execution>\n                </executions>\n            </plugin>\n\n        </plugins>\n\n        <!-- 仅声明构建的插件 -->\n        <pluginManagement>\n\n            <plugins>\n                <plugin>\n                    <groupId>org.apache.maven.plugins</groupId>\n                    <artifactId>maven-assembly-plugin</artifactId>\n                    <version>3.0.0</version>\n                    <executions>\n                        <execution>\n                            <id>make-assembly</id>\n                            <phase>package</phase>\n                            <goals>\n                                <goal>single</goal>\n                            </goals>\n                        </execution>\n                    </executions>\n                </plugin>\n            </plugins>\n\n        </pluginManagement>\n\n    </build>\n\n</project>\n\n```\n\n在创建一个Maven的子项目sparkcore，在sparkcore中创建spark-wordcount项目\n\n### sparkcore\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>spark</artifactId>\n        <groupId>com.hph</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>spark-core</artifactId>\n    <packaging>pom</packaging>\n    <modules>\n        <module>spark-wordcount</module>\n    </modules>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.apache.spark</groupId>\n            <artifactId>spark-core_2.11</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n```\n\n#### spark-wordcount\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>spark-core</artifactId>\n        <groupId>com.hph</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>spark-wordcount</artifactId>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-assembly-plugin</artifactId>\n                <configuration>\n                    <archive>\n                        <manifest>\n                            <mainClass>com.hph.WordCount</mainClass>\n                        </manifest>\n                    </archive>\n                    <descriptorRefs>\n                        <descriptorRef>jar-with-dependencies</descriptorRef>\n                    </descriptorRefs>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\n```scala\npackage com.hph\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n    //声明配置\n    val sparkConf = new SparkConf().setAppName(\"WordCount\")\n\n    //创建SparkContext\n    val sc = new SparkContext(sparkConf)\n\n\n    //设置日志等级\n    sc.setLogLevel(\"INFO\")\n    //读取输入的文件路径\n    val file = sc.textFile(args(0))\n    //对输入的文本信息进行分割压平\n    val words = file.flatMap(_.split(\" \"))\n    //对文本信息进行映射成K,1  \n    val word2Count = words.map((_, 1))\n    //相同的Key相加  \n    val result = word2Count.reduceByKey(_ + _)\n     //输入存储路径\n    result.saveAsTextFile(args(1))\n\t\n     //关闭资源\n    sc.stop()\n\n  }\n}\n```\n\n打包将我们的包更名为wordcunt.jar执行命令\n\n```shell\nspark-submit --class com.hph.WordCount --master spark://datanode1:7077 --executor-memory 1G --total-executor-cores 2 spark-wordcount-1.0-SNAPSHOT.jar hdfs://datanode1:9000//input/test.txt  hdfs://datanode1:9000//output/SPARK_WordCount\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527193259.png)\n\n当然这种就打包就比较麻烦因此我们可以尝试以下别的方法来运行以下。\n\n#### 远程运行\n\n```scala\npackage com.hph\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n    //配置用户名\n    val properties = System.getProperties\n    properties.setProperty(\"HADOOP_USER_NAME\", \"hadoop\")\n    //声明配置\n    val sparkConf = new SparkConf().setAppName(\"WordCount\").setMaster(\"spark://datanode1:7077\")\n      .setJars(List(\"E:\\\\spark2\\\\sparkcore\\\\spark-wordcount\\\\target\\\\spark-wordcount-1.0-SNAPSHOT.jar\"))\n      .setIfMissing(\"spark.driver.host\", \"192.168.1.1\")\n\n    //创建SparkContext\n    val sc = new SparkContext(sparkConf)\n\n\n    //设置日志等级\n    sc.setLogLevel(\"INFO\")\n    //业务处理\n    val file = sc.textFile(\"hdfs://datanode1:9000/input/test.txt\")\n    val words = file.flatMap(_.split(\" \"))\n    val word2Count = words.map((_, 1))\n    val result = word2Count.reduceByKey(_ + _)\n    result.saveAsTextFile(\"hdfs://datanode1:9000/output/Spark_Driver_On_W10\")\n\n    sc.stop()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527194605.png)\n\n这个相当于在W10上执行了任务，宿主机Windos当作了Driver。\n\n#### 本地调试\n\n```scala\npackage com.hph\n\nimport org.apache.spark.{SparkConf, SparkContext}\n\nobject WordCount {\n\n  def main(args: Array[String]): Unit = {\n    //配置用户名\n    val properties = System.getProperties\n    properties.setProperty(\"HADOOP_USER_NAME\", \"hadoop\")\n    //声明配置\n    val sparkConf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\")\n//      .setJars(List(\"E:\\\\spark2\\\\sparkcore\\\\spark-wordcount\\\\target\\\\spark-wordcount-1.0-SNAPSHOT.jar\"))\n//      .setIfMissing(\"spark.driver.host\", \"192.168.1.1\")\n\n    //创建SparkContext\n    val sc = new SparkContext(sparkConf)\n\n\n    //设置日志等级\n    sc.setLogLevel(\"INFO\")\n    //业务处理\n    val file = sc.textFile(\"D:\\\\input\\\\words.txt\")\n    val words = file.flatMap(_.split(\" \"))\n    val word2Count = words.map((_, 1))\n    val result = word2Count.reduceByKey(_ + _)\n    result.saveAsTextFile(\"D:\\\\output\\\\SPARK_ON_local\")\n\n    sc.stop()\n\n  }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527195233.png)\n\n如果你遇到了错误\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527195302.png)\n\n可以尝试以下方法修复\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/20190527195331.png)\n\n\n\n\n\n","tags":["Spark"],"categories":["大数据"]},{"title":"数据结构之哈希表","url":"/2019/05/26/数据结构之哈希表/","content":"\n {{ \"哈希表的相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n\n\n## 哈希表\n\n散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。\n\n给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。\n\n## 哈希函数\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526104304.png)\n\n哈希表是时间与空间之间的平衡,因此哈希函数的设计很重要。所以哈希函数应该尽量减少Hash冲突。也就是说“键”通过哈希函数得到的“索引”分布越均匀越好。\n\n### 整形\n\n对于小范围的整数比如`-100~100`,我们完全可以对整数直接使用把它映射到`0~200`之间,而对于身份证这种的大整数,通常我们采用的是取模,比如大整数的后四位相当于`mod 10000`,这里有一个小问题,如果我们选择的数字如果不好的话,就有可能可能导致数据映射分布不均匀,因此我们最好寻找一个素数.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526105544.png)\n\n如果选择一个合适的素数呢,这里有一个选择:\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526105723.png)\n\n图片来源:<https://planetmath.org/goodhashtableprimes>\n\n### 浮点型\n\n在计算机中都是32位或者64位的二进制表表示,只不过计算j级解析成了浮点数\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526115339.png)\n\n### 字符串\n\n字符串我们需要把它也转成整型处理\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526115537.png)\n\n我们对上面的方法进行优化一下,这就涉及到数学方面的知识了\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526115755.png)\n\n对于字符串来说,计算出来的大整形如果特别大的话可能会出现内存溢出,因此我们可以对取模的过程分别挪到式子里面.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526120023.png)\n\n对于整个字符串来说我们可以写程序也是十分容易地写出来他的处理函数\n\n```java\n        int hash = 0;\n        for (int i = 0; i < s.length; i++) {\n            hash = (hash * B + s.charAt(i)) % M;\n```\n\n###  案例\n\n```java\npublic class Student {\n\n    int grade;\n    int cls;\n    String firstName;\n    String lastName;\n\n    Student(int grade, int cls, String firstName, String lastName){\n        this.grade = grade;\n        this.cls = cls;\n        this.firstName = firstName;\n        this.lastName = lastName;\n    }\n\n    @Override\n    public int hashCode(){\n\n        int B = 31;\n        int hash = 0;\n        hash = hash * B + ((Integer)grade).hashCode();\n        hash = hash * B + ((Integer)cls).hashCode();\n        hash = hash * B + firstName.toLowerCase().hashCode();\n        hash = hash * B + lastName.toLowerCase().hashCode();\n        return hash;\n    }\n\n    //重写\n    @Override\n    public boolean equals(Object o){\n\n        if(this == o)\n            return true;\n\n        if(o == null)\n            return false;\n\n        if(getClass() != o.getClass())\n            return false;\n\n        Student another = (Student)o;\n        return this.grade == another.grade &&\n                this.cls == another.cls &&\n                this.firstName.toLowerCase().equals(another.firstName.toLowerCase()) &&\n                this.lastName.toLowerCase().equals(another.lastName.toLowerCase());\n    }\n\n    @Override\n    public String toString() {\n        return \"Student{\" +\n                \"grade=\" + grade +\n                \", cls=\" + cls +\n                \", firstName='\" + firstName + '\\'' +\n                \", lastName='\" + lastName + '\\'' +\n                '}';\n    }\n}\n```\n\n```java\nimport java.util.HashSet;\nimport java.util.HashMap;\n\npublic class Main {\n\n    public static void main(String[] args) {\n\n        int a = 42;\n        System.out.println(((Integer)a).hashCode());\n\n        int b = -42;\n        System.out.println(((Integer)b).hashCode());\n\n        double c = 3.1415926;\n        System.out.println(((Double)c).hashCode());\n\n        String d = \"imooc\";\n        System.out.println(d.hashCode());\n\n        System.out.println(Integer.MAX_VALUE + 1);\n        System.out.println();\n\n        Student student = new Student(3, 2, \"penghui\", \"Han\");\n        System.out.println(student.hashCode());\n\n        HashSet<Student> set = new HashSet<>();\n        set.add(student);\n        for (Student s : set) {\n            System.out.println(s.toString());\n\n        }\n\n        HashMap<Student, Integer> scores = new HashMap<>();\n        scores.put(student, 100);\n        System.out.println(scores.toString());\n\n        Student student2 = new Student(3, 2, \"Penghui\", \"han\");\n        System.out.println(student2.hashCode());\n        System.out.println(student.equals(student2));\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526152303.png)\n\n我们可以看到在实际的运行过程中对于整数的负数来说,依旧存在整数类型int中,对于浮点数和字符串来说都有都i是按照上面的方法.来进行计算。对于hashCode值相同我们并没有办法取判断是否属于一个对象，因此在equals和hashCode相同鼓的时候我们才能够说这个两个对象是相同的。\n\n## 哈希冲突处理\n\n### 链地址法\n\n哈希表本质就是一个数组，\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526153109.png)\n\n对于哈希表我们只需要让他求出K1然后在模于M，当然这个大M是一个素数。对于负数来说，可以直接用绝对值来解决负数的问题。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526153139.png)\n\n当然我们有时候看别人的代码或者源码的时候会看到\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526153453.png)\n\n用16进制法表示的整型，先和一个16进制表示的`0x7fffffff`的结果我们在对M取模，这个表示的是用二进制表示的话是31个`1`,整型有32位，最高位是符号位，32位和31位相与，这样做是吧最高位的32位，模成了0，符号位的问题我们就解决了。因此如果我们记录的`k1`的值为`4`，那么我们就可可以把k1存储到地址4这个位置中去。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526154202.png)\n\n如果k2的索引位置为1，那么假设k3的位置也为1，那么我们就产生了hash冲突，如何解决呢？\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526154417.png)\n\n这里我们可以采用链表的方式，对于整个哈希表我们开M个空间，由于会出现hash冲突，我们可以把它做成链表，这种方法也叫*separate chaining*，当然我们已可以存红黑树，或者TreeMap。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/哈希表/20190526154901.png)\n\n本质上来说，HashMap就是一个TreeMap数组，HashSet就是一个TreeSet数组。对于Java8来说，Java8之前每一个位置对应的是一个链表，Java8开始之后，当哈希冲突达到了一定的程度，每一个位置从链表转化为红黑树，这个阈值为8；\n\n### 代码实现\n\n```java\nimport java.util.LinkedList;\nimport java.util.List;\n\npublic class HashTable<T>{\n    public HashTable() {\n        this(DEFAULT_TABLE_SIZE);\n    }\n    public HashTable(int size) {\n        theLists=new LinkedList[nextPrime(size)];\n        for(int i=0;i<theLists.length;i++) {\n            theLists[i]=new LinkedList<>();//初始化链表数组\n        }\n    }\n    \n    /*\n     * 哈希表插入元素\n     * */\n    public void insert(T x) {\n        List<T> whichList=theLists[myhash(x)];\n        /*\n         * 如果当前哈希地址的链表不含有元素，则链表中添加该元素\n         * */\n        if(!whichList.contains(x)) {\n            whichList.add(x);\n            if(++currentSize>theLists.length)//如果表长度不够，则扩容\n                rehash();\n        }\n    }\n    public void remove(T x) {\n        List<T> whichList=theLists[myhash(x)];\n        if(whichList.contains(x)) {\n            whichList.remove(x);\n            currentSize--;\n        }\n    }\n    public boolean contains(T x) {\n        List<T> whilchList=theLists[myhash(x)];\n        return whilchList.contains(x);\n    }\n    public void makeEmpty() {\n        for(int i=0;i<theLists.length;i++)\n            theLists[i].clear();\n        currentSize=0;\n    }\n    \n    private static final int DEFAULT_TABLE_SIZE=101;\n    \n    private List<T> [] theLists;\n    private int currentSize;\n    \n    /*\n     * 哈希表扩容，表长度为下一个素数\n     * */\n    private void rehash() {\n        List<T>[] oldLists=theLists;\n        theLists=new List[nextPrime(2*theLists.length)];\n        for(int j=0;j<theLists.length;j++)\n            theLists[j]=new LinkedList<>();\n        \n        currentSize=0;\n        /*\n         * 更新哈希表\n         * */\n        for(List<T> list:oldLists)\n            for(T item:list)\n                insert(item);\n    }\n    /*\n     * myhash()方法获得哈希表的地址\n     * */\n    private int myhash(T x) {\n        int hashVal=x.hashCode();//hashCode()方法返回该对象的哈希码值\n        hashVal%=theLists.length;//对哈希表长度取余数\n        if(hashVal<0)\n            hashVal+=theLists.length;\n        return hashVal;\n    }\n    //下一个素数\n    private static int nextPrime(int n) {\n        if( n % 2 == 0 )\n            n++;\n\n        for( ; !isPrime( n ); n += 2 )\n            ;\n\n        return n;\n    }\n    //判断是否是素数\n    private static boolean isPrime(int n) {\n         if( n == 2 || n == 3 )\n                return true;\n\n            if( n == 1 || n % 2 == 0 )\n                return false;\n\n            for( int i = 3; i * i <= n; i += 2 )\n                if( n % i == 0 )\n                    return false;\n\n            return true;\n    }\n}\n```\n\n### 开放地址法\n\n这个方法的基本思想是：当发生地址冲突时，按照某种方法继续探测哈希表中的其他存储单元，直到找到空位置为止。这个过程可用下式描述： \nH i ( key ) = ( H ( key )+ d i ) mod m ( i = 1,2,…… ， k ( k ≤ m – 1)) \n其中： H ( key ) 为关键字 key 的直接哈希地址， m 为哈希表的长度， di 为每次再探测时的地址增量。 \n采用这种方法时，首先计算出元素的直接哈希地址 H ( key ) ，如果该存储单元已被其他元素占用，则继续查看地址为 H ( key ) + d 2 的存储单元，如此重复直至找到某个存储单元为空时，将关键字为 key 的数据元素存放到该单元。 \n增量 d 可以有不同的取法，并根据其取法有不同的称呼： \n（ 1 ） d i ＝ 1 ， 2 ， 3 ， …… 线性探测再散列； \n（ 2 ） d i ＝ 1^2 ，－ 1^2 ， 2^2 ，－ 2^2 ， k^2， -k^2…… 二次探测再散列； \n（ 3 ） d i ＝ 伪随机序列 伪随机再散列； \n\n例1设有哈希函数 H ( key ) = key mod 7 ，哈希表的地址空间为 0 ～ 6 ，对关键字序列（ 32 ， 13 ， 49 ， 55 ， 22 ， 38 ， 21 ）按线性探测再散列和二次探测再散列的方法分别构造哈希表。 \n解：\n（ 1 ）线性探测再散列： \n32 ％ 7 = 4 ； 13 ％ 7 = 6 ； 49 ％ 7 = 0 ； \n55 ％ 7 = 6 发生冲突，下一个存储地址（ 6 ＋ 1 ）％ 7 ＝ 0 ，仍然发生冲突，再下一个存储地址：（ 6 ＋ 2 ）％ 7 ＝ 1 未发生冲突，可以存入。 \n22 ％ 7 ＝ 1 发生冲突，下一个存储地址是：（ 1 ＋ 1 ）％ 7 ＝ 2 未发生冲突； \n38 ％ 7 ＝ 3 ； \n21 ％ 7 ＝ 0 发生冲突，按照上面方法继续探测直至空间 5 ，不发生冲突，所得到的哈希表对应存储位置： \n下标： 0 1 2 3 4 5 6 \n\n49 55 22 38 32 21 13 \n\n当然还有其他的方法比如`再哈希法`，Coalesced Hashing法（综合了Seperate Chainging 和 Open Addressiing）等。\n\n### 代码实现\n\n```java\nclass DataItem { //数据                             \n   private int iData;    // data item (key)  \n  \n   public DataItem(int ii) {   \n    iData = ii;   \n  }  \n      public int getKey(){  \n       return iData;   \n   }  \n  \n   }    \n  \nclass HashTable{//数组实现的哈希表，开放地址法之线性探测  \n   private DataItem[] hashArray; //存放数据的数组  \n   private int arraySize;  \n   private DataItem nonItem; //用作删除标志  \n  \n   public HashTable(int size) {//构造函数  \n      arraySize = size;  \n      hashArray = new DataItem[arraySize];  \n      nonItem = new DataItem(-1);   // deleted item key is -1  \n   }  \n  \n   public void displayTable(){//显示哈希表  \n      System.out.print(\"Table: \");  \n      for(int j=0; j<arraySize; j++)  \n         {  \n         if(hashArray[j] != null)  \n            System.out.print(hashArray[j].getKey() + \" \");  \n         else  \n            System.out.print(\"** \");  \n         }  \n      System.out.println(\"\");  \n      }  \n  \n   //哈希函数  \n   public int hashFunc(int key)  \n      {  \n      return key % arraySize;        \n      }  \n  \n  \n   //在哈希表中插入数据  \n   public void insert(DataItem item){  \n      int key = item.getKey();      // 获取数据的键值  \n      int hashVal = hashFunc(key);  // 计算其哈希值  \n                                    \n      while(hashArray[hashVal] != null && hashArray[hashVal].getKey() != -1){  \n         ++hashVal;                 // 插入位置被占，线性探测下一位置  \n         hashVal %= arraySize;   // 不让超过数组的大小  \n     }  \n      hashArray[hashVal] = item;  // 找到空位后插入  \n   }    \n  \n   //在哈希表中删除  \n   public DataItem delete(int key) {  \n      int hashVal = hashFunc(key);  // 计算其哈希值  \n  \n      while(hashArray[hashVal] != null){                               \n         if(hashArray[hashVal].getKey() == key){  \n            DataItem temp = hashArray[hashVal]; // 记录已删除的数据  \n            hashArray[hashVal] = nonItem;       // 删除它  \n            return temp;                          \n         }  \n         ++hashVal;  // 到下一单元找  \n         hashVal %= arraySize;      \n      }  \n      return null;    // 没有找到要删除的数据  \n      }   \n  \n   //在哈希表中查找  \n   public DataItem find(int key) {  \n      int hashVal = hashFunc(key);  //哈希这个键  \n  \n      while(hashArray[hashVal] != null) { // 直到空的单元                       \n         if(hashArray[hashVal].getKey() == key)  \n            return hashArray[hashVal];   // 找到  \n         ++hashVal;                 // 去下一单元找  \n         hashVal %= arraySize;      // 不让超过数组的大小  \n         }  \n      return null;  // 没有找到  \n  }  \n  \n}\n```\n\n\n\n\n\n## 参考资料\n\n<https://www.cnblogs.com/vincentme/p/7920237.html>\n\nhttps://blog.csdn.net/w_fenghui/article/details/2010387 \n\n<https://128kj.iteye.com/blog/1744810>\n\n","tags":["哈希表"],"categories":["数据结构"]},{"title":"数据结构之红黑树","url":"/2019/05/23/数据结构之红黑树/","content":"\n {{ \"2-3树和红黑树相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n\n\n红黑树和2-3树本身是等价的,在学习红黑树之前我们不妨去了解一下2-3树的特性。当我们理解了2-3树之后，对于红黑树和通常用于磁盘存储，文件系统，数据库相应的B类树也是有帮助的。\n\n## 2-3树\n\n2-3树是最简单的B-树（或-树）结构，其每个非叶节点都有两个或三个子女，而且所有叶都在统一层上。2-3树不是二叉树(满足二分搜索树的基本性质)，其节点可拥有3个孩子。不过，2-3树与满二叉树相似。高为h的2-3树包含的节点数大于等于高度为h的满二叉树的节点数，即至少有2^h-1个节点。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523223433.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523220039.png)\n\nb的左孩子点小于b的值，bc中间的值再bc之间，c的右孩子的值大于c。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523220622.png)\n\n2-3树是一颗绝对平衡的树：从根节点到任意一个节点所经过的节点数量一定是相同的\n\n### 插入\n\n如果插入2-节点那么\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523222021.png)\n\n如果插入3-节点\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523222055.png)\n\n如果插入3-节点，父亲节点为2-节点的话\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523222740.png)\n\n如果插入3-节点，父亲节点也为3-节点\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523222843.png)\n\n## 红黑树\n\n红黑树（英语：Red–black tree）是一种自平衡二叉查找树，计算机科学中用到的一种数据结构，典型的用途是实现关联数组。它在1972年由鲁道夫·贝尔发明，被称为`对称二叉B树`，它现代的名字源于LeoJ. Guibas和Robert Sedgewick于1978年写的一篇论文。红黑树的结构复杂，但它的操作有着良好的最坏情况运行时间，并且在实践中高效：它可以在 $\\mathrm{O}(\\log n)$时间内完成查找，插入和删除，这里的$n$是树中元素的数目。\n\n### 性质\n\n1. 节点是红色或黑色。\n2. 根是黑色。\n3. 所有叶子都是黑色（叶子是NIL节点）。\n4. 每个红色节点必须有两个黑色的子节点。（从每个叶子到根的所有路径上不能有两个连续的红色节点。）\n5. 从任一节点到其每个叶子的所有`简单路径`都包含相同数目的黑色节点。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190523224136.png)\n\n### Java实现\n\n红黑树的基本操作是**添加**、**删除**和**旋转**。在对红黑树进行添加或删除后，会用到旋转方法。添加或删除红黑树中的节点之后，红黑树就发生了变化，可能不满足红黑树的5条性质，也就不再是一颗红黑树了，而是一颗普通的树。而通过旋转，可以使这颗树重新成为红黑树，旋转的目的是让树保持红黑树的特性。\n旋转包括两种：**左旋** 和 **右旋**。\n\n#### 基本定义\n\n```java\npublic class RBTree<T extends Comparable<T>> {\n\n    private RBTNode<T> mRoot;    // 根结点\n\n    private static final boolean RED   = false;\n    private static final boolean BLACK = true;\n\n    public class RBTNode<T extends Comparable<T>> {\n        boolean color;        // 颜色\n        T key;                // 关键字(键值)\n        RBTNode<T> left;    // 左孩子\n        RBTNode<T> right;    // 右孩子\n        RBTNode<T> parent;    // 父结点\n\n        public RBTNode(T key, boolean color, RBTNode<T> parent, RBTNode<T> left, RBTNode<T> right) {\n            this.key = key;\n            this.color = color;\n            this.parent = parent;\n            this.left = left;\n            this.right = right;\n        }\n\n    }\n    ...\n}\n```\n\n\n\n #### 左旋\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190524115121.png)\n\n对x进行左旋，意味着\"将x变成一个左节点\"。\n\n```java\n/* \n * 对红黑树的节点(x)进行左旋转\n *\n * 左旋示意图(对节点x进行左旋)：\n *      px                              px\n *     /                               /\n *    x                               y                \n *   /  \\      --(左旋)-.           / \\                #\n *  lx   y                          x  ry     \n *     /   \\                       /  \\\n *    ly   ry                     lx  ly  \n *\n *\n */\nprivate void leftRotate(RBTNode<T> x) {\n    // 设置x的右孩子为y\n    RBTNode<T> y = x.right;\n\n    // 将 “y的左孩子” 设为 “x的右孩子”；\n    // 如果y的左孩子非空，将 “x” 设为 “y的左孩子的父亲”\n    x.right = y.left;\n    if (y.left != null)\n        y.left.parent = x;\n\n    // 将 “x的父亲” 设为 “y的父亲”\n    y.parent = x.parent;\n\n    if (x.parent == null) {\n        this.mRoot = y;            // 如果 “x的父亲” 是空节点，则将y设为根节点\n    } else {\n        if (x.parent.left == x)\n            x.parent.left = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”\n        else\n            x.parent.right = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”\n    }\n    \n    // 将 “x” 设为 “y的左孩子”\n    y.left = x;\n    // 将 “x的父节点” 设为 “y”\n    x.parent = y;\n}\n```\n\n\n\n#### 右旋\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190524115243.png)\n\n对y进行左旋，意味着\"将y变成一个右节点\"。\n\n```java\n/* \n * 对红黑树的节点(y)进行右旋转\n *\n * 右旋示意图(对节点y进行左旋)：\n *            py                               py\n *           /                                /\n *          y                                x                  \n *         /  \\      --(右旋)-.            /  \\                     #\n *        x   ry                           lx   y  \n *       / \\                                   / \\                   #\n *      lx  rx                                rx  ry\n * \n */\nprivate void rightRotate(RBTNode<T> y) {\n    // 设置x是当前节点的左孩子。\n    RBTNode<T> x = y.left;\n\n    // 将 “x的右孩子” 设为 “y的左孩子”；\n    // 如果\"x的右孩子\"不为空的话，将 “y” 设为 “x的右孩子的父亲”\n    y.left = x.right;\n    if (x.right != null)\n        x.right.parent = y;\n\n    // 将 “y的父亲” 设为 “x的父亲”\n    x.parent = y.parent;\n\n    if (y.parent == null) {\n        this.mRoot = x;            // 如果 “y的父亲” 是空节点，则将x设为根节点\n    } else {\n        if (y == y.parent.right)\n            y.parent.right = x;    // 如果 y是它父节点的右孩子，则将x设为“y的父节点的右孩子”\n        else\n            y.parent.left = x;    // (y是它父节点的左孩子) 将x设为“x的父节点的左孩子”\n    }\n\n    // 将 “y” 设为 “x的右孩子”\n    x.right = y;\n\n    // 将 “y的父节点” 设为 “x”\n    y.parent = x;\n}\n```\n\n#### 添加\n\n将一个节点插入到红黑树中，需要执行哪些步骤呢？首先，将红黑树当作一颗二叉查找树，将节点插入；然后，将节点着色为红色；最后，通过\"旋转和重新着色\"等一系列操作来修正该树，使之重新成为一颗红黑树。详细描述如下：\n**第一步: 将红黑树当作一颗二叉查找树，将节点插入。**\n       红黑树本身就是一颗二叉查找树，将节点插入后，该树仍然是一颗二叉查找树。也就意味着，树的键值仍然是有序的。此外，无论是左旋还是右旋，若旋转之前这棵树是二叉查找树，旋转之后它一定还是二叉查找树。这也就意味着，任何的旋转和重新着色操作，都不会改变它仍然是一颗二叉查找树的事实。\n好吧？那接下来，我们就来想方设法的旋转以及重新着色，使这颗树重新成为红黑树！\n\n**第二步：将插入的节点着色为\"红色\"。**\n       为什么着色成红色，而不是黑色呢？为什么呢？在回答之前，我们需要重新温习一下红黑树的特性：\n(1) 每个节点或者是黑色，或者是红色。\n(2) 根节点是黑色。\n(3) 每个叶子节点是黑色。 [注意：这里叶子节点，是指为空的叶子节点！]\n(4) 如果一个节点是红色的，则它的子节点必须是黑色的。\n(5) 从一个节点到该节点的子孙节点的所有路径上包含相同数目的黑节点。\n      将插入的节点着色为红色，不会违背\"特性(5)\"！少违背一条特性，就意味着我们需要处理的情况越少。接下来，就要努力的让这棵树满足其它性质即可；满足了的话，它就又是一颗红黑树了。o(∩∩)o...哈哈\n\n**第三步: 通过一系列的旋转或着色等操作，使之重新成为一颗红黑树。**\n       第二步中，将插入节点着色为\"红色\"之后，不会违背\"特性(5)\"。那它到底会违背哪些特性呢？\n       对于\"特性(1)\"，显然不会违背了。因为我们已经将它涂成红色了。\n       对于\"特性(2)\"，显然也不会违背。在第一步中，我们是将红黑树当作二叉查找树，然后执行的插入操作。而根据二叉查找数的特点，插入操作不会改变根节点。所以，根节点仍然是黑色。\n       对于\"特性(3)\"，显然不会违背了。这里的叶子节点是指的空叶子节点，插入非空节点并不会对它们造成影响。\n       对于\"特性(4)\"，是有可能违背的！\n       那接下来，想办法使之\"满足特性(4)\"，就可以将树重新构造成红黑树了。\n\n```java\n/* \n * 将结点插入到红黑树中\n *\n * 参数说明：\n *     node 插入的结点        // 对应《算法导论》中的node\n */\nprivate void insert(RBTNode<T> node) {\n    int cmp;\n    RBTNode<T> y = null;\n    RBTNode<T> x = this.mRoot;\n\n    // 1. 将红黑树当作一颗二叉查找树，将节点添加到二叉查找树中。\n    while (x != null) {\n        y = x;\n        cmp = node.key.compareTo(x.key);\n        if (cmp < 0)\n            x = x.left;\n        else\n            x = x.right;\n    }\n\n    node.parent = y;\n    if (y!=null) {\n        cmp = node.key.compareTo(y.key);\n        if (cmp < 0)\n            y.left = node;\n        else\n            y.right = node;\n    } else {\n        this.mRoot = node;\n    }\n\n    // 2. 设置节点的颜色为红色\n    node.color = RED;\n\n    // 3. 将它重新修正为一颗二叉查找树\n    insertFixUp(node);\n}\n\n/* \n * 新建结点(key)，并将其插入到红黑树中\n *\n * 参数说明：\n *     key 插入结点的键值\n */\npublic void insert(T key) {\n    RBTNode<T> node=new RBTNode<T>(key,BLACK,null,null,null);\n\n    // 如果新建结点失败，则返回。\n    if (node != null)\n        insert(node);\n}\n```\n\n**内部接口** -- insert(node)的作用是将\"node\"节点插入到红黑树中。\n**外部接口** -- insert(key)的作用是将\"key\"添加到红黑树中。\n\n#### 平衡\n\n```java\n/*\n * 红黑树插入修正函数\n *\n * 在向红黑树中插入节点之后(失去平衡)，再调用该函数；\n * 目的是将它重新塑造成一颗红黑树。\n *\n * 参数说明：\n *     node 插入的结点        // 对应《算法导论》中的z\n */\nprivate void insertFixUp(RBTNode<T> node) {\n    RBTNode<T> parent, gparent;\n\n    // 若“父节点存在，并且父节点的颜色是红色”\n    while (((parent = parentOf(node))!=null) && isRed(parent)) {\n        gparent = parentOf(parent);\n\n        //若“父节点”是“祖父节点的左孩子”\n        if (parent == gparent.left) {\n            // Case 1条件：叔叔节点是红色\n            RBTNode<T> uncle = gparent.right;\n            if ((uncle!=null) && isRed(uncle)) {\n                setBlack(uncle);\n                setBlack(parent);\n                setRed(gparent);\n                node = gparent;\n                continue;\n            }\n\n            // Case 2条件：叔叔是黑色，且当前节点是右孩子\n            if (parent.right == node) {\n                RBTNode<T> tmp;\n                leftRotate(parent);\n                tmp = parent;\n                parent = node;\n                node = tmp;\n            }\n\n            // Case 3条件：叔叔是黑色，且当前节点是左孩子。\n            setBlack(parent);\n            setRed(gparent);\n            rightRotate(gparent);\n        } else {    //若“z的父节点”是“z的祖父节点的右孩子”\n            // Case 1条件：叔叔节点是红色\n            RBTNode<T> uncle = gparent.left;\n            if ((uncle!=null) && isRed(uncle)) {\n                setBlack(uncle);\n                setBlack(parent);\n                setRed(gparent);\n                node = gparent;\n                continue;\n            }\n\n            // Case 2条件：叔叔是黑色，且当前节点是左孩子\n            if (parent.left == node) {\n                RBTNode<T> tmp;\n                rightRotate(parent);\n                tmp = parent;\n                parent = node;\n                node = tmp;\n            }\n\n            // Case 3条件：叔叔是黑色，且当前节点是右孩子。\n            setBlack(parent);\n            setRed(gparent);\n            leftRotate(gparent);\n        }\n    }\n\n    // 将根节点设为黑色\n    setBlack(this.mRoot);\n}\n```\n\n#### 删除\n\n将红黑树内的某一个节点删除。需要执行的操作依次是：首先，将红黑树当作一颗二叉查找树，将该节点从二叉查找树中删除；然后，通过\"旋转和重新着色\"等一系列来修正该树，使之重新成为一棵红黑树。详细描述如下：\n**第一步：将红黑树当作一颗二叉查找树，将节点删除。**\n       这和\"删除常规二叉查找树中删除节点的方法是一样的\"。分3种情况：\n① 被删除节点没有儿子，即为叶节点。那么，直接将该节点删除就OK了。\n② 被删除节点只有一个儿子。那么，直接删除该节点，并用该节点的唯一子节点顶替它的位置。\n③ 被删除节点有两个儿子。那么，先找出它的后继节点；然后把“它的后继节点的内容”复制给“该节点的内容”；之后，删除“它的后继节点”。在这里，后继节点相当于替身，在将后继节点的内容复制给\"被删除节点\"之后，再将后继节点删除。这样就巧妙的将问题转换为\"删除后继节点\"的情况了，下面就考虑后继节点。 在\"被删除节点\"有两个非空子节点的情况下，它的后继节点不可能是双子非空。既然\"的后继节点\"不可能双子都非空，就意味着\"该节点的后继节点\"要么没有儿子，要么只有一个儿子。若没有儿子，则按\"情况① \"进行处理；若只有一个儿子，则按\"情况② \"进行处理。\n\n**第二步：通过\"旋转和重新着色\"等一系列来修正该树，使之重新成为一棵红黑树。**\n        因为\"第一步\"中删除节点之后，可能会违背红黑树的特性。所以需要通过\"旋转和重新着色\"来修正该树，使之重新成为一棵红黑树\n\n```java\n/* \n * 删除结点(node)，并返回被删除的结点\n *\n * 参数说明：\n *     node 删除的结点\n */\nprivate void remove(RBTNode<T> node) {\n    RBTNode<T> child, parent;\n    boolean color;\n\n    // 被删除节点的\"左右孩子都不为空\"的情况。\n    if ( (node.left!=null) && (node.right!=null) ) {\n        // 被删节点的后继节点。(称为\"取代节点\")\n        // 用它来取代\"被删节点\"的位置，然后再将\"被删节点\"去掉。\n        RBTNode<T> replace = node;\n\n        // 获取后继节点\n        replace = replace.right;\n        while (replace.left != null)\n            replace = replace.left;\n\n        // \"node节点\"不是根节点(只有根节点不存在父节点)\n        if (parentOf(node)!=null) {\n            if (parentOf(node).left == node)\n                parentOf(node).left = replace;\n            else\n                parentOf(node).right = replace;\n        } else {\n            // \"node节点\"是根节点，更新根节点。\n            this.mRoot = replace;\n        }\n\n        // child是\"取代节点\"的右孩子，也是需要\"调整的节点\"。\n        // \"取代节点\"肯定不存在左孩子！因为它是一个后继节点。\n        child = replace.right;\n        parent = parentOf(replace);\n        // 保存\"取代节点\"的颜色\n        color = colorOf(replace);\n\n        // \"被删除节点\"是\"它的后继节点的父节点\"\n        if (parent == node) {\n            parent = replace;\n        } else {\n            // child不为空\n            if (child!=null)\n                setParent(child, parent);\n            parent.left = child;\n\n            replace.right = node.right;\n            setParent(node.right, replace);\n        }\n\n        replace.parent = node.parent;\n        replace.color = node.color;\n        replace.left = node.left;\n        node.left.parent = replace;\n\n        if (color == BLACK)\n            removeFixUp(child, parent);\n\n        node = null;\n        return ;\n    }\n\n    if (node.left !=null) {\n        child = node.left;\n    } else {\n        child = node.right;\n    }\n\n    parent = node.parent;\n    // 保存\"取代节点\"的颜色\n    color = node.color;\n\n    if (child!=null)\n        child.parent = parent;\n\n    // \"node节点\"不是根节点\n    if (parent!=null) {\n        if (parent.left == node)\n            parent.left = child;\n        else\n            parent.right = child;\n    } else {\n        this.mRoot = child;\n    }\n\n    if (color == BLACK)\n        removeFixUp(child, parent);\n    node = null;\n}\n\n/* \n * 删除结点(z)，并返回被删除的结点\n *\n * 参数说明：\n *     tree 红黑树的根结点\n *     z 删除的结点\n */\npublic void remove(T key) {\n    RBTNode<T> node; \n\n    if ((node = search(mRoot, key)) != null)\n        remove(node);\n}\n```\n\n**内部接口** -- remove(node)的作用是将\"node\"节点插入到红黑树中。\n**外部接口** -- remove(key)删除红黑树中键值为key的节点。\n\n#### 平衡\n\n```java\n/*\n * 红黑树删除修正函数\n *\n * 在从红黑树中删除插入节点之后(红黑树失去平衡)，再调用该函数；\n * 目的是将它重新塑造成一颗红黑树。\n *\n * 参数说明：\n *     node 待修正的节点\n */\nprivate void removeFixUp(RBTNode<T> node, RBTNode<T> parent) {\n    RBTNode<T> other;\n\n    while ((node==null || isBlack(node)) && (node != this.mRoot)) {\n        if (parent.left == node) {\n            other = parent.right;\n            if (isRed(other)) {\n                // Case 1: x的兄弟w是红色的  \n                setBlack(other);\n                setRed(parent);\n                leftRotate(parent);\n                other = parent.right;\n            }\n\n            if ((other.left==null || isBlack(other.left)) &&\n                (other.right==null || isBlack(other.right))) {\n                // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  \n                setRed(other);\n                node = parent;\n                parent = parentOf(node);\n            } else {\n\n                if (other.right==null || isBlack(other.right)) {\n                    // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  \n                    setBlack(other.left);\n                    setRed(other);\n                    rightRotate(other);\n                    other = parent.right;\n                }\n                // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。\n                setColor(other, colorOf(parent));\n                setBlack(parent);\n                setBlack(other.right);\n                leftRotate(parent);\n                node = this.mRoot;\n                break;\n            }\n        } else {\n\n            other = parent.left;\n            if (isRed(other)) {\n                // Case 1: x的兄弟w是红色的  \n                setBlack(other);\n                setRed(parent);\n                rightRotate(parent);\n                other = parent.left;\n            }\n\n            if ((other.left==null || isBlack(other.left)) &&\n                (other.right==null || isBlack(other.right))) {\n                // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  \n                setRed(other);\n                node = parent;\n                parent = parentOf(node);\n            } else {\n\n                if (other.left==null || isBlack(other.left)) {\n                    // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  \n                    setBlack(other.right);\n                    setRed(other);\n                    leftRotate(other);\n                    other = parent.left;\n                }\n\n                // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。\n                setColor(other, colorOf(parent));\n                setBlack(parent);\n                setBlack(other.left);\n                rightRotate(parent);\n                node = this.mRoot;\n                break;\n            }\n        }\n    }\n\n    if (node!=null)\n        setBlack(node);\n}\n```\n\nremoveFixup(node, parent)是对应\"上面所讲的第三步\"。它是一个内部接口。\n\n### 完整代码\n\n```java\n/**\n * Java 语言: 红黑树\n *\n * @author skywang\n * @date 2013/11/07\n */\n\npublic class RBTree<T extends Comparable<T>> {\n\n    private RBTNode<T> mRoot;    // 根结点\n\n    private static final boolean RED   = false;\n    private static final boolean BLACK = true;\n\n    public class RBTNode<T extends Comparable<T>> {\n        boolean color;        // 颜色\n        T key;                // 关键字(键值)\n        RBTNode<T> left;    // 左孩子\n        RBTNode<T> right;    // 右孩子\n        RBTNode<T> parent;    // 父结点\n\n        public RBTNode(T key, boolean color, RBTNode<T> parent, RBTNode<T> left, RBTNode<T> right) {\n            this.key = key;\n            this.color = color;\n            this.parent = parent;\n            this.left = left;\n            this.right = right;\n        }\n\n        public T getKey() {\n            return key;\n        }\n\n        public String toString() {\n            return \"\"+key+(this.color==RED?\"(R)\":\"B\");\n        }\n    }\n\n    public RBTree() {\n        mRoot=null;\n    }\n\n    private RBTNode<T> parentOf(RBTNode<T> node) {\n        return node!=null ? node.parent : null;\n    }\n    private boolean colorOf(RBTNode<T> node) {\n        return node!=null ? node.color : BLACK;\n    }\n    private boolean isRed(RBTNode<T> node) {\n        return ((node!=null)&&(node.color==RED)) ? true : false;\n    }\n    private boolean isBlack(RBTNode<T> node) {\n        return !isRed(node);\n    }\n    private void setBlack(RBTNode<T> node) {\n        if (node!=null)\n            node.color = BLACK;\n    }\n    private void setRed(RBTNode<T> node) {\n        if (node!=null)\n            node.color = RED;\n    }\n    private void setParent(RBTNode<T> node, RBTNode<T> parent) {\n        if (node!=null)\n            node.parent = parent;\n    }\n    private void setColor(RBTNode<T> node, boolean color) {\n        if (node!=null)\n            node.color = color;\n    }\n\n    /*\n     * 前序遍历\"红黑树\"\n     */\n    private void preOrder(RBTNode<T> tree) {\n        if(tree != null) {\n            System.out.print(tree.key+\" \");\n            preOrder(tree.left);\n            preOrder(tree.right);\n        }\n    }\n\n    public void preOrder() {\n        preOrder(mRoot);\n    }\n\n    /*\n     * 中序遍历\"红黑树\"\n     */\n    private void inOrder(RBTNode<T> tree) {\n        if(tree != null) {\n            inOrder(tree.left);\n            System.out.print(tree.key+\" \");\n            inOrder(tree.right);\n        }\n    }\n\n    public void inOrder() {\n        inOrder(mRoot);\n    }\n\n\n    /*\n     * 后序遍历\"红黑树\"\n     */\n    private void postOrder(RBTNode<T> tree) {\n        if(tree != null)\n        {\n            postOrder(tree.left);\n            postOrder(tree.right);\n            System.out.print(tree.key+\" \");\n        }\n    }\n\n    public void postOrder() {\n        postOrder(mRoot);\n    }\n\n\n    /*\n     * (递归实现)查找\"红黑树x\"中键值为key的节点\n     */\n    private RBTNode<T> search(RBTNode<T> x, T key) {\n        if (x==null)\n            return x;\n\n        int cmp = key.compareTo(x.key);\n        if (cmp < 0)\n            return search(x.left, key);\n        else if (cmp > 0)\n            return search(x.right, key);\n        else\n            return x;\n    }\n\n    public RBTNode<T> search(T key) {\n        return search(mRoot, key);\n    }\n\n    /*\n     * (非递归实现)查找\"红黑树x\"中键值为key的节点\n     */\n    private RBTNode<T> iterativeSearch(RBTNode<T> x, T key) {\n        while (x!=null) {\n            int cmp = key.compareTo(x.key);\n\n            if (cmp < 0) \n                x = x.left;\n            else if (cmp > 0) \n                x = x.right;\n            else\n                return x;\n        }\n\n        return x;\n    }\n\n    public RBTNode<T> iterativeSearch(T key) {\n        return iterativeSearch(mRoot, key);\n    }\n\n    /* \n     * 查找最小结点：返回tree为根结点的红黑树的最小结点。\n     */\n    private RBTNode<T> minimum(RBTNode<T> tree) {\n        if (tree == null)\n            return null;\n\n        while(tree.left != null)\n            tree = tree.left;\n        return tree;\n    }\n\n    public T minimum() {\n        RBTNode<T> p = minimum(mRoot);\n        if (p != null)\n            return p.key;\n\n        return null;\n    }\n     \n    /* \n     * 查找最大结点：返回tree为根结点的红黑树的最大结点。\n     */\n    private RBTNode<T> maximum(RBTNode<T> tree) {\n        if (tree == null)\n            return null;\n\n        while(tree.right != null)\n            tree = tree.right;\n        return tree;\n    }\n\n    public T maximum() {\n        RBTNode<T> p = maximum(mRoot);\n        if (p != null)\n            return p.key;\n\n        return null;\n    }\n\n    /* \n     * 找结点(x)的后继结点。即，查找\"红黑树中数据值大于该结点\"的\"最小结点\"。\n     */\n    public RBTNode<T> successor(RBTNode<T> x) {\n        // 如果x存在右孩子，则\"x的后继结点\"为 \"以其右孩子为根的子树的最小结点\"。\n        if (x.right != null)\n            return minimum(x.right);\n\n        // 如果x没有右孩子。则x有以下两种可能：\n        // (01) x是\"一个左孩子\"，则\"x的后继结点\"为 \"它的父结点\"。\n        // (02) x是\"一个右孩子\"，则查找\"x的最低的父结点，并且该父结点要具有左孩子\"，找到的这个\"最低的父结点\"就是\"x的后继结点\"。\n        RBTNode<T> y = x.parent;\n        while ((y!=null) && (x==y.right)) {\n            x = y;\n            y = y.parent;\n        }\n\n        return y;\n    }\n     \n    /* \n     * 找结点(x)的前驱结点。即，查找\"红黑树中数据值小于该结点\"的\"最大结点\"。\n     */\n    public RBTNode<T> predecessor(RBTNode<T> x) {\n        // 如果x存在左孩子，则\"x的前驱结点\"为 \"以其左孩子为根的子树的最大结点\"。\n        if (x.left != null)\n            return maximum(x.left);\n\n        // 如果x没有左孩子。则x有以下两种可能：\n        // (01) x是\"一个右孩子\"，则\"x的前驱结点\"为 \"它的父结点\"。\n        // (01) x是\"一个左孩子\"，则查找\"x的最低的父结点，并且该父结点要具有右孩子\"，找到的这个\"最低的父结点\"就是\"x的前驱结点\"。\n        RBTNode<T> y = x.parent;\n        while ((y!=null) && (x==y.left)) {\n            x = y;\n            y = y.parent;\n        }\n\n        return y;\n    }\n\n    /* \n     * 对红黑树的节点(x)进行左旋转\n     *\n     * 左旋示意图(对节点x进行左旋)：\n     *      px                              px\n     *     /                               /\n     *    x                               y                \n     *   /  \\      --(左旋)-.           / \\                #\n     *  lx   y                          x  ry     \n     *     /   \\                       /  \\\n     *    ly   ry                     lx  ly  \n     *\n     *\n     */\n    private void leftRotate(RBTNode<T> x) {\n        // 设置x的右孩子为y\n        RBTNode<T> y = x.right;\n\n        // 将 “y的左孩子” 设为 “x的右孩子”；\n        // 如果y的左孩子非空，将 “x” 设为 “y的左孩子的父亲”\n        x.right = y.left;\n        if (y.left != null)\n            y.left.parent = x;\n\n        // 将 “x的父亲” 设为 “y的父亲”\n        y.parent = x.parent;\n\n        if (x.parent == null) {\n            this.mRoot = y;            // 如果 “x的父亲” 是空节点，则将y设为根节点\n        } else {\n            if (x.parent.left == x)\n                x.parent.left = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”\n            else\n                x.parent.right = y;    // 如果 x是它父节点的左孩子，则将y设为“x的父节点的左孩子”\n        }\n        \n        // 将 “x” 设为 “y的左孩子”\n        y.left = x;\n        // 将 “x的父节点” 设为 “y”\n        x.parent = y;\n    }\n\n    /* \n     * 对红黑树的节点(y)进行右旋转\n     *\n     * 右旋示意图(对节点y进行左旋)：\n     *            py                               py\n     *           /                                /\n     *          y                                x                  \n     *         /  \\      --(右旋)-.            /  \\                     #\n     *        x   ry                           lx   y  \n     *       / \\                                   / \\                   #\n     *      lx  rx                                rx  ry\n     * \n     */\n    private void rightRotate(RBTNode<T> y) {\n        // 设置x是当前节点的左孩子。\n        RBTNode<T> x = y.left;\n\n        // 将 “x的右孩子” 设为 “y的左孩子”；\n        // 如果\"x的右孩子\"不为空的话，将 “y” 设为 “x的右孩子的父亲”\n        y.left = x.right;\n        if (x.right != null)\n            x.right.parent = y;\n\n        // 将 “y的父亲” 设为 “x的父亲”\n        x.parent = y.parent;\n\n        if (y.parent == null) {\n            this.mRoot = x;            // 如果 “y的父亲” 是空节点，则将x设为根节点\n        } else {\n            if (y == y.parent.right)\n                y.parent.right = x;    // 如果 y是它父节点的右孩子，则将x设为“y的父节点的右孩子”\n            else\n                y.parent.left = x;    // (y是它父节点的左孩子) 将x设为“x的父节点的左孩子”\n        }\n\n        // 将 “y” 设为 “x的右孩子”\n        x.right = y;\n\n        // 将 “y的父节点” 设为 “x”\n        y.parent = x;\n    }\n\n    /*\n     * 红黑树插入修正函数\n     *\n     * 在向红黑树中插入节点之后(失去平衡)，再调用该函数；\n     * 目的是将它重新塑造成一颗红黑树。\n     *\n     * 参数说明：\n     *     node 插入的结点        // 对应《算法导论》中的z\n     */\n    private void insertFixUp(RBTNode<T> node) {\n        RBTNode<T> parent, gparent;\n\n        // 若“父节点存在，并且父节点的颜色是红色”\n        while (((parent = parentOf(node))!=null) && isRed(parent)) {\n            gparent = parentOf(parent);\n\n            //若“父节点”是“祖父节点的左孩子”\n            if (parent == gparent.left) {\n                // Case 1条件：叔叔节点是红色\n                RBTNode<T> uncle = gparent.right;\n                if ((uncle!=null) && isRed(uncle)) {\n                    setBlack(uncle);\n                    setBlack(parent);\n                    setRed(gparent);\n                    node = gparent;\n                    continue;\n                }\n\n                // Case 2条件：叔叔是黑色，且当前节点是右孩子\n                if (parent.right == node) {\n                    RBTNode<T> tmp;\n                    leftRotate(parent);\n                    tmp = parent;\n                    parent = node;\n                    node = tmp;\n                }\n\n                // Case 3条件：叔叔是黑色，且当前节点是左孩子。\n                setBlack(parent);\n                setRed(gparent);\n                rightRotate(gparent);\n            } else {    //若“z的父节点”是“z的祖父节点的右孩子”\n                // Case 1条件：叔叔节点是红色\n                RBTNode<T> uncle = gparent.left;\n                if ((uncle!=null) && isRed(uncle)) {\n                    setBlack(uncle);\n                    setBlack(parent);\n                    setRed(gparent);\n                    node = gparent;\n                    continue;\n                }\n\n                // Case 2条件：叔叔是黑色，且当前节点是左孩子\n                if (parent.left == node) {\n                    RBTNode<T> tmp;\n                    rightRotate(parent);\n                    tmp = parent;\n                    parent = node;\n                    node = tmp;\n                }\n\n                // Case 3条件：叔叔是黑色，且当前节点是右孩子。\n                setBlack(parent);\n                setRed(gparent);\n                leftRotate(gparent);\n            }\n        }\n\n        // 将根节点设为黑色\n        setBlack(this.mRoot);\n    }\n\n    /* \n     * 将结点插入到红黑树中\n     *\n     * 参数说明：\n     *     node 插入的结点        // 对应《算法导论》中的node\n     */\n    private void insert(RBTNode<T> node) {\n        int cmp;\n        RBTNode<T> y = null;\n        RBTNode<T> x = this.mRoot;\n\n        // 1. 将红黑树当作一颗二叉查找树，将节点添加到二叉查找树中。\n        while (x != null) {\n            y = x;\n            cmp = node.key.compareTo(x.key);\n            if (cmp < 0)\n                x = x.left;\n            else\n                x = x.right;\n        }\n\n        node.parent = y;\n        if (y!=null) {\n            cmp = node.key.compareTo(y.key);\n            if (cmp < 0)\n                y.left = node;\n            else\n                y.right = node;\n        } else {\n            this.mRoot = node;\n        }\n\n        // 2. 设置节点的颜色为红色\n        node.color = RED;\n\n        // 3. 将它重新修正为一颗二叉查找树\n        insertFixUp(node);\n    }\n\n    /* \n     * 新建结点(key)，并将其插入到红黑树中\n     *\n     * 参数说明：\n     *     key 插入结点的键值\n     */\n    public void insert(T key) {\n        RBTNode<T> node=new RBTNode<T>(key,BLACK,null,null,null);\n\n        // 如果新建结点失败，则返回。\n        if (node != null)\n            insert(node);\n    }\n\n\n    /*\n     * 红黑树删除修正函数\n     *\n     * 在从红黑树中删除插入节点之后(红黑树失去平衡)，再调用该函数；\n     * 目的是将它重新塑造成一颗红黑树。\n     *\n     * 参数说明：\n     *     node 待修正的节点\n     */\n    private void removeFixUp(RBTNode<T> node, RBTNode<T> parent) {\n        RBTNode<T> other;\n\n        while ((node==null || isBlack(node)) && (node != this.mRoot)) {\n            if (parent.left == node) {\n                other = parent.right;\n                if (isRed(other)) {\n                    // Case 1: x的兄弟w是红色的  \n                    setBlack(other);\n                    setRed(parent);\n                    leftRotate(parent);\n                    other = parent.right;\n                }\n\n                if ((other.left==null || isBlack(other.left)) &&\n                    (other.right==null || isBlack(other.right))) {\n                    // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  \n                    setRed(other);\n                    node = parent;\n                    parent = parentOf(node);\n                } else {\n\n                    if (other.right==null || isBlack(other.right)) {\n                        // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  \n                        setBlack(other.left);\n                        setRed(other);\n                        rightRotate(other);\n                        other = parent.right;\n                    }\n                    // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。\n                    setColor(other, colorOf(parent));\n                    setBlack(parent);\n                    setBlack(other.right);\n                    leftRotate(parent);\n                    node = this.mRoot;\n                    break;\n                }\n            } else {\n\n                other = parent.left;\n                if (isRed(other)) {\n                    // Case 1: x的兄弟w是红色的  \n                    setBlack(other);\n                    setRed(parent);\n                    rightRotate(parent);\n                    other = parent.left;\n                }\n\n                if ((other.left==null || isBlack(other.left)) &&\n                    (other.right==null || isBlack(other.right))) {\n                    // Case 2: x的兄弟w是黑色，且w的俩个孩子也都是黑色的  \n                    setRed(other);\n                    node = parent;\n                    parent = parentOf(node);\n                } else {\n\n                    if (other.left==null || isBlack(other.left)) {\n                        // Case 3: x的兄弟w是黑色的，并且w的左孩子是红色，右孩子为黑色。  \n                        setBlack(other.right);\n                        setRed(other);\n                        leftRotate(other);\n                        other = parent.left;\n                    }\n\n                    // Case 4: x的兄弟w是黑色的；并且w的右孩子是红色的，左孩子任意颜色。\n                    setColor(other, colorOf(parent));\n                    setBlack(parent);\n                    setBlack(other.left);\n                    rightRotate(parent);\n                    node = this.mRoot;\n                    break;\n                }\n            }\n        }\n\n        if (node!=null)\n            setBlack(node);\n    }\n\n    /* \n     * 删除结点(node)，并返回被删除的结点\n     *\n     * 参数说明：\n     *     node 删除的结点\n     */\n    private void remove(RBTNode<T> node) {\n        RBTNode<T> child, parent;\n        boolean color;\n\n        // 被删除节点的\"左右孩子都不为空\"的情况。\n        if ( (node.left!=null) && (node.right!=null) ) {\n            // 被删节点的后继节点。(称为\"取代节点\")\n            // 用它来取代\"被删节点\"的位置，然后再将\"被删节点\"去掉。\n            RBTNode<T> replace = node;\n\n            // 获取后继节点\n            replace = replace.right;\n            while (replace.left != null)\n                replace = replace.left;\n\n            // \"node节点\"不是根节点(只有根节点不存在父节点)\n            if (parentOf(node)!=null) {\n                if (parentOf(node).left == node)\n                    parentOf(node).left = replace;\n                else\n                    parentOf(node).right = replace;\n            } else {\n                // \"node节点\"是根节点，更新根节点。\n                this.mRoot = replace;\n            }\n\n            // child是\"取代节点\"的右孩子，也是需要\"调整的节点\"。\n            // \"取代节点\"肯定不存在左孩子！因为它是一个后继节点。\n            child = replace.right;\n            parent = parentOf(replace);\n            // 保存\"取代节点\"的颜色\n            color = colorOf(replace);\n\n            // \"被删除节点\"是\"它的后继节点的父节点\"\n            if (parent == node) {\n                parent = replace;\n            } else {\n                // child不为空\n                if (child!=null)\n                    setParent(child, parent);\n                parent.left = child;\n\n                replace.right = node.right;\n                setParent(node.right, replace);\n            }\n\n            replace.parent = node.parent;\n            replace.color = node.color;\n            replace.left = node.left;\n            node.left.parent = replace;\n\n            if (color == BLACK)\n                removeFixUp(child, parent);\n\n            node = null;\n            return ;\n        }\n\n        if (node.left !=null) {\n            child = node.left;\n        } else {\n            child = node.right;\n        }\n\n        parent = node.parent;\n        // 保存\"取代节点\"的颜色\n        color = node.color;\n\n        if (child!=null)\n            child.parent = parent;\n\n        // \"node节点\"不是根节点\n        if (parent!=null) {\n            if (parent.left == node)\n                parent.left = child;\n            else\n                parent.right = child;\n        } else {\n            this.mRoot = child;\n        }\n\n        if (color == BLACK)\n            removeFixUp(child, parent);\n        node = null;\n    }\n\n    /* \n     * 删除结点(z)，并返回被删除的结点\n     *\n     * 参数说明：\n     *     tree 红黑树的根结点\n     *     z 删除的结点\n     */\n    public void remove(T key) {\n        RBTNode<T> node; \n\n        if ((node = search(mRoot, key)) != null)\n            remove(node);\n    }\n\n    /*\n     * 销毁红黑树\n     */\n    private void destroy(RBTNode<T> tree) {\n        if (tree==null)\n            return ;\n\n        if (tree.left != null)\n            destroy(tree.left);\n        if (tree.right != null)\n            destroy(tree.right);\n\n        tree=null;\n    }\n\n    public void clear() {\n        destroy(mRoot);\n        mRoot = null;\n    }\n\n    /*\n     * 打印\"红黑树\"\n     *\n     * key        -- 节点的键值 \n     * direction  --  0，表示该节点是根节点;\n     *               -1，表示该节点是它的父结点的左孩子;\n     *                1，表示该节点是它的父结点的右孩子。\n     */\n    private void print(RBTNode<T> tree, T key, int direction) {\n\n        if(tree != null) {\n\n            if(direction==0)    // tree是根节点\n                System.out.printf(\"%2d(B) is root\\n\", tree.key);\n            else                // tree是分支节点\n                System.out.printf(\"%2d(%s) is %2d's %6s child\\n\", tree.key, isRed(tree)?\"R\":\"B\", key, direction==1?\"right\" : \"left\");\n\n            print(tree.left, tree.key, -1);\n            print(tree.right,tree.key,  1);\n        }\n    }\n\n    public void print() {\n        if (mRoot != null)\n            print(mRoot, mRoot.key, 0);\n    }\n}\n```\n\n### 测试\n\n```java\n/**\n * Java 语言: 二叉查找树\n *\n * @author skywang\n * @date 2013/11/07\n */\npublic class RBTreeTest {\n\n    private static final int a[] = {10, 40, 30, 60, 90, 70, 20, 50, 80};\n    private static final boolean mDebugInsert = false;    // \"插入\"动作的检测开关(false，关闭；true，打开)\n    private static final boolean mDebugDelete = false;    // \"删除\"动作的检测开关(false，关闭；true，打开)\n\n    public static void main(String[] args) {\n        int i, ilen = a.length;\n        RBTree<Integer> tree=new RBTree<Integer>();\n\n        System.out.printf(\"== 原始数据: \");\n        for(i=0; i<ilen; i++)\n            System.out.printf(\"%d \", a[i]);\n        System.out.printf(\"\\n\");\n\n        for(i=0; i<ilen; i++) {\n            tree.insert(a[i]);\n            // 设置mDebugInsert=true,测试\"添加函数\"\n            if (mDebugInsert) {\n                System.out.printf(\"== 添加节点: %d\\n\", a[i]);\n                System.out.printf(\"== 树的详细信息: \\n\");\n                tree.print();\n                System.out.printf(\"\\n\");\n            }\n        }\n\n        System.out.printf(\"== 前序遍历: \");\n        tree.preOrder();\n\n        System.out.printf(\"\\n== 中序遍历: \");\n        tree.inOrder();\n\n        System.out.printf(\"\\n== 后序遍历: \");\n        tree.postOrder();\n        System.out.printf(\"\\n\");\n\n        System.out.printf(\"== 最小值: %s\\n\", tree.minimum());\n        System.out.printf(\"== 最大值: %s\\n\", tree.maximum());\n        System.out.printf(\"== 树的详细信息: \\n\");\n        tree.print();\n        System.out.printf(\"\\n\");\n\n        // 设置mDebugDelete=true,测试\"删除函数\"\n        if (mDebugDelete) {\n            for(i=0; i<ilen; i++)\n            {\n                tree.remove(a[i]);\n\n                System.out.printf(\"== 删除节点: %d\\n\", a[i]);\n                System.out.printf(\"== 树的详细信息: \\n\");\n                tree.print();\n                System.out.printf(\"\\n\");\n            }\n        }\n        // 销毁二叉树\n        tree.clear();\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/20190524205857.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/红黑树/红黑树插入.gif)\n\n## 参考资料\n\n《玩转数据结构》\n\n<https://www.cnblogs.com/skywang12345/p/3624343.html>\n\n\n\n\n\n\n\n\n\n\n","tags":["红黑树"],"categories":["数据结构"]},{"title":"数据结构之AVL树","url":"/2019/05/22/数据结构之AVL树/","content":"\n {{ \"AVL树相关学习与实现\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\n**AVL树**是最早被发明的`自平衡二叉查找树`。在AVL树中，任一节点对应的两棵子树的最大高度差为1，因此它也被称为**高度平衡树**。查找、插入和删除在平均和最坏情况下的时间复杂度(都是$O(\\log n)$。增加和删除元素的操作则可能需要借由一次或多次`树旋转`，以实现树的重新平衡。AVL树得名于它的发明者**G. M. Adelson-Velsky**和**Evgenii Landis**，他们在1962年的论文《An algorithm for the organization of information》中公开了这一数据结构。\n\n节点的**平衡因子**是它的左子树的高度减去它的右子树的高度（有时相反）。带有平衡因子1、0或 -1的节点被认为是平衡的。带有平衡因子 -2或2的节点被认为是不平衡的，并需要重新平衡这个树。平衡因子可以直接存储在每个节点中，或从可能存储在节点中的子树高度计算出来。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522123959.png)\n\n## 删除\n\n从AVL树中删除，可以通过把要删除的节点向下旋转成一个叶子节点，接着直接移除这个叶子节点来完成。因为在旋转成叶子节点期间最多有个节点被旋转，而每次AVL旋转耗费固定的时间，所以删除处理在整体上耗费$\\mathrm{O}(\\log n)$时间。\n\n## 搜索\n\n可以像普通二叉查找树一样的进行，所以耗费时间$O(\\log n)$，因为AVL树总是保持平衡的。不需要特殊的准备，树的结构不会由于查找而改变。\n\n## 平衡操作\n\n假设平衡因子是左子树的高度减去右子树的高度所得到的值，又假设由于在二叉排序树上插入节点而失去平衡的最小子树根节点的指针为a（即a是离插入点最近，且平衡因子绝对值超过1的祖先节点），则失去平衡后进行的规律可归纳为下列四种情况：\n\n1. 单向右旋平衡处理LL：由于在*a的左子树根节点的左子树上插入节点，*a的平衡因子由1增至2，致使以*a为根的子树失去平衡，则需进行一次右旋转操作；\n2. 单向左旋平衡处理RR：由于在*a的右子树根节点的右子树上插入节点，*a的平衡因子由-1变为-2，致使以*a为根的子树失去平衡，则需进行一次左旋转操作；\n3. 双向旋转（先左后右）平衡处理LR：由于在*a的左子树根节点的右子树上插入节点，*a的平衡因子由1增至2，致使以*a为根的子树失去平衡，则需进行两次旋转（先左旋后右旋）操作。\n4. 双向旋转（先右后左）平衡处理RL：由于在*a的右子树根节点的左子树上插入节点，*a的平衡因子由-1变为-2，致使以*a为根的子树失去平衡，则需进行两次旋转（先右旋后左旋）操作。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522115224.png)\n\n在平衡的二叉排序树BBST (Balancing Binary Search Tree)上插入一个新的数据元素e的递归算法可描述如下：\n\n1. 若BBST为空树，则插入一个数据元素为e的新节点作为BBST的根节点，树的深度增1；\n2. 若e的关键字和BBST的根节点的关键字相等，则不进行；\n3. 若e的关键字小于BBST的根节点的关键字，而且在BBST的左子树中不存在和e有相同关键字的节点，则将e插入在BBST的左子树上，并且当插入之后的左子树深度增加（+1）时，分别就下列不同情况处理之：\n    1. BBST的根节点的平衡因子为-1（右子树的深度大于左子树的深度，则将根节点的平衡因子更改为0，BBST的深度不变；\n    2. BBST的根节点的平衡因子为0（左、右子树的深度相等）：则将根节点的平衡因子更改为1，BBST的深度增1；\n    3. BBST的根节点的平衡因子为1（左子树的深度大于右子树的深度）：则若BBST的左子树根节点的平衡因子为1：则需进行单向右旋平衡处理，并且在右旋处理之后，将根节点和其右子树根节点的平衡因子更改为0，树的深度不变；\n4. 若e的关键字大于BBST的根节点的关键字，而且在BBST的右子树中不存在和e有相同关键字的节点，则将e插入在BBST的右子树上，并且当插入之后的右子树深度增加（+1）时，分别就不同情况处理之。\n\n## Java实现\n\n### 节点\n\n```java\npublic class AVLTree<T extends Comparable<T>> {\n    private AVLTreeNode<T> mRoot;    // 根结点\n\n    // AVL树的节点(内部类)\n    class AVLTreeNode<T extends Comparable<T>> {\n        T key;                // 关键字(键值) -->排序使用\n        int height;         // 高度\n        AVLTreeNode<T> left;    // 左孩子\n        AVLTreeNode<T> right;    // 右孩子\n\n        public AVLTreeNode(T key, AVLTreeNode<T> left, AVLTreeNode<T> right) {\n            this.key = key;\n            this.left = left;\n            this.right = right;\n            this.height = 0;\n        }\n    }\n    \n}\n```\n\n### 高度\n\n```java\n/*\n * 获取树的高度\n */\nprivate int height(AVLTreeNode<T> tree) {\n    if (tree != null)\n        return tree.height;\n\n    return 0;\n}\n\npublic int height() {\n    return height(mRoot);\n}\n```\n\n### 比较大小\n\n```java\n/*\n * 比较两个值的大小\n */\nprivate int max(int a, int b) {\n    return a>b ? a : b;\n}\n```\n\n### 旋转\n\n#### LL的旋转\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522122401.png)\n\n```java\n/*\n * LL：左左对应的情况(左单旋转)。\n *\n * 返回值：旋转后的根节点\n */\nprivate AVLTreeNode<T> leftLeftRotation(AVLTreeNode<T> k2) {\n    AVLTreeNode<T> k1;\n\n    k1 = k2.left;\n    k2.left = k1.right;\n    k1.right = k2;\n\n    k2.height = max( height(k2.left), height(k2.right)) + 1;\n    k1.height = max( height(k1.left), k2.height) + 1;\n\n    return k1;\n}\n```\n\n#### RR的旋转\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522122726.png)\n\n```java\n/*\n * RR：右右对应的情况(右单旋转)。\n *\n * 返回值：旋转后的根节点\n */\nprivate AVLTreeNode<T> rightRightRotation(AVLTreeNode<T> k1) {\n    AVLTreeNode<T> k2;\n\n    k2 = k1.right;\n    k1.right = k2.left;\n    k2.left = k1;\n\n    k1.height = max( height(k1.left), height(k1.right)) + 1;\n    k2.height = max( height(k2.right), k1.height) + 1;\n\n    return k2;\n}\n```\n\n#### LR的旋转\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522122838.png)\n\n```java\n/*\n * LR：左右对应的情况(左双旋转)。\n *\n * 返回值：旋转后的根节点\n */\nprivate AVLTreeNode<T> leftRightRotation(AVLTreeNode<T> k3) {\n    k3.left = rightRightRotation(k3.left);\n\n    return leftLeftRotation(k3);\n}\n```\n\n#### RL的旋转\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522123047.png)\n\n```java\n/*\n * RL：右左对应的情况(右双旋转)。\n *\n * 返回值：旋转后的根节点\n */\nprivate AVLTreeNode<T> rightLeftRotation(AVLTreeNode<T> k1) {\n    k1.right = leftLeftRotation(k1.right);\n\n    return rightRightRotation(k1);\n}\n```\n\n#### 删除\n\n```java\n/* \n * 删除结点(z)，返回根节点\n *\n * 参数说明：\n *     tree AVL树的根结点\n *     z 待删除的结点\n * 返回值：\n *     根节点\n */\nprivate AVLTreeNode<T> remove(AVLTreeNode<T> tree, AVLTreeNode<T> z) {\n    // 根为空 或者 没有要删除的节点，直接返回null。\n    if (tree==null || z==null)\n        return null;\n\n    int cmp = z.key.compareTo(tree.key);\n    if (cmp < 0) {        // 待删除的节点在\"tree的左子树\"中\n        tree.left = remove(tree.left, z);\n        // 删除节点后，若AVL树失去平衡，则进行相应的调节。\n        if (height(tree.right) - height(tree.left) == 2) {\n            AVLTreeNode<T> r =  tree.right;\n            if (height(r.left) > height(r.right))\n                tree = rightLeftRotation(tree);\n            else\n                tree = rightRightRotation(tree);\n        }\n    } else if (cmp > 0) {    // 待删除的节点在\"tree的右子树\"中\n        tree.right = remove(tree.right, z);\n        // 删除节点后，若AVL树失去平衡，则进行相应的调节。\n        if (height(tree.left) - height(tree.right) == 2) {\n            AVLTreeNode<T> l =  tree.left;\n            if (height(l.right) > height(l.left))\n                tree = leftRightRotation(tree);\n            else\n                tree = leftLeftRotation(tree);\n        }\n    } else {    // tree是对应要删除的节点。\n        // tree的左右孩子都非空\n        if ((tree.left!=null) && (tree.right!=null)) {\n            if (height(tree.left) > height(tree.right)) {\n                // 如果tree的左子树比右子树高；\n                // 则(01)找出tree的左子树中的最大节点\n                //   (02)将该最大节点的值赋值给tree。\n                //   (03)删除该最大节点。\n                // 这类似于用\"tree的左子树中最大节点\"做\"tree\"的替身；\n                // 采用这种方式的好处是：删除\"tree的左子树中最大节点\"之后，AVL树仍然是平衡的。\n                AVLTreeNode<T> max = maximum(tree.left);\n                tree.key = max.key;\n                tree.left = remove(tree.left, max);\n            } else {\n                // 如果tree的左子树不比右子树高(即它们相等，或右子树比左子树高1)\n                // 则(01)找出tree的右子树中的最小节点\n                //   (02)将该最小节点的值赋值给tree。\n                //   (03)删除该最小节点。\n                // 这类似于用\"tree的右子树中最小节点\"做\"tree\"的替身；\n                // 采用这种方式的好处是：删除\"tree的右子树中最小节点\"之后，AVL树仍然是平衡的。\n                AVLTreeNode<T> min = maximum(tree.right);\n                tree.key = min.key;\n                tree.right = remove(tree.right, min);\n            }\n        } else {\n            AVLTreeNode<T> tmp = tree;\n            tree = (tree.left!=null) ? tree.left : tree.right;\n            tmp = null;\n        }\n    }\n\n    return tree;\n}\n\npublic void remove(T key) {\n    AVLTreeNode<T> z; \n\n    if ((z = search(mRoot, key)) != null)\n        mRoot = remove(mRoot, z);\n}\n```\n\n### 测试\n\n```java\n/**\n * Java 语言: AVL树\n *\n * @author skywang\n * @date 2013/11/07\n */\n\npublic class AVLTreeTest {\n    private static int arr[]= {3,2,1,4,5,6,7,16,15,14,13,12,11,10,8,9};\n\n    public static void main(String[] args) {\n        int i;\n        AVLTree<Integer> tree = new AVLTree<Integer>();\n\n        System.out.printf(\"== 依次添加: \");\n        for(i=0; i<arr.length; i++) {\n            System.out.printf(\"%d \", arr[i]);\n            tree.insert(arr[i]);\n        }\n\n        System.out.printf(\"\\n== 前序遍历: \");\n        tree.preOrder();\n\n        System.out.printf(\"\\n== 中序遍历: \");\n        tree.inOrder();\n\n        System.out.printf(\"\\n== 后序遍历: \");\n        tree.postOrder();\n        System.out.printf(\"\\n\");\n\n        System.out.printf(\"== 高度: %d\\n\", tree.height());\n        System.out.printf(\"== 最小值: %d\\n\", tree.minimum());\n        System.out.printf(\"== 最大值: %d\\n\", tree.maximum());\n        System.out.printf(\"== 树的详细信息: \\n\");\n        tree.print();\n\n        i = 8;\n        System.out.printf(\"\\n== 删除根节点: %d\", i);\n        tree.remove(i);\n\n        System.out.printf(\"\\n== 高度: %d\", tree.height());\n        System.out.printf(\"\\n== 中序遍历: \");\n        tree.inOrder();\n        System.out.printf(\"\\n== 树的详细信息: \\n\");\n        tree.print();\n\n        // 销毁二叉树\n        tree.destroy();\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522123616.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/AVL树/20190522123633.png)\n\n## 参考资料\n\n《维基百科》\n\n<https://www.cnblogs.com/skywang12345/p/3577479.html>\n\n\n\n\n\n","tags":["AVL树"],"categories":["数据结构"]},{"title":"数据结构之并查集","url":"/2019/05/14/数据结构之并查集/","content":"\n {{ \"数据结构并查集的学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n并查集\n\n在计算机科学中， **并查集**是一种树型的数据结构 ，用于处理一些不交集（Disjoint Sets）的合并及查询问题。 有一个**联合-查找算法** （ **union-find algorithm** ）定义了两个用于此数据结构的操作：\n\n- Find：确定元素属于哪一个子集。 它可以被用来确定两个元素是否属于同一子集。\n- Union：将两个子集合并成同一个集合。\n\n由于支持这两种操作，一个不相交集也常被称为联合-查找数据结构（union-find data structure）或合并-查找集合（merge-find set）。 其他的重要方法，MakeSet，用于建立单元素集合。 有了这些方法，许多经典的划分问题可以被解决。\n\n为了更加精确的定义这些方法，需要定义如何表示集合。 一种常用的策略是为每个集合选定一个固定的元素，称为代表，以表示整个集合。 接着，Find(x) 返回x 所属集合的代表，而Union 使用两个集合的代表作为参数。\n\n### UF\n\n```java\npublic interface UF {\n    int getSize();\n    boolean isConnected(int p,int q);\n    void UnionElements(int p,int q);\n}\n```\n\n### QuickFind\n\n```java\n//第一版Union-Find 查询为O(1)级别十分块 缺点:Union的时间复杂度为O(n)\npublic class UnionFind1 implements UF {\n    private int[] id;\n\n    public UnionFind1(int size) {\n        id = new int[size];\n\n        for (int i = 0; i < id.length; i++) {\n            id[i] = i;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return id.length;\n    }\n\n    //查找元素p所对应的集合编号\n    private int find(int p) {\n        if (p < 0 && p >= id.length) {\n            throw new IllegalArgumentException(\"p  is out of bound.\");\n        }\n        return id[p];\n    }\n\n    //查看元素p和元素q是否属于一个集合\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n    //合并元素p和元素q是否属于一个集合\n    @Override\n    public void UnionElements(int p, int q) {\n        int pID = find(p);\n        int qID = find(q);\n\n        if (pID == qID) {\n            return;\n        }\n        for (int i = 0; i < id.length; i++) {\n            if (id[i] == pID) {\n                id[i] = qID;\n            }\n        }\n    }\n}\n```\n\n```java\n//第二版Union-Find\npublic class UniomFind2 implements UF {\n    private int[] parent;\n\n    public UniomFind2(int size) {\n\n        parent = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        parent[pRoot] = qRoot;\n    }\n}\n\n```\n\n### QuickUnion\n\n```java\n//第二版Union-Find 将每一个元素看作一个节点,节点之间相连接我们看成一个树结构,,孩子指向父亲.通常的实现思路\npublic class UniomFind2 implements UF {\n    private int[] parent;\n\n    public UniomFind2(int size) {\n\n        parent = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        parent[pRoot] = qRoot;\n    }\n}\n```\n\n### 测试\n\n```java\nimport java.util.Random;\n\npublic class Main {\n    private static double testUF(UF uf, int m) {\n        int size = uf.getSize();\n        Random random = new Random();\n\n        long startTime = System.nanoTime();\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.UnionElements(a, b);\n        }\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.isConnected(a, b);\n        }\n\n        long endTime = System.nanoTime();\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n        int size = 100000;\n        int m = 100000;\n        UnionFind1 uf1 = new UnionFind1(size);\n        System.out.println(\"UnionFind1: \" + testUF(uf1, m) + \"s\");\n\n        UniomFind2 uf2 = new UniomFind2(size);\n        System.out.println(\"UnionFind2: \" + testUF(uf2, m) + \"s\");\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516204544.png)\n\n#### 分析:\n\nUnionFind1整体使用的是一个数组,合并操作,是对连续的内存进行一次循环操作,JVM,对此有很好的优化,对于UnionFind2来说,查询的过程是一个不断索引的过程,不是顺次访问一片连续的内存空间,要在不同的地址之间进行跳转.因此速度会慢一些.在UnionFInd2中Find的时间复杂度为O(h)级别的,操作数越大Union中更多的元素被组织到了一个集合中去.这样树会非常大,\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/并查集退化.gif)\n\n但是又有退化成链表的风险.\n\n### Size优化\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/size优化.gif)\n\n```java\n//第三版Union-Find    Size优化 降低树高\npublic class UniomFind3 implements UF {\n    private int[] parent;\n    private int[] sz; //sz[i]表示以i为根的集合中元素个数\n\n\n    public UniomFind3(int size) {\n\n        parent = new int[size];\n        sz = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n            sz[i] = 1;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        //根据两个元素所在树的元素个数不同判断合并方向\n        //将元素个数少的集合合并到元素个数多的集合\n        if (sz[pRoot] < sz[qRoot]) {\n            parent[pRoot] = qRoot;\n            sz[pRoot] += sz[qRoot];\n        } else {\n            parent[qRoot] = pRoot;\n            sz[pRoot] += sz[qRoot];\n        }\n    }\n}\n```\n\n#### 测试\n\n```java\nimport java.util.Random;\n\npublic class Main {\n    private static double testUF(UF uf, int m) {\n        int size = uf.getSize();\n        Random random = new Random();\n\n        long startTime = System.nanoTime();\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.UnionElements(a, b);\n        }\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.isConnected(a, b);\n        }\n\n        long endTime = System.nanoTime();\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n        int size = 100000;\n        int m = 100000;\n        UnionFind1 uf1 = new UnionFind1(size);\n        System.out.println(\"UnionFind1: \" + testUF(uf1, m) + \"s\");\n\n        UniomFind2 uf2 = new UniomFind2(size);\n        System.out.println(\"UnionFind2: \" + testUF(uf2, m) + \"s\");\n        UniomFind3 uf3 = new UniomFind3(size);\n  \t\tSystem.out.println(\"UnionFind3: \" + testUF(uf3, m) + \"s\");\n\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516211621.png)\n\n### 分析:\n\n在UnionFInd3中,我们可以确保树的深度是十分浅的,虽然时间复杂度为O(h)但是h很小因此,效率会很高.对于UnionFind2来说,在极端情况下,会退化成一个链表.\n\n### Rank优化\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/rank优化.gif)\n\n```java\n//第四版Union-Find  基于Rank优化\npublic class UniomFind4 implements UF {\n    private int[] parent;\n    private int[] rank; //rank[i]表示以i为根的集合所表示的树的层数\n\n    public UniomFind4(int size) {\n\n        parent = new int[size];\n        rank = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n            rank[i] = 1;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        //根据两个元素所在树的rank不同判断合并方向\n        //rank低的集合合并到rank高的集合上\n        if (rank[pRoot] < rank[qRoot]) {\n            parent[pRoot] = qRoot;\n            //qRoot的树高没有发生变化\n        } else if (rank[qRoot] < rank[pRoot]) {\n            parent[qRoot] = pRoot;\n        } else {  //rank[qRoot] == rank[pRoot]\n            parent[qRoot] = pRoot;\n            rank[pRoot] += 1;\n        }\n    }\n}\n```\n\n#### 测试\n\n```java\nimport java.util.Random;\n\npublic class Main {\n    private static double testUF(UF uf, int m) {\n        int size = uf.getSize();\n        Random random = new Random();\n\n        long startTime = System.nanoTime();\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.UnionElements(a, b);\n        }\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.isConnected(a, b);\n        }\n\n        long endTime = System.nanoTime();\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n        int size = 10000000;\n        int m = 10000000;\n        UniomFind3 uf3 = new UniomFind3(size);\n\n        System.out.println(\"UnionFind3: \" + testUF(uf3, m) + \"s\");\n        UniomFind4 uf4 = new UniomFind4(size);\n        System.out.println(\"UnionFind4: \" + testUF(uf4, m) + \"s\");\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516212847.png)\n\n对于千万级别的数据的,由于实现并查集的时候,基于Rank的要比基于Size逻辑上更加合理,因此我们大多情况下实现一个并查集是基于Rank实现的.\n\n###  路径压缩\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/路径压缩.gif)\n\n```java\n//第五版Union-Find 路径压缩\npublic class UniomFind5 implements UF {\n    private int[] parent;\n    private int[] rank; //rank[i]表示以i为根的集合所表示的树的层数\n\n    public UniomFind5(int size) {\n\n        parent = new int[size];\n        rank = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n            rank[i] = 1;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            parent[p] =     parent[parent[p]];\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        //根据两个元素所在树的rank不同判断合并方向\n        //rank低的集合合并到rank高的集合上\n        if (rank[pRoot] < rank[qRoot]) {\n            parent[pRoot] = qRoot;\n            //qRoot的树高没有发生变化\n        } else if (rank[qRoot] < rank[pRoot]) {\n            parent[qRoot] = pRoot;\n        } else {  //rank[qRoot] == rank[pRoot]\n            parent[qRoot] = pRoot;\n            rank[pRoot] += 1;\n        }\n    }\n}\n```\n\n### 测试\n\n```java\nimport java.util.Random;\n\npublic class Main {\n    private static double testUF(UF uf, int m) {\n        int size = uf.getSize();\n        Random random = new Random();\n\n        long startTime = System.nanoTime();\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.UnionElements(a, b);\n        }\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.isConnected(a, b);\n        }\n\n        long endTime = System.nanoTime();\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n        int size = 10000000;\n        int m = 10000000;\n        \n        UniomFind3 uf3 = new UniomFind3(size);\n\n        System.out.println(\"UnionFind3: \" + testUF(uf3, m) + \"s\");\n        UniomFind4 uf4 = new UniomFind4(size);\n        System.out.println(\"UnionFind4: \" + testUF(uf4, m) + \"s\");\n        UniomFind5 uf5 = new UniomFind5(size);\n        System.out.println(\"UnionFind5: \" + testUF(uf5, m) + \"s\");\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516214106.png)\n\n### 路径压缩[递归]\n\n实际上当我们使用路径压缩的时候,希望路径直接压缩成这个样子的.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516214249.png)\n\n```java\n//第六版Union-Find 路径压缩递归实现\npublic class UniomFind6 implements UF {\n    private int[] parent;\n    private int[] rank; //rank[i]表示以i为根的集合所表示的树的层数\n\n    public UniomFind6(int size) {\n\n        parent = new int[size];\n        rank = new int[size];\n\n        for (int i = 0; i < size; i++) {\n            parent[i] = i;\n            rank[i] = 1;\n        }\n    }\n\n    @Override\n    public int getSize() {\n        return parent.length;\n    }\n\n    //查找过程,查找元素p所对应的集合编号\n    //O(h)复杂度,h为树的高度\n    private int find(int p) {\n\n        if ((p < 0 && p >= parent.length)) {\n            throw new IllegalArgumentException(\"p is out of bound.\");\n        }\n        while (p != parent[p]) {\n            parent[p] = find(parent[p]);\n            p = parent[p];\n        }\n        return p;\n    }\n\n    //产看元素是否属于一个集合\n    //O(h)复杂度,h为树的高度\n    @Override\n    public boolean isConnected(int p, int q) {\n        return find(p) == find(q);\n    }\n\n\n    //合并元素p和元素q所属的元素\n    //O(h)复杂度,h为树的高度\n    @Override\n    public void UnionElements(int p, int q) {\n        int pRoot = find(p);\n        int qRoot = find(q);\n\n        if (qRoot == pRoot) {\n            return;\n        }\n        //根据两个元素所在树的rank不同判断合并方向\n        //rank低的集合合并到rank高的集合上\n        if (rank[pRoot] < rank[qRoot]) {\n            parent[pRoot] = qRoot;\n            //qRoot的树高没有发生变化\n        } else if (rank[qRoot] < rank[pRoot]) {\n            parent[qRoot] = pRoot;\n        } else {  //rank[qRoot] == rank[pRoot]\n            parent[qRoot] = pRoot;\n            rank[pRoot] += 1;\n        }\n    }\n}\n```\n\n#### 测试\n\n```java\nimport java.util.Random;\n\npublic class Main {\n    private static double testUF(UF uf, int m) {\n        int size = uf.getSize();\n        Random random = new Random();\n\n        long startTime = System.nanoTime();\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.UnionElements(a, b);\n        }\n        for (int i = 0; i < m; i++) {\n            int a = random.nextInt(size);\n            int b = random.nextInt(size);\n            uf.isConnected(a, b);\n        }\n\n        long endTime = System.nanoTime();\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n        int size = 10000000;\n        int m = 10000000;\n\n        UniomFind3 uf3 = new UniomFind3(size);\n\n        System.out.println(\"UnionFind3: \" + testUF(uf3, m) + \"s\");\n        UniomFind4 uf4 = new UniomFind4(size);\n        System.out.println(\"UnionFind4: \" + testUF(uf4, m) + \"s\");\n        UniomFind5 uf5 = new UniomFind5(size);\n        System.out.println(\"UnionFind5: \" + testUF(uf5, m) + \"s\");\n        UniomFind6 uf6 = new UniomFind6(size);\n        System.out.println(\"UnionFind6: \"  + testUF(uf6, m) + \"s\");\n\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516220116.png)\n\n虽然UnionFind6理论上最大压缩了树,但是由于时基于递归实现 虽然会有一丁点的时间消耗但是这次我们测试的数据总量数据千万级别的,因此使用递归实现的压缩,要比非递归实现的要差那么一点点.\n\n### 并查集时间复杂度\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/并查集/20190516215459.png)\n\n### 参考资料\n\n《玩转数据结构》","tags":["并查集"],"categories":["数据结构"]},{"title":"数据结构之字典树","url":"/2019/05/14/数据结构之字典树/","content":"\n {{ \"数据结构字典树的学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## Tire\n\n<p>Trie 树，也叫“字典树”,是一个树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。</p>\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/字典树/字典树.gif)\n\nTrie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/字典树/字典树查找.gif)\n\n## 实现\n\n将字符串集合构造成 Trie 树。这个过程分解开来的话，就是一个将字符串插入到 Trie 树的过程。另一个是在 Trie 树中查询一个字符串。\n\n```java\nimport java.util.HashMap;\n\npublic class Trie_Tree{\n\t \n\n\tprivate class Node{\n\t\tprivate int dumpli_num;////该字串的重复数目，  该属性统计重复次数的时候有用,取值为0、1、2、3\n\t\tprivate int prefix_num;///以该字串为前缀的字串数， 应该包括该字串本身！！！！！\n\t\tprivate Node childs[];////此处用数组实现，当然也可以map或list实现以节省空间\n\t\tprivate boolean isLeaf;///是否为单词节点\n\t\tpublic Node(){\n\t\t\tdumpli_num=0;\n\t\t\tprefix_num=0;\n\t\t\tisLeaf=false;\n\t\t\tchilds=new Node[26];\n\t\t}\n\t}\t\n\t\n\tprivate Node root;///树根  \n\tpublic Trie_Tree(){\n\t\t///初始化trie 树\n\t\troot=new Node();\n\t}\n\t\n\t/**\n\t * 插入字串，用循环代替迭代实现\n\t * @param words\n\t */\n\tpublic void insert(String words){\n\t\tinsert(this.root, words);\n\t}\n\t/**\n\t * 插入字串，用循环代替迭代实现\n\t * @param root\n\t * @param words\n\t */\n\tprivate void insert(Node root,String words){\n\t\twords=words.toLowerCase();////转化为小写\n\t\tchar[] chrs=words.toCharArray();\n\t\t\n\t\tfor(int i=0,length=chrs.length; i<length; i++){\n\t\t\t///用相对于a字母的值作为下标索引，也隐式地记录了该字母的值\n\t\t\tint index=chrs[i]-'a';\n\t\t\tif(root.childs[index]!=null){\n\t\t\t\t////已经存在了，该子节点prefix_num++\n\t\t\t\troot.childs[index].prefix_num++;\n\t\t\t}else{\n\t\t\t\t///如果不存在\n\t\t\t\troot.childs[index]=new Node();\n\t\t\t\troot.childs[index].prefix_num++;\t\t\t\t\n\t\t\t}\t\n\t\t\t\n\t\t\t///如果到了字串结尾，则做标记\n\t\t\tif(i==length-1){\n\t\t\t\troot.childs[index].isLeaf=true;\n\t\t\t\troot.childs[index].dumpli_num++;\n\t\t\t}\n\t\t\t///root指向子节点，继续处理\n\t\t\troot=root.childs[index];\n\t\t}\n\t\t\n\t}\n\t\n\t\n\t/**\n\t * 遍历Trie树，查找所有的words以及出现次数\n\t * @return HashMap<String, Integer> map\n\t */\n\tpublic HashMap<String,Integer> getAllWords(){\t\t\n\t\treturn preTraversal(this.root, \"\");\n\t}\n\t\n\t/**\n\t * 前序遍历。。。\n\t * @param root\t\t子树根节点\n\t * @param prefixs\t查询到该节点前所遍历过的前缀\n\t * @return\n\t */\n\tprivate  HashMap<String,Integer> preTraversal(Node root,String prefixs){\n\t\tHashMap<String, Integer> map=new HashMap<String, Integer>();\n\t\t\n\t\tif(root!=null){\n\t\t\t\n\t\t\tif(root.isLeaf==true){\n\t\t\t////当前即为一个单词\n\t\t\t\tmap.put(prefixs, root.dumpli_num);\n\t\t\t}\n\t\t\t\n\t\t\tfor(int i=0,length=root.childs.length; i<length;i++){\n\t\t\t\tif(root.childs[i]!=null){\n\t\t\t\t\tchar ch=(char) (i+'a');\n\t\t\t\t\t////递归调用前序遍历\n\t\t\t\t\tString tempStr=prefixs+ch;\n\t\t\t\t\tmap.putAll(preTraversal(root.childs[i], tempStr));\n\t\t\t\t}\n\t\t\t}\n\t\t}\t\t\n\t\t\n\t\treturn map;\n\t}\n\t\n\t\n\t\n\t\n\t/**\n\t * 判断某字串是否在字典树中\n\t * @param word\n\t * @return true if exists ,otherwise  false \n\t */\n\tpublic boolean isExist(String word){\n\t\treturn search(this.root, word);\n\t}\n\t/**\n\t * 查询某字串是否在字典树中\n\t * @param word\n\t * @return true if exists ,otherwise  false \n\t */\n\tprivate boolean search(Node root,String word){\n\t\tchar[] chs=word.toLowerCase().toCharArray();\n\t\tfor(int i=0,length=chs.length; i<length;i++){\n\t\t\tint index=chs[i]-'a';\n\t\t\tif(root.childs[index]==null){\n\t\t\t\t///如果不存在，则查找失败\n\t\t\t\treturn false;\n\t\t\t}\t\t\t\n\t\t\troot=root.childs[index];\t\t\t\n\t\t}\n\t\t\n\t\treturn true;\n\t}\n\t\n\t/**\n\t * 得到以某字串为前缀的字串集，包括字串本身！ 类似单词输入法的联想功能\n\t * @param prefix 字串前缀\n\t * @return 字串集以及出现次数，如果不存在则返回null\n\t */\n\tpublic HashMap<String, Integer> getWordsForPrefix(String prefix){\n\t\treturn getWordsForPrefix(this.root, prefix);\n\t}\n\t/**\n\t * 得到以某字串为前缀的字串集，包括字串本身！\n\t * @param root\n\t * @param prefix\n\t * @return 字串集以及出现次数\n\t */\n\tprivate HashMap<String, Integer> getWordsForPrefix(Node root,String prefix){\n\t\tHashMap<String, Integer> map=new HashMap<String, Integer>();\n\t\tchar[] chrs=prefix.toLowerCase().toCharArray();\n\t\t////\n\t\tfor(int i=0, length=chrs.length; i<length; i++){\n\t\t\t\n\t\t\tint index=chrs[i]-'a';\n\t\t\tif(root.childs[index]==null){\n\t\t\t\treturn null;\n\t\t\t}\n\t\t\t\n\t\t\troot=root.childs[index];\n\t\t\n\t\t}\n\t\t///结果包括该前缀本身\n\t\t///此处利用之前的前序搜索方法进行搜索\n\t\treturn preTraversal(root, prefix);\n\t}\n\t   \n}\n```\n\n```java\nimport java.util.HashMap;\n\npublic class Trie_Test {\n\n    public static void main(String args[])  //Just used for test\n    {\n        Trie_Tree trie = new Trie_Tree();\n        trie.insert(\"HELLO\");\n        trie.insert(\"Hadoop\");\n        trie.insert(\"Hadoop\");\n        trie.insert(\"Spark\");\n        trie.insert(\"Flink\");\n        trie.insert(\"Hbase\");\n        trie.insert(\"Hive\");\n        trie.insert(\"Flume\");\n        trie.insert(\"Kafka\");\n\n        HashMap<String, Integer> map = trie.getAllWords();\n\n        for (String key : map.keySet()) {\n            System.out.println(key + \" 出现: \" + map.get(key) + \"次\");\n        }\n\n\n        map = trie.getWordsForPrefix(\"H\");\n\n        System.out.println(\"\\n\\n包含H（包括本身）前缀的单词及出现次数：\");\n        for (String key : map.keySet()) {\n            System.out.println(key + \" 出现: \" + map.get(key) + \"次\");\n        }\n\n        if (trie.isExist(\"Storm\") == false) {\n            System.out.println(\"\\n\\n字典树中不存在：Storm \");\n        }\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/字典树/20190514113602.png)\n\n### 参考资料\n\n<https://blog.csdn.net/abcd_d_/article/details/40116485>\n\n","tags":["字典树"],"categories":["数据结构"]},{"title":"数据结构之堆与优先队列","url":"/2019/05/10/数据结构之堆与优先队列/","content":"\n {{ \" 堆与优先队列\"}}：<Excerpt in index | 首页摘要><!-- more --> \n堆\n\n堆必须是一个完全二叉树。除了最后一层，其他层的节点个数都是满的，最后一层的节点都靠左排列\n\n堆中的每个节点的值必须大于等于（或者小于等于）其子树中每个节点的值或者说堆中每个节点的值都大于等于（或者小于等于）其左右子节点的值。这两种表述是等价的。\n\n<p>对于每个节点的值都大于等于子树中每个节点值的堆，我们叫作“大顶堆”。对于每个节点的值都小于等于子树中每个节点值的堆，我们叫作“小顶堆”。</p>\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510214135.png)\n\n在图中1和2是大顶堆,3是小顶堆,4不是堆（最后一层不是右子节点）。\n\n## 实现\n\n完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510214714.png)\n\n数组中下标为 i 的节点的左子节点，就是下标为 $i$的节点，右子节点就是下标为$i*2+1$的节点，父节点就是下标为 $$\\frac{i}{2}$$的节点。\n\n## 堆化\n\n新插入的元素放到堆的最后,我们需要进行调整，让其重新满足堆的特性，这个过程就叫作堆化（heapify）。  堆化实际上有两种，从下往上和从上往下。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510215315.png)\n\n新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的大小关系，就互换两个节点。重复这个过程，直到父子节点之间满足刚说的那种大小关系。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510215720.png)\n\n删除堆顶元素\n\n堆顶元素存储的其实是堆中数据中的最大值或者最小值。  如果我们构造的是大顶堆，堆顶元素就是最大的元素。当我们删除堆顶元素之后，就需要把第二大的元素放到堆顶，那第二大元素肯定会出现在左右子节点中。然后我们再迭代地删除第二大节点，以此类推，直到叶子节点被删除。\n\n![img](https://static001.geekbang.org/resource/image/59/81/5916121b08da6fc0636edf1fc24b5a81.jpg)\n\n## 代码\n\n复用原先的Array代码并且加以改造\n\n```java\npublic class Array<E> {\n\n    private E[] data;\n    private int size;\n\n    // 构造函数，传入数组的容量capacity构造Array\n    public Array(int capacity){\n        data = (E[])new Object[capacity];\n        size = 0;\n    }\n\n    // 无参数的构造函数，默认数组的容量capacity=10\n    public Array(){\n        this(10);\n    }\n\n    public Array(E[] arr){\n        data = (E[])new Object[arr.length];\n        for(int i = 0 ; i < arr.length ; i ++)\n            data[i] = arr[i];\n        size = arr.length;\n    }\n\n    // 获取数组的容量\n    public int getCapacity(){\n        return data.length;\n    }\n\n    // 获取数组中的元素个数\n    public int getSize(){\n        return size;\n    }\n\n    // 返回数组是否为空\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    // 在index索引的位置插入一个新元素e\n    public void add(int index, E e){\n\n        if(index < 0 || index > size)\n            throw new IllegalArgumentException(\"Add failed. Require index >= 0 and index <= size.\");\n\n        if(size == data.length)\n            resize(2 * data.length);\n\n        for(int i = size - 1; i >= index ; i --)\n            data[i + 1] = data[i];\n\n        data[index] = e;\n\n        size ++;\n    }\n\n    // 向所有元素后添加一个新元素\n    public void addLast(E e){\n        add(size, e);\n    }\n\n    // 在所有元素前添加一个新元素\n    public void addFirst(E e){\n        add(0, e);\n    }\n\n    // 获取index索引位置的元素\n    public E get(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Get failed. Index is illegal.\");\n        return data[index];\n    }\n\n    // 修改index索引位置的元素为e\n    public void set(int index, E e){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Set failed. Index is illegal.\");\n        data[index] = e;\n    }\n\n    // 查找数组中是否有元素e\n    public boolean contains(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return true;\n        }\n        return false;\n    }\n\n    // 查找数组中元素e所在的索引，如果不存在元素e，则返回-1\n    public int find(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return i;\n        }\n        return -1;\n    }\n\n    // 从数组中删除index位置的元素, 返回删除的元素\n    public E remove(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Remove failed. Index is illegal.\");\n\n        E ret = data[index];\n        for(int i = index + 1 ; i < size ; i ++)\n            data[i - 1] = data[i];\n        size --;\n        data[size] = null; // loitering objects != memory leak\n\n        if(size == data.length / 4 && data.length / 2 != 0)\n            resize(data.length / 2);\n        return ret;\n    }\n\n    // 从数组中删除第一个元素, 返回删除的元素\n    public E removeFirst(){\n        return remove(0);\n    }\n\n    // 从数组中删除最后一个元素, 返回删除的元素\n    public E removeLast(){\n        return remove(size - 1);\n    }\n\n    // 从数组中删除元素e\n    public void removeElement(E e){\n        int index = find(e);\n        if(index != -1)\n            remove(index);\n    }\n\n    public void swap(int i, int j){\n\n        if(i < 0 || i >= size || j < 0 || j >= size)\n            throw new IllegalArgumentException(\"Index is illegal.\");\n\n        E t = data[i];\n        data[i] = data[j];\n        data[j] = t;\n    }\n\n    @Override\n    public String toString(){\n\n        StringBuilder res = new StringBuilder();\n        res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length));\n        res.append('[');\n        for(int i = 0 ; i < size ; i ++){\n            res.append(data[i]);\n            if(i != size - 1)\n                res.append(\", \");\n        }\n        res.append(']');\n        return res.toString();\n    }\n\n    // 将数组空间的容量变成newCapacity大小\n    private void resize(int newCapacity){\n\n        E[] newData = (E[])new Object[newCapacity];\n        for(int i = 0 ; i < size ; i ++)\n            newData[i] = data[i];\n        data = newData;\n    }\n}\n```\n\n```java\npublic class MaxHeap<E extends Comparable<E>> {\n\n    private Array<E> data;\n\n    public MaxHeap(int capacity){\n        data = new Array<>(capacity);\n    }\n\n    public MaxHeap(){\n        data = new Array<>();\n    }\n\n    public MaxHeap(E[] arr){\n        data = new Array<>(arr);\n        for(int i = parent(arr.length - 1) ; i >= 0 ; i --)\n            siftDown(i);\n    }\n\n    // 返回堆中的元素个数\n    public int size(){\n        return data.getSize();\n    }\n\n    // 返回一个布尔值, 表示堆中是否为空\n    public boolean isEmpty(){\n        return data.isEmpty();\n    }\n\n    // 返回完全二叉树的数组表示中，一个索引所表示的元素的父亲节点的索引\n    private int parent(int index){\n        if(index == 0)\n            throw new IllegalArgumentException(\"index-0 doesn't have parent.\");\n        return (index - 1) / 2;\n    }\n\n    // 返回完全二叉树的数组表示中，一个索引所表示的元素的左孩子节点的索引\n    private int leftChild(int index){\n        return index * 2 + 1;\n    }\n\n    // 返回完全二叉树的数组表示中，一个索引所表示的元素的右孩子节点的索引\n    private int rightChild(int index){\n        return index * 2 + 2;\n    }\n\n    // 向堆中添加元素\n    public void add(E e){\n        data.addLast(e);\n        siftUp(data.getSize() - 1);\n    }\n\n    private void siftUp(int k){\n\n        while(k > 0 && data.get(parent(k)).compareTo(data.get(k)) < 0 ){\n            data.swap(k, parent(k));\n            k = parent(k);\n        }\n    }\n\n    // 看堆中的最大元素\n    public E findMax(){\n        if(data.getSize() == 0)\n            throw new IllegalArgumentException(\"Can not findMax when heap is empty.\");\n        return data.get(0);\n    }\n\n    // 取出堆中最大元素\n    public E extractMax(){\n\n        E ret = findMax();\n\n        data.swap(0, data.getSize() - 1);\n        data.removeLast();\n        siftDown(0);\n\n        return ret;\n    }\n\n    private void siftDown(int k){\n\n        while(leftChild(k) < data.getSize()){\n            int j = leftChild(k); // 在此轮循环中,data[k]和data[j]交换位置\n            if( j + 1 < data.getSize() &&\n                    data.get(j + 1).compareTo(data.get(j)) > 0 )\n                j ++;\n            // data[j] 是 leftChild 和 rightChild 中的最大值\n\n            if(data.get(k).compareTo(data.get(j)) >= 0 )\n                break;\n\n            data.swap(k, j);\n            k = j;\n        }\n    }\n\n    // 取出堆中的最大元素，并且替换成元素e\n    public E replace(E e){\n\n        E ret = findMax();\n        data.set(0, e);\n        siftDown(0);\n        return ret;\n    }\n}\n```\n\n```java\nimport java.util.Random;\n\npublic class Main {\n\n    private static double testHeap(Integer[] testData, boolean isHeapify){\n\n        long startTime = System.nanoTime();\n\n        MaxHeap<Integer> maxHeap;\n        if(isHeapify)\n            maxHeap = new MaxHeap<>(testData);\n        else{\n            maxHeap = new MaxHeap<>();\n            for(int num: testData)\n                maxHeap.add(num);\n        }\n\n        int[] arr = new int[testData.length];\n        for(int i = 0 ; i < testData.length ; i ++)\n            arr[i] = maxHeap.extractMax();\n\n        for(int i = 1 ; i < testData.length ; i ++)\n            if(arr[i-1] < arr[i])\n                throw new IllegalArgumentException(\"Error\");\n        System.out.println(\"Test MaxHeap completed.\");\n\n        long endTime = System.nanoTime();\n\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n\n        int n = 1000000;\n\n        Random random = new Random();\n        Integer[] testData = new Integer[n];\n        for(int i = 0 ; i < n ; i ++)\n            testData[i] = random.nextInt(Integer.MAX_VALUE);\n\n        double time1 = testHeap(testData, false);\n        System.out.println(\"Without heapify: \" + time1 + \" s\");\n\n        double time2 = testHeap(testData, true);\n        System.out.println(\"With heapify: \" + time2 + \" s\");\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510220210.png)\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510220241.png)\n\n## 优先队列\n\n普通的队列是一种先进先出的数据结构，元素在队列尾追加，而从队列头删除。在优先队列中，元素被赋予优先级。当访问元素时，具有最高优先级的元素最先删除。优先队列具有最高级先出 （first in, largest out）的行为特征。通常采用堆数据结构来实现。\n\n```java\npublic class PriorityQueue<E extends Comparable<E>> implements Queue<E> {\n\n    private MaxHeap<E> maxHeap;\n\n    public PriorityQueue(){\n        maxHeap = new MaxHeap<>();\n    }\n\n    @Override\n    public int getSize(){\n        return maxHeap.size();\n    }\n\n    @Override\n    public boolean isEmpty(){\n        return maxHeap.isEmpty();\n    }\n\n    @Override\n    public E getFront(){\n        return maxHeap.findMax();\n    }\n\n    @Override\n    public void enqueue(E e){\n        maxHeap.add(e);\n    }\n\n    @Override\n    public E dequeue(){\n        return maxHeap.extractMax();\n    }\n```\n\n```java\nimport java.util.Random;\n\npublic class PriorityQueueMain {\n    public static void main(String[] args) {\n\n\n        PriorityQueue<Integer> priorityQueue = new PriorityQueue<>();\n        Random random = new Random();\n        long startTime = System.nanoTime();\n        int size = 1000000;\n        for (int i = 0; i < size; i++) {\n            priorityQueue.enqueue(random.nextInt(Integer.MAX_VALUE));\n            if (i % 3 == 0)\n                priorityQueue.dequeue();\n        }\n        long endTime = System.nanoTime();\n\n\n        System.out.println(\"The priorityQueue size is :\" + size +\"\"+\"\\n After operation size is \"+priorityQueue.getSize()+ \"\\n operation time is \" + (((endTime - startTime) / 1000000000.0)) + \"s\");\n\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/堆/20190510222335.png)\n\n## 参考资料\n\n《大话数据结构》\n\n《数据结构与算法之美》\n\n《玩转数据结构》\n\n\n\n","tags":["二叉树","堆","优先队列"],"categories":["数据结构"]},{"title":"数据结构之二叉树","url":"/2019/05/07/数据结构之二叉树/","content":"\n {{ \" 二叉树的学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 树\n\n树(Tree)是n(n$\\geq$0)个节点的有限集,当n=0时称为空树。在任意以可非空树中：\n\n1. 有且只有一个特定的根（Root）节点；\n\n2. 当n$\\geq$0的时候，其余节点分为m(m>0)个互不相交的有限集$T_{1}$，$T_{2}$……$T_{m}$，其中每一台集合本身优势一棵树，被称为根的子树。\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507201059.png)\n\n“树”这种数据结构很像我们生活中的“树”，这里面每个元素我们叫作“节 点”；用来连线相邻节点之间的关系，我们叫作“父子关系”\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507201130.png)\n\nA 节点就是 B 节点的父节点，B 节点是 A 节点的子节点。B、C、D 这三个节点 的父节点是同一个节点，所以它们之间互称为兄弟节点。我们把没有父节点的节点叫作根节点，也就是图中的节点 E。我们把没有子节点的节点叫作叶子节点或者叶节点，比如图中的 G、H、I、 J、K、L 都是叶子节点。\n\n关于“树”，还有三个比较相似的概念：高度（Height）、深 度（Depth）、层（Level）。\n\n节点的高度=节点到叶子节点的``最长路径``(边数)\n\n节点的深度=根节点到这个节点所经历的`边的个数`\n\n节点的层数=节点的深度+1\n\n树的高度=根节点的高度\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507201721.png)\n\n高度计数起点为0，从最底层开始计数。\n\n深度从根开始计算向下计数。\n\n层数计数起点为1，根节点位于第一层。\n\n## 二叉树\n\n二叉树:每个节点最多有两个“叉”，也就是两个子节点，分别是左子节点和右子节点。二叉树并不要求每个节点都有两个子节点，有的节点只有左子节点，有的节点只有右子节点。当然可以由四叉树，八叉树。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507203709.png)\n\n编号 2 的二叉树中，叶子节点全都在最底层，除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫作满二叉树。\n编号 3 的二叉树中，叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一 层，其他层的节点个数都要达到最大，这种二叉树叫作完全二叉树。\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507203903.png)\n\n在满足满二叉树的性质后，最后一层的叶子节点均需在最左边。\n\n### 实现\n\n想要存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。\n每个节点有三个字段，其中一个存储数据，另外两个是指向左右子节点的指针。我们只要拎住根节点，就可以通过左\n右子节点的指针，把整棵树都串起来。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507210732.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507210745.png)完全二叉树，仅仅“浪费”了一个下标为 0 的存储位置。但是如果是非完全二叉树，会浪费很多的数组存储空间。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507210858.png)\n\n### 遍历\n\n- 前序遍历：对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印\n    它的右子树。\n- 中序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它\n    的右子树。\n    后序遍历：对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打\n    印这个节点本身\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/树/20190507211036.png)\n\n二叉树的遍历是一个递归的过程。\n\n## 代码\n\n```java\nimport java.util.LinkedList;\nimport java.util.Queue;\nimport java.util.Stack;\n\npublic class BST<E extends Comparable<E>> {\n\n    private class Node{\n        public E e;\n        public Node left, right;\n\n        public Node(E e){\n            this.e = e;\n            left = null;\n            right = null;\n        }\n    }\n\n    private Node root;\n    private int size;\n\n    public BST(){\n        root = null;\n        size = 0;\n    }\n\n    public int size(){\n        return size;\n    }\n\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    // 向二分搜索树中添加新的元素e\n    public void add(E e){\n        root = add(root, e);\n    }\n\n    // 向以node为根的二分搜索树中插入元素e，递归算法\n    // 返回插入新节点后二分搜索树的根\n    private Node add(Node node, E e){\n\n        if(node == null){\n            size ++;\n            return new Node(e);\n        }\n\n        if(e.compareTo(node.e) < 0)\n            node.left = add(node.left, e);\n        else if(e.compareTo(node.e) > 0)\n            node.right = add(node.right, e);\n\n        return node;\n    }\n\n    // 看二分搜索树中是否包含元素e\n    public boolean contains(E e){\n        return contains(root, e);\n    }\n\n    // 看以node为根的二分搜索树中是否包含元素e, 递归算法\n    private boolean contains(Node node, E e){\n\n        if(node == null)\n            return false;\n\n        if(e.compareTo(node.e) == 0)\n            return true;\n        else if(e.compareTo(node.e) < 0)\n            return contains(node.left, e);\n        else // e.compareTo(node.e) > 0\n            return contains(node.right, e);\n    }\n\n    // 二分搜索树的前序遍历\n    public void preOrder(){\n        preOrder(root);\n    }\n\n    // 前序遍历以node为根的二分搜索树, 递归算法\n    private void preOrder(Node node){\n\n        if(node == null)\n            return;\n\n        System.out.println(node.e);\n        preOrder(node.left);\n        preOrder(node.right);\n    }\n\n    // 二分搜索树的非递归前序遍历\n    public void preOrderNR(){\n\n        Stack<Node> stack = new Stack<>();\n        stack.push(root);\n        while(!stack.isEmpty()){\n            Node cur = stack.pop();\n            System.out.println(cur.e);\n\n            if(cur.right != null)\n                stack.push(cur.right);\n            if(cur.left != null)\n                stack.push(cur.left);\n        }\n    }\n\n    // 二分搜索树的中序遍历\n    public void inOrder(){\n        inOrder(root);\n    }\n\n    // 中序遍历以node为根的二分搜索树, 递归算法\n    private void inOrder(Node node){\n\n        if(node == null)\n            return;\n\n        inOrder(node.left);\n        System.out.println(node.e);\n        inOrder(node.right);\n    }\n\n    // 二分搜索树的后序遍历\n    public void postOrder(){\n        postOrder(root);\n    }\n\n    // 后序遍历以node为根的二分搜索树, 递归算法\n    private void postOrder(Node node){\n\n        if(node == null)\n            return;\n\n        postOrder(node.left);\n        postOrder(node.right);\n        System.out.println(node.e);\n    }\n\n    // 二分搜索树的层序遍历\n    public void levelOrder(){\n\n        Queue<Node> q = new LinkedList<>();\n        q.add(root);\n        while(!q.isEmpty()){\n            Node cur = q.remove();\n            System.out.println(cur.e);\n\n            if(cur.left != null)\n                q.add(cur.left);\n            if(cur.right != null)\n                q.add(cur.right);\n        }\n    }\n\n    // 寻找二分搜索树的最小元素\n    public E minimum(){\n        if(size == 0)\n            throw new IllegalArgumentException(\"BST is empty!\");\n\n        return minimum(root).e;\n    }\n\n    // 返回以node为根的二分搜索树的最小值所在的节点\n    private Node minimum(Node node){\n        if(node.left == null)\n            return node;\n        return minimum(node.left);\n    }\n\n    // 寻找二分搜索树的最大元素\n    public E maximum(){\n        if(size == 0)\n            throw new IllegalArgumentException(\"BST is empty\");\n\n        return maximum(root).e;\n    }\n\n    // 返回以node为根的二分搜索树的最大值所在的节点\n    private Node maximum(Node node){\n        if(node.right == null)\n            return node;\n\n        return maximum(node.right);\n    }\n\n    // 从二分搜索树中删除最小值所在节点, 返回最小值\n    public E removeMin(){\n        E ret = minimum();\n        root = removeMin(root);\n        return ret;\n    }\n\n    // 删除掉以node为根的二分搜索树中的最小节点\n    // 返回删除节点后新的二分搜索树的根\n    private Node removeMin(Node node){\n\n        if(node.left == null){\n            Node rightNode = node.right;\n            node.right = null;\n            size --;\n            return rightNode;\n        }\n\n        node.left = removeMin(node.left);\n        return node;\n    }\n\n    // 从二分搜索树中删除最大值所在节点\n    public E removeMax(){\n        E ret = maximum();\n        root = removeMax(root);\n        return ret;\n    }\n\n    // 删除掉以node为根的二分搜索树中的最大节点\n    // 返回删除节点后新的二分搜索树的根\n    private Node removeMax(Node node){\n\n        if(node.right == null){\n            Node leftNode = node.left;\n            node.left = null;\n            size --;\n            return leftNode;\n        }\n\n        node.right = removeMax(node.right);\n        return node;\n    }\n\n    // 从二分搜索树中删除元素为e的节点\n    public void remove(E e){\n        root = remove(root, e);\n    }\n\n    // 删除掉以node为根的二分搜索树中值为e的节点, 递归算法\n    // 返回删除节点后新的二分搜索树的根\n    private Node remove(Node node, E e){\n\n        if( node == null )\n            return null;\n\n        if( e.compareTo(node.e) < 0 ){\n            node.left = remove(node.left , e);\n            return node;\n        }\n        else if(e.compareTo(node.e) > 0 ){\n            node.right = remove(node.right, e);\n            return node;\n        }\n        else{   // e.compareTo(node.e) == 0\n\n            // 待删除节点左子树为空的情况\n            if(node.left == null){\n                Node rightNode = node.right;\n                node.right = null;\n                size --;\n                return rightNode;\n            }\n\n            // 待删除节点右子树为空的情况\n            if(node.right == null){\n                Node leftNode = node.left;\n                node.left = null;\n                size --;\n                return leftNode;\n            }\n\n            // 待删除节点左右子树均不为空的情况\n\n            // 找到比待删除节点大的最小节点, 即待删除节点右子树的最小节点\n            // 用这个节点顶替待删除节点的位置\n            Node successor = minimum(node.right);\n            successor.right = removeMin(node.right);\n            successor.left = node.left;\n\n            node.left = node.right = null;\n\n            return successor;\n        }\n    }\n\n    @Override\n    public String toString(){\n        StringBuilder res = new StringBuilder();\n        generateBSTString(root, 0, res);\n        return res.toString();\n    }\n\n    // 生成以node为根节点，深度为depth的描述二叉树的字符串\n    private void generateBSTString(Node node, int depth, StringBuilder res){\n\n        if(node == null){\n            res.append(generateDepthString(depth) + \"null\\n\");\n            return;\n        }\n\n        res.append(generateDepthString(depth) + node.e +\"\\n\");\n        generateBSTString(node.left, depth + 1, res);\n        generateBSTString(node.right, depth + 1, res);\n    }\n\n    private String generateDepthString(int depth){\n        StringBuilder res = new StringBuilder();\n        for(int i = 0 ; i < depth ; i ++)\n            res.append(\"-- \");\n        return res.toString();\n    }\n}\n```\n\n```java\npublic class Order {\n\n    public static void main(String[] args) {\n\n        BST<Integer> bst = new BST<>();\n        int[] nums = {5, 3, 6, 8, 4, 2};\n        for(int num: nums)\n            bst.add(num);\n\n        /////////////////\n        //      5      //\n        //    /   \\    //\n        //   3    6    //\n        //  / \\    \\   //\n        // 2  4     8  //\n        /////////////////\n        System.out.println(bst);\n        \n        \n        /* *      深度优先遍历            * */\n        //前序遍历\n        bst.preOrder();\n        System.out.println();\n\t    //中序遍历\n        bst.inOrder();\n        System.out.println();\n\t\t//后序遍历\n        bst.postOrder();\n        System.out.println();\n       /* *      深度优先遍历            * */\n \t\t\n      /* *      广度优先遍历            * */\n        //层序遍历 \n        bst.levelOrder();\n        System.out.println();\n     /* *      广度优先遍历            * */\n   \n    }\n}\n```\n### 遍历\n\n```java\n\n\n        /////////////////\n        //      5      //\n        //    /   \\    //\n        //   3    6    //\n        //  / \\    \\   //\n        // 2  4     8  //\n        /////////////////\n\n        /* *      深度优先遍历            * */\n\n5\n--3\n----2\n------null\n------null\n----4\n------null\n------null\n--6\n----null\n----8\n------null\n------null\n        //前序遍历\n\n5\n3\n2\n4\n6\n8\n        //中序遍历\n2\n3\n4\n5\n6\n8\n        //后序遍历\n\n2\n4\n3\n8\n6\n5\n        //层序遍历\n\n5\n3\n6\n2\n4\n8\n```\n\n### 删除\n\n```java\nimport java.util.ArrayList;\nimport java.util.Random;\n\npublic class Remove {\n\n    public static void main(String[] args) {\n\n        BST<Integer> bst = new BST<>();\n        Random random = new Random();\n\n        int n = 10;\n\n        // test removeMin\n        for(int i = 0 ; i < n ; i ++)\n            bst.add(random.nextInt(10000));\n        System.out.println(bst);\n        ArrayList<Integer> nums = new ArrayList<>();\n        while(!bst.isEmpty())\n            nums.add(bst.removeMin());\n\n        System.out.println(nums);\n        for(int i = 1 ; i < nums.size() ; i ++)\n            if(nums.get(i - 1) > nums.get(i))\n                throw new IllegalArgumentException(\"Error!\");\n        System.out.println(\"removeMin test completed.\");\n\n\n        // test removeMax\n        for(int i = 0 ; i < n ; i ++)\n            bst.add(random.nextInt(10000));\n\n        nums = new ArrayList<>();\n        while(!bst.isEmpty())\n            nums.add(bst.removeMax());\n\n        System.out.println(nums);\n        for(int i = 1 ; i < nums.size() ; i ++)\n            if(nums.get(i - 1) < nums.get(i))\n                throw new IllegalArgumentException(\"Error!\");\n        System.out.println(\"removeMax test completed.\");\n    }\n}\n```\n\n````java\n5022\n--1869\n----617\n------null\n------1370\n--------null\n--------null\n----3497\n------null\n------null\n--5729\n----null\n----9598\n------8387\n--------6796\n----------null\n----------8167\n------------null\n------------null\n--------null\n------null\n\n[617, 1370, 1869, 3497, 5022, 5729, 6796, 8167, 8387, 9598]\nremoveMin test completed.\n7926\n--5532\n----309\n------null\n------2171\n--------null\n--------4294\n----------null\n----------null\n----7031\n------6053\n--------null\n--------6737\n----------null\n----------null\n------null\n--8619\n----null\n----8677\n------null\n------null\n\n[8677, 8619, 7926, 7031, 6737, 6053, 5532, 4294, 2171, 309]\nremoveMax test completed.\n````\n\n\n\n","tags":["二叉树"],"categories":["数据结构"]},{"title":"数据结构之递归","url":"/2019/05/06/数据结构之递归/","content":"\n {{ \" 数据结构递归的学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n简介\n\n递归（英语：Recursion），又译为递回，在数学与计算机科学中，是指在函数的定义中使用函数自身的方法。递归一词还较常用于描述以自相似方法重复事物的过程。例如，当两面镜子相互之间近似平行时，镜中嵌套的图像是以无限递归的形式出现的。也可以理解为自我复制的过程。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/递归/20190506144833.png)\n\n在谷歌搜索中由这样的彩蛋。\n\n\n\n## 满足条件\n\n1. 一个问题的解可以分解为几个子问题的解\n2. 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样\n3.  存在递归终止条件\n\n## 秘诀\n\n写递归代码的关键就是找到如何将大问题分解为小问题的规律，并且基于此写出递推公式，然后再推敲终止条件，最后将递推公式和终止条件翻译成代码。\n\n## 案例\n\n### 斐波那契数列\n\n```java\npublic class PrintFib {\n\t\n\tpublic static int fib(int num) {\n\t\tif(num == 1 || num == 2) {\n\t\t\treturn 1;\n\t\t}else {\n\t\t\treturn fib(num - 2) + fib(num - 1);\n\t\t}\n\t}\n\t\n\tpublic static void main(String[] args) {\n\t\t\n\t\tfor(int i = 1;i <= 10;i++) {\n\t\t\tSystem.out.print(fib(i) + \"\\t\");\n\t\t}\t\n\t}\t\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/递归/20190506151043.png)\n\n### 汉诺塔\n\n汉诺塔（又称河内塔）问题是源于印度一个古老传说的益智玩具。大梵天创造世界的时候做了三根金刚石]柱子，在一根柱子上从下往上按照大小顺序摞着64片黄金圆盘。大梵天命令婆罗门把圆盘从下面开始按大小顺序重新摆放在另一根柱子上。并且规定，在小圆盘上不能放大圆盘，在三根柱子之间一次只能移动一个圆盘。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/递归/汉诺塔.gif)\n\n图片来源:<https://www.zhihu.com/question/24385418>\n\n```java\nimport java.util.Scanner;\n \npublic class Hanoi {\n\tstatic long s = 0;\n \n\tpublic static void main(String args[]) {\n \n\t\tint n = 0;\n\t\tSystem.out.println(\"请输入汉诺塔的层数:\");\n\t\tScanner console = new Scanner(System.in);\n\t\tn = console.nextInt();\n\t\tSystem.out.println(\"汉诺塔层数为\" + n);\n\t\tSystem.out.println(\"移动方案为：\");\n\t\thanoi(n, 'a', 'b', 'c');\n\t\tSystem.out.println(\"需要移动次数：\" + s);\n \n\t}\n \n\tstatic void hanoi(int n, char a, char b, char c) { //a为初始塔，b为中间塔，c为目标塔\n\t\tif (n == 1){  \n            System.out.println(\"n=\" + n + \" \" + a + \"-->\" + c);  \n            s++;\n        }\n\t\telse{  \n            hanoi(n-1,a,c,b);\n            System.out.println(\"n=\" + n + \" \" + a + \"-->\" + c);  \n            hanoi(n-1,b,a,c);  \n            s++;\n        }\n\t}\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/递归/20190506164324.png)\n\n","tags":["递归"],"categories":["数据结构"]},{"title":"数据结构之链表","url":"/2019/05/04/数据结构之链表/","content":"\n {{ \"数据结构链表的学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 链表\n\n数组要一块连续的内存空间来存储，对内存的要求比较高。如果我们申请一个100MB大小的数组，当内存中没有连续的、足够大的存储空间 时，即便内存的剩余总可用空间大于 100MB，仍然会申请失败。\n链表恰恰相反，它并不需要一块连续的内存空间，它通过“指针”将一组零散的内存块串联起来使用，所以如果我们申请的是 100MB 大小的链表，根本不会有问题。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504172620.png)\n\n链表通过指针将一组零散的内存块串联在一起。其中，我们把内存块称为链表的“结点”。为了将所有的结点串起来，每个链表的结点除了存储数据之外，还需要记录链上的下一个结点 的地址。如图所示，我们把这个记录下个结点地址的指针叫作后继指针 next。\n\n### 单链表(数组)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504172746.png)\n\n其中有两个结点是比较特殊的，它们分别是第一个结点和最后一个结点。我们习惯性地把第一个结点叫作头结点，把最后一个结点叫作尾结点。其中，头结点用来记录链表的基地址。有了它，我们就可以遍历得到整条链表。而尾结点特殊的地方是：指针 不是指向下一个结点，而是指向一个空地址 NULL，表示这是链表上最后一个结点。与数组一样，链表也支持数据的查找、插入和删除操作。只不过效率会不高。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504174653.png)\n\n### 循环链表\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504174905.png)\n\n循环链表是一种特殊的单链表。它跟单链表唯一的区别就在尾结点：单链表的尾结点指针指向空地址，表示这就是最后的结点了。而循环链表的尾结点指针是指向链表的头结点。\n\n### 双向链表\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504175352.png)\n\n双向链表需要额外的两个空间来存储后继结点和前驱结点的地址。所以，如果存储同样多的数据，双向链表要比单链表占用更多的内存空间。虽然两个指针比较浪费存储空间，但可以支持双向遍历，这样也带来了双向链表操作的灵活性。那相比单链表，双向链表适解决删除结点中“值等于某个给定值”的结点，删除给定指针指向的结点。\n\n### 双向循环链表\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504175412.png)\n\n双向链表也叫双链表，是链表的一种，它的每个数据结点中都有两个指针，分别指向直接后继和直接前驱。所以，从双向链表中的任意一个结点开始，都可以很方便地访问它的前驱结点和后继结点。一般我们都构造双向循环链表。\n\n## 代码实现\n\n### 单向链表\n\n```java\npublic class LinkedList<E> {\n    private class Node {\n        public E e;\n        public Node next;\n\n        public Node(E e, Node next) {\n            this.e = e;\n            this.next = next;\n        }\n\n        public Node(E e) {\n            this(e, null);\n        }\n\n        public Node() {\n            this(null, null);\n        }\n\n        @Override\n        public String toString() {\n            return e.toString();\n        }\n    }\n\n    private Node dummyHead;\n    private int size;\n\n    public LinkedList() {\n        dummyHead = new Node(null, null);\n        size = 0;\n    }\n\n    public int getSize() {\n        return size;\n    }\n\n    public boolean isEmpty() {\n        return size == 0;\n    }\n\n    //为链表头添加新的元素E\n    public void addFirst(E e) {\n        add(0, e);\n    }\n\n    //在链表的index(0-based)位置添加新的元素e\n    //链表中不是一个常用的操作\n    public void add(int index, E e) {\n        if (index < 0 || index > size) {\n            throw new IllegalArgumentException(\"Add Failed.Illegal index.\");\n        }\n\n        Node prev = dummyHead;\n        for (int i = 0; i < index; i++) {\n            prev = prev.next;\n        }\n\n        prev.next = new Node(e, prev.next);\n        size++;\n    }\n\n    public void addLast(E e) {\n        add(size, e);\n    }\n\n    public E get(int index) {\n        if (index < 0 || index >= size) {\n            throw new IllegalArgumentException(\"Get Failed.Illegal index\");\n        }\n        Node cur = dummyHead.next;\n        for (int i = 0; i < index; i++) {\n            cur = cur.next;\n        }\n        return cur.e;\n    }\n\n    //获取链表的第一个元素\n    public E getFirst() {\n        return get(0);\n    }\n\n    //获取链表的最后一个元素怒\n    public E getLast() {\n        return get(size - 1);\n    }\n\n    //修改链表的index元素(0-based)个位置的元素\n    //不常用的操作\n    public void set(int index, E e) {\n        if (index < 0 || index >= size) {\n            throw new IllegalArgumentException(\"Update Failed.Illegal index\");\n        } else {\n            Node cur = dummyHead.next;\n            for (int i = 0; i < index; i++) {\n                cur = cur.next;\n            }\n            cur.e = e;\n        }\n    }\n\n    //查找链表是否存在元素E\n    public boolean contains(E e) {\n        Node cur = dummyHead.next;\n        while (cur != null) {\n            if (cur.e.equals(e)) {\n                return true;\n            }\n            cur = cur.next;\n        }\n        return false;\n    }\n\n    public E remove(int index) {\n        if (index < 0 || index >= size) {\n            throw new IllegalArgumentException(\"Update Failed.Illegal index\");\n        } else {\n            Node prev = dummyHead;\n            for (int i = 0; i < index; i++) {\n                prev = prev.next;\n            }\n            Node retNode = prev.next;\n            prev.next = retNode.next;\n            retNode.next = null;\n            size--;\n\n            return retNode.e;\n        }\n\n    }\n\n    public E removeFirst() {\n        return remove(0);\n    }\n\n    public E removeLast() {\n        return remove(size - 1);\n    }\n\n    @Override\n    public String toString() {\n        StringBuilder res = new StringBuilder();\n        \n        for (Node cur = dummyHead.next; cur != null; cur = cur.next) {\n            res.append(cur + \"->\");\n        }\n        res.append(\"NULL\");\n        return res.toString();\n    }\n\n}\n```\n\n````java\npublic class Main {\n\n    public static void main(String[] args) {\n\n        LinkedList<Integer> linkedList = new LinkedList<>();\n        for (int i = 0; i < 5; i++) {\n            linkedList.addFirst(i);\n            System.out.println(linkedList);\n        }\n\n        linkedList.add(2, 666);\n        System.out.println(linkedList);\n        linkedList.remove(2);\n        System.out.println(linkedList);\n        linkedList.removeFirst();\n        System.out.println(linkedList);\n        linkedList.removeLast();\n        System.out.println(linkedList);\n\n    }\n}\n````\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190504175729.png)\n\n### 循环链表\n\n```java\npublic class SinglyLoopLink {\n    public Node head;\n    public Node tail;\n    public int size;\n\n    public SinglyLoopLink(){\n        this.size=0;\n        this.head=new Node(0);\n        this.tail=new Node(0);\n        head.next=tail;\n        tail.next=head;\n    }\n\n    class Node {\n        Node next;\n        int data;\n        public Node(){\n\n        }\n        public Node(int data){\n            this.data=data;\n        }\n    }\n\n    public void add(int data){\n        Node newNode=new Node(data);\n        if (head == null) {\n            newNode.next=tail;\n            head.next=newNode;\n\n        }else{\n            Node temp=head;\n            while(temp.next!=tail){\n                temp=temp.next;\n            }\n            temp.next=newNode;\n            newNode.next=tail;\n        }\n        size++;\n    }\n\n    public String add(int index,int data){\n        Node temp=head;\n        Node newNode=new Node(data);\n        if(index<0||index>size){\n            return \"数组指标有误\";\n        }\n        for(int i=0;i<size;i++){\n            temp=temp.next;\n\n            if(index==0){\n                head.next=newNode;\n                newNode.next=temp;\n                size++;\n                break;\n            }else if(i==index-1){\n                System.out.println(\"this is \"+size);\n                newNode.next=temp.next;\n                temp.next=newNode;\n                size++;\n                break;\n            }\n        }\n        return null;\n    }\n\n    public String addFirst(int data){\n        return add(0,data);\n    }\n\n    public String addLast(int data){\n        return add(size,data);\n    }\n\n\n    public String disPlay(){\n        SinglyLoopLink.Node temp=head;\n        StringBuffer result=new StringBuffer(\"[=>head=>\");\n        for(int i=0;i<size;i++){\n            temp=temp.next;\n            result.append(temp.data+\"=>\");\n\n        }\n        result.append(\"tail=>]\");\n        return result.toString();\n    }\n\n    public static void main(String[] args) {\n        SinglyLoopLink singlyLoopLink=new SinglyLoopLink();\n        singlyLoopLink.add(1);\n        singlyLoopLink.add(2);\n        singlyLoopLink.add(0,-9);\n        singlyLoopLink.add(1,9);\n        singlyLoopLink.add(4,10);\n        singlyLoopLink.addFirst(-99);\n        singlyLoopLink.addLast(100);\n        singlyLoopLink.add(0,-155);\n        singlyLoopLink.add(3,10);\n        System.out.println(singlyLoopLink.disPlay());\n    }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505132821.png)\n\n### 双向链表\n\n```java\npublic class LinkList {\n    Node head;\n    Node tail;\n    //申明该变量不会被持久化\n    transient int size=0;\n    int data;\n    class Node{\n        int data;\n        Node pre;\n        Node next;\n        Node(int data,Node pre,Node next){\n            this.data=data;\n            this.pre=pre;\n            this.next=next;\n        }\n        Node(int data){\n            this.data=data;\n        }\n        Node(){\n        }\n    }\n\n    public LinkList(int size,Node head,Node tail){\n        this.size=0;\n        this.head=head;\n        this.tail=tail;\n    }\n\n    public LinkList(){\n        head=new Node();\n        tail=new Node();\n    }\n\n\n    //添加头部节点(只有一个元素的时候)\n    public void addHeadNode(int data){\n        Node node=new Node(data,null,null);\n        head.next=node;\n        node.pre=head;\n        node.next=tail;\n        tail.pre=node;\n        size++;\n    }\n\n    //尾插法\n    public void addLast(int data){\n        if(head.next==null){\n            addHeadNode(data);\n        }else{\n            Node temp=head;\n            Node newNode=new Node(data);\n            for(int i=0;i<size;i++){\n                temp=temp.next;\n            }\n            newNode.pre=temp;\n            newNode.next=tail;\n            temp.next=newNode;\n            tail.pre=newNode;\n            size++;\n        }\n    }\n\n    //头插法\n    public void addBegin(int data){\n//        add(0,data);\n        if(head.next==null){\n            addHeadNode(data);\n            size++;\n        }else {\n            Node newNode = new Node(data);\n            newNode.next = head.next;\n            newNode.pre = head;\n            head.next = newNode;\n            size++;\n        }\n    }\n\n    //从指定索引位置插入相应的内容\n//    public void add(int index,int data){\n//        //链表里面已经有相应的元素了\n////        if(head.next!=null){\n//            Node temp=head;\n//            Node newNode=new Node(data);\n//            if(index==0){\n//                newNode.pre=head;\n//                newNode.next=head.next;\n//                head.next=newNode;\n//            }\n//            if(index<size&&index>0){ //最后一个节点处插入需要特殊处理\n//                for(int i=0;i<index;i++){\n//                    temp=temp.next;\n//                }\n//                newNode.next=temp.next;\n//                newNode.pre=temp;\n//                temp.next.pre=newNode;\n//                temp.next=newNode;\n//            }\n//            if(index==size&&size!=0){\n//                for(int i=0;i<index;i++){\n//                    temp=temp.next;\n//                }\n//                temp.next=newNode;\n//                newNode.pre=temp;\n//                newNode.next=tail;\n//                tail.pre=newNode;\n//            }\n//            size++;\n////        }\n//\n//    }\n\n\n    public Node get(int index){\n        if(index>size || index<0){\n            throw new RuntimeException(\"链表下标超出范围了！\");\n        }\n        Node currentNode=head;\n        for(int j=0;j<size;j++){\n            currentNode=head.next;\n            if(j==index){\n                return currentNode;\n            }\n        }\n        return null;\n    }\n\n    //根据索引，删除某一个链表元素内容\n    public String remove(int index){\n        Node temp=head;\n        if(index>=size){\n            return \"下标超值\";\n        }\n        for(int i=0;i<size;i++){\n            //并非最后一个数值\n            if(i==index){\n                temp.next=temp.next.next;\n                temp.next.pre=temp;\n                size--;\n                break;\n            }\n            temp=temp.next;\n        }\n        return null;\n    }\n\n    public void removeData(Integer data){\n        Node temp=head;\n        Node preNode;\n        for(int i=0;i<size;i++){\n            preNode=temp;\n            temp=temp.next;\n            if(temp.data==data){\n                preNode.next=temp.next;\n                size--;\n                break;\n            }\n        }\n    }\n\n    public void removeNode(Node itemNode){\n      Node preNode=itemNode.pre;\n      Node nextNode=itemNode.next;\n      preNode.next=nextNode;\n      size--;\n    }\n\n    public String removeFirst(){\n        return remove(0);\n    }\n\n    public String removeLast(){\n//        if(size==2){ //剩余头尾两个节点和一个元素的时候\n//            remove(0);\n//        }\n        return remove(size-1);\n    }\n\n\n    public String disPlay(){\n        Node temp=head.next;\n        StringBuffer result=new StringBuffer(\"[head<==>\");\n        int index=0;\n        for(int i=0;i<size;i++){\n            index++;\n            result.append(temp.data+\"<==>\");\n            temp=temp.next;\n        }\n        result.append(\"tail]\");\n        return result.toString();\n    }\n\n    public static void main(String[] args) {\n        LinkList linkList=new LinkList();\n        linkList.addLast(1);\n        linkList.addLast(2);\n        linkList.addLast(3);\n        linkList.addBegin(34);\n        linkList.addLast(44);\n        System.out.println(linkList.disPlay());\n        linkList.removeData(2);\n        linkList.removeData(3);\n        System.out.println(linkList.disPlay());\n        Node node=linkList.get(0);\n\n        linkList.removeNode(node);\n        System.out.println(linkList.disPlay());\n    }\n```\n\n### 双向循环链表\n\n```java\npublic class DoubleLoopLink {\n    Node head;\n    Node tail;\n    //申明该变量不会被持久化\n    transient int size=0;\n    int data;\n    class Node{\n        int data;\n        Node pre;\n        Node next;\n        Node(int data,Node pre,Node next){\n            this.data=data;\n            this.pre=pre;\n            this.next=next;\n        }\n        Node(int data){\n            this.data=data;\n        }\n        Node(){\n        }\n    }\n\n    public DoubleLoopLink(int size,Node head,Node tail){\n        this.size=0;\n        this.head=head;\n        this.tail=tail;\n\n    }\n\n    public DoubleLoopLink(){\n        head=new Node();\n        head.pre=tail;\n        head.next=tail;\n        tail=new Node();\n        tail.pre=head;\n        tail.next=head;\n    }\n\n\n    //添加头部节点(只有一个元素的时候)\n    public void addHeadNode(int data){\n        Node node=new Node(data,null,null);\n        head.next=node;\n        node.pre=head;\n        node.next=tail;\n        tail.pre=node;\n        size++;\n    }\n\n    //尾插法\n    public void add(int data){\n        if(head.next==null){\n            addHeadNode(data);\n        }else{\n            Node temp=head;\n            Node newNode=new Node(data);\n            for(int i=0;i<size;i++){\n                temp=temp.next;\n            }\n            newNode.pre=temp;\n            newNode.next=tail;\n            temp.next=newNode;\n            tail.pre=newNode;\n            size++;\n        }\n    }\n\n    //头插法\n    public void addBegin(int data){\n        if(head.next==null){\n            addHeadNode(data);\n            size++;\n        }else {\n            Node newNode = new Node(data);\n            newNode.next = head.next;\n            newNode.pre = head;\n            head.next = newNode;\n            size++;\n        }\n    }\n\n    //从指定索引位置插入相应的内容\n    public void add(int index,int data){\n        //链表里面已经有相应的元素了\n//        if(head.next!=null){\n        Node temp=head;\n        Node newNode=new Node(data);\n        if(index==0){\n            newNode.pre=head;\n            newNode.next=head.next;\n            head.next=newNode;\n        }\n        if(index<size&&index>0){ //最后一个节点处插入需要特殊处理\n            for(int i=0;i<index;i++){\n                temp=temp.next;\n            }\n            newNode.next=temp.next;\n            newNode.pre=temp;\n            temp.next.pre=newNode;\n            temp.next=newNode;\n        }\n        if(index==size&&size!=0){\n            for(int i=0;i<index;i++){\n                temp=temp.next;\n            }\n            temp.next=newNode;\n            newNode.pre=temp;\n            newNode.next=tail;\n            tail.pre=newNode;\n        }\n        size++;\n//        }\n\n    }\n\n    //根据索引，删除某一个链表元素内容\n    public String remove(int index){\n        Node temp=head;\n        if(index>=size){\n            return \"下标超值\";\n        }\n        for(int i=0;i<size;i++){\n            //并非最后一个数值\n            if(i==index){\n                temp.next=temp.next.next;\n                temp.next.pre=temp;\n                size--;\n                break;\n            }\n            temp=temp.next;\n        }\n        return null;\n    }\n\n    public String removeFirst(){\n        return remove(0);\n    }\n\n    public String removeLast(){\n        return remove(size);\n    }\n\n\n    public String disPlay(){\n        Node temp=head.next;\n        StringBuffer result=new StringBuffer(\"[<==>head<==>\");\n        int index=0;\n        for(int i=0;i<size;i++){\n            index++;\n            result.append(temp.data+\"<==>\");\n            temp=temp.next;\n        }\n\n        result.append(\"<==>tail<==>]\");\n        return result.toString();\n    }\n\n    public static void main(String[] args) {\n        DoubleLoopLink doubleLoopLink=new DoubleLoopLink();\n        doubleLoopLink.add(-1);\n        doubleLoopLink.add(-2);\n        doubleLoopLink.add(-3);\n        System.out.println(doubleLoopLink.disPlay());\n    }\n}\n```\n\n## 时间复杂度分析\n\n| 添加操作     | O(n)        |\n| ------------ | ----------- |\n| addFirst     | O(1)        |\n| addLast      | O(n)        |\n| add(index,e) | O(n/2)=O(n) |\n\n| 删除操作        | O(n)        |\n| --------------- | ----------- |\n| rremoveFirst(e) | O(1)        |\n| removeLast      | O(n)        |\n| remove(index,e) | O(n/2)=O(n) |\n\n| 修改操作     | O(n) |\n| ------------ | ---- |\n| set(index,e) | O(n) |\n\n| 查找操作    | O(n) |\n| ----------- | ---- |\n| get(index)  | O(n) |\n| contains(e) | O(n) |\n\n如果值对链表头进行操作时间复杂度为O(1)\n\n## 链表栈\n\n```java\npublic interface Stack<E> {\n\n    int getSize();\n    boolean isEmpty();\n    void push(E e);\n    E pop();\n    E peek();\n}\n\n```\n\n```java\npublic class LinkedList<E> {\n\n    private class Node{\n        public E e;\n        public Node next;\n\n        public Node(E e, Node next){\n            this.e = e;\n            this.next = next;\n        }\n\n        public Node(E e){\n            this(e, null);\n        }\n\n        public Node(){\n            this(null, null);\n        }\n\n        @Override\n        public String toString(){\n            return e.toString();\n        }\n    }\n\n    private Node dummyHead;\n    private int size;\n\n    public LinkedList(){\n        dummyHead = new Node();\n        size = 0;\n    }\n\n    // 获取链表中的元素个数\n    public int getSize(){\n        return size;\n    }\n\n    // 返回链表是否为空\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    // 在链表的index(0-based)位置添加新的元素e\n    // 在链表中不是一个常用的操作，练习用：）\n    public void add(int index, E e){\n\n        if(index < 0 || index > size)\n            throw new IllegalArgumentException(\"Add failed. Illegal index.\");\n\n        Node prev = dummyHead;\n        for(int i = 0 ; i < index ; i ++)\n            prev = prev.next;\n\n        prev.next = new Node(e, prev.next);\n        size ++;\n    }\n\n    // 在链表头添加新的元素e\n    public void addFirst(E e){\n        add(0, e);\n    }\n\n    // 在链表末尾添加新的元素e\n    public void addLast(E e){\n        add(size, e);\n    }\n\n    // 获得链表的第index(0-based)个位置的元素\n    // 在链表中不是一个常用的操作，练习用：）\n    public E get(int index){\n\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Get failed. Illegal index.\");\n\n        Node cur = dummyHead.next;\n        for(int i = 0 ; i < index ; i ++)\n            cur = cur.next;\n        return cur.e;\n    }\n\n    // 获得链表的第一个元素\n    public E getFirst(){\n        return get(0);\n    }\n\n    // 获得链表的最后一个元素\n    public E getLast(){\n        return get(size - 1);\n    }\n\n    // 修改链表的第index(0-based)个位置的元素为e\n    // 在链表中不是一个常用的操作，练习用：）\n    public void set(int index, E e){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Update failed. Illegal index.\");\n\n        Node cur = dummyHead.next;\n        for(int i = 0 ; i < index ; i ++)\n            cur = cur.next;\n        cur.e = e;\n    }\n\n    // 查找链表中是否有元素e\n    public boolean contains(E e){\n        Node cur = dummyHead.next;\n        while(cur != null){\n            if(cur.e.equals(e))\n                return true;\n            cur = cur.next;\n        }\n        return false;\n    }\n\n    // 从链表中删除index(0-based)位置的元素, 返回删除的元素\n    // 在链表中不是一个常用的操作，练习用：）\n    public E remove(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Remove failed. Index is illegal.\");\n\n        // E ret = findNode(index).e; // 两次遍历\n\n        Node prev = dummyHead;\n        for(int i = 0 ; i < index ; i ++)\n            prev = prev.next;\n\n        Node retNode = prev.next;\n        prev.next = retNode.next;\n        retNode.next = null;\n        size --;\n\n        return retNode.e;\n    }\n\n    // 从链表中删除第一个元素, 返回删除的元素\n    public E removeFirst(){\n        return remove(0);\n    }\n\n    // 从链表中删除最后一个元素, 返回删除的元素\n    public E removeLast(){\n        return remove(size - 1);\n    }\n\n    // 从链表中删除元素e\n    public void removeElement(E e){\n\n        Node prev = dummyHead;\n        while(prev.next != null){\n            if(prev.next.e.equals(e))\n                break;\n            prev = prev.next;\n        }\n\n        if(prev.next != null){\n            Node delNode = prev.next;\n            prev.next = delNode.next;\n            delNode.next = null;\n            size --;\n        }\n    }\n\n    @Override\n    public String toString(){\n        StringBuilder res = new StringBuilder();\n\n        Node cur = dummyHead.next;\n        while(cur != null){\n            res.append(cur + \"->\");\n            cur = cur.next;\n        }\n        res.append(\"NULL\");\n\n        return res.toString();\n    }\n}\n```\n\n```java\npublic class LinkedListStack<E> implements Stack<E> {\n\n    private LinkedList<E> list;\n\n    public LinkedListStack(){\n        list = new LinkedList<>();\n    }\n\n    @Override\n    public int getSize(){\n        return list.getSize();\n    }\n\n    @Override\n    public boolean isEmpty(){\n        return list.isEmpty();\n    }\n\n    @Override\n    public void push(E e){\n        list.addFirst(e);\n    }\n\n    @Override\n    public E pop(){\n        return list.removeFirst();\n    }\n\n    @Override\n    public E peek(){\n        return list.getFirst();\n    }\n\n    @Override\n    public String toString(){\n        StringBuilder res = new StringBuilder();\n        res.append(\"Stack: top \");\n        res.append(list);\n        return res.toString();\n    }\n\n    public static void main(String[] args) {\n\n        LinkedListStack<Integer> stack = new LinkedListStack<>();\n\n        for(int i = 0 ; i < 5 ; i ++){\n            stack.push(i);\n            System.out.println(stack);\n        }\n\n        stack.pop();\n        System.out.println(stack);\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505142637.png)\n\n### 性能比较\n\n数组栈和链表栈对比测试:\n\n```java\nimport java.util.Random;\n\npublic class Main {\n\n    // 测试使用stack运行opCount个push和pop操作所需要的时间，单位：秒\n    private static double testStack(Stack<Integer> stack, int opCount){\n\n        long startTime = System.nanoTime();\n\n        Random random = new Random();\n        for(int i = 0 ; i < opCount ; i ++)\n            stack.push(random.nextInt(Integer.MAX_VALUE));\n        for(int i = 0 ; i < opCount ; i ++)\n            stack.pop();\n\n        long endTime = System.nanoTime();\n\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n\n        int opCount = 20000000;\n\n        ArrayStack<Integer> arrayStack = new ArrayStack<>();\n        System.out.println(\"opCount:\"+opCount);\n        double time1 = testStack(arrayStack, opCount);\n        System.out.println(\"ArrayStack, time: \" + time1 + \" s\");\n\n        LinkedListStack<Integer> linkedListStack = new LinkedListStack<>();\n        double time2 = testStack(linkedListStack, opCount);\n        System.out.println(\"LinkedListStack, time: \" + time2 + \" s\");\n\n        // 其实这个时间比较很复杂，因为LinkedListStack中包含更多的new操作\n    }\n}\n\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505145051.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505145150.png)\n\n## 链表队列\n\n```java\npublic class LinkedListQueue<E> implements Queue<E> {\n\n    private class Node{\n        public E e;\n        public Node next;\n\n        public Node(E e, Node next){\n            this.e = e;\n            this.next = next;\n        }\n\n        public Node(E e){\n            this(e, null);\n        }\n\n        public Node(){\n            this(null, null);\n        }\n\n        @Override\n        public String toString(){\n            return e.toString();\n        }\n    }\n\n    private Node head, tail;\n    private int size;\n\n    public LinkedListQueue(){\n        head = null;\n        tail = null;\n        size = 0;\n    }\n\n    @Override\n    public int getSize(){\n        return size;\n    }\n\n    @Override\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    @Override\n    public void enqueue(E e){\n        if(tail == null){\n            tail = new Node(e);\n            head = tail;\n        }\n        else{\n            tail.next = new Node(e);\n            tail = tail.next;\n        }\n        size ++;\n    }\n\n    @Override\n    public E dequeue(){\n        if(isEmpty())\n            throw new IllegalArgumentException(\"Cannot dequeue from an empty queue.\");\n\n        Node retNode = head;\n        head = head.next;\n        retNode.next = null;\n        if(head == null)\n            tail = null;\n        size --;\n        return retNode.e;\n    }\n\n    @Override\n    public E getFront(){\n        if(isEmpty())\n            throw new IllegalArgumentException(\"Queue is empty.\");\n        return head.e;\n    }\n\n    @Override\n    public String toString(){\n        StringBuilder res = new StringBuilder();\n        res.append(\"Queue: front \");\n\n        Node cur = head;\n        while(cur != null) {\n            res.append(cur + \"->\");\n            cur = cur.next;\n        }\n        res.append(\"NULL tail\");\n        return res.toString();\n    }\n\n    public static void main(String[] args){\n\n        LinkedListQueue<Integer> queue = new LinkedListQueue<>();\n        for(int i = 0 ; i < 10 ; i ++){\n            queue.enqueue(i);\n            System.out.println(queue);\n\n            if(i % 3 == 2){\n                queue.dequeue();\n                System.out.println(queue);\n            }\n        }\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505161851.png)\n\n## 队列性能\n\n### 性能比较\n\n```java\nimport java.util.Random;\n\npublic class Main {\n\n    // 测试使用q运行opCount个enqueueu和dequeue操作所需要的时间，单位：秒\n    private static double testQueue(Queue<Integer> q, int opCount){\n\n        long startTime = System.nanoTime();\n\n        Random random = new Random();\n        for(int i = 0 ; i < opCount ; i ++)\n            q.enqueue(random.nextInt(Integer.MAX_VALUE));\n        for(int i = 0 ; i < opCount ; i ++)\n            q.dequeue();\n\n        long endTime = System.nanoTime();\n\n        return (endTime - startTime) / 1000000000.0;\n    }\n\n    public static void main(String[] args) {\n\n        int opCount = 100000;\n\n        ArrayQueue<Integer> arrayQueue = new ArrayQueue<>();\n        double time1 = testQueue(arrayQueue, opCount);\n        System.out.println(\"ArrayQueue, time: \" + time1 + \" s\");\n\n        LoopQueue<Integer> loopQueue = new LoopQueue<>();\n        double time2 = testQueue(loopQueue, opCount);\n        System.out.println(\"LoopQueue, time: \" + time2 + \" s\");\n\n        LinkedListQueue<Integer> linkedListQueue = new LinkedListQueue<>();\n        double time3 = testQueue(linkedListQueue, opCount);\n        System.out.println(\"LinkedListQueue, time: \" + time3 + \" s\");\n    }\n\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/20190505162053.png)\n\n## 资料参考\n\n<<数据结构与算法之美>>\n\n<<玩转数据结构>>\n\nGitee: <https://gitee.com/IdeaHome_admin/projects>\n\n## 斯坦福链表问题\n\n{% pdf https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/链表/LinkedListProblems.pdf %}\n\n\n\n\n\n\n\n\n\n\n\n","tags":["链表"],"categories":["数据结构"]},{"title":"数据结构之队列","url":"/2019/05/03/数据结构之队列/","content":"\n {{ \" 数据结构队列的相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\n队列是是只允许在一端进行插入操作,而在另一端进行删除操作的线性表。\n\n队列是一种先进先出的线性表，简称FIFO允许插入的以端称为队尾，允许删除的一端被称为队头。\n\n## 入队\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/队列入队.gif)\n\n## 出队\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/队列出队.gif)\n\n## 数组实现队列\n\n```java\npublic class Array<E> {\n\n    private E[] data;\n    private int size;\n\n    // 构造函数，传入数组的容量capacity构造Array\n    public Array(int capacity){\n        data = (E[])new Object[capacity];\n        size = 0;\n    }\n\n    // 无参数的构造函数，默认数组的容量capacity=10\n    public Array(){\n        this(10);\n    }\n\n    // 获取数组的容量\n    public int getCapacity(){\n        return data.length;\n    }\n\n    // 获取数组中的元素个数\n    public int getSize(){\n        return size;\n    }\n\n    // 返回数组是否为空\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    // 在index索引的位置插入一个新元素e\n    public void add(int index, E e){\n\n        if(index < 0 || index > size)\n            throw new IllegalArgumentException(\"Add failed. Require index >= 0 and index <= size.\");\n\n        if(size == data.length)\n            resize(2 * data.length);\n\n        for(int i = size - 1; i >= index ; i --)\n            data[i + 1] = data[i];\n\n        data[index] = e;\n\n        size ++;\n    }\n\n    // 向所有元素后添加一个新元素\n    public void addLast(E e){\n        add(size, e);\n    }\n\n    // 在所有元素前添加一个新元素\n    public void addFirst(E e){\n        add(0, e);\n    }\n\n    // 获取index索引位置的元素\n    public E get(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Get failed. Index is illegal.\");\n        return data[index];\n    }\n\n    public E getLast(){\n        return get(size - 1);\n    }\n\n    public E getFirst(){\n        return get(0);\n    }\n\n    // 修改index索引位置的元素为e\n    public void set(int index, E e){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Set failed. Index is illegal.\");\n        data[index] = e;\n    }\n\n    // 查找数组中是否有元素e\n    public boolean contains(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return true;\n        }\n        return false;\n    }\n\n    // 查找数组中元素e所在的索引，如果不存在元素e，则返回-1\n    public int find(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return i;\n        }\n        return -1;\n    }\n\n    // 从数组中删除index位置的元素, 返回删除的元素\n    public E remove(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Remove failed. Index is illegal.\");\n\n        E ret = data[index];\n        for(int i = index + 1 ; i < size ; i ++)\n            data[i - 1] = data[i];\n        size --;\n        data[size] = null; // loitering objects != memory leak\n\n        if(size == data.length / 4 && data.length / 2 != 0)\n            resize(data.length / 2);\n        return ret;\n    }\n\n    // 从数组中删除第一个元素, 返回删除的元素\n    public E removeFirst(){\n        return remove(0);\n    }\n\n    // 从数组中删除最后一个元素, 返回删除的元素\n    public E removeLast(){\n        return remove(size - 1);\n    }\n\n    // 从数组中删除元素e\n    public void removeElement(E e){\n        int index = find(e);\n        if(index != -1)\n            remove(index);\n    }\n\n    @Override\n    public String toString(){\n\n        StringBuilder res = new StringBuilder();\n        res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length));\n        res.append('[');\n        for(int i = 0 ; i < size ; i ++){\n            res.append(data[i]);\n            if(i != size - 1)\n                res.append(\", \");\n        }\n        res.append(']');\n        return res.toString();\n    }\n\n    // 将数组空间的容量变成newCapacity大小\n    private void resize(int newCapacity){\n\n        E[] newData = (E[])new Object[newCapacity];\n        for(int i = 0 ; i < size ; i ++)\n            newData[i] = data[i];\n        data = newData;\n    }\n}\n```\n\n```java\npublic interface Queue<E> {\n    int getSize();\n    boolean isEmpty();\n    void enqueue(E e);\n    E dequeue();\n    E getFront();\n}\n```\n\n```java\npublic class ArrayQueue<E> implements Queue<E> {\n    private Array<E> array;\n\n    public ArrayQueue(int capacity) {\n        array = new Array<>(capacity);\n    }\n\n    public ArrayQueue() {\n        array = new Array<>();\n    }\n\n    @Override\n    public int getSize() {\n        return array.getSize();\n    }\n\n    @Override\n    public boolean isEmpty() {\n        return array.isEmpty();\n    }\n\n    public int getCapacity() {\n        return array.getCapacity();\n    }\n\n    @Override\n    public void enqueue(E e) {\n        array.addLast(e);\n    }\n\n    @Override\n    public E dequeue() {\n        return array.removeFirst();\n    }\n\n    @Override\n    public E getFront() {\n        return array.getFirst();\n    }\n\n    @Override\n    public String toString() {\n\n        StringBuilder res = new StringBuilder();\n        res.append(\"Queue:  \");\n        res.append(\"front [ \");\n        for (int i = 0; i < array.getSize(); i++) {\n            res.append(array.get(i));\n            if (i != array.getSize() - 1)\n                res.append(\", \");\n        }\n        res.append(\"] tail\");\n\n        return res.toString();\n    }\n\n    public static void main(String[] args) {\n        ArrayQueue<Integer> queue = new ArrayQueue <> ();\n        System.out.println();\n        for (int i = 0; i < 100000; i++) {\n            queue.enqueue(i);\n            System.out.println(queue);\n            if (i % 3 == 2) {\n                queue.dequeue();\n                System.out.println(queue);\n            }\n        }\n    }\n}\n```\n\n### 测试\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503172849.png)\n\n### 复杂度分析\n\n| ArrayQueue&lt;E&gt; |                               |\n| ------------------- | ----------------------------- |\n| void enqueue(E e)   | O(1)均摊                      |\n| E dequeue()         | <font color=\"red\">O(n)</font> |\n| E getFront          | o(1)                          |\n| int   getSize       | O(1)                          |\n| boolean isEmpty()   | O(1)                          |\n\n如果使用顺序队列的话，试想一下如果我们队列十分的大，那么我们需要耗费的时间是多少呢。\n\n## 循环队列\n\n队列首位相接的顺序存储结构。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/循环队列.gif)\n\n通过这样的方法，我们成功避免了数据搬移操作。看起来不难理解在用数组实现的非循环队列中，队满的判断条件是 tail == n，队空的判断条件是 head == tail。那针对循环队列，如何判断队空和队满呢？\n队列为空的判断条件仍然是 head == tail。但队列满的判断条件就稍微有点复杂了。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503175043.png)\n\ntail=3，head=4，n=8，所以总结一下规律就是：(3+1)%8=4。当队满时，(tail+1)%n=head，当队列满时，图中的 tail 指向的位置实际上是没有存储数据的。所以，循环队列会 浪费一个数组的存储空间。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503175243.png)\n\n### 基于数组实现\n\n循环队列需要我们单独去实现。\n\n```java\npublic interface Queue<E> {\n    int getSize();\n    boolean isEmpty();\n    void enqueue(E e);\n    E dequeue();\n    E getFront();\n}\n```\n\n\n\n ```java\npublic class LoopQueue<E> implements Queue<E> {\n\n    private E[] data;\n    private int front, tail;\n    private int size;  // 有兴趣的同学，在完成这一章后，可以思考一下：\n    // LoopQueue中不声明size，如何完成所有的逻辑？\n    // 这个问题可能会比大家想象的要难一点点：）\n\n    public LoopQueue(int capacity){\n        data = (E[])new Object[capacity + 1];\n        front = 0;\n        tail = 0;\n        size = 0;\n    }\n\n    public LoopQueue(){\n        this(10);\n    }\n\n    public int getCapacity(){\n        return data.length - 1;\n    }\n\n    @Override\n    public boolean isEmpty(){\n        return front == tail;\n    }\n\n    @Override\n    public int getSize(){\n        return size;\n    }\n\n    @Override\n    public void enqueue(E e){\n\n        if((tail + 1) % data.length == front)\n            resize(getCapacity() * 2);\n\n        data[tail] = e;\n        tail = (tail + 1) % data.length;\n        size ++;\n    }\n\n    @Override\n    public E dequeue(){\n\n        if(isEmpty())\n            throw new IllegalArgumentException(\"Cannot dequeue from an empty queue.\");\n\n        E ret = data[front];\n        data[front] = null;\n        front = (front + 1) % data.length;\n        size --;\n        if(size == getCapacity() / 4 && getCapacity() / 2 != 0)\n            resize(getCapacity() / 2);\n        return ret;\n    }\n\n    @Override\n    public E getFront(){\n        if(isEmpty())\n            throw new IllegalArgumentException(\"Queue is empty.\");\n        return data[front];\n    }\n\n    private void resize(int newCapacity){\n\n        E[] newData = (E[])new Object[newCapacity + 1];\n        for(int i = 0 ; i < size ; i ++)\n            newData[i] = data[(i + front) % data.length];\n\n        data = newData;\n        front = 0;\n        tail = size;\n    }\n\n    @Override\n    public String toString(){\n\n        StringBuilder res = new StringBuilder();\n        res.append(String.format(\"Queue: size = %d , capacity = %d\\n\", size, getCapacity()));\n        res.append(\"front [\");\n        for(int i = front ; i != tail ; i = (i + 1) % data.length){\n            res.append(data[i]);\n            if((i + 1) % data.length != tail)\n                res.append(\", \");\n        }\n        res.append(\"] tail\");\n        return res.toString();\n    }\n\n    public static void main(String[] args){   //测试\n\n        LoopQueue<Integer> queue = new LoopQueue<>();\n        for(int i = 0 ; i < 10 ; i ++){\n            queue.enqueue(i);\n            System.out.println(queue);\n\n            if(i % 3 == 2){\n                queue.dequeue();\n                System.out.println(queue);\n            }\n        }\n    }\n}\n ```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503191623.png)\n\n### 对比\n\n```java\nimport java.util.Random;\n\npublic class Main {\n\n    // 测试使用q运行opCount个enqueueu和dequeue操作所需要的时间，单位：秒\n    private static double testQueue(Queue<Integer> q, int opCount) {\n\n        long startTime = System.nanoTime();\n\n        Random random = new Random();\n        for (int i = 0; i < opCount; i++) {\n            q.enqueue(random.nextInt(Integer.MAX_VALUE));\n        }\n        for (int i = 0; i < opCount; i++) {\n            q.dequeue();\n        }\n\n        long endTime = System.nanoTime();\n\n        return (endTime - startTime) / 1000000000.0;  //纳秒10^9\n    }\n\n    public static void main(String[] args) {\n\n        int opCount = 100000;\t//随机10万个数插入队列\n\n        ArrayQueue<Integer> arrayQueue = new ArrayQueue<>();\n        double time1 = testQueue(arrayQueue, opCount);\n        System.out.println(\"ArrayQueue, time: \" + time1 + \" s\");\n\n        LoopQueue<Integer> loopQueue = new LoopQueue<>();\n        double time2 = testQueue(loopQueue, opCount);\n        System.out.println(\"LoopQueue, time: \" + time2 + \" s\");\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503192442.png)\n\n可见时间差距是将近200多倍。\n\n##  应用\n\n### 阻塞队列\n\n阻塞队列在队列基础上增加了阻塞操作。在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503193222.png)\n\n基于阻塞队列实现的“生产者 - 消费者模型”，可以有效地协调生产和消费的速度。当“生产者”生 产数据的速度过快，“消费者”来不及消费时，存储数据的队列很快就会满了。这个时候，生产者就阻塞等待，直到“消费者”消费了数据，“生产者”才会被唤醒继续“生产”。\n而且不仅如此，基于阻塞队列，我可以通过协调“生产者”和“消费者”的个数，来提高数据的处理效率。比如前面的例子，我们可以多配置几个“消费者”，来应对一个“生产者”。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/队列/20190503193422.png)\n\n### 并发队列\n\n线程安全的队列我们叫作并发队列。最简单直接的实现方式是直接在 enqueue()、dequeue() 方法 上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。实际上，基于数组的循环队列，利用 CAS 原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用 更加广泛的原因。\n\n## 参考资料\n\n《数据结构与算法之美》\n\n《玩转数据结构》\n\n《大话数据结构》\n\n\n\n\n\n\n\n","tags":["队列"],"categories":["数据结构"]},{"title":"数据结构之栈","url":"/2019/05/03/数据结构之栈/","content":"\n {{ \"数据结构栈的相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\n限定仅在表尾进行插入和删除操作的线性表。允许插入和删除的一端成为栈顶，另一端成为栈低，不含任何元素的栈成为空栈，栈又称为先进先出的线性表，简称LIFO结构。\n\n栈的插入操作，叫做进栈，也称压栈，入栈。\n\n栈的删除操作，也叫出战，也有的叫做弹栈。\n\n## 入栈\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/入栈操作.gif)\n\n## 出栈\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/出栈操作.gif)\n\n## 实现一个栈\n\n栈主要包含两个操作，入栈和出栈，也就是在栈顶插入一个数据和从栈顶删除一个数据。栈既可以用数组来实现，也可以用链表来实现。用数组实现的栈，我们叫作顺序栈，用链表实现的栈，我们叫作链式栈。\n根据上一节我们实现的数组代码我们来实现一个自己的栈。\n\n```java\npublic interface Stack<E> {\n\n    int getSize();\t//获取栈大小\n    boolean isEmpty();\t//判断是否为空\n    void push(E e);\t\t//压栈\n    E pop();\t\t//弹栈\n    E peek();\t\t//查看栈顶\n}\n```\n\n```java\npublic class ArrayStack<E> implements Stack<E> {\n\n    private Array<E> array;\n\n    public ArrayStack(int capacity){\n        array = new Array<>(capacity);\n    }\n\n    public ArrayStack(){\n        array = new Array<>();\n    }\n\n    @Override\n    public int getSize(){\n        return array.getSize();\n    }\n\n    @Override\n    public boolean isEmpty(){\n        return array.isEmpty();\n    }\n\n    public int getCapacity(){\n        return array.getCapacity();\n    }\n\n    @Override\n    public void push(E e){\n        array.addLast(e);\n    }\n\n    @Override\n    public E pop(){\n        return array.removeLast();\n    }\n\n    @Override\n    public E peek(){\n        return array.getLast();\n    }\n\n    @Override\n    public String toString(){\n        StringBuilder res = new StringBuilder();\n        res.append(\"Stack: \");\n        res.append('[');\n        for(int i = 0 ; i < array.getSize() ; i ++){\n            res.append(array.get(i));\n            if(i != array.getSize() - 1)\n                res.append(\", \");\n        }\n        res.append(\"] top\");\n        return res.toString();\n    }\n}\n```\n\n### 测试\n\n```java\npublic class Main {\n\n    public static void main(String[] args) {\n\n        ArrayStack<Integer> stack = new ArrayStack<>();\n\n        for(int i = 0 ; i < 6 ; i ++){\n            stack.push(i); //压栈\n            System.out.println(stack);\n        }\n\n        stack.pop(); //弹栈\n        System.out.println(stack);\n        System.out.println(\"栈顶为:\"+stack.peek());\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190503112010.png)\n\n## 分析\n\n不管是顺序栈还是链式栈，我们存储数据只要一个大小为 n 的数组就够了。入栈和出栈过程中，只需要一两个临时变量存储空间，所以空间复杂度是 O(1)。\n这里存储数据需要一个大小为 n 的数组，并不是说空间复杂度就是 O(n)。因为，这 n 个空间 是必须的，无法省掉。所以我们说空间复杂度的时候，是指除了原本的数据存储空间外，算法运行还需要额外的存储空间。\n\n出栈的时间复杂度仍然是 O(1)。但是，对于入栈操作来说，情况就不一样。当栈中有空闲空间时，入栈操作的时间复杂度 为 O(1)。但当空间不够时，就需要重新申请内存和数据搬移，所以时间复杂度就变成了 O(n)。\n\n## 摊还分析\n\n栈空间不够时，我们重新申请一个是原来大小两倍的数组；为了简化分析，假设只有入栈操作没有出栈操作；定义不涉及内存搬移的入栈操作为 simple-push 操作，时间复杂度为 O(1)。如果当前栈大小为 K，并且已满，当再有新的数据要入栈时，就需要重新申请 2 倍大小的内存，并 且做 K 个数据的搬移操作，然后再入栈。但是，接下来的 K-1 次入栈操作，我们都不需要再重新申请内存和搬移数据，所以这 K-1 次入栈操作都只需要一个 simple-push 操作就可以完成。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190503113631.png)\n\n## 栈应用\n\n### 函数应用\n\n操作系统给每个线程分配了一块独立的内存空间，这块内存被组织成“栈”这种结构, 用来存储函数调用时的临时变量。每进入一个函数，就会将临时变量作为一个栈帧入栈，当被调用函数执行完成，返回之后，将这个函数对应的栈帧出栈。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/栈的应用函数调用.gif)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190503114934.png)\n\n<div id='marguee' style='text-align:center;font-size:20px;color:red'>这是一个比较费流量的GIF！！！</div>\n### 表达式求值\n\n我将算术表达式简化为只包含加减乘除四则运算，比如：34+13*9+44-12/3。对于这个四则运算编译器就是通过两个栈来实现的。其中一个保存操作数的栈，另一个是保存运算符的栈。我们从左向右遍历表达式，当遇到数字，我们就直接压入操作数栈；当遇到运算符，就与运算符栈的栈顶元素进行比较。\n如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取 2 个操作数，然后进行计算，再把计算 完的结果压入操作数栈，继续比较。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/栈的表达式.gif)\n\n### 在括号匹配\n\n我们假设表达式中只包含三种括号，圆括号 ()、方括号 [] 和花括号{}，并且它们可以任意嵌套。比如，{[{}]}或 [{()}([])] 等都为合法格式，而{[}()] 或 [({)] 为不合法的格式。在给你一个包含三种括号的表达式字符串。我们用栈来保存未匹配的左括号，从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。如果能够匹配，比 如“(”跟“)”匹配，“[”跟“]”匹配，“{”跟“}”匹配，则继续扫描剩下的字符串。如果扫描的过程中，遇到不 能配对的右括号，或者栈中没有数据，则说明为非法格式。当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。\n\n```java\nimport java.util.Stack;\n\nclass Solution {\n\n    public boolean isValid(String s) {\n\n        Stack<Character> stack = new Stack<>();\n        for (int i = 0; i < s.length(); i++) {\n            char c = s.charAt(i);\n            if (c == '(' || c == '[' || c == '{')\n                stack.push(c);\n            else {\n                if (stack.isEmpty())\n                    return false;\n\n                char topChar = stack.pop();\n                if (c == ')' && topChar != '(')\n                    return false;\n                if (c == ']' && topChar != '[')\n                    return false;\n                if (c == '}' && topChar != '{')\n                    return false;\n            }\n        }\n        return stack.isEmpty();\n    }\n\n\n    public static void main(String[] args) {\n\n        System.out.println((new Solution()).isValid(\"({[]})[]{}\"));\n        System.out.println((new Solution()).isValid(\"([)]\"));\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190503121838.png)\n\n## 参考资料\n\n《数据结构与算法之美》\n\n《大话数据结构》\n\n《玩转数据结构》\n\n\n\n","tags":["栈"],"categories":["数据结构"]},{"title":"数据结构之数组","url":"/2019/05/02/数据结构之数组/","content":"\n {{ \"数据结构数组的学习和自己实现一个数组\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\n（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。\n\n### 线性表\n\n零个或多个数据元素的有限序列。每个线性表上的数据最多只有前和后两个方向。其实除了数组，链表、队列、栈等也是线性表结构。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190502212724.png)\n\n它相对立的概念是非线性表，比如二叉树、堆、图等。之所以叫非线性，是因为，在非线性表中，数据之间并不是简单的前后关系。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190502212847.png)\n\n连续的内存空间和相同类型的数据:这两个限制也让数组的很多操作变得非常低效，比如要想在数组中删除、插入一个数据，为了保证连续性，就需要做大量的数据搬移工作。但是数组的随机访问效率确实十分的高。\n\n### 案例\n\n拿一个长度为 10 的 int 类型的数组 int[] a = new int[10] 来举例\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190502213326.png)\n\n计算机给数组 a[10]，分配了一块连续内存空间 1000～1039，其中，内存块的首地址为 base_address = 1000。当我们要随机访问一个地址的时候，，它会首先通过下面的寻址公式，计算出该元素存储的内存地址：\n\n> a[i]_address = base_address + i * data_type_size\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/数组O1添加.gif)\n\n数组支持随机访问，根据下标随机访问的时间复杂度为 O(1)。\n\n数组的插入和删除都十分的低效。\n\n## 插入\n\n假设数组的长度为 n，将一个数据插入到数组中的第 k 个位置。为了把第 k 个 位置腾出来，给新来的数据，我们需要将第 k～n 这部分的元素都顺序地往后挪一位。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/数组中插入数据.gif)\n\n如果在数组的末尾插入元素，那就不需要移动数据了，这时的时间复杂度为 O(1)。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/数组末尾插入数据.gif)\n\n但如果在数组的开头插入元素，那所有的数据都需要依次往后移动一位，所以最坏时间复杂度是 O(n)。 因为我们在 每个位置插入元素的概率是一样的，所以平均情况时间复杂度为 (1+2+…n)/n=O(n)。\n\n如果数组中的数据是有序的，我们在某个位置插入一个新的元素时，就必须按照刚才的方法搬移 k 之后的数据。但是，如果数组中存储的数据并没有任何规律，数组只是被当作一个存储数据的集 合。在这种情况下，如果要将某个数组插入到第 k 个位置，为了避免大规模的数据搬移，我们还有 一个简单的办法就是，直接将第 k 位的数据搬移到数组元素的最后，把新的元素直接放入第 k 个位 置。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/快排思想.gif)\n\n在特定场景下，在第 k 个位置插入一个元素的时间复杂度就会降为 O(1)。这个 处理思想在快排中会用。\n\n## 删除\n\n插入数据类似，我们要删除第 k 个位置的数据，为了内存的连续性，也需要搬移数据，不然中间就会出现空洞，内存就不连续了。在某些特殊场景下，我们并不一定非得追求数组中数据的连续性。如果我们将多次删除操作集中在一起执行，这种就类似于Java虚拟中的标记清除。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/标记清除.gif)\n\n当数组没有更多空间存储数据时，我们再触发执行一次真正的删除操作，这样就大大减少了删除操作导致的数据搬移。\n\n## 容器\n\nArrayList 最大的优势就是可以将很多数组操作的细节封装起来。比如前面提到的数组 插入、删除数据时需要搬移其他数据等。另外，它还有一个优势，就是支持动态扩容。\n数组本身在定义的时候需要预先指定大小，因为需要分配连续的内存空间。如果我们申请了大小为 10 的数组，当第 11 个数据需要存储到数组中时，我们就需要重新分配一块更大的空间，将原来的 数据复制过去，然后再将新的数据插入。\n如果使用 ArrayList，我们就完全不需要关心底层的扩容逻辑，ArrayList 已经帮我们实现好了。每次 存储空间不够的时候，它都会将空间自动扩容为 1.5 倍大小。\n不过，这里需要注意一点，因为扩容操作涉及内存申请和数据搬移，是比较耗时的。所以，如果事 先能确定需要存储的数据大小，最好在创建 ArrayList 的时候事先指定数据大小。\n比如我们要从数据库中取出 10000 条数据放入 ArrayList。我们看下面这几行代码，你会发现，相比 之下，事先指定数据大小可以省掉很多次内存申请和数据搬移操作。\n\n```java\npublic class ListTest {\n    public static void main(String[] args) {\n        ArrayList<Integer> list = new ArrayList(10000);\n        for (int i = 0; i < 10000; ++i) {\n            list.add(i);\n        }\n        System.out.println(list);\n    }\n}\n```\n\nJava ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。\n\n如果数据大小事先已知，并且对数据的操作非常简单，用不到 ArrayList 提供的大部分方法，也可 以直接使用数组。\n\n当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：\n\nArrayList &lt;ArrayList &gt;array。\n\n对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。\n\n## 小结\n\n从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移（offset）”。前面也讲到，如果用 a 来表示数组的首地址，a[0] 就是偏移为 0 的位置，也就是首地址，a[k] 就表示偏移 k 个 type_size 的位置，所以计算 a[k] 的内存地址只需要用这个公式：\n\n```\na[k]_address = base_address + k * type_size\n```\n\n但是，如果数组从 1 开始计数，那我们计算数组元素 a[k] 的内存地址就会变为：\n\n```\na[k]_address = base_address + (k-1)*type_siz\n```\n\n从 1 开始编号，每次随机访问数组元素都多了一次减法运算，对于 CPU 来说，就是多了一次减法指令。\n\n数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。\n\n## 实现\n\n```java\npublic class Array<E> {\n\n    private E[] data;\n    private int size;\n\n    // 构造函数，传入数组的容量capacity构造Array\n    public Array(int capacity){\n        data = (E[])new Object[capacity];\n        size = 0;\n    }\n\n    // 无参数的构造函数，默认数组的容量capacity=10\n    public Array(){\n        this(10);\n    }\n\n    // 获取数组的容量\n    public int getCapacity(){\n        return data.length;\n    }\n\n    // 获取数组中的元素个数\n    public int getSize(){\n        return size;\n    }\n\n    // 返回数组是否为空\n    public boolean isEmpty(){\n        return size == 0;\n    }\n\n    // 在index索引的位置插入一个新元素e\n    public void add(int index, E e){\n\n        if(index < 0 || index > size)\n            throw new IllegalArgumentException(\"Add failed. Require index >= 0 and index <= size.\");\n\n        if(size == data.length)\n            resize(2 * data.length);\n\n        for(int i = size - 1; i >= index ; i --)\n            data[i + 1] = data[i];\n\n        data[index] = e;\n\n        size ++;\n    }\n\n    // 向所有元素后添加一个新元素\n    public void addLast(E e){\n        add(size, e);\n    }\n\n    // 在所有元素前添加一个新元素\n    public void addFirst(E e){\n        add(0, e);\n    }\n\n    // 获取index索引位置的元素\n    public E get(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Get failed. Index is illegal.\");\n        return data[index];\n    }\n\n    // 修改index索引位置的元素为e\n    public void set(int index, E e){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Set failed. Index is illegal.\");\n        data[index] = e;\n    }\n\n    // 查找数组中是否有元素e\n    public boolean contains(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return true;\n        }\n        return false;\n    }\n\n    // 查找数组中元素e所在的索引，如果不存在元素e，则返回-1\n    public int find(E e){\n        for(int i = 0 ; i < size ; i ++){\n            if(data[i].equals(e))\n                return i;\n        }\n        return -1;\n    }\n\n    // 从数组中删除index位置的元素, 返回删除的元素\n    public E remove(int index){\n        if(index < 0 || index >= size)\n            throw new IllegalArgumentException(\"Remove failed. Index is illegal.\");\n\n        E ret = data[index];\n        for(int i = index + 1 ; i < size ; i ++)\n            data[i - 1] = data[i];\n        size --;\n        data[size] = null; // loitering objects != memory leak\n\n        if(size == data.length / 4 && data.length / 2 != 0)\n            resize(data.length / 2);\n        return ret;\n    }\n\n    // 从数组中删除第一个元素, 返回删除的元素\n    public E removeFirst(){\n        return remove(0);\n    }\n\n    // 从数组中删除最后一个元素, 返回删除的元素\n    public E removeLast(){\n        return remove(size - 1);\n    }\n\n    // 从数组中删除元素e\n    public void removeElement(E e){\n        int index = find(e);\n        if(index != -1)\n            remove(index);\n    }\n\n    @Override\n    public String toString(){\n\n        StringBuilder res = new StringBuilder();\n        res.append(String.format(\"Array: size = %d , capacity = %d\\n\", size, data.length));\n        res.append('[');\n        for(int i = 0 ; i < size ; i ++){\n            res.append(data[i]);\n            if(i != size - 1)\n                res.append(\", \");\n        }\n        res.append(']');\n        return res.toString();\n    }\n\n    // 将数组空间的容量变成newCapacity大小\n    private void resize(int newCapacity){\n\n        E[] newData = (E[])new Object[newCapacity];\n        for(int i = 0 ; i < size ; i ++)\n            newData[i] = data[i];\n        data = newData;\n    }\n}\n```\n\n### 测试\n\n```java\npublic class Main {\n\n    public static void main(String[] args) {\n\n        Array<Integer> arr = new Array<>();\n        for(int i = 0 ; i < 10 ; i ++)\n            arr.addLast(i);\n        System.out.println(arr);\n\n        arr.add(1, 100);\n        System.out.println(arr);\n\n        arr.addFirst(-1);\n        System.out.println(arr);\n\n        arr.remove(2);\n        System.out.println(arr);\n\n        arr.removeElement(4);\n        System.out.println(arr);\n\n        arr.removeFirst();\n        System.out.println(arr);\n\n        for(int i = 0 ; i < 4 ; i ++){\n            arr.removeFirst();\n            System.out.println(arr);\n        }\n    }\n}\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/数组/20190502233933.png)\n\n## 参考资料\n\n《玩转数据结构》\n\n《数据结构与算法之美》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["数组"],"categories":["数据结构"]},{"title":"数据结构与算法前置","url":"/2019/05/02/数据结构与算法前提/","content":"\n {{ \"数据结构绪论学习笔记\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n\n\n## 简介\n\n数据结构是一门研究非等值计算的程序设计问题的操作对象,以及他们之间的关系核操作等相关问题的学科。\n\n简而言之数据结构是计算机存储、组织数据的方式。数据结构是指相互之间存在一种或多种特定关系的数据元素的集合。通常情况下，精心选择的数据结构可以带来更高的运行或者存储效率。数据结构往往同高效的检索算法和索引技术有关。\n\n### 数据\n\n客观描述事务的符号，计算机中可以操作的对象，可以被计算机识别，并输入给计算机处理的符号集合。不仅仅包括整型、实型等数值类型，还包括字符、图像、视频等非数值型。前提为：\n\n1. 可以被输入到计算机。\n2. 可以被计算机程序处理。\n\n### 数据元素\n\n组成数据的有意义的基本单位，通常作为整体被称为记录。通俗来讲是一个类的子集，比如禽类的子集：鸭\n\n### 数据项\n\n一个数据元素可以由若干个数据项组成，数据详是数据不可分割的最小单位。\n\n### 数据对象\n\n性质相同的数据元素的集合，是数据的子集。\n\n## 小结\n\n数据结构是相互之间存在一种或者多种特定关系的数据元素的集合。\n\n## 划分\n\n### 逻辑结构\n\n逻数据对象中元素之间的相互关系。\n\n#### 集合结构\n\n元素中的数据元素除了同属于一个集合外，没有其他关系。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502115443.png)\n\n#### 线性结构\n\n线性结构中的数据元素之间是一对一的关系\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502115559.png)\n\n#### 树形结构\n\n树型结构中的数据元素之间存在一种一对多的层次关系\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502115645.png)\n\n#### 图形数据结构\n\n图形数据结构的元素是多对多的关系\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502115749.png)\n\n### 物理结构\n\n数据的逻辑结构在计算机中的存储形式\n\n#### 顺序结构存储:\n\n数据元素存放在地址连续的存储单元内，数据之间的逻辑关系核物理关系是一致的。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502120210.png)\n\n#### 链式存储结构\n\n元素存放在任意的存储单元里，存储单元可以连续，也可以不连续。\n\n### 抽象数据类型\n\n一组性质相同的值的集合及定义在此几何上的一些操作的总称。\n\n#### 原子类型\n\n不可以再分解的基本类型，包括整型、实型、字符型等。\n\n#### 结构类型\n\n由若干个类型组合而成，是可以再分解的，比如整型数组是由若干个整型数据组成的。\n\n## 算法\n\n解决特定问题求解步骤的描述，在计算机中表现为指令集的有限序列，并且每条指令集表示一个或者多个操作。\n\n ### 特性\n\n#### 输入输出\n\n零个或多个输入，至少由一个或者多个输出。\n\n#### 有穷性\n\n算法再执行有限步骤之后，自动结束而不是无限循环，并且每一个步骤要再可接受的时间内完成。\n\n#### 确定性\n\n算法的每一步骤都具有具体的含义，不会出现二义性。即在相同的输入输出只能由相同的结果。\n\n#### 可行性\n\n算法的每一步必须可行，每一步都能通过执行有限次数完成。\n\n### 设计要求\n\n#### 正确性\n\n算法至少应该具有输入、输出和加工处理无歧义性，能够正确反映问题的需求，能够得到问题的正确答案。\n\n1. 程序没有语法错误。\n2. 算法程序对于合法的输入数据能够得到满足要求的输出结果。\n3. 算法程序对于非法的输入数据能够满足规格说明的结果。\n4. 算法程序对于精心选择的，甚至刁难的测试数据都要有满足要求的输入输出。\n\n#### 可读性\n\n便于阅读理解、和交流。\n\n#### 健壮性\n\n输入数据不合法时，算法能够做出相关处理，而不是产生异常或莫名奇妙的结果。\n\n#### 时间效率高和存储量低\n\n时间效率是指算法执行的时间，对于同一个问题有多种算法解决。执行时间短的算法效率高，执行时间长的效率低。存储量需求是指在执行过程中需要的最大存储空间。设计算法应该要尽量满足时间效率高和存储量低的需求。\n\n## 算法度量方法\n\n### 事后统计方法\n\n通过设计好的测试程序和数据，利用计算机计时对不同算法表程序运行的时间进行比较，从而确定算法效率的高低。\n\n#### 缺陷\n\n1. 需要提前编制好程序。\n2. 依赖计算机硬件和软件等环境。\n3. 算法测试数据设计困难。\n\n### 事前分析估算方法\n\n计算机程序编制之前，需要统计方法对算法进行估算。\n\n#### 影响因素\n\n高级语言程序编写的程序在计算机上运行时间取决于：\n\n1. 算法采用的策略和方法 (**根本**)\n2. 编译产生的代码质量\n3. 问题输入的规模\n4. 机器执行指令的熟度，\n\n## 算法时间复杂度\n\n语句总的执行次数$T(n)$是关于问题规模$n$的函数，进而分析$T(n)$随着$n$的情况变化而确定$T(n)$的规模级。算法的时间复杂度，也就是算法的时间度量，记作：$T(n)$=$O(f(n))$，它表示随着问题规模$n$的增大，算法的执行时间的增长率和$f(n)$的增长率相同，称作算法的逐渐时间复杂度，简称时间复杂度。其中$f(n)$是问题规模$n$的某个函数。\n\n假设每行代码执行的时间都一样，为 unit_time。\n\n```java\n int cal(int n) {\n   int sum = 0;\t\t//执行一次\n   int i = 1;\t\t//执行一次\n   for (; i <= n; ++i) {\t//执行n次\n     sum = sum + i;\t//执行n次\n   }\n   return sum;\t   //总的时间复杂度为2n+2\n }\n```\n\n```java\n int cal(int n) {\n   int sum = 0;\t//执行一次\n   int i = 1;\t//执行一次\n   int j = 1;\t//执行一次\n   for (; i <= n; ++i) {\t//执行n次\n     j = 1;\t\t\t\t\t//执行n次\n     for (; j <= n; ++j) {\t//执行n*n次\n       sum = sum +  i * j;\t//执行n*n次\n     }\n   }\t\t\t\t\t\t//总的时间复杂度为2n^2+2n+3\n }\n\n```\n\n注意：当$n$很大的时候，我们通常会忽略掉公式中的低阶、常量、系数三部分并不左右增长趋势，而公式中的低阶、常量、系数三部分并不左右增 长趋势，所以都可以忽略。我们只需要记录一个最大量级就可以了。\n\n### 技巧\n\n#### 只关注循环执行次数最多的一段代码\n\n```java\n int cal(int n) {\n   int sum = 0;\n   int i = 1;\n   for (; i <= n; ++i) { //n次\n     sum = sum + i;    // n 次\n   }\n   return sum;\n }                        //去掉常数项系数O(n)\n\n```\n\n#### 加法法则\n\n总复杂度等于量级最大的那段代码的复杂度\n\n```java\nint cal(int n) {\n   int sum_1 = 0;\n   int p = 1;\n   for (; p < 100; ++p) {   //常数 \n     sum_1 = sum_1 + p;\n   }\n\n   int sum_2 = 0;\n   int q = 1;\n   for (; q < n; ++q) {\t\t//n次\n     sum_2 = sum_2 + q;\t//n次\n   }\n \n   int sum_3 = 0;\n   int i = 1;\n   int j = 1;\n   for (; i <= n; ++i) {  //n次\n     j = 1; \n     for (; j <= n; ++j) {   // n^2\n       sum_3 = sum_3 +  i * j;  //n^2\n     }\n   }\n \n   return sum_1 + sum_2 + sum_3;\n }\n\n```\n\n$\\mathrm{T} 1(\\mathrm{n})=\\mathrm{O}(\\mathrm{f}(\\mathrm{n}))$,$\\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}(\\mathrm{g}(\\mathrm{n}))$那么$T(n)=T 1(n)+T 2(n)=\\max (O(n)), O(g(n)) )=O(\\max (f(n)g(n) )$\n\n#### 乘法法则\n\n嵌套代码的复杂度等于嵌套内外代码复杂度的乘积\n\n$\\mathrm{T} 1(\\mathrm{n})=\\mathrm{O}(\\mathrm{f}(\\mathrm{n})), \\quad \\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}(\\mathrm{g}(\\mathrm{n}))$那么$\\mathrm{T}(\\mathrm{n})=\\mathrm{T} 1(\\mathrm{n})^{\\star} \\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}(\\mathrm{f}(\\mathrm{n}))^{\\star} \\mathrm{O}(\\mathrm{g}(\\mathrm{n}))=\\mathrm{O}\\left(\\mathrm{f}(\\mathrm{n})^{\\star} \\mathrm{g}(\\mathrm{n})\\right)$\n\n假设$\\mathrm{T} 1(\\mathrm{n})=\\mathrm{O}(\\mathrm{n}), \\quad \\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}\\left(\\mathrm{n}^{2}\\right)$，那么$\\mathrm{T} 1(\\mathrm{n})^{*} \\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}\\left(\\mathrm{n}^{3}\\right)$.\n\n```java\nint cal(int n) {\n   int ret = 0; \n   int i = 1;\n   for (; i < n; ++i) {  //n次\n     ret = ret + f(i);\n   } \n } \n \n int f(int n) {\n  int sum = 0;\n  int i = 1;\n  for (; i < n; ++i) {    //n次\n    sum = sum + i;\n  } \n  return sum;\n }\n\n```\n\n### 常见复杂度\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502150942.png)\n\n#### $O(1)$\n\nO(1) 只是常量级时间复杂度的一种表示方法，并不是指只执行了一行代码。也就是说算法执行的步骤是有限的。\n\n```java\n int i = 8;\n int j = 6;\n int sum = i + j;\n```\n\n#### $O(\\log n)$O($ nlogn $)\n\n```java\n i=1;\n while (i <= n)  {\n   i = i * 2;\n }\n```\n\n这段代码停止的时的时候为i<=n,那么如何求n呢，\n\n$2^0*2^1*2^2*2^3*2^4*2^5**2^6**2^6*2^7........2^x$,通过我们学习的数学知识基本上可以知道$2^n=n$求解的答案，为\n\n$x=\\log _{2} n$,也就是说这段代码的时间复杂度为$O(log _{2} n)$\n\n我们把代码改一下呢\n\n```java\n i=1;\n while (i <= n)  {\n   i = i * 3;\n }\n```\n\n这段代码的时间复杂度就应该为$O\\left(\\log _{3} n\\right)$，但是$\\log _{3} n$=$\\log _{3} 2^{*} \\log _{2} n$，$\\mathrm{O}\\left(\\log _{3} n\\right)=\\mathrm{O}\\left(\\mathrm{C}^{*}\\right.$$\\log _{2} n )$,其中$\\mathrm{C}=\\log _{3} 2$为常量，可忽略，即$O(C(n))=O(f(n))$,所以$\\mathrm{O}\\left(\\log _{2} n\\right)$等价于$\\mathrm{O}\\left(\\log _{3} \\mathrm{n}\\right)$，因此在对数复杂度中，我们可以忽略底的对数，统一标识为$O(logn)$\n\n#### $O(n \\log n)$\n\n一段代码的复杂度为$O(logn)$，我们循环执行n遍就是$O(n \\log n)$。\n\n#### $O(m+n)$\n\n```java\nint cal(int m, int n) {\n  int sum_1 = 0;\n  int i = 1;\n  for (; i < m; ++i) {\n    sum_1 = sum_1 + i;\n  }\n\n  int sum_2 = 0;\n  int j = 1;\n  for (; j < n; ++j) {\n    sum_2 = sum_2 + j;\n  }\n\n  return sum_1 + sum_2;\n}\n```\n\nm 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂 度就是 O(m+n)。\n\n#### O(m*n)\n\n$\\mathrm{T} 1(\\mathrm{m})^{\\star} \\mathrm{T} 2(\\mathrm{n})=\\mathrm{O}(\\mathrm{f}(\\mathrm{m}) * \\mathrm{f}(\\mathrm{n}))$仍然有效。\n\n## 空间复杂度分析\n\n空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的 存储空间与数据规模之间的增长关系。\n\n```java\nvoid print(int n) {\n  int i = 0; //空间存储变量\n  int[] a = new int[n]; //申请大小为n的int类型数组空间量为O(n)\n  for (i; i <n; ++i) {\n    a[i] = i * i;     //其余代码未占用更多空间整段代码的空间的空间复杂度为O(n)\n  }\n\n  for (i = n-1; i >= 0; --i) {\n    print out a[i]\n  }\n}\n```\n\n## 复杂度图\n\n$O(1)<O(\\log n)<O(n)<O(n \\log n)<O\\left(n^{2}\\right)<O\\left(n^{3}\\right)<O\\left(n^{n}\\right)<O\\left(n^{n}\\right)$\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/数据结构/绪论/20190502154912.png)\n\n### 最好、最坏情况时间复杂度\n\n```java\n// n 表示数组 array 的长度\nint find(int[] array, int n, int x) {\n  int i = 0;\n  int pos = -1;\n  for (; i < n; ++i) {\n    if (array[i] == x) pos = i;\n  }\n  return pos;\n}\n```\n\n在一个无序的数组（array）中，查找变量 x 出现的 位置。如果没有找到，就返回 -1。按这段代码的复杂度是 $O(n)$，其中，n 为数组的长度。假设我们在中途中找到了 x 我们就可以结束循环了。我们修改一下代码。\n\n```java\n// n 表示数组 array 的长度\nint find(int[] array, int n, int x) {\n  int i = 0;\n  int pos = -1;\n  for (; i < n; ++i) {\n    if (array[i] == x) {\n       pos = i;\n       break;  //找到他的时候跳出\n    }\n  }\n  return pos;\n}\n```\n\n如果我们在第一个位置就找到了它，那么它的时间复杂度就应该为$O(1)$,如果我们在最后找到那么它的时间复杂度为$O(n)$\n\n这里我们就引入了三个新的概念：<font color=\"red\"> 最好情况时间复杂度、最坏情况时间复杂度和平均情况时间复杂度</font>\n\n因此相对应的是最好情况时间复杂度为$O(1)$，最坏情况时间复杂度为$O(n)$.\n\n### 平均情况时间复杂度\n\n最好情况时间复杂度和最坏情况时间复杂度对应的都是极端情况下的代码复杂度，发生的概率其实并不大。为了更好地表示平均情况下的复杂度，我们引入另一个概念：平均情况时间复杂度(平均时间复杂度)。\n\n要查找的变量 x 在数组中的位置，有 n+1 种情况：在数组的 0～n-1 位置中和不在数组中。我们把 每种情况下，查找需要遍历的元素个数累加起来，然后再除以 n+1，就可以得到需要遍历的元素个 数的平均值：\n\n$\\frac{1+2+3+\\cdots+n+n}{n+1}=\\frac{n(n+3)}{2(n+1)}$\n\n大 O 标记法中，可以省略掉系数、低阶、常量因此我们得到的平均时间复杂度为$O(n)$\n\n但是我们忽略了一个问题这 n+1 种 情况，出现的概率并不是一样的。因此我们需要引入概率论的一些知识。\n\n要查找的变量 x，要么在数组里，要么就不在数组里。这两种情况对应的概率统计起来很麻烦，我们假设在数组中与不在数组中的概率都为 1/2。另外，要查找的数据 出现在 0～n-1 这 n 个位置的概率也是一样的，为 1/n。所以，根据概率乘法法则，要查找的数据出 现在 0～n-1 中任意位置的概率就是 $=\\frac{1}{2 n}$。\n\n因此我们将式子完善一下。\n\n$1 \\times \\frac{1}{2 n}+2 \\times \\frac{1}{2 n}+3 \\times \\frac{1}{2 n}+\\dots+n \\times \\frac{1}{2 n}+n \\times \\frac{1}{2}=\\frac{3 n+1}{4}$\n\n$\\frac{3 n+1}{4}$这个值是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度。根据大$O$表示法来表示时间复杂度依然是$O(n)$\n\n### 均摊时间复杂度\n\n均摊时间复杂度又叫摊还分析（或者叫平摊分析）。\n\n```java\n // array 表示一个长度为 n 的数组\n // 代码中的 array.length 就等于 n\n int[] array = new int[n];\n int count = 0;\n \n void insert(int val) {\n    if (count == array.length) { \n       int sum = 0;\n       for (int i = 0; i < array.length; ++i)  {//for 循环遍历数组\n          sum = sum + array[i];\t//求和\n       }\n       array[0] = sum;\t//sum 值放到数组的第一个位\n       count = 1;\n    }\n\n    array[count] = val; //清空数组\n    ++count; \n }\n\n```\n\n数组中有空闲空间，我们只需将数据插入到数组下标为 count 的位置就可以 了，最好情况时间复杂度为 O(1)。最坏的情况下，数组中没有空闲空间了，我们需要先做一次 数组的遍历求和，然后再将数据插入，所以最坏情况时间复杂度为O(n)。\n用概率论的方法来分析平均时间复杂度。\n数组的长度是 n，根据数据插入的位置的不同，我们可以分为 n 种情况，每种情况的时间复杂 度是 O(1)。除此之外，还有一种“额外”的情况，就是在数组没有空闲空间时插入一个数据，这个时 候的时间复杂度是 O(n)。而且，这 n+1 种情况发生的概率一样，都是$\\frac{1}{n+1}$。根据加权平均的计算方法，我们求得的平均时间复杂度就是：\n\n$1 \\times \\frac{1}{n+1}+1 \\times \\frac{1}{n+1}+\\dots+1 \\times \\frac{1}{n+1}+n \\times \\frac{1}{n+1}=O(1)$\n\n这里不需要引入概率论的知识也可以：find() 函数在极端情况下，复杂度才为 O(1)。但 insert() 在大部分情况下，时间复杂度都为 O(1)。只有个别情况下，复杂度才比较高，为 O(n)。这是 insert()第一个区别于 find() 的地方。\n\n对于 insert() 函数来说，O(1) 时间复杂度的插入和 O(n) 时间复杂度 的插入，出现的频率是非常有规律的，而且有一定的前后时序关系，一般都是一个 O(n) 插入之后， 紧跟着 n-1 个 O(1) 的插入操作，循环往复。\n\n针对这样一种特殊场景的复杂度分析，我们并不需要像之前讲平均复杂度分析方法那样，找出所有的输入情况及相应的发生概率，然后再计算加权平均值。我们引入了**摊还分析法**，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。\n\n每一次 O(n) 的插入操作，都会跟着 n-1 次 O(1) 的 插入操作，所以把耗时多的那次操作均摊到接下来的 n-1 次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是 O(1)。\n\n## 参考资料\n\n《数据结构与算法之美》\n\n《大话数据结构》\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["绪论"],"categories":["数据结构"]},{"title":"Java项目架构演进和SpringCloud总结","url":"/2019/04/28/Java项目架构演进和SpringCloud总结/","content":"\n {{ \"Java项目架构演进过程和SpirngCloud知识总结\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n#### 技术梳理\n\n1. 开发技术栈以SpringCloud为主，单个微服务模块以SpringMVC+SpringBoot/Spring+MyBatis组合进行开发\n2. 前端层，页面H5+thymeleaf/样式CSS3+Bootstrap/前端框架JQuery+Node|Vue等\n3. 负载层，前端访问通过Http或Https协议到达服务端的LB，可以是F5等硬件做负载均衡，还可以自行部署LVS+Keepalived等（前期量小可以直接使用Nginx） \n4. 网关层，请求通过LB后，会到达整个微服务体系的网关层Zuul（Gateway），内嵌Ribbon做客户端负载均衡，Hystrix做熔断降级等\n5. 服务注册，采用Eureka来做服务治理，Zuul会从Eureka集群获取已发布的微服务访问地址，然后根据配置把请求代理到相应的微服务去\n6. docker容器，所有的微服务模块都部署在Docker容器里面，而且前后端的服务完全分开，各自独立部署后前端微服务调用后端微服务，后端微服务之间会有相互调用\n7. 服务调用，微服务模块间调用都采用标准的Http/Https+REST+JSON的方式，调用技术采用Feign+HttpClient+Ribbon+Hystrix\n8. 统一配置，每个微服务模块会跟Eureka集群、配置中心（SpringCloudConfig）等进行交互\n9. 第3方框架，每个微服务模块根据实现的需要，通常还需要使用一些第三发框架，比如常见的有：缓存服务（Redis）、图片服务（FastDFS）、搜索服务（ElasticSearch）、安全管理（Shiro）等等\n10. Mysql数据库，可以按照微服务模块进行拆分，统一访问公共库或者单独自己库，可以单独构建MySQL集群或者分库分表MyCat等\n\n## 其他组件\n\nSpringCloud Stream：数据流操作开发包\nSpringCloud Turbine是聚合服务器发送事件流数据的一个工具，用来监控集群下hystrix的metrics情况。\nSpringCloud Task：提供云端计划任务管理、任务调度。\nSpringCloud Sleuth：日志收集工具包实现了一种分布式追踪解决方案，封装了Dapper和log-based追踪以及Zipkin和HTrace操作，\nSpringCloud Security：基于spring security的安全工具包，为应用程序添加安全控制\n服务部署：Kubernetes ， OpenStack\n全链路追踪：Zipkin，brave\n服务监控：zabbix\nSpringCloud CLI：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。\n全局控制：选举leader、全局锁、全局唯一id\n安全鉴权： auth2、 openId connect\n自动化构建与部署： gitlab + jenkins + docker \n服务监控和告警（Spring Boot Admin）\n\n### 技术架构演进\n\n1. 第1阶段单体\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428110033.png)\n\n2. 第1-2阶段\n\n    随着访问量的增加,单台服务器无法满足需求,在假设数据库服务器没有压力的情况下,我们把应用服务器从一台变成两台或者多台,把用户的请求分散到不同的服务器上,提高负载均衡.\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428111032.png)\n\n3. 阶段2-1\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/阶段2-1.png)\n\n    该阶段引入了Nginx他的主要功能是:反向代理+动静分离+分在均衡\n\n4. 阶段2-2Niginx+应用服务器配置集群+HA\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428111839.png)\n\n5. 阶段3负载均衡服务器配置集群\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428112227.png)\n\n6. 阶段4CDN+Varnish服务器配置集群\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428112540.png)\n\n7. 阶段5数据库读写分离\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428112844.png)\n\n8.  阶段6NOSQL+分布式搜索引擎\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428113122.png)\n\n9. 阶段7NOSQL(HA)+分表分库+MyCat\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428113524.png)\n\n10. 第8阶段分布式文件系统存储图片等\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428113905.png)\n\n11. 第9阶段应用服务化拆分+消息中间键\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428115053.png)\n\n12. 第10阶段微服务架构\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428115933.png)\n\n图为jhipster:<https://www.jhipster.tech/monitoring/>\n\n## 总体预览\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428121001.png)\n\n## 参考资料\n\n尚硅谷SpringCloud\n\n \n\n​    \n\n​    \n\n​    \n\n​    \n\n\n\n\n\n","tags":["SpringCloud","JavaWeb"],"categories":["微服务"]},{"title":"SpringCloud与SpringConfig分布式配置中心","url":"/2019/04/27/SpringCloud与SpringConfig分布式配置中心/","content":"\n {{ \"Github上配置SpringCloud的配置文件\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 问题\n\n 微服务意味着要将单体应用中的业务拆分成一个个子服务，每个服务的粒度相对较小，因此系统中会出现大量的服务。由于每个服务都需要必要的配置信息才能运行，所以一套集中式的、动态的配置管理设施是必不可少的。SpringCloud提供了ConfigServer来解决这个问题，我们每一个微服务自己带着一个application.yml，上百个配置文件管理起来势必使以减十分麻烦的事情因此我们引入了SpringCloud Config。\n\n## 简介\n\nSpringConfig图解\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427193726.png)\n\nSpringCloud Config为微服务架构中的微服务提供集中化的外部配置支持，配置服务器为各个不同微服务应用的所有环境提供了一个中心化的外部配置。\n\nSpringCloud Config分为服务端和客户端两部分。\n\n服务端也称为分布式配置中心，它是一个独立的微服务应用，用来连接配置服务器并为客户端提供获取配置信息，加密/解密信息等访问接口\n\n客户端则是通过指定的配置中心来管理应用资源，以及与业务相关的配置内容，并在启动的时候从配置中心获取和加载配置信息配置服务器默认采用git来存储配置信息，这样就有助于对环境配置进行版本管理，并且可以通过git客户端工具来方便的管理和访问配置内容。\n\n## 作用\n\n1. 集中管理配置文件\n\n2. 不同环境不同配置，动态化的配置更新，分环境部署比如dev/test/prod/beta/release\n\n3. 运行期间动态调整配置，不需要在每个服务部署的机器上编写配置文件，服务会向配置中心统一拉取配置自己的信息\n\n4. 当配置发生变动时，服务不需要重启即可感知到配置的变化并应用新的配置\n\n5. 将配置信息以REST接口的形式暴露\n\n## GitHub整合\n\n1. 在GitHub上创建一个repository\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427194648.png)\n\n2. 获取到git的链接`git@github.com:bigdataxiaohan/microservicecloud-config.git`在本地创建一个仓库\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427195034.png)\n\n3. 在本地的仓库在创建一个application.yml&nbsp;注意保存的编码格式必要是UTF-8\n\n    ```yaml\n    spring:\n      profiles:\n        active:\n        - dev\n    ---\n    spring:\n      profiles: dev     #开发环境\n      application: \n        name: microservicecloud-config-hphblog-dev\n    ---\n    spring:\n      profiles: test   #测试环境\n      application: \n        name: microservicecloud-config-hphblog-test\n    #  请保存为UTF-8格式\n    ```\n\n4. 将本地的配置文件推送到github上去\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427200208.png)\n\n5. 查看github上的配置信息\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427200254.png)\n\n6. 新建一个Moudle `microservicecloud-config-3344`作为配置中心模块\n\n    pom.xml文件修改\n\n    ```xml\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n             xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n             xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n        <parent>\n            <artifactId>microservicecloud</artifactId>\n            <groupId>com.hph.springcloud</groupId>\n            <version>1.0-SNAPSHOT</version>\n        </parent>\n        <modelVersion>4.0.0</modelVersion>\n    \n        <artifactId>microservicecloud-config-3344</artifactId>\n        <dependencies>\n            <!-- springCloud Config -->\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-config-server</artifactId>\n            </dependency>\n            <!-- 图形化监控 -->\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-actuator</artifactId>\n            </dependency>\n            <!-- 熔断 -->\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-hystrix</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-eureka</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-starter-config</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-jetty</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-web</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-starter-test</artifactId>\n            </dependency>\n            <!-- 热部署插件 -->\n            <dependency>\n                <groupId>org.springframework</groupId>\n                <artifactId>springloaded</artifactId>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-devtools</artifactId>\n            </dependency>\n        </dependencies>\n    \n    </project>\n    ```\n\n    \n\n7. application.yml\n\n    ```yaml\n    server:\n      port: 3344\n    \n    spring:\n      application:\n        name:  microservicecloud-config\n      cloud:\n        config:\n          server:\n            git:\n              uri: https://github.com/bigdataxiaohan/microservicecloud-config.git #GitHub上面的git仓库名字\n    ```\n\n8. Config_3344_StartSpringCloudApp\n\n    ```java\n    package com.hph.springcloud;\n    \n    import org.springframework.boot.SpringApplication;\n    import org.springframework.boot.autoconfigure.SpringBootApplication;\n    import org.springframework.cloud.config.server.EnableConfigServer;\n    \n    @SpringBootApplication\n    @EnableConfigServer\n    public class Config_3344_StartSpringCloudApp\n    {\n      public static void main(String[] args)\n      {\n       SpringApplication.run(Config_3344_StartSpringCloudApp.class,args);\n      }\n    }\n    ```\n\n9. windows下修改hosts文件，增加映射\n\n    ```properties\n    127.0.0.1  config-3344.com\n    ```\n\n\n### 测试\n\n1. 启动Config_3344_StartSpringCloudApp\n\n2. 访问<http://config-3344.com:3344/application-dev.yml>\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427204401.png)\n\n3. 访问<http://config-3344.com:3344/application-test.yml>\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427204514.png)\n\n4. 访问<http://config-3344.com:3344/application-xxx.yml>(不存在的配置)\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427204606.png)\n\n### 访问方式\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427205614.png)\n\n链接地址：<https://github.com/spring-cloud/spring-cloud-config/issues/292>\n\n#### 方式一\n\n`/{application}/{profile}[/{label}]`\n\n访问:<http://config-3344.com:3344/application/dev/master>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427205902.png)\n\n访问：<http://config-3344.com:3344/application/test/master>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427210036.png)\n\n访问： <http://config-3344.com:3344/application/xxx/master>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427210117.png)\n\n#### 方式二\n\n`/{label}/{application}-{profile}.yml`\n\n访问<http://config-3344.com:3344/master/application-dev.yml>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427210323.png)\n\n访问<http://config-3344.com:3344/master/application-test.yml>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427210349.png)\n\n## 配置客户端\n\n创建一个microservicecloud-config-client.yml文件\n\n### microservicecloud-config-client.yml\n\n```yaml\nspring:\n  profiles:\n    active:\n    - dev\n---\nserver: \n  port: 8201 \nspring:\n  profiles: dev\n  application: \n    name: microservicecloud-config-client\neureka: \n  client: \n    service-url: \n      defaultZone: http://eureka-dev.com:7001/eureka/   \n---\nserver: \n  port: 8202 \nspring:\n  profiles: test\n  application: \n    name: microservicecloud-config-client\neureka: \n  client: \n    service-url: \n      defaultZone: http://eureka-test.com:7001/eureka/\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427211654.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427223239.png)\n\n### 新建microservicecloud-config-client-3355\n\npom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-config-client-3355</artifactId>\n    <dependencies>\n        <!-- SpringCloud Config客户端 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-hystrix</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n```\n\n### bootstrap.yml\n\napplicaiton.yml是用户级的资源配置项\n\n```yaml\nspring:\n  cloud:\n    config:\n      name: microservicecloud-config-client #需要从github上读取的资源名称，注意没有yml后缀名\n      profile: test   #本次访问的配置项\n      label: master   \n      uri: http://config-3344.com:3344  #本微服务启动后先去找3344号服务，通过SpringCloudConfig获取GitHub的服务地址\n \n```\n\nbootstrap.yml是系统级的，优先级更加高\n\n### application.yml\n\n```yaml\nspring:\n  application:\n    name: microservicecloud-config-client\n```\n\nSpring Cloud会创建一个`Bootstrap Context`，作为Spring应用的`Application Context`的父上下文。初始化的时候，`Bootstrap Context`负责从外部源加载配置属性并解析配置。这两个上下文共享一个从外部获取的`Environment`。`Bootstrap`属性有高优先级，默认情况下，它们不会被本地配置覆盖。 `Bootstrap context`和`Application Context`有着不同的约定，\n所以新增了一个`bootstrap.yml`文件，保证`Bootstrap Context`和`Application Context`配置的分离。\n\n增加映射\n\n```properties\n127.0.0.1  client-config.com\n```\n\n新建rest类，验证是否能从GitHub上读取配置\n\n```java\npackage com.hph.springcloud.rest;\n\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n\n@RestController\npublic class ConfigClientRest {\n\n    @Value(\"${spring.application.name}\")\n    private String applicationName;\n\n    @Value(\"${eureka.client.service-url.defaultZone}\")\n    private String eurekaServers;\n\n    @Value(\"${server.port}\")\n    private String port;\n\n    @RequestMapping(\"/config\")\n    public String getConfig()\n    {\n        String str = \"applicationName: \"+applicationName+\"\\t eurekaServers:\"+eurekaServers+\"\\t port: \"+port;\n        System.out.println(\"******str: \"+ str);\n        return \"applicationName: \"+applicationName+\"\\t eurekaServers:\"+eurekaServers+\"\\t port: \"+port;\n    }\n}\n```\n\n## 测试\n\n启动Config配置中心3344微服务并自测\n\n访问<http://config-3344.com:3344/application-dev.yml>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427223505.png)\n\n因为我们配置的文件是test访问的端口是8202 \n\n访问测试端口\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190427223908.png)\n\n## SpringCloud Config配置实战\n\n注意:<font color= red>编码方式一定要是UTF-8</font>\n\n### 新建microservicecloud-config-eureka-client.yml\n\n```yaml\nspring: \n  profiles: \n    active: \n    - dev\n---\nserver: \n  port: 7001 #注册中心占用7001端口,冒号后面必须要有空格\n   \nspring: \n  profiles: dev\n  application:\n    name: microservicecloud-config-eureka-client\n    \neureka: \n  instance: \n    hostname: eureka7001.com #冒号后面必须要有空格\n  client: \n    register-with-eureka: false #当前的eureka-server自己不注册进服务列表中\n    fetch-registry: false #不通过eureka获取注册信息\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka/\n---\nserver: \n  port: 7001 #注册中心占用7001端口,冒号后面必须要有空格\n   \nspring: \n  profiles: test\n  application:\n    name: microservicecloud-config-eureka-client\n    \neureka: \n  instance: \n    hostname: eureka7001.com #冒号后面必须要有空格\n  client: \n    register-with-eureka: false #当前的eureka-server自己不注册进服务列表中\n    fetch-registry: false #不通过eureka获取注册信息\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka/\n```\n\n### 新建microservicecloud-config-dept-client.yml\n\n```yaml\nspring: \n  profiles:\n    active:\n    - dev\n--- \nserver:\n  port: 8001\nspring: \n   profiles: dev\n   application: \n    name: microservicecloud-config-dept-client\n   datasource:\n    type: com.alibaba.druid.pool.DruidDataSource\n    driver-class-name: org.gjt.mm.mysql.Driver\n    url: jdbc:mysql://192.168.1.110:3306/cloudDB01\n    username: root\n    password: 123456\n    dbcp2:\n      min-idle: 5\n      initial-size: 5\n      max-total: 5\n      max-wait-millis: 200 \nmybatis:\n  config-location: classpath:mybatis/mybatis.cfg.xml\n  type-aliases-package: com.hph.springcloud.entities\n  mapper-locations:\n  - classpath:mybatis/mapper/**/*.xml\n \neureka: \n  client: #客户端注册进eureka服务列表内\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka\n  instance:\n    instance-id: dept-8001.com\n    prefer-ip-address: true\n \ninfo:\n  app.name: hphblog-microservicecloud-springcloudconfig01\n  company.name: www.hphnlog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n---\nserver:\n  port: 8001\nspring: \n   profiles: test\n   application: \n    name: microservicecloud-config-dept-client\n   datasource:\n    type: com.alibaba.druid.pool.DruidDataSource\n    driver-class-name: org.gjt.mm.mysql.Driver\n    url: jdbc:mysql://192.168.1.110:3306/cloudDB02\n    username: root\n    password: 123456\n    dbcp2:\n      min-idle: 5\n      initial-size: 5\n      max-total: 5\n      max-wait-millis: 200  \n  \n  \nmybatis:\n  config-location: classpath:mybatis/mybatis.cfg.xml\n  type-aliases-package: com.hph.springcloud.entities\n  mapper-locations:\n  - classpath:mybatis/mapper/**/*.xml\n \neureka: \n  client: #客户端注册进eureka服务列表内\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka\n  instance:\n    instance-id: dept-8001.com\n    prefer-ip-address: true\n \ninfo:\n  app.name: hphblog-microservicecloud-springcloudconfig01\n  company.name: www.hphnlog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n```\n\n### 新建工程microservicecloud-config-eureka-client-7001\n\n#### pom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-config-eureka-client-7001</artifactId>\n    <dependencies>\n        <!-- SpringCloudConfig配置 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka-server</artifactId>\n        </dependency>\n        <!-- 热部署插件 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n    \n</project>\n```\n\n#### bootstrap.yml\n\n```yaml\nspring:\n  cloud:\n    config:\n      name: microservicecloud-config-eureka-client     #需要从github上读取的资源名称，注意没有yml后缀名\n      profile: dev\n      label: master\n      uri: http://config-3344.com:3344      #SpringCloudConfig获取的服务地址\n```\n\n#### application.yml\n\n```yaml\nspring:\n  application:\n    name: microservicecloud-config-eureka-client\n```\n\n#### Config_Git_EurekaServerApplication\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer\npublic class Config_Git_EurekaServerApplication \n{\n  public static void main(String[] args) \n  {\n   SpringApplication.run(Config_Git_EurekaServerApplication.class, args);\n  }\n}\n```\n\n### 测试\n\n1. 先启动microservicecloud-config-3344微服务，保证Config总配置状态可用\n\n2. 再启动microservicecloud-config-eureka-client-7001微服务\n\n3. 访问<http://eureka7001.com:7001/> \n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190428091607.png)\n\n### microservicecloud-config-dept-client-8001\n\n#### pom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-config-dept-client-8001</artifactId>\n\n    <dependencies>\n        <!-- SpringCloudConfig配置 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n```\n\n#### bootstrap.yml\n\n```yaml\nspring:\n  cloud:\n    config:\n      name: microservicecloud-config-dept-client #需要从github上读取的资源名称，注意没有yml后缀名\n      #profile配置是什么就取什么配置dev or test\n      #profile: dev\n      profile: test\n      label: master\n      uri: http://config-3344.com:3344  #SpringCloudConfig获取的服务地址\n```\n\n#### application.yml\n\n```yaml\nspring:\n  application:\n    name: microservicecloud-config-dept-client\n```\n\n#### 主启动类\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n@EnableEurekaClient //本服务启动后会自动注册进eureka服务中\n@EnableDiscoveryClient //服务发现\npublic class DeptProvider8001_App {\n    public static void main(String[] args) {\n        SpringApplication.run(DeptProvider8001_App.class, args);\n    }\n}\n```\n\n#### 其他配置\n\n参考microservicecloud-provider-dept-8001\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190428095701.png)\n\n​    \n\n### 测试\n\n1. 启动microservicecloud-config-3344主启动类\n\n2. 启动microservicecloud-config-client-3355主启动类\n\n3. 启动microservicecloud-config-eureka-client-7001主启动类\n\n4. 启动microservicecloud-config-dept-client-8001主启动类\n\n    由于我们激活的配置是test所以我们访问一下<<http://localhost:8001/dept/list>>\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190428100653.png)\n\n5. 修改microservicecloud-config-dept-client-8001中的profile属性改成dev\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190428102056.png)\n\n    \n\n6. 我们将本地的文件修改一下让dev模式下访问3号数据库重新启动所有服务\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/SpringConfig/20190428102753.png)\n\n7. ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190428121518.png)\n\n\n\n### 完整代码\n\nGithub地址: &nbsp;<https://github.com/bigdataxiaohan/microservicecloud/tree/master/SpringConfig>\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud","SpringConfig"],"categories":["微服务"]},{"title":"SpringCloud与zuul","url":"/2019/04/27/SpringCloud与zuul/","content":"\n {{ \"微服务的网关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 简介\n\nZuul包含了对请求的路由和过滤两个最主要的功能：\n其中路由功能负责将外部请求转发到具体的微服务实例上，是实现外部访问统一入口的基础而过滤器功能则负责对请求的处理过程进行干预，是实现请求校验、服务聚合等功能的基础Zuul和Eureka进行整合，将Zuul自身注册为Eureka服务治理下的应用，同时从Eureka中获得其他微服务的消息，也即以后的访问微服务都是通过Zuul跳转后获得。\n\n注意：Zuul服务最终还是会注册进Eureka\n\n提供=代理+路由+过滤三大功能\n\n## 步骤\n\n### 创建microservicecloud-zuul-gateway-9527\n\n#### pom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-zuul-gateway-9527</artifactId>\n    <dependencies>\n        <!-- zuul路由网关 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-zuul</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <!-- actuator监控 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <!--  hystrix容错-->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-hystrix</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <!-- 日常标配 -->\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <!-- 热部署插件 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n    \n</project>\n```\n\n#### application.yml\n\n```yaml\nserver:\n  port: 9527\n\nspring:\n  application:\n    name: microservicecloud-zuul-gateway\n\neureka:\n  client:\n    service-url:\n      defaultZone: http://eureka7001.com:7001/eureka,http://eureka7002.com:7002/eureka,http://eureka7003.com:7003/eureka\n  instance:\n    instance-id: gateway-9527.com\n    prefer-ip-address: true\n\n\ninfo:\n  app.name: hph-microservicecloud\n  company.name: www.hphblog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n```\n\n#### hosts修改\n\n```java\n127.0.0.1  myzuul.com\n```\n\n#### Zuul_9527_StartSpringCloudApp\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.zuul.EnableZuulProxy;\n\n@SpringBootApplication\n@EnableZuulProxy\npublic class Zuul_9527_StartSpringCloudApp\n{\n  public static void main(String[] args)\n  {\n   SpringApplication.run(Zuul_9527_StartSpringCloudApp.class, args);\n  }\n}\n```\n\n## 测试\n\n1. 启动三个eurekaserver集群。\n2. 启动microservicecloud-provider-dept-8001类。\n3. 启动Zuul_9527_StartSpringCloudApp。\n4. 不使用路由访问&nbsp;<http://localhost:8001/dept/get/2>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427173636.png)\n\n5. 使用路由访问&nbsp;http://myzuul.com:9527/microservicecloud-dept/dept/get/2\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427173803.png)\n\n## 添加映射\n\n### application.yml\n\n添加\n\n```yaml\nzuul: \n  routes: \n    mydept.serviceId: microservicecloud-dept\n    mydept.path: /mydept/**\n```\n\n访问：<http://myzuul.com:9527/mydept/dept/get/1>\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427174336.png)\n\n但是这个路径还存在我们可以屏蔽一下。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427174614.png)\n\n```yaml\nzuul:\n  ignored-services: microservicecloud-dept\n  routes:\n    mydept.serviceId: microservicecloud-dept\n    mydept.path: /mydept/**\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427175033.png)\n\n如果我们想批量的屏蔽真实的地址我们可以使用`“*”`\n\n```yaml\nzuul:\n  ignored-services: \"*\"\n  routes:\n    mydept.serviceId: microservicecloud-dept\n    mydept.path: /mydept/**\n```\n\n设置同一的前缀安全加固\n\n```yaml\nzuul: \n  prefix: /hphblog\n  ignored-services: \"*\"\n  routes: \n    mydept.serviceId: microservicecloud-dept\n    mydept.path: /mydept/**\n \n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427175609.png)\n\n## 完整代码\n\nGithub: <https://github.com/bigdataxiaohan/microservicecloud/tree/master/zuul>\n\n\n\n","tags":["SpringCloud","zuul"],"categories":["微服务"]},{"title":"SpringCloud与Hystrix断路器","url":"/2019/04/27/SpringCloud与Hystrix断路器/","content":"\n {{ \"服务熔断,服务降级,服务监控的学习笔记\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n## 问题\n\n复杂分布式体系结构中的应用程序有数十个依赖关系，每个依赖关系在某些时候将不可避免地失败。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427103000.png)\n\n### 服务雪崩\n多个微服务之间调用的时候，假设微服务A调用微服务B和微服务C，微服务B和微服务C又调用其它的微服务，这就是所谓的“扇出”。如果扇出的链路上某个微服务的调用响应时间过长或者不可用，对微服务A的调用就会占用越来越多的系统资源，进而引起系统崩溃，所谓的“雪崩效应”.\n\n对于高流量的应用来说，单一的后端依赖可能会导致所有服务器上的所有资源都在几秒钟内饱和。比失败更糟糕的是，这些应用程序还可能导致服务之间的延迟增加，备份队列，线程和其他系统资源紧张，导致整个系统发生更多的级联故障。这些都表示需要对故障和延迟进行隔离和管理，以便单个依赖关系的失败，不能取消整个应用程序或系统。\n\n\n备注：一般情况对于服务依赖的保护主要有3中解决方案：\n\n- 熔断模式：这种模式主要是参考电路熔断，如果一条线路电压过高，保险丝会熔断，防止火灾。放到我们的系统中，如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。\n- 隔离模式：这种模式就像对系统请求按类型划分成一个个小岛的一样，当某个小岛被火少光了，不会影响到其他的小岛。例如可以对不同类型的请求使用线程池来资源隔离，每种类型的请求互不影响，如果一种类型的请求线程资源耗尽，则对后续的该类型请求直接返回，不再调用后续资源。这种模式使用场景非常多，例如将一个服务拆开，对于重要的服务使用单独服务器来部署，再或者公司最近推广的多中心。\n- 限流模式：上述的熔断模式和隔离模式都属于出错后的容错处理机制，而限流模式则可以称为预防模式。限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。\n\n## Hystrix解决之道\n\n### 服务降级\n\nHystrix服务降级，其实就是线程池中单个线程障处理，防止单个线程请求时间太长，导致资源长期被占有而得不到释放，从而导致线程池被快速占用完，导致服务崩溃。\nHystrix能解决如下问题：\n1.请求超时降级，线程资源不足降级，降级之后可以返回自定义数据\n2.线程池隔离降级，分布式服务可以针对不同的服务使用不同的线程池，从而互不影响\n3.自动触发降级与恢复\n4.实现请求缓存和请求合并\n\n### 服务熔断\n\n熔断模式：这种模式主要是参考电路熔断，如果一条线路电压过高，保险丝会熔断，防止火灾。放到我们的系统中，如果某个目标服务调用慢或者有大量超时，此时，熔断该服务的调用，对于后续调用请求，不在继续调用目标服务，直接返回，快速释放资源。如果目标服务情况好转则恢复调用。\n\n### 服务限流\n\n限流模式主要是提前对各个类型的请求设置最高的QPS阈值，若高于设置的阈值则对该请求直接返回，不再调用后续资源。这种模式不能解决服务依赖的问题，只能解决系统整体资源分配问题，因为没有被限流的请求依然有可能造成雪崩效应。\n\n## 近实时监控\n\nHystrixCommand和HystrixObservableCommand在执行时，会生成执行结果和运行指标，比如每秒执行的请求数、成功数等，这些监控数据对分析应用系统的状态很有用。使用Hystrix的模块 hystrix-metrics-event-stream ，就可将这些监控的指标信息以 text/event-stream 的格式暴露给外部系统。spring-cloud-starter-hystrix包含该模块，在此基础上，只须为项目添加spring-boot-starter-actuator，就可使用 /hystrix.stream 端点获取Hystrix的监控信息了。\n\n官方资料 <https://github.com/Netflix/Hystrix/wiki/How-To-Use#Hello-World>\n\n## 服务熔断\n\n熔断机制是应对雪崩效应的一种微服务链路保护机制。当扇出链路的某个微服务不可用或者响应时间太长时，会进行服务的降级，进而熔断该节点微服务的调用，快速返回\"错误\"的响应信息。当检测到该节点微服务调用响应正常后恢复调用链路。在SpringCloud框架里熔断机制通过Hystrix实现。Hystrix会监控微服务间调用的状况，当失败的调用到一定阈值，缺省是5秒内20次调用失败就会启动熔断机制。熔断机制的注解是@HystrixCommand。\n\n## 步骤\n\n## 创建microservicecloud-provider-dept-hystrix-8001\n\n我们需要参考microservicecloud-provider-dept-8001\n\n### pom.xml\n\npom文件修改为这样的\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-provider-dept-hystrix-8001</artifactId>\n    <dependencies>\n        <!--  hystrix -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-hystrix</artifactId>\n        </dependency>\n        <!-- 引入自己定义的api通用包，可以使用Dept部门Entity -->\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <!-- 将微服务provider侧注册进eureka -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <!-- actuator监控信息完善 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n### application.yml\n\n```java\nserver:\n  port: 8001\n  \nmybatis:\n  config-location: classpath:mybatis/mybatis.cfg.xml  #mybatis所在路径\n  type-aliases-package: com.hph.springcloud.entities #entity别名类\n  mapper-locations:\n  - classpath:mybatis/mapper/**/*.xml #mapper映射文件\n    \nspring:\n   application:\n    name: microservicecloud-dept \n   datasource:\n    type: com.alibaba.druid.pool.DruidDataSource\n    driver-class-name: org.gjt.mm.mysql.Driver\n    url: jdbc:mysql://192.168.1.110:3306/cloudDB01\n    username: root\n    password: 123456\n    dbcp2:\n      min-idle: 5\n      initial-size: 5\n      max-total: 5\n      max-wait-millis: 200\n      \neureka:\n  client: #客户端注册进eureka服务列表内\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/\n  instance:\n    instance-id: microservicecloud-dept8001-hystrix   #自定义hystrix相关的服务名称信息\n    prefer-ip-address: true     #访问路径可以显示IP地址\n      \ninfo:\n  app.name: hph-microservicecloud\n  company.name: www.hphblog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n```\n\n### DeptMapper.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n\"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n\n<mapper namespace=\"com.hph.springcloud.dao.DeptDao\">\n\t<select id=\"findById\" resultType=\"Dept\" parameterType=\"Long\">\n\t\tselect deptno,dname,db_source from dept where deptno=#{deptno};\n\t</select>\n\t<select id=\"findAll\" resultType=\"Dept\">\n\t\tselect deptno,dname,db_source from dept;\n\t</select>\n\t<insert id=\"addDept\" parameterType=\"Dept\">\n\t\tINSERT INTO dept(dname,db_source) VALUES(#{dname},DATABASE());\n\t</insert>\n</mapper>\n```\n\n### mybatis.cfg.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE configuration\n  PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n  \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n\n<configuration>\n\n\t<settings>\n\t\t<setting name=\"cacheEnabled\" value=\"true\" /><!-- 二级缓存开启 -->\n\t</settings>\n\n</configuration>\n```\n\n### 修改DeptController\n\n```java\npackage com.hph.springcloud.controller;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cloud.client.ServiceInstance;\nimport org.springframework.cloud.client.discovery.DiscoveryClient;\nimport org.springframework.web.bind.annotation.*;\n\nimport com.hph.springcloud.entities.Dept;\nimport com.hph.springcloud.service.DeptService;\nimport com.netflix.hystrix.contrib.javanica.annotation.HystrixCommand;\n\nimport java.util.List;\n\n@RestController\npublic class DeptController\n{\n\t@Autowired\n\tprivate DeptService service = null;\n\n\n\t@Autowired\n\tprivate DiscoveryClient client;\n\n\t@RequestMapping(value = \"/dept/get/{id}\", method = RequestMethod.GET)\n\t//一旦调用服务方法失败并抛出了错误信息后，会自动调用@HystrixCommand标注好的fallbackMethod调用类中的指定方法\n\t@HystrixCommand(fallbackMethod = \"processHystrix_Get\")\n\tpublic Dept get(@PathVariable(\"id\") Long id)\n\t{\n\n\t\tDept dept = this.service.get(id);\n\t\t\n\t\tif (null == dept) {\n\t\t\tthrow new RuntimeException(\"该ID：\" + id + \"没有没有对应的信息\");\n\t\t}\n\t\t\n\t\treturn dept;\n\t}\n\n\tpublic Dept processHystrix_Get(@PathVariable(\"id\") Long id)\n\t{\n\t\treturn new Dept().setDeptno(id).setDname(\"该ID：\" + id + \"没有没有对应的信息,null--@HystrixCommand\")\n\t\t\t\t.setDb_source(\"no this database in MySQL\");\n\t}\n\n\t@RequestMapping(value = \"/dept/add\", method = RequestMethod.POST)\n\tpublic boolean add(@RequestBody Dept dept)\n\t{\n\t\treturn service.add(dept);\n\t}\n\t@RequestMapping(value = \"/dept/list\", method = RequestMethod.GET)\n\tpublic List<Dept> list()\n\t{\n\t\treturn service.list();\n\t}\n\n\n\t//\t@Autowired\n//\tprivate DiscoveryClient client;\n\t@RequestMapping(value = \"/dept/discovery\", method = RequestMethod.GET)\n\tpublic Object discovery()\n\t{\n\t\tList<String> list = client.getServices();\n\t\tSystem.out.println(\"**********\" + list);\n\n\t\tList<ServiceInstance> srvList = client.getInstances(\"MICROSERVICECLOUD-DEPT\");\n\t\tfor (ServiceInstance element : srvList) {\n\t\t\tSystem.out.println(element.getServiceId() + \"\\t\" + element.getHost() + \"\\t\" + element.getPort() + \"\\t\"\n\t\t\t\t\t+ element.getUri());\n\t\t}\n\t\treturn this.client;\n\t}\n}\n```\n\n### 修改主启动类DeptProvider8001_Hystrix_App\n\n添加新注解@EnableCircuitBreake\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n@EnableEurekaClient //本服务启动后会自动注册进eureka服务中\n@EnableDiscoveryClient //服务发现\n@EnableCircuitBreaker//对hystrixR熔断机制的支持\npublic class DeptProvider8001_Hystrix_App\n{\n\tpublic static void main(String[] args)\n\t{\n\t\tSpringApplication.run(DeptProvider8001_Hystrix_App.class, args);\n\t}\n}\n```\n\n### 测试\n\n1. 首先启动3个eurekaserver\n2. 启动微服务microservicecloud-provider-dept-8001\n3. 启动microservicecloud-consumer-dept-80\n4. 访问http://localhost/consumer/dept/get/2048\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427105045.png)\n\n### 不足\n\n这样写耦合度太高,我们需要对它进行解耦\n\n## 服务降级\n\n整体资源快不够了，将某些服务先关掉，待渡过难关，再开启回来。服务降级处理是在客户端实现完成的，与服务端没有关系\n\n### 修改microservicecloud-api 内容\n\n根据已经有的DeptClientService接口新建一个实现FallbackFactory接口的类DeptClientServiceFallbackFactory\n\n```java\npackage com.hph.springcloud.service;\n\nimport java.util.List;\n\nimport org.springframework.stereotype.Component;\nimport com.hph.springcloud.entities.Dept;\nimport feign.hystrix.FallbackFactory;\n\n\n@Component//这个注解比较重要不要忘记添加\npublic class DeptClientServiceFallbackFactory implements FallbackFactory<DeptClientService>\n{\n  @Override\n  public DeptClientService create(Throwable throwable)\n  {\n   return new DeptClientService() {\n     @Override\n     public Dept get(long id)\n     {\n       return new Dept().setDeptno(id)\n               .setDname(\"该ID：\"+id+\"没有没有对应的信息,Consumer客户端提供的降级信息,此刻服务Provider已经关闭\")\n               .setDb_source(\"no this database in MySQL\");\n     }\n \n     @Override\n     public List<Dept> list()\n     {\n       return null;\n     }\n \n     @Override\n     public boolean add(Dept dept)\n     {\n       return false;\n     }\n   };\n  }\n}\n```\n\n### DeptClientService\n\n```java\npackage com.hph.springcloud.service;\n\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.cloud.netflix.feign.FeignClient;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\n\nimport java.util.List;\n\n@FeignClient(value = \"MICROSERVICECLOUD-DEPT\",fallbackFactory=DeptClientServiceFallbackFactory.class)\npublic interface DeptClientService {\n    @RequestMapping(value = \"/dept/get/{id}\",method = RequestMethod.GET)\n    public Dept get(@PathVariable(\"id\") long id);\n\n    @RequestMapping(value = \"/dept/list\",method = RequestMethod.GET)\n    public List<Dept> list();\n\n    @RequestMapping(value = \"/dept/add\",method = RequestMethod.POST)\n    public boolean add(Dept dept);\n\n}\n```\n\n我们首先mvn&nbsp;&nbsp;clean一下  然后mvn&nbsp;&nbsp;install一下更新一下Maven中的api包为修改后的包\n\n### 修改microservicecloud-consumer-dept-feign\n\napplication.yml\n\n```yml\nserver:\n  port: 80\n\nfeign:\n  hystrix:\n    enabled: true\n\neureka:\n  client:\n    register-with-eureka: false\n    service-url:\n      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/\n```\n\n### 测试\n\n1. 首先启动3个eurekaserver\n\n2. 启动微服务microservicecloud-provider-dept-8001\n\n3. 启动microservicecloud-consumer-dept-80\n\n4. 访问http://localhost/consumer/dept/get/1\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427131825.png)\n\n    5.高并发请求下模拟访问\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/测试服务降级.gif)\n\n    此时服务端provider已经down了，但是我们做了服务降级处理，让客户端在服务端不可用时也会获得提示信息而不会挂起耗死服务器。这次我们使用的工具时apache的ab&nbsp;-是发送请求的数量&nbsp;-c是并发数。我们可以看到服务被临时被降级了。\n\n    \n\n## 服务监控\n\n除了隔离依赖服务的调用以外，Hystrix还提供了准实时的调用监控（Hystrix Dashboard），Hystrix会持续地记录所有通过Hystrix发起的请求的执行信息，并以统计报表和图形的形式展示给用户，包括每秒执行多少请求多少成功，多少失败等。Netflix通过hystrix-metrics-event-stream项目实现了对以上指标的监控。Spring Cloud也提供了Hystrix Dashboard的整合，对监控内容转化成可视化界面。\n\n### microservicecloud-consumer-hystrix-dashboard\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-provider-dept-hystrix-8001</artifactId>\n    <dependencies>\n        <!--  hystrix -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-hystrix</artifactId>\n        </dependency>\n        <!-- 引入自己定义的api通用包，可以使用Dept部门Entity -->\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <!-- 将微服务provider侧注册进eureka -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <!-- actuator监控信息完善 -->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-actuator</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n    \n</project>\n```\n\n### application.ym\n\n```yaml\nserver:\n  port: 9001\n```\n\n### DeptConsumer_DashBoard_App\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;\n\n@SpringBootApplication\n@EnableHystrixDashboard\npublic class DeptConsumer_DashBoard_App\n{\n  public static void main(String[] args)\n  {\n   SpringApplication.run(DeptConsumer_DashBoard_App.class,args);\n  }\n}\n \n```\n\n 所有Provider微服务提供类(8001/8002/8003)都需要监控依赖配置\n\n```xml\n <!-- actuator监控信息完善 -->\n   <dependency>\n     <groupId>org.springframework.boot</groupId>\n     <artifactId>spring-boot-starter-actuator</artifactId>\n   </dependency>\n```\n\n### 测试\n\n1. 启动microservicecloud-consumer-hystrix-dashboard访问http://localhost:9001/hystrix\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427145014.png)\n\n2. 启动3个eurekaserver\n\n3. 启动microservicecloud-provider-dept-hystrix-8001\n\n4. 访问http://localhost:8001/dept/get/1\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427145151.png)\n\n5. 访问http://localhost:8001/hystrix.stream\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/服务监控测试非图形界面.gif)\n\n6. 这种访问不是很友好我们可以试一下其他的方法。\n\n    将http://localhost:8001/hystrix.stream填入地址栏\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/服务监控测试界面初始化.gif)\n\n### 如何观察\n\n其中颜色在右上角代表着不同的信息。\n\n实心圆：共有两种含义。它通过颜色的变化代表了实例的健康程度，它的健康度从绿色<黄色<橙色<红色递减。\n该实心圆除了颜色的变化之外，它的大小也会根据实例的请求流量发生变化，流量越大该实心圆就越大。所以通过该实心圆的展示，就可以在大量的实例中快速的发现故障实例和高压力实例。\n\n曲线：用来记录2分钟内流量的相对变化，可以通过它来观察到流量的上升和下降趋势。\n\n​    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/20190427151441.png)\n\n动态图展示监控 我们用ab模拟访问。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Hystrix/服务监控测试图形化界面.gif)\n\n## 完整代码\n\nGithub: <https://github.com/bigdataxiaohan/microservicecloud/tree/master/Hystrix>\n\n​    \n\n​    \n\n​    \n\n​    \n\n​    \n\n​    \n\n​    \n\n​    \n\n​    \n\n\n\n\n\n","tags":["SpringCloud","Hystrix"],"categories":["微服务"]},{"title":"SpringCloud与Feign","url":"/2019/04/26/SpringCloud与Feign负载均衡/","content":"\n {{ \"SpringCloud只Feign如何实现负载均衡\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n# 简介\n\n Feign是一个声明式WebService客户端。使用Feign能让编写Web Service客户端更加简单, 它的使用方法是定义一个接口，然后在上面添加注解，同时也支持JAX-RS标准的注解。Feign也支持可拔插式的编码器和解码器。Spring Cloud对Feign进行了封装，使其支持了Spring MVC标准注解和HttpMessageConverters。Feign可以与Eureka和Ribbon组合使用以支持负载均衡。\n\n Feign是一个声明式的Web服务客户端，使得编写Web服务客户端变得非常容易，只需要创建一个接口，然后在上面添加注解即可。\n\n# 作用\n\nFeign旨在使编写Java Http客户端变得更容易。\n前面在使用Ribbon+RestTemplate时，利用RestTemplate对http请求的封装处理，形成了一套模版化的调用方法。但是在实际开发中，由于对服务依赖的调用可能不止一处，往往一个接口会被多处调用，所以通常都会针对每个微服务自行封装一些客户端类来包装这些依赖服务的调用。所以，Feign在此基础上做了进一步封装，由他来帮助我们定义和实现依赖服务接口的定义。在Feign的实现下，我们只需创建一个接口并使用注解的方式来配置它(以前是Dao接口上面标注Mapper注解,现在是一个微服务接口上面标注一个Feign注解即可)，即可完成对服务提供方的接口绑定，简化了使用Spring cloud Ribbon时，自动封装服务调用客户端的开发量。\n\n# Feign集成了Ribbon\n\n利用Ribbon维护了MicroServiceCloud-Dept的服务列表信息，并且通过轮询实现了客户端的负载均衡。而与Ribbon不同的是，通过feign只需要定义服务绑定接口且以声明式的方法，优雅而简单的实现了服务调用\n\n# 步骤\n\n## 新建microservicecloud-consumer-dept-feign\n\n我们可以直接复制microservicecloud-consumer-dept-80这个项目,然后进行相应的修改\n\n1. 修改主启动类名字为DeptConsumer80_Feign_App\n\n### pom.xm\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-consumer-dept-feig</artifactId>\n\n    <dependencies>\n        <dependency><!-- 自己定义的api -->\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-feign</artifactId>\n        </dependency>\n        <!-- Ribbon相关 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-ribbon</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-config</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n### DeptConsumer80_Feign_App\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\nimport org.springframework.cloud.netflix.feign.EnableFeignClients;\n\n@SpringBootApplication\n@EnableEurekaClient\n//在启动该微服务的时候就能去加载我们的自定义Ribbon配置类，从而使配置生效\n@EnableFeignClients(basePackages= {\"com.hph.springcloud\"})\npublic class DeptConsumer80_Feign_App\n{\n    public static void main(String[] args)\n    {\n        SpringApplication.run(DeptConsumer80_Feign_App.class, args);\n    }\n}\n\n```\n\n## 修改microservicecloud-api工程\n\n### \n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-api</artifactId>\n<dependencies>\n    <dependency>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n    </dependency>\n\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-feign</artifactId>\n    </dependency>\n\n</dependencies>\n\n</project>\n```\n\n### pom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-api</artifactId>\n<dependencies>\n    <dependency>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n    </dependency>\n\n    <dependency>\n        <groupId>org.springframework.cloud</groupId>\n        <artifactId>spring-cloud-starter-feign</artifactId>\n    </dependency>\n\n</dependencies>\n\n</project>\n```\n\n### DeptClientService\n\n```java\npackage com.hph.springcloud.service;\n\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.cloud.netflix.feign.FeignClient;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\n\nimport java.util.List;\n\n@FeignClient(value = \"MICROSERVICECLOUD-DEPT\") //指定微服务\npublic interface DeptClientService {\n    @RequestMapping(value = \"/dept/get/{id}\",method = RequestMethod.GET)\n    public Dept get(@PathVariable(\"id\") long id);\n\n    @RequestMapping(value = \"/dept/list\",method = RequestMethod.GET)\n    public List<Dept> list();\n\n    @RequestMapping(value = \"/dept/add\",method = RequestMethod.POST)\n    public boolean add(Dept dept);\n\n}\n```\n\n然后执行`mvn clean` `mvn install `更新本地中的依赖包为我们修改过的依赖包。\n\n## 修改microservicecloud-consumer-dept-feign\n\n###  Controller\n\n添加上一步新建的DeptClientService接口\n\n```java\npackage com.hph.springcloud.controller;\n\nimport com.hph.springcloud.entities.Dept;\nimport com.hph.springcloud.service.DeptClientService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\nimport java.util.List;\n\n@RestController\n\npublic class DeptController_Consumer {\n    @Autowired\n    private DeptClientService service = null;\n\n    @RequestMapping(value = \"/consumer/dept/get/{id}\")\n    public Dept get(@PathVariable(\"id\") Long id)\n    {\n        return this.service.get(id);\n    }\n\n    @RequestMapping(value = \"/consumer/dept/list\")\n    public List<Dept> list()\n    {\n        return this.service.list();\n    }\n\n    @RequestMapping(value = \"/consumer/dept/add\")\n    public Object add(Dept dept)\n    {\n        return this.service.add(dept);\n    }\n\n}\n```\n\n### 修改主启动类\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\nimport org.springframework.cloud.netflix.feign.EnableFeignClients;\n\n@SpringBootApplication\n@EnableEurekaClient\n//在启动该微服务的时候就能去加载我们的自定义Ribbon配置类，从而使配置生效\n@EnableFeignClients(basePackages= {\"com.hph.springcloud\"})\npublic class DeptConsumer80_Feign_App\n{\n    public static void main(String[] args)\n    {\n        SpringApplication.run(DeptConsumer80_Feign_App.class, args);\n    }\n}\n```\n\n## 测试\n\n1. 首先一次启动E启动3个eureka集群\n\n2. 启动部门微服务8001/8002/8003\n\n3. 启动microservicecloud-consumer-dept-feig\n\n4. 访问http://localhost/consumer/dept/get/1\n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/测试Feign.gif)\n\n    \n\n## 小结\n\n\nFeign通过接口的方法调用Rest服务（之前是Ribbon+RestTemplate），该请求发送给Eureka服务器<http://localhost/consumer/dept/get/1>,通过Feign直接找到服务接口，由于在进行服务调用的时候融合了Ribbon技术，所以也支持负载均衡作用。\n\n## 代码\n\nGithub代码地址： <https://github.com/bigdataxiaohan/microservicecloud/tree/master/Feign>\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud","Feign"],"categories":["微服务"]},{"title":"SpringCloud的Ribbon负载均衡","url":"/2019/04/25/SpringCloud的Ribbon负载均衡/","content":"\n {{ \"Spring Cloud Ribbon相关学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n# 简介\n\n\nSpring Cloud Ribbon是基于Netflix Ribbon实现的一套客户端负载均衡的工具。\n\n简单的说，Ribbon是Netflix发布的开源项目，主要功能是提供客户端的软件负载均衡算法，将Netflix的中间层服务连接在一起。Ribbon客户端组件提供一系列完善的配置项如连接超时，重试等。简单的说，就是在配置文件中列出Load Balancer（简称LB）后面所有的机器，Ribbon会自动的帮助你基于某种规则（如简单轮询，随机连接等）去连接这些机器。我们也很容易使用Ribbon实现自定义的负载均衡算法。\n\n# 负载均衡\n\nLB，即负载均衡(Load Balance)，在微服务或分布式集群中经常用的一种应用。负载均衡简单的说就是将用户的请求平摊的分配到多个服务上，从而达到系统的HA。常见的负载均衡有软件Nginx，LVS，硬件 F5等。相应的在中间件，例如：dubbo和SpringCloud中均给我们提供了负载均衡，SpringCloud的负载均衡算法可以自定义。 \n\n## 集中式LB\n\n即在服务的消费方和提供方之间使用独立的LB设施(可以是硬件，如F5, 也可以是软件，如nginx), 由该设施负责把访问请求通过某种策略转发至服务的提供方；\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190425231310.png)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\tF5硬件图例\n\n## 进程内LB\n\n将LB逻辑集成到消费方，消费方从服务注册中心获知有哪些地址可用，然后自己再从这些地址中选择出一个合适的服务器。\n\nRibbon就属于进程内LB，它只是一个类库，集成于消费方进程，消费方通过它来获取到服务提供方的地址。\n\n# 步骤\n\n##  microservicecloud-consumer-dept-80\n\n## pom.xml\n\n添加\n\n```xml\n   <!-- Ribbon相关 -->\n<dependency>\n     <groupId>org.springframework.cloud</groupId>\n     <artifactId>spring-cloud-starter-eureka</artifactId>\n   </dependency>\n   <dependency>\n     <groupId>org.springframework.cloud</groupId>\n     <artifactId>spring-cloud-starter-ribbon</artifactId>\n   </dependency>\n   <dependency>\n     <groupId>org.springframework.cloud</groupId>\n     <artifactId>spring-cloud-starter-config</artifactId>\n</dependency>\n```\n\n## application.yml \n\n```yaml\nserver:\n  port: 80\n \neureka:\n  client:\n    register-with-eureka: false\n    service-url: \n      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/\n```\n\n## ConfigBean\n\n对ConfigBean进行新注解@LoadBalanced    获得Rest时加入Ribbon的配置\n\n```java\npackage com.hph.springcloud.cfbeans;\n\nimport org.springframework.cloud.client.loadbalancer.LoadBalanced;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.client.RestTemplate;\n\n@Configuration\npublic class ConfigBean {\n\n    @Bean\n    @LoadBalanced   //Spring Cloud Ribbon 是基于Netflix Ribbon实现的的一套客户端 负载 均衡的工具\n    public RestTemplate getRestTemplate() {\n        return new RestTemplate();\n    }\n}\n```\n\n## DeptConsumer80_App\n\n 主启动类DeptConsumer80_App添加@EnableEurekaClient\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n@EnableEurekaClient\n//在启动该微服务的时候就能去加载我们的自定义Ribbon配置类，从而使配置生效,如果没有启动是默认的轮询\npublic class DeptConsumer80_App\n{\n    public static void main(String[] args)\n    {\n        SpringApplication.run(DeptConsumer80_App.class, args);\n    }\n}\n\n```\n\n## DeptController_Consumer\n\n修改DeptController_Consumer客户端访问类,把REST_URL_PREFIX写成我们的微服务名称。\n\n```java\npackage com.hph.springcloud.controller;\n\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\nimport java.util.List;\n\n@RestController\npublic class DeptController_Consumer {\n\n//    private  static  final  String REST_URL_PREFIX = \"http://localhost:8001\";\n    private  static  final  String REST_URL_PREFIX = \"http://MICROSERVICECLOUD-DEPT\";\n    @Autowired\n    private RestTemplate restTemplate;\n\n    @RequestMapping(value = \"/consumer/dept/add\")\n    public boolean add(Dept dept)\n    {\n        return restTemplate.postForObject(REST_URL_PREFIX + \"/dept/add\", dept, Boolean.class);\n    }\n\n    @RequestMapping(value = \"/consumer/dept/get/{id}\")\n    public Dept get(@PathVariable(\"id\") Long id)\n    {\n        return restTemplate.getForObject(REST_URL_PREFIX + \"/dept/get/\" + id, Dept.class);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @RequestMapping(value = \"/consumer/dept/list\")\n    public List<Dept> list()\n    {\n        return restTemplate.getForObject(REST_URL_PREFIX + \"/dept/list\", List.class);\n    }\n\n    // 测试@EnableDiscoveryClient,消费端可以调用服务发现\n    @RequestMapping(value = \"/consumer/dept/discovery\")\n    public Object discovery()\n    {\n        return restTemplate.getForObject(REST_URL_PREFIX + \"/dept/discovery\", Object.class);\n    }\n}\n```\n\n## 测试\n\n先启动3个eureka集群后，再启动`microservicecloud-provider-dept-8001`并注册进eureka\n\n启动microservicecloud-consumer-dept-80\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426110653.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426110727.png)\n\n\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426110813.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426110934.png)\n\n小结 `Ribbon和Eureka整合后Consumer可以直接调用服务而不用再关心地址和端口号`\n\n# Ribbon架构\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426110956.png)\n\nRibbon在工作时分成两步\n第一步先选择 EurekaServer ,它优先选择在同一个区域内负载较少的server.\n第二步再根据用户指定的策略，在从server取到的服务注册列表中选择一个地址。\n其中Ribbon提供了多种策略：比如轮询、随机和根据响应时间加权。\n\n# Ribbon负载均衡\n\n## microservicecloud-provider-dept-8002\n\n数据准备\n\n```sql\n \nDROP DATABASE IF EXISTS cloudDB02;\n \nCREATE DATABASE cloudDB02 CHARACTER SET UTF8;\n \nUSE cloudDB02;\n \nCREATE TABLE dept\n(\n  deptno BIGINT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n  dname VARCHAR(60),\n  db_source   VARCHAR(60)\n);\n \nINSERT INTO dept(dname,db_source) VALUES('开发部',DATABASE());\nINSERT INTO dept(dname,db_source) VALUES('人事部',DATABASE());\nINSERT INTO dept(dname,db_source) VALUES('财务部',DATABASE());\nINSERT INTO dept(dname,db_source) VALUES('市场部',DATABASE());\nINSERT INTO dept(dname,db_source) VALUES('运维部',DATABASE());\n \nSELECT * FROM dept;\n```\n\n### application.yml\n\n```yaml\nserver:\n  port: 8002\n\nmybatis:\n  config-location: classpath:mybatis/mybatis.cfg.xml        # mybatis配置文件所在路径\n  type-aliases-package: com.hph.springcloud.entities    # 所有Entity别名类所在包\n  mapper-locations:\n    - classpath:mybatis/mapper/**/*.xml                       # mapper映射文件\n\nspring:\n  application:\n    name: microservicecloud-dept\n  datasource:\n    type: com.alibaba.druid.pool.DruidDataSource            # 当前数据源操作类型\n    driver-class-name: org.gjt.mm.mysql.Driver              # mysql驱动包\n    url: jdbc:mysql://192.168.1.110:3306/cloudDB02         # 数据库名称\n    username: root\n    password: 123456\n    dbcp2:\n      min-idle: 5                                           # 数据库连接池的最小维持连接数\n      initial-size: 5                                       # 初始化连接数\n      max-total: 5                                          # 最大连接数\n      max-wait-millis: 200                                  # 等待连接获取的最大超时时间\n\neureka:\n  client: #客户端注册进eureka服务列表内\n    service-url:\n      defaultZone:  http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/\n  instance:\n    instance-id: microservicecloud-dept8002   #自定义服务名称信息\n    prefer-ip-address: true     #访问路径可以显示IP地址\n\ninfo:\n  app.name: hph-microservicecloud\n  company.name: www.hphblog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n\n```\n\n其他基本相同我们需要把启动类的配置类做适当修改,完整代码请参考Github链接。\n\n## 测试\n\n1. 启动3个eureka集群配置区\n\n2. 启动3个Dept微服务并各自测试通过 \n\n    ![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/测试.gif)\n\n3. 客户端通过Ribbo完成负载均衡并访问上一步的Dept微服务\n\n    我们可以看得到的访问到的是不同的数据库信息。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/负载均衡动图.gif)\n\n## 小结\n\nRibbon其实就是一个软负载均衡的客户端组件，他可以和其他所需请求的客户端结合使用，和eureka结合只是其中的一个实例。\n\n# 自定义负载均衡\n\n## Ribbon核心组件IRule\n\nIRule：根据特定算法中从服务列表中选取一个要访问的服务\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426122015.png)\n\n## 指定轮询算法\n\n### 创建com.hph.myrule\n\n在指定中添加指定为随机轮询\n\n```java\npackage com.hph.myrule;\n\nimport com.netflix.loadbalancer.IRule;\nimport com.netflix.loadbalancer.RandomRule;\nimport org.springframework.context.annotation.Bean;\n\npublic class MySelfRule {\n    @Bean\n    public IRule myRule() {\n        return new RandomRule();//Ribbon默认是轮询，我自定义为随机\n\n    }\n}\n```\n\n修改我我们的主启动类\n\n```java\npackage com.hph.springcloud;\n\nimport com.hph.myrule.MySelfRule;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\nimport org.springframework.cloud.netflix.ribbon.RibbonClient;\n\n@SpringBootApplication\n@EnableEurekaClient\n//在启动该微服务的时候就能去加载我们的自定义Ribbon配置类，从而使配置生效\n@RibbonClient(name=\"MICROSERVICECLOUD-DEPT\",configuration= MySelfRule.class)\npublic class DeptConsumer80_App\n{\n    public static void main(String[] args)\n    {\n        SpringApplication.run(DeptConsumer80_App.class, args);\n    }\n}\n```\n\n\n\n我们测试一下结果是随机访问你的微服务的数据库。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/随机算法测试.gif)\n\n## 自定义注意\n\n官方文档明确给出了警告：\n这个自定义配置类不能放在@ComponentScan所扫描的当前包下以及子包下，否则我们自定义的这个配置类就会被所有的Ribbon客户端所共享，也就是说我们达不到特殊化定制的目的了。\n\n因此我们创新创建一个包，不让自己定义的规则和SpirngApplication在同一包下，所以我们选择重新创建一个包\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/20190426135238.png)\n\n这里我们可以看到相关的信息。根据[github源码](https://github.com/Netflix/ribbon/blob/master/ribbon-loadbalancer/src/main/java/com/netflix/loadbalancer/RandomRule.java)我们自己写一个轮询策略。\n\n\n\n```java\npackage com.hph.myrule;\n\nimport com.netflix.client.config.IClientConfig;\nimport com.netflix.loadbalancer.AbstractLoadBalancerRule;\nimport com.netflix.loadbalancer.ILoadBalancer;\nimport com.netflix.loadbalancer.Server;\n\nimport java.util.List;\n \npublic class RandomRule_ZY extends AbstractLoadBalancerRule {\n \n  private int total = 0;    //总共被调用的次数，目前要求每台被调用5次\n  private int currentIndex = 0;//当前提供服务的机器号\n  \n    public Server choose(ILoadBalancer lb, Object key) {\n        if (lb == null) {\n            return null;\n        }\n        Server server = null;\n \n        while (server == null) {\n            if (Thread.interrupted()) {\n                return null;\n            }\n            List<Server> upList = lb.getReachableServers();\n            List<Server> allList = lb.getAllServers();\n \n            int serverCount = allList.size();\n            if (serverCount == 0) {\n                /*\n                 * No servers. End regardless of pass, because subsequent passes\n                 * only get more restrictive.\n                 */\n                return null;\n            }\n            \n//            int index = rand.nextInt(serverCount);\n//            server = upList.get(index);\n            if(total < 5)\n            {\n            server = upList.get(currentIndex);\n            total++;\n            }else {\n            total = 0;\n            currentIndex++;\n            if(currentIndex >= upList.size())\n            {\n              currentIndex = 0;\n            }\n            \n            }\n           \n       \n            if (server == null) {\n                /*\n                 * The only time this should happen is if the server list were\n                 * somehow trimmed. This is a transient condition. Retry after\n                 * yielding.\n                 */\n                Thread.yield();\n                continue;\n            }\n \n            if (server.isAlive()) {\n                return (server);\n            }\n \n            // Shouldn't actually happen.. but must be transient or a bug.\n            server = null;\n            Thread.yield();\n        }\n \n        return server;\n \n    }\n \n  @Override\n  public Server choose(Object key) {\n   return choose(getLoadBalancer(), key);\n  }\n \n  @Override\n  public void initWithNiwsConfig(IClientConfig clientConfig) {\n   \n  }\n}\n```\n\n## 测试\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Ribbon/测试5次访问.gif)\n\n自定义完成。\n\n## 完整代码\n\nGithub地址 : https://github.com/bigdataxiaohan/microservicecloud/tree/master/Ribbon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud","Ribbon"],"categories":["微服务"]},{"title":"SpringCloud注册与发现Eureka","url":"/2019/04/23/SpringCloud注册与发现Eureka/","content":"\n {{ \"SpringCloud注册于发现Eureka服务学习\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n# 简介\n\nEureka是Netflix开发的服务发现框架，本身是一个基于REST的服务，主要用于定位运行在AWS域中的中间层服务，以达到负载均衡和中间层服务故障转移的目的。SpringCloud将它集成在其子项目spring-cloud-netflix中，以实现SpringCloud的服务发现功能。\n\nEureka包含两个组件：Eureka Server和Eureka Client。\n\nEureka Server  采用了 C-S 的设计架构,提供服务注册服务，各个节点启动后，会在Eureka Server中进行注册，这样EurekaServer中的服务注册表中将会存储所有可用服务节点的信息，服务节点的信息可以在界面中直观的看到。\n\nEureka Client是一个java客户端，用于简化与Eureka Server的交互，客户端同时也就是一个内置的、使用轮询(round-robin)负载算法的负载均衡器。\n\n在应用启动后，将会向Eureka Server发送心跳,默认周期为30秒，如果Eureka Server在多个心跳周期内没有接收到某个节点的心跳，Eureka Server将会从服务注册表中把这个服务节点移除(默认90秒)。\n\nEureka Server之间通过复制的方式完成数据的同步，Eureka还提供了客户端缓存机制，即使所有的Eureka Server都挂掉，客户端依然可以利用缓存中的信息消费其他服务的API。综上，Eureka通过心跳检查、客户端缓存等机制，确保了系统的高可用性、灵活性和可伸缩性。\n\nSpring Cloud 封装了 Netflix 公司开发的 Eureka 模块来实现服务注册和发现(请对比Zookeeper)。\n\n而系统中的其他微服务，使用 Eureka 的客户端连接到 Eureka Server并维持心跳连接。这样系统的维护人员就可以通过 Eureka Server 来监控系统中各个微服务是否正常运行。SpringCloud 的一些其他模块（比如Zuul）就可以通过 Eureka Server 来发现系统中的其他微服务，并执行相关的逻辑。\n\n## 与Dubbo的架构对比\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423125949.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423130056.png)\n\n## 小结\n\n- Eureka Server 提供服务注册和发现\n- Service Provider服务提供方将自身服务注册到Eureka，从而使服务消费方能够找到\n- Service Consumer服务消费方从Eureka获取注册服务列表，从而能够消费服务\n\n## microservicecloud-eureka-7001\n\n### pom\n\npom文件配置信息。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-eureka-7001</artifactId>\n\n    <dependencies>\n        <!--eureka-server服务端 -->\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-eureka-server</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n\n</project>\n```\n\n### application.yml\n\n```yml\nserver: \n  port: 7001\n \neureka:\n  instance:\n    hostname: localhost #eureka服务端的实例名称\n  client:\n    register-with-eureka: false #false表示不向注册中心注册自己。\n    fetch-registry: false #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务\n    service-url:\n      defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/        #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。\n```\n\n### main\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer //EurekaServer服务器端启动类,接受其它微服务注册进来\n\npublic class EurekaServer7001_App {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaServer7001_App.class, args);\n    }\n}\n```\n\n启动方法我们可以看到\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423113258.png)\n\n看到上图说明Eureka的服务端就启动了，服务端启动之后我们将microservicecloud-provider-dept-8001的服务注册进服务端。\n\n## microservicecloud-provider-dept-8001\n\n### pom\n\n继续上一章的内容，将服务注册进去我们需要添加一些依赖\n\n```xml\n   <!-- 将微服务provider侧注册进eureka -->\n   <dependency>\n     <groupId>org.springframework.cloud</groupId>\n     <artifactId>spring-cloud-starter-eureka</artifactId>\n   </dependency>\n   <dependency>\n     <groupId>org.springframework.cloud</groupId>\n     <artifactId>spring-cloud-starter-config</artifactId>\n   </dependency>\n```\n\n### main\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.EnableEurekaClient;\n\n@SpringBootApplication\n@EnableEurekaClient //服务启动后自动注册进入eureka服务\npublic class DeptProvider8001_App {\n    public static void main(String[] args) {\n        SpringApplication.run(DeptProvider8001_App.class,args);\n    }\n}\n```\n\n# 测试\n\n启动DeptProvider8001_App，观察服务端信息。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423114444.png)\n\n服务已经注册。\n\n该微服务应用名称是由microservicecloud-provider-dept-8001中的application.yml指定的。 \n\n```yaml\nspring:\n  application:\n    name: microservicecloud-dept\n```\n\n## 问题1\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423120433.png)\n\n当前的问题就是微服务的IP我们不知道，那么如何添加近服务的IP地址嗯我们可以在application中添加，\n\n```yaml\n  instance:\n    instance-id: microservicecloud-dept8001   #自定义服务名称信息\n    prefer-ip-address: true     #访问路径可以显示IP地址\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423120302.png)\n\n## 问题2\n\n我们点击超链接之后爆出了Error Page 如何解决这个问题呢？\n\n首先我们修改 microservicecloud-provider-dept-8001中的pom文件\n\n```xml\n<dependency>\n       <groupId>org.springframework.boot</groupId>\n       <artifactId>spring-boot-starter-actuator</artifactId>\n   </dependency>\n```\n\n总的父工程microservicecloud修改pom.xml添加构建build信息。\n\n```xml\n   <build>\n        <finalName>microservicecloud</finalName>\n        <resources>\n            <resource>\n                <directory>src/main/resources</directory>\n                <filtering>true</filtering>\n            </resource>\n        </resources>\n        <plugins>\n            <plugin>\n                <groupId>org.apache.maven.plugins</groupId>\n                <artifactId>maven-resources-plugin</artifactId>\n                <configuration>\n                    <delimiters>\n                        <delimit>$</delimit>\n                    </delimiters>\n                </configuration>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n在microservicecloud-provider-dept-8001中的application.yml中添加\n\n```yaml\ninfo:\n  app.name: hph-microservicecloud\n  company.name: www.hphblog.cn\n  build.artifactId: ${project.artifactId}\n  build.version: ${project.version}\n```\n\n热启动服务之后\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423125334.png)\n\n# 自我保护\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423114022.png)\n\n默认情况下，如果Eureka Server在一定时间内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与Eureka Server之间无法正常通信，这就可能变得非常危险了----因为微服务本身是健康的，此时本不应该注销这个微服务。\n\nEureka Server通过“自我保护模式”来解决这个问题----当Eureka Server节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。\n\n 自我保护模式是一种对网络异常的安全保护措施。使用自我保护模式，而已让Eureka集群更加的健壮、稳定。\n\n在分布式系统中有个著名的CAP定理（C-数据一致性；A-服务可用性；P-服务对网络分区故障的容错性，这三个特性在任何分布式系统中不能同时满足，最多同时满足两个）；\n\nNetflix在设计Eureaka时遵循的时`AP原则`\n\n## 复现\n\n我们通过修改8001端口的服务名称然后再修改回来,使不同的服务注册到Eureka中。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423182510.png)\n\n当我们点击进去的时候两个都可以点击进去提供的服务依旧可用，这是因为某时刻某一个微服务不可用了，eureka不会立刻清理，依旧会对该微服务的信息进行保存。\n\n# 服务发现\n\n首先我们再com.hph.springcloud.controller.DeptController中装配DiscoveryClient；\n\n```java\n@Autowired\nprivate DiscoveryClient discoveryClient;\n\n\n//添加服务\n    @RequestMapping(value = \"/deppt/discovery\", method = RequestMethod.GET)\n    public Object discovery() {\n        List<String> list = discoveryClient.getServices();\n        System.out.println(\"********\" + list);\n        List<ServiceInstance> serviceInstanceList = discoveryClient.getInstances(\"MICROSERVICECLOUD-DEPT\");\n        for (ServiceInstance serviceInstance : serviceInstanceList) {\n            System.out.println(serviceInstance.getServiceId() + \"\\t\" + serviceInstance.getHost() + \"\\t\" + serviceInstance.getPort() + \"\\t\" + serviceInstance.getUri());\n\n        }\n\n        return this.discoveryClient;\n\n    }\n```\n\n再main方法中添加。\n\n```java\n@EnableEurekaClient //本服务启动后会自动注册进eureka服务中\n@EnableDiscoveryClient //服务发现\n```\n\n启动EurekaServer7001_App服务端，和DeptProvider8001_App客户端。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423190331.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423190546.png)\n\n在`microservicecloud-provider-dept-8001`中的`com.hph.springcloud.DeptProvider8001_Ap`中的main方法中添加注解\n\n```java\n@EnableEurekaClient //本服务启动后会自动注册进eureka服务中\n@EnableDiscoveryClient //服务发现\n```\n\n消费端`microservicecloud-consumer-dept-80`中com.hph.springcloud.controller.DeptController_Consumer添加\n\n```java\n    // 测试@EnableDiscoveryClient,消费端可以调用服务发现\n    @RequestMapping(value = \"/consumer/dept/discovery\")\n    public Object discovery()\n    {\n        return restTemplate.getForObject(REST_URL_PREFIX + \"/dept/discovery\", Object.class);\n    }\n```\n\n在`com.hph.springcloud.DeptConsumer80_App`中添加\n\n```java\n@EnableEurekaClient\n//在启动该微服务的时候就能去加载我们的自定义Ribbon配置类，从而使配置生效\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423201532.png)\n\n# 集群配置\n\n配置映射方便访问`C:\\Windows\\System32\\drivers\\etc\\hosts`\n\n```properties\n127.0.0.1  eureka7001.com\n127.0.0.1  eureka7002.com\n127.0.0.1  eureka7003.com\n```\n\n新建`microservicecloud-eureka-7002`模块复制`icroservicecloud-eureka-7001`的启动类和配置文件进行相应的修改\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer ////EurekaServer服务器端启动类,接受其它微服务注册进来\n\npublic class EurekaServer7002_App {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaServer7002_App.class, args);\n    }\n}\n\n```\n\n```yaml\nserver:\n  port: 7002\n\neureka:\n  instance:\n    hostname: eureka7002.com #eureka服务端的实例名称\n  client:\n    register-with-eureka: false     #false表示不向注册中心注册自己。\n    fetch-registry: false     #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务\n    service-url:\n      #defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/       #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。\n      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7003.com:7003/eureka/\n```\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@SpringBootApplication\n@EnableEurekaServer ////EurekaServer服务器端启动类,接受其它微服务注册进来\n\npublic class EurekaServer7003_App {\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaServer7003_App.class, args);\n    }\n}\n```\n\n```java\nserver:\n  port: 7003\n\neureka:\n  instance:\n    hostname: eureka7003.com #eureka服务端的实例名称\n  client:\n    register-with-eureka: false     #false表示不向注册中心注册自己。\n    fetch-registry: false     #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务\n    service-url:\n      #defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/       #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址。\n      defaultZone: http://eureka7001.com:7001/eureka/,http://eureka7002.com:7002/eureka/\n```\n\n修改7001的文件为\n\n```yaml\nserver:\n  port: 7001\n\neureka:\n  instance:\n    hostname: eureka7001.com #eureka服务端的实例名称\n  client:\n    register-with-eureka: false     #false表示不向注册中心注册自己。\n    fetch-registry: false     #false表示自己端就是注册中心，我的职责就是维护服务实例，并不需要去检索服务\n    service-url:\n      #单机 defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/       #设置与Eureka Server交互的地址查询服务和注册服务都需要依赖这个地址（单机）。\n      defaultZone: http://eureka7002.com:7002/eureka/,http://eureka7003.com:7003/eureka/\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423204731.png)\n\n集群配置成功，点击`microservicecloud-dept8001`查看是否跳转。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423205436.png)\n\n可见对资源的消耗还是比较严重的，同时我们也差不多可以明白了为什么一个微服务就是一个进程这个说法。\n\n# 与Zookeeper对比\n\n**著名的CAP理论指出，一个分布式系统不可能同时满足C（一致性）、A（可用性）、和P（分区容错性）。由于分区容错性P在分布式系统中必须要保证的，因此我们只能在A和C之间进行权衡。**\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/Eureka/20190423211438.png)\n\n**因此：** \n**Zookeeper保证的是CP，** \n**Eureka则是AP。**\n\nZoopkeeper保证CP： \n当向注册中心查询服务列表时，我们可以容忍注册中心返回的是几分钟以前的注册信息，但是不能接受服务直接down掉不可用。也就是说，服务注册功能对可用性的要求要高于一致性。但是zk会出现这样的一种情况，当master节点因网路故障与其他节点失去联系时，剩余的节点会重新进行leader选举。问题在于，选举leader的时间太长，30~120s，且选举期间整个zk集群是都是不可用的，这就导致在选举期间注册服务瘫痪，在云部署的环境下，因网络问题使得zk集群失去master节点是较大概率会发生的事，虽然服务能够最终恢复，但是漫长的选举时间导致的注册长期不可用是不能容忍的。\n\nEureka保证AP： \nEureka看明白了这一点，因此在设计时就优先保证可用性。<font color=\"red\">Eureka各个节点都是平等的，</font>几个节点挂掉不影响正常节点的工作，剩余的节点依然可以提供注册和查询服务。而Eureka的客户端在向某个Eureka注册时如果发现连接失败，则会自动切换至其他的节点，只要有一台Eureka还在，就能保证注册服务可用（保证可用性），只不过查到的信息可能不是最新的（不保证一致性）。除此之外，Eureka还有一种自我保护机制，如果在15分钟内超过85%的节点都没有正常的心跳，那么Eureka就认为客户端与注册中心出现了网络故障，此时会出现以下几种情况： \n1.Eureka不再从注册列表中移除因为长时间没有收到心跳而应该过期的服务 \n2.Eureka仍然能够接受新服务的注册和查询请求，但是不会被同步到其它节点上（即保证当前节点依然可用） \n3.当前网络稳定时，当前实例新的注册信息会被同步到其它节点中 \n<font color=\"red\">因此，Eureka可以很好的应对因网络故障导致节点失去联系的情况，而不会像zookeeper那样使整个注册服务瘫痪。</font>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud","Eureka"],"categories":["微服务"]},{"title":"微服务与SpringCloud","url":"/2019/04/21/SpringCloud与REST微服务构建/","content":"\n {{ \"SpringClou微服务如何通过HTTP调用服务\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n#  简介\n\n  是资源呈现出来的形式，比如上述URI返回的HTML或JSON，包括HTTP Header等；\n   REST是一个无状态的架构模式，因为在任何时候都可以由客户端发出请求到服务端，最终返回自己想要的数据，当前请求不会受到上次请求的影响。也就是说，服务端将内部资源发布REST服务，客户端通过URL来定位这些资源并通过HTTP协议来访问它们。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/示例.png)\n\n# 项目结构\n\n为了实现REST风格的设计我们需要首先构建一个空项目,在添加一个项目作为父项目,设计统一的版本。在父项目中我们设计统一的资源。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160122.png)\n\n#  microservicecloud\n\n父项目microservicecloud只有这一个XML来统一管理版本问题。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.hph.springcloud</groupId>\n    <artifactId>microservicecloud</artifactId>\n    <packaging>pom</packaging>\n    <version>1.0-SNAPSHOT</version>\n    <modules>\n        <module>microservicecloud-api</module>\n        <module>microservicecloud-provider-dept-8001</module>\n        <module>microservicecloud-consumer-dept-80</module>\n    </modules>\n    <properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <maven.compiler.source>1.8</maven.compiler.source>\n        <maven.compiler.target>1.8</maven.compiler.target>\n        <junit.version>4.12</junit.version>\n        <log4j.version>1.2.17</log4j.version>\n        <lombok.version>1.16.18</lombok.version>\n    </properties>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>Dalston.SR1</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-dependencies</artifactId>\n                <version>1.5.9.RELEASE</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n            <dependency>\n                <groupId>mysql</groupId>\n                <artifactId>mysql-connector-java</artifactId>\n                <version>5.1.47</version>\n            </dependency>\n            <dependency>\n                <groupId>com.alibaba</groupId>\n                <artifactId>druid</artifactId>\n                <version>1.0.31</version>\n            </dependency>\n            <dependency>\n                <groupId>org.mybatis.spring.boot</groupId>\n                <artifactId>mybatis-spring-boot-starter</artifactId>\n                <version>1.3.0</version>\n            </dependency>\n            <dependency>\n                <groupId>ch.qos.logback</groupId>\n                <artifactId>logback-core</artifactId>\n                <version>1.2.3</version>\n            </dependency>\n            <dependency>\n                <groupId>junit</groupId>\n                <artifactId>junit</artifactId>\n                <version>${junit.version}</version>\n                <scope>test</scope>\n            </dependency>\n            <dependency>\n                <groupId>log4j</groupId>\n                <artifactId>log4j</artifactId>\n                <version>${log4j.version}</version>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n</project>\n```\n\n## microservicecloud-api\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-api</artifactId>\n<dependencies>\n    <dependency>\n        <groupId>org.projectlombok</groupId>\n        <artifactId>lombok</artifactId>\n    </dependency>\n</dependencies>\n\n</project>\n```\n\nmicroservicecloud-api只有这个类，为什么没有get，set呢，这是因为我们使用了lombok\n\n```java\npackage com.hph.springcloud.entities;\n\nimport lombok.Data;\nimport lombok.NoArgsConstructor;\nimport lombok.experimental.Accessors;\n\nimport java.io.Serializable;\n\n@SuppressWarnings(\"serial\")\n@NoArgsConstructor\n@Data\n@Accessors(chain = true)\npublic class Dept implements Serializable {\n    private Long deptno;   //主键\n    private String dname;   //部门名称\n    private String db_source;//来自那个数据库，因为微服务架构可以一个服务对应一个数据库，同一个信息被存储到不同数据库\n\n    public Dept(String dname) {\n        super();\n        this.dname = dname;\n    }\n}\n```\n\n在此之前我们需要执行 Maven install 来将这个公有使用的Bean安装到Maven依赖中。\n\n## microservicecloud-provider-dept-8001\n\n新建一个MAVEN在microserviceclou中作为microserviceclou的子项目。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-provider-dept-8001</artifactId>\n    <dependencies>\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160155.png)\n\n### mybatis.cfg.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE configuration\n        PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n\n<configuration>\n\n    <settings>\n        <setting name=\"cacheEnabled\" value=\"true\"/><!-- 二级缓存开启 -->\n    </settings>\n\n</configuration>\n```\n\n###  config\n\n#### application.yml\n\n```yml\nserver:\n  port: 8001\n\nmybatis:\n  config-location: classpath:mybatis/mybatis.cfg.xml        # mybatis配置文件所在路径\n  type-aliases-package: com.hph.springcloud.entities    # 所有Entity别名类所在包\n  mapper-locations:\n    - classpath:mybatis/mapper/**/*.xml                       # mapper映射文件\n\nspring:\n  application:\n    name: microservicecloud-dept\n  datasource:\n    type: com.alibaba.druid.pool.DruidDataSource            # 当前数据源操作类型\n    driver-class-name: org.gjt.mm.mysql.Driver              # mysql驱动包\n    url: jdbc:mysql://192.168.1.110:3306/cloudDB01              # 数据库名称\n    username: root\n    password: 123456\n    dbcp2:\n      min-idle: 5                                           # 数据库连接池的最小维持连接数\n      initial-size: 5                                       # 初始化连接数\n      max-total: 5                                          # 最大连接数\n      max-wait-millis: 200                                  # 等待连接获取的最大超时时间\n```\n\n### 数据准备\n\n```shell\n docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci\n```\n\n```sql\nDROP DATABASE IF EXISTS cloudDB01;\nCREATE DATABASE cloudDB01 CHARACTER SET UTF8;\nUSE cloudDB01;\nCREATE TABLE dept\n(\n  deptno BIGINT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n  dname VARCHAR(60),\n  db_source   VARCHAR(60)\n);\n```\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160235.png)\n\n### service\n\n```java\npackage com.hph.springcloud.service;\n\nimport com.hph.springcloud.entities.Dept;\n\nimport java.util.List;\n\npublic interface DeptService\n{\n  public boolean add(Dept dept);\n  public Dept    get(Long id);\n  public List<Dept> list();\n}\n```\n\n```java\npackage com.hph.springcloud.service;\n\nimport com.hph.springcloud.dao.DeptDao;\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\nimport java.util.List;\n\n@Service\npublic class DeptServiceImpl implements DeptService\n{\n  @Autowired\n  private DeptDao dao ;\n  \n  @Override\n  public boolean add(Dept dept)\n  {\n   return dao.addDept(dept);\n  }\n \n  @Override\n  public Dept get(Long id)\n  {\n   return dao.findById(id);\n  }\n \n  @Override\n  public List<Dept> list()\n  {\n   return dao.findAll();\n  }\n \n}\n```\n\n###  mapper\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n\n<mapper namespace=\"com.hph.springcloud.dao.DeptDao\">\n\n    <select id=\"findById\" resultType=\"Dept\" parameterType=\"Long\">\n   select deptno,dname,db_source from dept where deptno=#{deptno};\n  </select>\n    <select id=\"findAll\" resultType=\"com.hph.springcloud.entities.Dept\">\n   select deptno,dname,db_source from dept;\n  </select>\n    <insert id=\"addDept\" parameterType=\"Dept\">\n   INSERT INTO dept(dname,db_source) VALUES(#{dname},DATABASE());\n  </insert>\n\n</mapper>\n```\n\n### DeptServiceImpl\n\n```java\npackage com.hph.springcloud.service;\n\nimport com.hph.springcloud.dao.DeptDao;\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\nimport java.util.List;\n\n@Service\npublic class DeptServiceImpl implements DeptService\n{\n  @Autowired\n  private DeptDao dao ;\n  \n  @Override\n  public boolean add(Dept dept)\n  {\n   return dao.addDept(dept);\n  }\n \n  @Override\n  public Dept get(Long id)\n  {\n   return dao.findById(id);\n  }\n \n  @Override\n  public List<Dept> list()\n  {\n   return dao.findAll();\n  }\n \n}\n```\n\n### DeptController\n\n微服务提供者REST风格\n\n```java\npackage com.hph.springcloud.controller;\n\nimport com.hph.springcloud.entities.Dept;\nimport com.hph.springcloud.service.DeptService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.*;\n\nimport java.util.List;\n\n@RestController\npublic class DeptController {\n    @Autowired\n    private DeptService service;\n\n    @RequestMapping(value = \"/dept/add\",method = RequestMethod.POST)\n    public boolean add(@RequestBody Dept dept) {\n        return service.add(dept);\n    }\n\n    @RequestMapping(value = \"/dept/get/{id}\",method = RequestMethod.GET)\n    public Dept get(@PathVariable(\"id\") Long id) {\n        return service.get(id);\n    }\n\n    @RequestMapping(value = \"/dept/list\",method = RequestMethod.GET)\n    public List<Dept> list() {\n        return service.list();\n    }\n}\n```\n\n### DeptProvider8001_App\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DeptProvider8001_App {\n    public static void main(String[] args) {\n        SpringApplication.run(DeptProvider8001_App.class,args);\n    }\n}\n```\n\n\n\n## microservicecloud-consumer-dept-80\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <parent>\n        <artifactId>microservicecloud</artifactId>\n        <groupId>com.hph.springcloud</groupId>\n        <version>1.0-SNAPSHOT</version>\n    </parent>\n    <modelVersion>4.0.0</modelVersion>\n\n    <artifactId>microservicecloud-provider-dept-8001</artifactId>\n    <dependencies>\n        <dependency>\n            <groupId>com.hph.springcloud</groupId>\n            <artifactId>microservicecloud-api</artifactId>\n            <version>${project.version}</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>com.alibaba</groupId>\n            <artifactId>druid</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>ch.qos.logback</groupId>\n            <artifactId>logback-core</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.mybatis.spring.boot</groupId>\n            <artifactId>mybatis-spring-boot-starter</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-jetty</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n        </dependency>\n        <!-- 修改后立即生效，热部署 -->\n        <dependency>\n            <groupId>org.springframework</groupId>\n            <artifactId>springloaded</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-devtools</artifactId>\n        </dependency>\n\n    </dependencies>\n\n</project>\n```\n\n\n\napplication.yml\n\n```yml\nserver:\n  port: 80\n```\n\n### cfbeans\n\n```java\npackage com.hph.springcloud.cfbeans;\n\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.client.RestTemplate;\n\n@Configuration\npublic class ConfigBean {\n\n    @Bean\n    public RestTemplate getRestTemplate(){\n        return  new RestTemplate();\n    }\n}\n```\n\n### controller\n\n```java\npackage com.hph.springcloud.controller;\n\nimport com.hph.springcloud.entities.Dept;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\nimport java.util.List;\n\n@RestController\npublic class DeptController_Consumer {\n\n    private  static  final  String REST_URL_PREFIX = \"http://localhost:8001\";\n   /* 使用restTemplate访问restful接口非常的简单粗暴无脑。\n            (url, requestMap, ResponseBean.class)这三个参数分别代表\n    REST请求地址、请求参数、HTTP响应转换被转换成的对象类型。*/\n   @Autowired\n    private RestTemplate restTemplate;\n\n   @RequestMapping(value = \"/consume/dept/add\",method = RequestMethod.POST)\n   public boolean add(Dept dept){\n       return  restTemplate.postForObject(REST_URL_PREFIX+\"/dept/add\",dept,Boolean.class);\n    }\n\n    @RequestMapping(value = \"/consumer/dept/get/{id}\")\n    public Dept get(@PathVariable(\"id\") Long id){\n        return  restTemplate.getForObject(REST_URL_PREFIX+\"/dept/get/\"+id,Dept.class);\n    }\n\n    @SuppressWarnings(\"unchecked\")\n    @RequestMapping(value = \"/consumer/dept/list\")\n    public List<Dept> list(){\n        return  restTemplate.getForObject(REST_URL_PREFIX+\"/dept/list\",List.class);\n    }\n\n\n}\n\n```\n\n### DeptConsumer80_App\n\n```java\npackage com.hph.springcloud;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class DeptConsumer80_App {\n    public static void main(String[] args) {\n        SpringApplication.run(DeptConsumer80_App.class, args);\n    }\n}\n```\n\n\n\n## 测试\n\n我们首先启动``microservicecloud-provider-dept-8001`的服务提供端，然后启动`microservicecloud-consumer-dept-80`消费端，注意我们在消费端仅仅只是编写了一个Controller和一个启动类，其他的什么也没有了。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160304.png)\n\n然而当我们查询的时候时候可以发现服务可以使用这是为什么呢？这是因为我们添加了RestTemplate，它提供了多种便捷访问远程Http服务的方法， 是一种简单便捷的访问restful服务模板类，是Spring提供的用于访问Rest服务的客户端模板工具集。\n\n使用restTemplate访问restful接口非常的简单粗暴无脑。(url, requestMap, ResponseBean.class)这三个参数分别代表 \nREST请求地址、请求参数、HTTP响应转换被转换成的对象类型。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160357.png)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422161215.png)\n\n使用Postman发送\n\n```shell\nhttp://localhost/consume/dept/add?dname=数据中台\n```\n\n我们再调用一下查询方法\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/REST/20190422160332.png)\n\n可以查询到数据了，一个简单的微服务通过HTTP调用服务就实现了。\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud","REST"],"categories":["微服务"]},{"title":"微服务与SpringCloud","url":"/2019/04/18/微服务与SpringCloud/","content":"\n {{ \"微服务的引入和SpringCloud与其他产品的对比\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 微服务简介\n\n“微服务架构”一词是在过去几年里涌现出来的，它用于描述一种独立部署的软件应用设计方式。这种架构方式并没有非常明确的定义，但有一些共同的特点就是围绕在业务能力、自动化布署、端到端的整合以及语言和数据的分散控制上面。目前为止,微服务我们也不太好给一个定义,但是绝大部分的微服务都有相似的特点。\n\n一种架构⻛风格，将单体应⽤用划分成一组⼩的服务，服务之间相互协作，实现业务功能。 \n\n 每个服务运⾏行在独⽴立的进程中，服务间采⽤用轻量量级的通信机制协作（通常是HTTP/ JSON） \n\n每个服务围绕业务能力力进⾏行行构建，并且能够通过⾃自动化机制独⽴立地部署 \n\n很少有集中式的服务管理，每个服务可以使⽤用不不同的语⾔言开发，使⽤用不不同的存储技术 \n\n Loosely coupled service oriented architecture with bounded context ：基于有界上下⽂文的，松散耦合的⾯面向服务的架构  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t————Adrian Cockcroft。\n\n 参考：https://www.martinfowler.com/articles/microservices.html\n\n![img](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/%E6%9E%B6%E6%9E%84/20190422161622.png)\n\n\n\n## 微服务技术栈\n\n| 微服务条目                             | 落地技术                                                     |\n| -------------------------------------- | ------------------------------------------------------------ |\n| 服务开发                               | Springboot、Spring、SpringMVC                                |\n| 服务配置与管理                         | Netflix公司的Archaius、阿里的Diamond等                       |\n| 服务注册与发现                         | Eureka、Consul、Zookeeper等                                  |\n| 服务调用                               | Rest、RPC、gRPC                                              |\n| 服务熔断器                             | Hystrix、Envoy等                                             |\n| 负载均衡                               | Ribbon、Nginx等                                              |\n| 服务接口调用(客户端调用服务的简化工具) | Feign等                                                      |\n| 消息队列                               | Kafka、RabbitMQ、ActiveMQ等                                  |\n| 服务配置中心管理                       | SpringCloudConfig、Chef等                                    |\n| 服务路由（API网关）                    | Zuul等                                                       |\n| 服务监控                               | Zabbix、Nagios、Metrics、Spectator等                         |\n| 全链路追踪                             | Zipkin，Brave、Dapper等                                      |\n| 服务部署                               | Docker、OpenStack、Kubernetes等                              |\n| 数据流操作开发包                       | SpringCloud Stream（封装与Redis,Rabbit、Kafka等发送接收消息） |\n| 事件消息总线                           | Spring Cloud Bus                                             |\n\n## SpringCloud\n\n![ESvKtf.png](http://hphblog.cn/SpringCloud/20190421180902.png)\nSpringCloud，基于SpringBoot提供了一套微服务解决方案，包括服务注册与发现，配置中心，全链路监控，服务网关，负载均衡，熔断器等组件，除了基于NetFlix的开源组件做高度抽象封装之外，还有一些选型中立的开源组件。\n\nSpringCloud利用SpringBoot的开发便利性巧妙地简化了分布式系统基础设施的开发，SpringCloud为开发人员提供了快速构建分布式系统的一些工具，包括配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等,它们都可以用SpringBoot的开发风格做到一键启动和部署。\n\nSpringBoot并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过SpringBoot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包\n\nSpringCloud=分布式微服务架构下的一站式解决方案，是各个微服务架构落地技术的集合体，俗称[微服务全家桶](<https://springcloud.cc/>)。\n\n## 二者关系\n\nSpringBoot专注于快速方便的开发单个个体微服务。\n\nSpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，\n为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务。\n\nSpringBoot可以离开SpringCloud独立使用开发项目，但是SpringCloud离不开SpringBoot，属于依赖的关系。\n\nSpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。\n\n\n\n## 目前架构\n\n分布式+服务治理Dubbo：应用服务化拆分+消息中间件。\n\n![](<https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/%E6%9E%B6%E6%9E%84/20190422161756.png>)\n\n## Dubbo和SpringCloud对比\n\n![](<https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/%E6%9E%B6%E6%9E%84/20190422161809.png>)\n\n**最大区别**：SpringCloud抛弃了Dubbo的RPC通信，采用的是基于HTTP的REST方式。严格来说，这两种方式各有优劣。虽然从一定程度上来说，后者牺牲了服务调用的性能，但也避免了上面提到的原生RPC带来的问题。而且REST相比RPC更为灵活，服务提供方和调用方的依赖只依靠一纸契约，不存在代码级别的强依赖，这在强调快速演化的微服务环境下，显得更加合适。\n\n**品牌机与组装机的区别**\n很明显，Spring Cloud的功能比DUBBO更加强大，涵盖面更广，而且作为Spring的拳头项目，它也能够与Spring Framework、Spring Boot、Spring Data、Spring Batch等其他Spring项目完美融合，这些对于微服务而言是至关重要的。使用Dubbo构建的微服务架构就像组装电脑，各环节我们的选择自由度很高，但是最终结果很有可能因为一条内存质量不行就点不亮了，总是让人不怎么放心，但是如果你是一名高手，那这些都不是问题；而Spring Cloud就像品牌机，在Spring Source的整合下，做了大量的兼容性测试，保证了机器拥有更高的稳定性，但是如果要在使用非原装组件外的东西，就需要对其基础有足够的了解。\n\n**社区支持与更新力度**：DUBBO停止了5年左右的更新，虽然2017.7重启了。对于技术发展的新需求，需要由开发者自行拓展升级（比如当当网弄出了DubboX），这对于很多想要采用微服务架构的中小软件组织，显然是不太合适的，中小公司没有这么强大的技术能力去修改Dubbo源码+周边的一整套解决方案，并不是每一个公司都有阿里的大牛+真实的线上生产环境测试过。\n\n**定位**：Dubbo的定位始终是一款RPC框架，而Spring Cloud的目标是微服务框架下的一站式解决方案，在面临 微服务基础框架选型时Dubbo与Spring Cloud只能二选一。\n\n选型依据： 整体解决方案和框架成熟度，社区热度，可维护性，学习曲线。\n\n## SpringBoot和其他框架对比\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190422223904.png)\n\n\n\n## 微服务的利利和弊\n\n| <font color=\"gree\">利</font> | <font color=\"red\">弊</font> |\n| ---------------------------- | --------------------------- |\n| 强模块化边界                 | 分布式系统复杂性            |\n| 可独⽴立部署                 | 最终一致性                  |\n| 技术多样性                   | 运维复杂性                  |\n|                              | 测试复杂性                  |\n\n## 康威法则\n\n中文直译大概的意思就是：设计系统的组织，其产生的设计等同于组织之内、组织之间的沟通结构。看看下面的图片，再想想Apple的产品、微软的产品设计，就能形象生动的理解这句话。\n\n微服务很多核心理念其实在半个世纪前的一篇文章中就被阐述过了，而且这篇文章中的很多论点在软件开发飞速发展的这半个世纪中竟然一再被验证，这就是[康威定律（Conway's Law）](http://www.melconway.com/Home/Conways_Law.html?spm=a2c4e.11153940.blogcont8611.5.7ea872f09xxfIQ)。\n\n在康威的这篇文章中，最有名的一句话就是：\n\n> Organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. - Melvin Conway(1967)\n\n用通俗的说法就是：组织形式等同系统设计。\n\n这里的系统按原作者的意思并不局限于软件系统。据说这篇文章最初投的哈佛商业评论，结果程序员屌丝的文章不入商业人士的法眼，无情被拒，康威就投到了一个编程相关的杂志，所以被误解为是针对软件开发的。最初这篇文章显然不敢自称定律（law），只是描述了作者自己的发现和总结。后来，在Brooks Law著名的人月神话中，引用这个论点，并将其“吹捧”成了现在我们熟知“康威定律”。\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/SpringCloud/架构/20190422223944.png)\n\n### 康威定律详细介绍\n\nMike从他的角度归纳这篇论文中的其他一些核心观点，如下：\n\n- 第一定律\n    - Communication dictates design\n    - 组织沟通方式会通过系统设计表达出来\n- 第二定律\n    - There is never enough time to do something right, but there is always enough time to do it over\n    - 时间再多一件事情也不可能做的完美，但总有时间做完一件事情\n- 第三定律\n    - There is a homomorphism from the linear graph of a system to the linear graph of its design organization\n    - 线型系统和线型组织架构间有潜在的异质同态特性\n- 第四定律\n    - The structures of large systems tend to disintegrate during development, qualitatively more so than with small systems\n    - 大的系统组织总是比小系统更倾向于分解\n\n## 何时引入微服务\n\n目前正在学习极客时间的微服务20讲，杨波老师的建议一开始并不是要直接使用微服务的，它需要前期的基础设施投入，复杂性很高，反而会影响我们业务的开展， 比较倾向于开始，单块应用的方式，当架构师对业务越来越清晰的时候，业务越来越大，架构师不断地拆分最后演化成微服务架构。\n\n\n\n\n\n\n\n\n\n","tags":["微服务","技术选型"],"categories":["微服务"]},{"title":"SpringBoot和监控管理","url":"/2019/04/14/SpringBoot和监控管理/","content":"\n {{ \"Spring  Boot 监控管理\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\n引入spring-boot-starter-actuator，我们可以使用Spring Boot为我们提供的准生产环境下的应用监控和管理功能。我们可以通过HTTP，JMX，SSH协议来进行操作，自动得到审计、健康及指标信息。\n\n## 准备\n\n![AXV3rQ.png](https://s2.ax1x.com/2019/04/14/AXV3rQ.png)\n\n我们在什么都不做的情况下启动项目\n\n![AjILvD.png](https://s2.ax1x.com/2019/04/15/AjILvD.png)\n\n这里映射了许多的方法比如把info映射到info.json调用的是哪一个方法。\n\n![Ajo0xO.png](https://s2.ax1x.com/2019/04/15/Ajo0xO.png)\n\n这个是我们引入了spring-boot-starter-actuator的原因如果我们不引入这个以来我们看一下效果会是怎么样的。\n\n![AjTFF1.png](https://s2.ax1x.com/2019/04/15/AjTFF1.png)\n\n心塞啊少了那么多信息，只剩下默认的/error了，可见这个模块的作用可以为我们提供的准生产环境下的应用监控和管理功能。下面我们访问一下它的关键的监控把。\n\n## 关键功能\n\n### beans\n\n![AjTT1K.png](https://s2.ax1x.com/2019/04/15/AjTT1K.png)\n\n需要我们在application.properties中添加\n\n```yml\nmanagement.security.enabled=false\n```\n\n我们安装了热部署插件可以按住ctrl+F9来重新运行服务。\n\n再次访问beans看到了不同的信息。beans是监控容器中的组件的信息的。\n\n![Aj7ajO.png](https://s2.ax1x.com/2019/04/15/Aj7ajO.png)\n\n### health\n\nhealth的信息\n\n![Aj7g8P.png](https://s2.ax1x.com/2019/04/15/Aj7g8P.png)\n\n### info\n\ninfo为空，那么info信息是怎么来的呢\n\n![Aj7T5n.png](https://s2.ax1x.com/2019/04/15/Aj7T5n.png)\n\n在配置文件中添加\n\n```yml\ninfo.app.id=app1\ninfo.app.versio=1.0.0\n```\n\n热部署之后\n\n![AjHSa9.png](https://s2.ax1x.com/2019/04/15/AjHSa9.png) \n\n### github\n\n在开发过程中我们有时需要将源码托管给github，我们要配置一些github的属性。创建application.properties文件。\n\n![AjHvSP.png](https://s2.ax1x.com/2019/04/15/AjHvSP.png)\n\n### dump\n\n暴露我们程序运行中的线程信息。\n\n![Ajb0kd.png](https://s2.ax1x.com/2019/04/15/Ajb0kd.png)\n\n### autoconfig\n\nSpringBoot的自动配置信息。\n\n![AjXXKx.png](https://s2.ax1x.com/2019/04/15/AjXXKx.png)\n\n### heapdump\n\n输入localhost:8080/heapdump即可下载信息\n\n![AjzmdK.png](https://s2.ax1x.com/2019/04/15/AjzmdK.png)\n\n### trace\n\n![AjzaFS.png](https://s2.ax1x.com/2019/04/15/AjzaFS.png)\n\n\n\n### mappings\n\n![AjzdJg.png](https://s2.ax1x.com/2019/04/15/AjzdJg.png)\n\n### metrics\n\n![AjzoO1.png](https://s2.ax1x.com/2019/04/15/AjzoO1.png)\n\n### env\n\n![AvSZlj.png](https://s2.ax1x.com/2019/04/15/AvSZlj.png)\n\n### configprops\n\n![AvSK00.png](https://s2.ax1x.com/2019/04/15/AvSK00.png)\n\n```properties\nendpoints.metrics.enabled=false\n```\n\n![AvSd76.png](https://s2.ax1x.com/2019/04/15/AvSd76.png)\n\n### shutdown\n\n在配置文件中配置\n\n```properties\nendpoints.shutdown.enabled=true\n```\n\n![AvS4N8.png](https://s2.ax1x.com/2019/04/15/AvS4N8.png)\n\n![AvS7cj.png](https://s2.ax1x.com/2019/04/15/AvS7cj.png)\n\n\n\n### 总结\n\n| **端点名**   | **描述**                    |\n| ------------ | --------------------------- |\n| *autoconfig* | 所有自动配置信息            |\n| auditevents  | 审计事件                    |\n| beans        | 所有Bean的信息              |\n| configprops  | 所有配置属性                |\n| dump         | 线程状态信息                |\n| env          | 当前环境信息                |\n| health       | 应用健康状况                |\n| info         | 当前应用信息                |\n| metrics      | 应用的各项指标              |\n| mappings     | 应用@RequestMapping映射路径 |\n| shutdown     | 关闭当前应用（默认关闭）    |\n| trace        | 追踪信息（最新的http请求）  |\n\n## 定制端点信息\n\n```properties\nendpoints.beans.id=myselfbean\nendpoints.beans.path=/myselfbeanspath\n```\n\n![Avp1bt.png](https://s2.ax1x.com/2019/04/15/Avp1bt.png)\n\n小结一下\n\n```tex\n定制端点一般通过endpoints+端点名+属性名来设置。\n修改端点id（endpoints.beans.id=mybeans）\n开启远程应用关闭功能（endpoints.shutdown.enabled=true）\n关闭端点（endpoints.beans.enabled=false）\n开启所需端点\nendpoints.enabled=false\nendpoints.beans.enabled=true\n定制端点访问根路径\nmanagement.context-path=/manage\n关闭http端点\nmanagement.port=-1  也就是说这些功能都访问不到了\n```\n\n### 健康监控\n\n在SpringBoot中健康监控组件会在org.springframework.boot.actuate包下，我们也可以在IDEA上看到SpringBoot的健康状态，以Redis为例，当SpringBootMaven加入了Redis的以来之后SpringBoot会对Redis的健康状态进行监控。\n\n![Av9MJU.png](https://s2.ax1x.com/2019/04/15/Av9MJU.png)\n\n在SpringBoot中添加\n\n```xml\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n```\n\n![Av9ryd.png](https://s2.ax1x.com/2019/04/15/Av9ryd.png)\n\n这是因为我们没有正确配置Redis的原因。在Docker中启动Redis，在application.properties中\n\n```properties\nspring.redis.host=192.168.1.110\n```\n\n![AvCVpD.png](https://s2.ax1x.com/2019/04/15/AvCVpD.png)\n\n### 自定义\n\n我们可以自定义健康状态的指示器，\n\n首先我们要编写一个健康指示器实现HealthIndicator接口\n\n```java\n@Component\npublic class MyAppHealthIndicator implements HealthIndicator {\n    @Override\n    public Health health() {\n        //自定义的查询方法\n        // return Health.up().build();代表健康\n        return Health.down().withDetail(\"msg\",\"服务器异常请排除错误\").build();\n    }\n}\n```\n\n![AvCvUP.png](https://s2.ax1x.com/2019/04/15/AvCvUP.png)\n\n这样没有图形界面的监控信息不是很友好我们可以在,SpringBoot的使用SpringBoot Admin\n\n## SpringBoot Admin\n\n### 简介\n\nSpring Boot Admin 是一个管理和监控Spring Boot 应用程序的开源软件。每个应用都认为是一个客户端，通过HTTP或者使用 Eureka注册到admin server中进行展示，Spring Boot Admin UI部分使用AngularJs将数据展示在前端。\n\nSpring Boot Admin 是一个针对spring-boot的actuator接口进行UI美化封装的监控工具。他可以：在列表中浏览所有被监控spring-boot项目的基本信息，详细的Health信息、内存信息、JVM信息、垃圾回收信息、各种配置信息（比如数据源、缓存列表和命中率）等，还可以直接修改logger的level。\n\n## 准备\n\n### server\n\n创建SpringBoot项目初始化如下这是服务端的安装\n\n![Av0enU.png](https://s2.ax1x.com/2019/04/16/Av0enU.png)\n\n我们首先为了不和其他的端口发生冲突,可以配置server端口为自己的自定义的端口号这里我们配置成8000。\n\n在SpringbootAdminServerApplication中我们添加一些注解\n\n```java\npackage com.hph.server;\n\nimport de.codecentric.boot.admin.config.EnableAdminServer;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration  //配置类\n@EnableAdminServer  //开启admin服务\n@SpringBootApplicatio\npublic class SpringbootAdminServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootAdminServerApplication.class, args);\n    }\n\n}\n```\n\n```java\npackage com.hph.server;\n\nimport de.codecentric.boot.admin.config.EnableAdminServer;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration \n@EnableAdminServer  //开启admin服务\n@SpringBootApplication\npublic class SpringbootAdminServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootAdminServerApplication.class, args);\n    }\n}\n```\n\n### client\n\n我们继续创建新的Module，选择Admin的Client端\n\n![AvBMqS.png](https://s2.ax1x.com/2019/04/16/AvBMqS.png)\n\n在client端中我们只需要在application.properties配置上其他的不需要再进行配置。\n\n```properties\n#client占用端口\nserver.port=8001    \n\n#server端的url地址\nspring.boot.admin.url=http://localhost:8000\\\n#修改权限  \nmanagement.security.enabled=false \n```\n\n启动client服务之后再去查看一下我们的信息，发现多了一条纪录。\n\n![AvBoid.png](https://s2.ax1x.com/2019/04/16/AvBoid.png)\n\n### 监控\n\n点击Details我们进入到\n\n#### Details\n\n![AvD4XV.png](https://s2.ax1x.com/2019/04/16/AvD4XV.png)\n\n#### Metrics\n\n![AvrmB8.png](https://s2.ax1x.com/2019/04/16/AvrmB8.png)\n\n#### Environment\n\n![AvrmB8.png](https://s2.ax1x.com/2019/04/16/AvrmB8.png)\n\n![Avr8cq.png](https://s2.ax1x.com/2019/04/16/Avr8cq.png)\n\n#### JXM\n\n![Avrsjx.png](https://s2.ax1x.com/2019/04/16/Avrsjx.png)\n\n#### Threads\n\n这里有一些线程信息\n\n![Avc3IU.png](https://s2.ax1x.com/2019/04/16/Avc3IU.png)\n\n#### Audit\n\n审计事件\n\n![Avg1OI.png](https://s2.ax1x.com/2019/04/16/Avg1OI.png)\n\n#### Trace\n\n![AvgT76.png](https://s2.ax1x.com/2019/04/16/AvgT76.png)\n\n#### Heapdump\n\n![AvgbtO.png](https://s2.ax1x.com/2019/04/16/AvgbtO.png)","tags":["SpringBoot Admin"],"categories":["SpringBoot"]},{"title":"SpringBoot与SpringCloud集成","url":"/2019/04/14/SpringBoot与SpringCloud集成/","content":"\n {{ \"SpringBoot与SpringCloud集成\"}} ：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nSpring Cloud是一系列框架的有序集合。它利用[Spring Boot](https://baike.baidu.com/item/Spring%20Boot/20249767)的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。\n\n关于[微服务相关文档](<https://martinfowler.com/articles/microservices.html>)可以点击此处查看。\n\n## 常用组件\n\n- 服务发现——Netflix Eureka\n- 客服端负载均衡——Netflix Ribbon\n- 断路器——Netflix Hystrix\n- 服务网关——Netflix Zuul\n- 分布式配置——Spring Cloud Config\n\n在后续的学习中,会给大家继续介绍。\n\n## 步骤\n\n### 创建项目\n\n项目结构如下,eureka-server外，在创建项目时选中Eureka Server 其余都应该为Eureka Discovery\n\n![AOHAxS.png](https://s2.ax1x.com/2019/04/14/AOHAxS.png)\n\n![AOHQP0.png](https://s2.ax1x.com/2019/04/14/AOHQP0.png)\n\npom文件配置\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>1.5.20.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n    <groupId>com.hph</groupId>\n    <artifactId>eureka-server</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>eureka-server</name>\n    <description>Demo project for Spring Boot</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Edgware.SR5</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-server</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\n在eureka-server中我们添加application.yml配置文件。\n\n```yml\nserver:\n  port: 8761\neureka:\n  instance:\n    hostname: eureka-server  # eureka实例的主机名\n  client:\n    register-with-eureka: false #不把自己注册到eureka上\n    fetch-registry: false #不从eureka上来获取服务的注册信息\n    service-url:\n      defaultZone: http://localhost:8761/eureka/\n```\n在EurekaServerApplication中我们添加@EnableEurekaServer 注解\n```java\npackage com.hph.eurek;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;\n\n@EnableEurekaServer  //添加\n@SpringBootApplication\npublic class EurekaServerApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(EurekaServerApplication.class, args);\n    }\n\n}\n```\n\n### 启动项目\n\n![AOH9UI.png](https://s2.ax1x.com/2019/04/14/AOH9UI.png)\n\n### provider\n\n下面我们来编写provider项目代码\n\npom文件配置如下\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n    <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>1.5.20.RELEASE</version>\n        <relativePath/> <!-- lookup parent from repository -->\n    </parent>\n    <groupId>com.hph</groupId>\n    <artifactId>provider-ticket</artifactId>\n    <version>0.0.1-SNAPSHOT</version>\n    <name>provider-ticket</name>\n    <description>Demo project for Spring Boot</description>\n\n    <properties>\n        <java.version>1.8</java.version>\n        <spring-cloud.version>Edgware.SR5</spring-cloud.version>\n    </properties>\n\n    <dependencies>\n        <dependency>\n            <groupId>org.springframework.cloud</groupId>\n            <artifactId>spring-cloud-starter-netflix-eureka-client</artifactId>\n        </dependency>\n\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-test</artifactId>\n            <scope>test</scope>\n        </dependency>\n    </dependencies>\n\n    <dependencyManagement>\n        <dependencies>\n            <dependency>\n                <groupId>org.springframework.cloud</groupId>\n                <artifactId>spring-cloud-dependencies</artifactId>\n                <version>${spring-cloud.version}</version>\n                <type>pom</type>\n                <scope>import</scope>\n            </dependency>\n        </dependencies>\n    </dependencyManagement>\n\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n\n</project>\n```\n\n#### 创建服务\n\n项目结构如图所使\n\n![AOLRo9.png](https://s2.ax1x.com/2019/04/14/AOLRo9.png)\n\n\n\n```java\npackage com.hph.provider.service;\n\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class TicketService {\n    public String getTicket(){\n        return \"《复仇者联盟4：终局之战》\";\n    }\n\n```\n\n配置视图控制器\n\n```java\npackage com.hph.provider.controller;\n\n\nimport com.hph.provider.service.TicketService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class TicketController {\n\n    @Autowired\n    TicketService ticketService;\n\n    @GetMapping(\"/ticket\")\n    public String getTicket() {\n        return ticketService.getTicket();\n    }\n}\n```\n\n编写配置文件\n\n```yml\nserver:\n  port: 8001\nspring:\n  application:\n    name: provider-ticket\n\n\neureka:\n  instance:\n    prefer-ip-address: true # 注册服务的时候使用服务的ip地址\n  client:\n    service-url:\n      defaultZone: http://localhost:8761/eureka/\n```\n\n#### 启动服务\n\n![AOLhJ1.png](https://s2.ax1x.com/2019/04/14/AOLhJ1.png)\n\n再去查看一下服务端\n\n服务成功注册。\t\n\n![AOL4Rx.png](https://s2.ax1x.com/2019/04/14/AOL4Rx.png)\n\n我们可以更换服务端口把它打成jar包实现多服务的注册测试。\n\n![AOOGf1.png](https://s2.ax1x.com/2019/04/14/AOOGf1.png)\n\n![AOOsfI.png](https://s2.ax1x.com/2019/04/14/AOOsfI.png)\n\n![AOOg6f.png](https://s2.ax1x.com/2019/04/14/AOOg6f.png)\n\n### consumer\n\nconsumer端项目结构如下\n\n![AOjlGR.png](https://s2.ax1x.com/2019/04/14/AOjlGR.png)\n\n```java\npackage com.hph.consumer;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cloud.client.discovery.EnableDiscoveryClient;\nimport org.springframework.cloud.client.loadbalancer.LoadBalanced;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.web.client.RestTemplate;\n\n@EnableDiscoveryClient //开启发现服务功能\n@SpringBootApplication\npublic class ConsumerUserApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(ConsumerUserApplication.class, args);\n    }\n\n    @LoadBalanced  //使用负载均衡机制\n    @Bean\n    public RestTemplate restTemplate() {\n        return new RestTemplate();\n    }\n}\n```\n\n```java\npackage com.hph.consumer.controller;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\nimport org.springframework.web.client.RestTemplate;\n\n@RestController\npublic class UserController {\n\n    @Autowired\n    RestTemplate restTemplate;\n\n    @GetMapping(\"/buy\")\n    public String buyTicket(String name) {\n        //此参数为注册在Eureka中的服务\n        String ticketName = restTemplate.getForObject(\"http://PROVIDER-TICKET/ticket\", String.class);\n        return name + \"购买了\" + ticketName;\n\n    }\n}\n\n```\n\n\n\n![AOjVMV.png](https://s2.ax1x.com/2019/04/14/AOjVMV.png)\n\n启动后我们发现在Eureka上注册了另外一个服务\n\n![AOjwid.png](https://s2.ax1x.com/2019/04/14/AOjwid.png)\n\n### 测试\n\n我们访问consumer8200端口的buy方法。我们可以说这个服务时负载均衡的。它使用了轮询的方式。\n\n![AjWgIK.png](https://s2.ax1x.com/2019/04/15/AjWgIK.png)\n\n![AOvFTe.png](https://s2.ax1x.com/2019/04/14/AOvFTe.png)\n\n后续会继续学习关于SpringClould的相关知识。\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringCloud"],"categories":["SpringBoot"]},{"title":"SpringBoot与安全","url":"/2019/04/13/SpringBoot安全/","content":"\n {{ \"SpringBoot  Security管理武当秘籍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\n### 安全框架\n\nSpring Security是针对Spring项目的安全框架，也是Spring Boot底层安全模块默认的技术选型。他可以实现强大的web安全控制。对于安全控制，我们仅需引入spring-boot-starter-security模块，进行少量的配置，即可实现强大的安全管理。\n\nApache Shiro是一个强大且易用的Java安全框架,执行身份验证、授权、密码和会话管理。使用Shiro的易于理解的API,您可以快速、轻松地获得任何应用程序,从最小的移动应用程序到最大的网络和企业应用程序。\n\n### Spring Security\n\n```java\nWebSecurityConfigurerAdapter：自定义Security策略\nAuthenticationManagerBuilder：自定义认证策略\n@EnableWebSecurity：开启WebSecurity模式\n```\n\n应用程序的两个主要区域是“认证”和“授权”（或者访问控制）。这两个主要区域是Spring Security 的两个目标。\n\n“认证”（Authentication），是建立一个他声明的主体的过程（一个“主体”一般是指用户，设备或一些可以在你的应用程序中执行动作的其他系统）。\n\n“授权”（Authorization）指确定一个主体是否允许在你的应用程序执行一个动作的过程。为了抵达需要授权的店，主体的身份已经有认证过程建立。\n\n这个概念是通用的而不只在Spring Security中。\n\n## 准备\n\n创建一个项目Springboot为1.5.20,并且导入thymeleaf支持。\n\n### 页面准备\n\n链接：https://pan.baidu.com/s/11UWsVj5rohGms24Xl8rFYQ  提取码：3pwj \n\n### Controller\n\n```java\npackage com.hph.springbootsecurity.controller;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\n\n@Controller\npublic class KungfuController {\n\tprivate final String PREFIX = \"pages/\";\n\t/**\n\t * 欢迎页\n\t * @return\n\t */\n\t@GetMapping(\"/\")\n\tpublic String index() {\n\t\treturn \"welcome\";\n\t}\n\t\n\t/**\n\t * 登陆页\n\t * @return\n\t */\n\t@GetMapping(\"/userlogin\")\n\tpublic String loginPage() {\n\t\treturn PREFIX+\"login\";\n\t}\n\t\n\t\n\t/**\n\t * level1页面映射\n\t * @param path\n\t * @return\n\t */\n\t@GetMapping(\"/level1/{path}\")\n\tpublic String level1(@PathVariable(\"path\")String path) {\n\t\treturn PREFIX+\"level1/\"+path;\n\t}\n\t\n\t/**\n\t * level2页面映射\n\t * @param path\n\t * @return\n\t */\n\t@GetMapping(\"/level2/{path}\")\n\tpublic String level2(@PathVariable(\"path\")String path) {\n\t\treturn PREFIX+\"level2/\"+path;\n\t}\n\t\n\t/**\n\t * level3页面映射\n\t * @param path\n\t * @return\n\t */\n\t@GetMapping(\"/level3/{path}\")\n\tpublic String level3(@PathVariable(\"path\")String path) {\n\t\treturn PREFIX+\"level3/\"+path;\n\t}\n\n}\n```\n\n![ALGYlR.png](https://s2.ax1x.com/2019/04/13/ALGYlR.png)\n\n启动之后报错这是因为我们的thymeleaf版本不支持，因此我们需要更改一下pom文件中的配置信息。\n\n```xml\n    <properties>\n        <java.version>1.8</java.version>\n        <thymeleaf.version>3.0.9.RELEASE</thymeleaf.version>\n        <thymeleaf-layout-dialect.version>2.3.0</thymeleaf-layout-dialect.version>\n    </properties>\n```\n\n![ALJGE8.png](https://s2.ax1x.com/2019/04/13/ALJGE8.png)\n\n现在这套武当派的秘籍管理系统不是很好我们需要完善一下。\n\n## 步骤\n\n### 引入Spring Security\n\n在pom文件中添加\n\n```xml\n<dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-security</artifactId>\n</dependency>\n```\n\n### Spring Security的配置\n\nSpringBoot帮助我们配置了大多数的Spring Security，因此我们只需要编写一个配置类即可。参考[官网](<https://docs.spring.io/spring-security/site/docs/current/guides/html5/helloworld-boot.html>)\n\n```java\npackage com.hph.springbootsecurity.config;\n\nimport org.springframework.security.config.annotation.web.builders.HttpSecurity;\nimport org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;\nimport org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;\n\n@EnableWebSecurity\n//编写SpringSecurity的配置类\npublic class MySecurityConfig extends WebSecurityConfigurerAdapter {\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        //注销掉父类的默认规则\n        //super.configure(http);\n\n        //定制请求的授权规则\n        http.authorizeRequests().antMatchers(\"/\").\n                permitAll()\n                .antMatchers(\"/level1/**\").hasRole(\"newbee\")\n                .antMatchers(\"/level2/**\").hasRole(\"senior\")\n                .antMatchers(\"/level3/**\").hasRole(\"master\");\n    }\n}\n```\n\n在配置之后我们先看一下效果。\n\n![ALtAYD.png](https://s2.ax1x.com/2019/04/13/ALtAYD.png)\n\n主页可以访问不过其他的组件时候。不可以访问。提示403，必须角色相互匹配。\n\n![ALteld.png](https://s2.ax1x.com/2019/04/13/ALteld.png)\n\n当我们开启自动配置登录的时候\n\n```java\n//开启自动配置的登录功能\n http.formLogin();\n//1.login请求来到登录页\n//2.如果登录错误,重定向到/login?error\n```\n\n\n\n![ALtvAf.png](https://s2.ax1x.com/2019/04/13/ALtvAf.png)\n\n### 自定义规则 \n\n开发过程中尽量不要使用中文\n\n```java\n  //定义认证规则\n    @Override\n    protected void configure(AuthenticationManagerBuilder auth) throws Exception {\n     //   super.configure(auth);\n        auth.inMemoryAuthentication()\n                .withUser(\"武当侠士\").password(\"cainiao\").roles(\"newbee\")\n                .and()\n                .withUser(\"无为真人\").password(\"zhongji\").roles(\"newbee\",\"senior\")\n                .and()\n                .withUser(\"武当天尊\").password(\"dalao\").roles(\"newbee\",\"senior\",\"master\");\n    }\n```\n\n经过测试相应角色都可以访问。\n\n### 添加注销\n\n首先我们在welcome.html中添加\n\n```html\n<h1 align=\"center\">欢迎光临武当秘籍管理系统</h1>\n<h2 align=\"center\">游客您好，如果想查看武当秘籍 <a th:href=\"@{/login}\">请成为武当弟子</a></h2>\n<!--添加-->\n<form th:action=\"@{/logout}\" method=\"post\">\n\t<input type=\"submit\" value=\"注销\"/>\n</form>\n```\n配置类中设置logout;\n```java\nhttp.logout();\n```\n\n注销成功之后会返回login?logout的登录页，当然我们也可以定制url\n\n![ALwSZn.png](https://s2.ax1x.com/2019/04/13/ALwSZn.png)\n\n定制注销后返回的url\n\n```java\nhttp.logout().logoutSuccessUrl(\"/\");\n```\n\n如何让游客显示为特定角色呢？我们需要引入\n\n```xml\n<properties>\n        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n        <project.reporting.outputEncoding>UTF-8</project.reporting.outputEncoding>\n        <java.version>1.8</java.version>\n        <thymeleaf.version>3.0.9.RELEASE</thymeleaf.version>\n        <thymeleaf-layout-dialect.version>2.3.0</thymeleaf-layout-dialect.version>\n        <thymeleaf-extras-springsecurity4.version>3.0.2.RELEASE</thymeleaf-extras-springsecurity4.version>\n    </properties>\n\n    <dependency>\n            <groupId>org.thymeleaf.extras</groupId>\n            <artifactId>thymeleaf-extras-springsecurity4</artifactId>\n        </dependency>\n```\n\n### 显示角色权限\n\n在welcome.html中添加\n\n```html\n<!--没有认证的情况下-->\n<div sec:authorize=\"!isAuthenticated()\">\n<h2 align=\"center\">游客您好，如果想查看武当秘籍 <a th:href=\"@{/login}\">请成为武当弟子</a></h2>\n</div>\n<!--认证了-->\n<div sec:authorize=\"isAuthenticated()\">\n\t<h2><span sec:authentication=\"name\"></span>，您好,您的角色有：\n\t\t<span sec:authentication=\"principal.authorities\"></span></h2>\n\t<form th:action=\"@{/logout}\" method=\"post\">\n\t\t<input type=\"submit\" value=\"注销\"/>\n\t</form>\n</div>\n```\n\n\n\n![AL0LuQ.png](https://s2.ax1x.com/2019/04/13/AL0LuQ.png)\n\n![AL6I4s.png](https://s2.ax1x.com/2019/04/13/AL6I4s.png)\n\n### 对应权限显示\n\n我们需要在welcome.html中修改\n\n```html\n<div sec:authorize=\"hasRole('newbee')\">\n    <h3>普通武功秘籍</h3>\n    <ul>\n        <li><a th:href=\"@{/level1/1}\">八卦掌</a></li>\n        <li><a th:href=\"@{/level1/2}\">犀牛望月</a></li>\n        <li><a th:href=\"@{/level1/3}\">太渊十三剑</a></li>\n    </ul>\n</div>\n\n<div sec:authorize=\"hasRole('senior')\">\n    <h3>高级武功秘籍</h3>\n    <ul>\n        <li><a th:href=\"@{/level2/1}\">梯云纵</a></li>\n        <li><a th:href=\"@{/level2/2}\">七星聚首</a></li>\n        <li><a th:href=\"@{/level2/3}\">天外飞仙</a></li>\n    </ul>\n</div>\n\n<div sec:authorize=\"hasRole('master')\">\n    <h3>绝世武功秘籍</h3>\n    <ul>\n        <li><a th:href=\"@{/level3/1}\">神照经</a></li>\n        <li><a th:href=\"@{/level3/2}\">九阴真经</a></li>\n        <li><a th:href=\"@{/level3/3}\">独孤九剑</a></li>\n    </ul>\n</div>\n```\n\n### 记住我功能\n\n在MySecurityConfig中添加\n\n```java\n//开启记住我功能\nhttp.rememberMe();\n```\n\n开启之后再次登录就有一个记住我的按钮了\n\n![ALgiJs.png](https://s2.ax1x.com/2019/04/13/ALgiJs.png)\n\n![ALgTXV.png](https://s2.ax1x.com/2019/04/13/ALgTXV.png)\n\n登录成功之后cookie发送给浏览器保存，以后登录带上这个cookie，只要通过检查就可以免登录，如果点击注销会删除这个cookie\n\n### 定制登录页\n\n在MySecurityConfig中使用\n\n```java\nhttp.formLogin().usernameParameter(\"user\").passwordParameter(\"passwd\").loginPage(\"/userlogin\");\n```\n\n在修改welcome.html 发送请求为userlogin\n\n```html\n<h2 align=\"center\">游客您好，如果想查看武当秘籍请<a th:href=\"@{/userlogin}\">成为武当弟子</a></h2>\n```\n\n默认post形式的/login代表处理登录，但是如果一旦定制了loginPage的post请求就是登录\n\nlogin.html\n\n```html\n<!DOCTYPE html>\n<html xmlns:th=\"http://www.thymeleaf.org\">\n<head>\n<meta charset=\"UTF-8\">\n<title>Insert title here</title>\n</head>\n<body>\n\t<h1 align=\"center\">欢迎登录武当秘籍管理系统</h1>\n\t<hr>\n\t<div align=\"center\">\n\t\t<form th:action=\"@{/userlogin}\" method=\"post\">\n\t\t\t用户名:<input name=\"user\"/><br>\n\t\t\t密&nbsp;&nbsp;&nbsp;码:<input name=\"passwd\"><br/>\n\t\t\t<input type=\"submit\" value=\"登陆\">\n\t\t</form>\n\t</div>\n</body>\n</html>\n```\n\n![ALheIS.png](https://s2.ax1x.com/2019/04/13/ALheIS.png)\n\n实现了登录界面的跳转。\n\n#### 添加记住我功能\n\n在MySecurityConfig中添加\n\n```java\n//开启记住我功能\n http.rememberMe().rememberMeParameter(\"remeber\");\n```\n\n在userlogin.html中添加\n\n```html\n<div align=\"center\">\n    <br th:action=\"@{/userlogin}\" method=\"post\">\n        用户名:<input name=\"user\"/><br>\n        密&nbsp;&nbsp;&nbsp;码:<input name=\"passwd\"><br/>\n        <!--添加记住我按钮-->\n        <input type=\"checkbox\" name=\"remeber\">记住我</br>\n        <input type=\"submit\" value=\"登陆\">\n    </form>\n</div>\n```\n\n![ALh5Lt.png](https://s2.ax1x.com/2019/04/13/ALh5Lt.png)\n\n没有提交表单之前式没有rember这个cookie的\n\n![AL4FW4.png](https://s2.ax1x.com/2019/04/13/AL4FW4.png)\n\n这样下次就不用手动输入密码了。\n\n\n\n\n\n\n\n","tags":["Spring Security"],"categories":["SpringBoot"]},{"title":"SpringBoot与任务","url":"/2019/04/13/SpringBoot与任务/","content":"\n {{ \"SpringBoot异步任务 定时任务 邮件任务\"}}：<Excerpt in index | 首页摘要><!-- more -->\n \n## 准备\n\n![ALp4vq.png](https://s2.ax1x.com/2019/04/13/ALp4vq.png)\n\n暂时只选中web模块\n\n![ALpOPJ.png](https://s2.ax1x.com/2019/04/13/ALpOPJ.png)\n\n## 异步任务\n\n```java\npackage com.hph.task.service;\n\nimport org.springframework.stereotype.Service;\n\nimport java.text.SimpleDateFormat;\nimport java.util.Calendar;\n\n@Service\npublic class Asyncservice {\n\n    public void dataprocessing() {\n        Calendar ago = Calendar.getInstance();\n        SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd :hh:mm:ss\");\n        System.out.println(\"数据处理前\"+dateFormat.format(ago.getTime()));\n\n        try {\n            Thread.sleep(3000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n\n        System.out.println(\"数据正在处理中......\");\n\n        Calendar now = Calendar.getInstance();\n        System.out.println(\"数据处理完毕\"+dateFormat.format(now.getTime()));\n    }\n\n}\n```\n\n```java\npackage com.hph.task.controller;\n\nimport com.hph.task.service.Asyncservice;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class AsyncController {\n    @Autowired\n    Asyncservice asyncservice;\n\n    @GetMapping(\"/dataprocessing\")\n    public String dataprocessing() {\n        asyncservice.dataprocessing();\n        return \"success\";\n    }\n}\n```\n\n三秒之后数据有响应。\n\n![ALCytg.png](https://s2.ax1x.com/2019/04/13/ALCytg.png)\n\n要完成数据的异步调用其实很简单我们只需要在SpringbootTaskApplication 开启异步注解功能\n\n```java\npackage com.hph.task;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.scheduling.annotation.EnableAsync;\n\n@EnableAsync //开启异步注解功能\n@SpringBootApplication\npublic class SpringbootTaskApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootTaskApplication.class, args);\n    }\n\n}\n\n```\n\n```java\npackage com.hph.task.service;\n\nimport org.springframework.scheduling.annotation.Async;\nimport org.springframework.stereotype.Service;\n\nimport java.text.SimpleDateFormat;\nimport java.util.Calendar;\n\n@Service\npublic class Asyncservice {\n    @Async   //异步任务\n    public void dataprocessing() {\n        Calendar ago = Calendar.getInstance();\n        SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd :hh:mm:ss\");\n        System.out.println(\"数据处理前\" + dateFormat.format(ago.getTime()));\n\n        try {\n            Thread.sleep(3000);\n        } catch (InterruptedException e) {\n            e.printStackTrace();\n        }\n\n        System.out.println(\"数据正在处理中......\");\n\n        Calendar now = Calendar.getInstance();\n        System.out.println(\"数据处理完毕\" + dateFormat.format(now.getTime()));\n    }\n\n}\n\n```\n\n![ALPnUS.png](https://s2.ax1x.com/2019/04/13/ALPnUS.png)\n\n## 定时任务\n\n定时任务可以按照自己的规则定时启动任务。\n\n```java\npublic @interface Scheduled {\n\t//这个cron比较重要 比较像Linux中的crontab\n \n\t//         秒  分   时   日   月  周几\n\t// {@code \"0   *    *    *    *  MON-FRI\"} 周一到周五每秒启动一次\n\tString cron() default \"\";\n\n\tString zone() default \"\";\n\n\tlong fixedDelay() default -1;\n\n\tString fixedDelayString() default \"\";\n\n\tlong fixedRate() default -1;\n\n\tString fixedRateString() default \"\";\n\n\tlong initialDelay() default -1;\n\n\tString initialDelayString() default \"\";\n\n}\n```\n\n### 准备\n\n#### service\n\n```java\npackage com.hph.task.service;\n\nimport org.springframework.scheduling.annotation.Scheduled;\nimport org.springframework.stereotype.Service;\n\nimport java.text.SimpleDateFormat;\nimport java.util.Calendar;\n\n@Service\npublic class ScheduledService {\n    @Scheduled(cron = \"0   *   *  *  *  MON-SAT\") //每分钟启动一次周一到周六\n    public void hello() {\n        Calendar now = Calendar.getInstance();\n        SimpleDateFormat dateFormat = new SimpleDateFormat(\"yyyy-MM-dd :hh:mm:ss\");\n        System.out.println(dateFormat.format(now.getTime())+\"  定时任务启动 ..  .. .. \");\n    }\n}\n```\n\n#### 开启注解\n\n需要在`SpringbootTaskApplication`开启注解\n\n```java\n@SpringBootApplication\n@EnableScheduling   //开启基于注解的定时任务\npublic class SpringbootTaskApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootTaskApplication.class, args);\n    }\n}\n```\n\n![ALiERJ.png](https://s2.ax1x.com/2019/04/13/ALiERJ.png)\n\n#### cron表达式\n\n![ALivFO.png](https://s2.ax1x.com/2019/04/13/ALivFO.png)\n\n```java\n@Scheduled(cron = \"0,1,2,3,4   *   *  *  *  MON-SAT\") //每分钟的头1-4秒启动定时任务\n```\n\n![ALFZtS.png](https://s2.ax1x.com/2019/04/13/ALFZtS.png)\n\n```java\n@Scheduled(cron = \"0-4  *   *  *  *  MON-SAT\") //每分钟的头1-4秒启动定时任务\n```\n\n![ALFD76.png](https://s2.ax1x.com/2019/04/13/ALFD76.png)\n\n```java\n@Scheduled(cron = \"0/4  *   *  *  *  MON-SAT\") //每4秒启动定时任务\n```\n\n![ALFRcd.png](https://s2.ax1x.com/2019/04/13/ALFRcd.png)\n\n其他例子\n\n```text\n  \t* 0 * * * * MON-FRI\n     *  【0 0/5 14,18 * * ?】 每天14点整，和18点整，每隔5分钟执行一次\n     *  【0 15 10 ? * 1-6】 每个月的周一至周六10:15分执行一次\n     *  【0 0 2 ? * 6L】每个月的最后一个周六凌晨2点执行一次\n     *  【0 0 2 LW * ?】每个月的最后一个工作日凌晨2点执行一次\n     *  【0 0 2-4 ? * 1#1】每个月的第一个周一凌晨2点到4点期间，每个整点都执行一次；\n```\n\n##  邮件任务\n\n### 准备\n\n我们需要在邮件中引入依赖\n\n```xml\n <dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-mail</artifactId>\n </dependency>\n```\n\n### 自动配置\n\n```java\n@Configuration\n@ConditionalOnClass(Session.class)\n@ConditionalOnProperty(prefix = \"spring.mail\", name = \"jndi-name\")\n@ConditionalOnJndi\nclass MailSenderJndiConfiguration {\n\n\tprivate final MailProperties properties;\n\n\tMailSenderJndiConfiguration(MailProperties properties) {\n\t\tthis.properties = properties;\n\t}\n\n\t@Bean   //用来发送邮件的 \n\tpublic JavaMailSenderImpl mailSender(Session session) {\n\t\tJavaMailSenderImpl sender = new JavaMailSenderImpl();\n\t\tsender.setDefaultEncoding(this.properties.getDefaultEncoding().name());\n\t\tsender.setSession(session);\n\t\treturn sender;\n\t}\n\n\t@Bean\n\t@ConditionalOnMissingBean\n\tpublic Session session() {\n\t\tString jndiName = this.properties.getJndiName();\n\t\ttry {\n\t\t\treturn new JndiLocatorDelegate().lookup(jndiName, Session.class);\n\t\t}\n\t\tcatch (NamingException ex) {\n\t\t\tthrow new IllegalStateException(\n\t\t\t\t\tString.format(\"Unable to find Session in JNDI location %s\", jndiName),\n\t\t\t\t\tex);\n\t\t}\n\t}\n\n}\n\n```\n\n可以配置的选项\n\n```java\n@ConfigurationProperties(prefix = \"spring.mail\")\npublic class MailProperties {\n\n\tprivate static final Charset DEFAULT_CHARSET = Charset.forName(\"UTF-8\");\n\n\tprivate String host;\n    \n\tprivate Integer port;\n\n\tprivate String username;\n\n\tprivate String password;\n\n\tprivate String protocol = \"smtp\";\n\n\tprivate Charset defaultEncoding = DEFAULT_CHARSET;\n\n\tprivate Map<String, String> properties = new HashMap<String, String>();\n\n\tprivate String jndiName;\n    ......\n}\n```\n\n### 配置邮箱\n\n需要将QQ邮箱中设置一下\n\n![ALV1v4.png](https://s2.ax1x.com/2019/04/13/ALV1v4.png)\n\n在`application.properties`中配置\n\n```properties\nspring.mail.username=467008580@qq.com\nspring.mail.password=meqkusfmrwxxbhag   #授权码\nspring.mail.host=smtp.qq.com\n```\n### 简单邮件\n\n```java\n    @Autowired\n    JavaMailSender mailSender;\n\n    @Test\n    public void sendMail() {\n        SimpleMailMessage message = new SimpleMailMessage();\n        //邮件设置\n        message.setSubject(\"邮件测试通知来自QQ邮箱\");\n        message.setText(\"SpringBoot的邮件测试\");\n        message.setTo(\"han_penghui@sina.com\"); //给新浪发送邮箱\n        message.setFrom(\"467008580@qq.com\");\n        mailSender.send(message);\n    }\n```\n\n启动测试类\n\n![ALVQ8U.png](https://s2.ax1x.com/2019/04/13/ALVQ8U.png)\n\n如果运行程序出错在`application.properties`中添加配置\n\n```properties\nspring.mail.properties.mail.smtp.ssl.enable=true\n```\n\n### 复杂邮件\n\n```java\n//复杂邮件发送需要将第二个参数设置为ture\t\npublic MimeMessageHelper(MimeMessage mimeMessage, boolean multipart) throws MessagingException {\n\t\tthis(mimeMessage, multipart, null);\n}\n```\n\n```java\n    @Test\n    public void sendMimeMail() throws MessagingException {\n        //创建一个复杂的消息右键\n        MimeMessage mimeMessage = mailSender.createMimeMessage();\n        MimeMessageHelper helper = new MimeMessageHelper(mimeMessage,true);\n\n        helper.setSubject(\"复杂邮件测试来自QQ邮箱\");\n        helper.setText(\"<b style='color:red'>SpringBoot</b>的<em>邮件测试</em>\",true);  //如果没有设置true默认是false，标签不生效\n        helper.setTo(\"han_penghui@sina.com\"); //给新浪发送邮箱\n\n        //helpr上传文件\n        helper.addAttachment(\"背景.jpg\",new File(\"E:\\\\mail\\\\bg.jpg\"));\n        helper.addAttachment(\"Java.pdf\",new File(\"E:\\\\mail\\\\Java知识.pdf\"));\n        helper.setFrom(\"467008580@qq.com\");\n        mailSender.send(mimeMessage);\n\n    }\n```\n\n![ALZ1FP.png](https://s2.ax1x.com/2019/04/13/ALZ1FP.png)\n\n\n\n![ALZMdI.png](https://s2.ax1x.com/2019/04/13/ALZMdI.png)\n\n发送成功\n\n\n\n","tags":["任务"],"categories":["SpringBoot"]},{"title":"SpringBoot和Elasticsearch集成","url":"/2019/04/12/SpringBoot和ElasticSearch集成/","content":"\n {{ \"SpringBoot和Elasticsearch的集成\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 步骤\n\n![AqUQhD.png](https://s2.ax1x.com/2019/04/12/AqUQhD.png)\n\n![AqUbHx.png](https://s2.ax1x.com/2019/04/12/AqUbHx.png)\n\n## 依赖\n\n在Maven的pom文件中\n\n```xml\n  \t <!--SpringBoot默认使用SpringData ElasticSearch模块进行操作-->\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-data-elasticsearch</artifactId>\n        </dependency>\n        <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n```\n\n## 自动配置\n\n![AqDcDJ.png](https://s2.ax1x.com/2019/04/12/AqDcDJ.png)\n\n![AqDovD.png](https://s2.ax1x.com/2019/04/12/AqDovD.png)\n\n在SpringBoot中默认支持两种技术来和ES进行数据交互操作:\n\n1.Jest(默认不生效的)\n\n![AqrGPx.png](https://s2.ax1x.com/2019/04/12/AqrGPx.png)\n\n需要导入Jest的工具包` io.searchbox.client.JestClient;`\n\n2.SpringData ElasticSearch\n\n```java\n\t@Bean\n\t@ConditionalOnMissingBean\n\t//链接es的客户端\n\tpublic Client elasticsearchClient() {\n\t\ttry {\n\t\t\treturn createClient();\n\t\t}\n\t\tcatch (Exception ex) {\n\t\t\tthrow new IllegalStateException(ex);\n\t\t}\n\t}\n\t//创建客户端\n\tprivate Client createClient() throws Exception {\n        //获得每一个节点的信息\n\t\tif (StringUtils.hasLength(this.properties.getClusterNodes())) {\n\t\t\treturn createTransportClient();\n\t\t}\n\t\treturn createNodeClient();\n\t}\n\t//创创建连接点客户端\n\tprivate Client createNodeClient() throws Exception {\n\t\tSettings.Builder settings = Settings.settingsBuilder();\n\t\tfor (Map.Entry<String, String> entry : DEFAULTS.entrySet()) {\n\t\t\tif (!this.properties.getProperties().containsKey(entry.getKey())) {\n\t\t\t\tsettings.put(entry.getKey(), entry.getValue());\n\t\t\t}\n\t\t}\n\t\tsettings.put(this.properties.getProperties());\n\t\tNode node = new NodeBuilder().settings(settings)\n\t\t\t\t.clusterName(this.properties.getClusterName()).node();\n\t\tthis.releasable = node;\n\t\treturn node.client();\n\t}\n\n\tprivate Client createTransportClient() throws Exception {\n\t\tTransportClientFactoryBean factory = new TransportClientFactoryBean();\n\t\tfactory.setClusterNodes(this.properties.getClusterNodes());\n\t\tfactory.setProperties(createProperties());\n\t\tfactory.afterPropertiesSet();\n\t\tTransportClient client = factory.getObject();\n\t\tthis.releasable = client;\n\t\treturn client;\n\t}\n\n\tprivate Properties createProperties() {\n\t\tProperties properties = new Properties();\n\t\tproperties.put(\"cluster.name\", this.properties.getClusterName());\n\t\tproperties.putAll(this.properties.getProperties());\n\t\treturn properties;\n\t}\n\n\t@Override\n\tpublic void destroy() throws Exception {\n\t\tif (this.releasable != null) {\n\t\t\ttry {\n\t\t\t\tif (logger.isInfoEnabled()) {\n\t\t\t\t\tlogger.info(\"Closing Elasticsearch client\");\n\t\t\t\t}\n\t\t\t\ttry {\n\t\t\t\t\tthis.releasable.close();\n\t\t\t\t}\n\t\t\t\tcatch (NoSuchMethodError ex) {\n\t\t\t\t\t// Earlier versions of Elasticsearch had a different method name\n\t\t\t\t\tReflectionUtils.invokeMethod(\n\t\t\t\t\t\t\tReflectionUtils.findMethod(Releasable.class, \"release\"),\n\t\t\t\t\t\t\tthis.releasable);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (final Exception ex) {\n\t\t\t\tif (logger.isErrorEnabled()) {\n\t\t\t\t\tlogger.error(\"Error closing Elasticsearch client: \", ex);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n```\n\n```java\n@Configuration\n@ConditionalOnClass({ Client.class, ElasticsearchTemplate.class })\n@AutoConfigureAfter(ElasticsearchAutoConfiguration.class)\npublic class ElasticsearchDataAutoConfiguration {\n\n\t@Bean\n\t@ConditionalOnMissingBean\n\t@ConditionalOnBean(Client.class)\n    //操作es\n\tpublic ElasticsearchTemplate elasticsearchTemplate(Client client,\n\t\t\tElasticsearchConverter converter) {\n\t\ttry {\n\t\t\treturn new ElasticsearchTemplate(client, converter);\n\t\t}\n\t\tcatch (Exception ex) {\n\t\t\tthrow new IllegalStateException(ex);\n\t\t}\n\t}\n\n\t@Bean\n\t@ConditionalOnMissingBean\n\tpublic ElasticsearchConverter elasticsearchConverter(\n\t\t\tSimpleElasticsearchMappingContext mappingContext) {\n\t\treturn new MappingElasticsearchConverter(mappingContext);\n\t}\n\n\t@Bean\n\t@ConditionalOnMissingBean\n\tpublic SimpleElasticsearchMappingContext mappingContext() {\n\t\treturn new SimpleElasticsearchMappingContext();\n\t}\n\n}\n\n```\n\n```java\n@Configuration\n//启用ElasticsearchRepository接口\n@ConditionalOnClass({ Client.class, ElasticsearchRepository.class })\n@ConditionalOnProperty(prefix = \"spring.data.elasticsearch.repositories\",\n\t\tname = \"enabled\", havingValue = \"true\", matchIfMissing = true)\n@ConditionalOnMissingBean(ElasticsearchRepositoryFactoryBean.class)\n@Import(ElasticsearchRepositoriesRegistrar.class)\npublic class ElasticsearchRepositoriesAutoConfiguration {\n\n}\n\n```\n\n```java\n@NoRepositoryBean\npublic interface ElasticsearchRepository<T, ID extends Serializable> extends ElasticsearchCrudRepository<T, ID> {\n\n\t<S extends T> S index(S entity);\n\n\tIterable<T> search(QueryBuilder query);\n\n\tPage<T> search(QueryBuilder query, Pageable pageable);\n\n\tPage<T> search(SearchQuery searchQuery);\n\n\tPage<T> searchSimilar(T entity, String[] fields, Pageable pageable);\n\n\tvoid refresh();\n\n\tClass<T> getEntityClass();\n}\n```\n\n我们需要编写一个一个ElasticsearchRepository的子接口来操作ES\n\n## Jest\n\n我们需要将`spring-boot-starter-data-elasticsearch`注释掉在maven中添加jest依赖由于ES版本为5所以我们引入最新版的Jest\n\n```java\n<dependency>\n   <groupId>io.searchbox</groupId>\n   <artifactId>jest</artifactId>\n   <version>5.3.3</version>\n</dependency>\n```\n\n![Aqsh6O.png](https://s2.ax1x.com/2019/04/12/Aqsh6O.png)\n\n引入依赖成功.\n\n```java\nprotected HttpClientConfig createHttpClientConfig() {\n\t\tHttpClientConfig.Builder builder = new HttpClientConfig.Builder(\n            \t//getUris比较重要\n\t\t\t\tthis.properties.getUris());\n    \t//用户名\n\t\tif (StringUtils.hasText(this.properties.getUsername())) {\n\t\t\tbuilder.defaultCredentials(this.properties.getUsername(),\n\t\t\t\t\tthis.properties.getPassword());\n\t\t}\n    \t//主机\n\t\tString proxyHost = this.properties.getProxy().getHost();\n\t\tif (StringUtils.hasText(proxyHost)) {\n            //主机端口\n\t\t\tInteger proxyPort = this.properties.getProxy().getPort();\n\t\t\tAssert.notNull(proxyPort, \"Proxy port must not be null\");\n\t\t\tbuilder.proxy(new HttpHost(proxyHost, proxyPort));\n\t\t}\n\t\tGson gson = this.gsonProvider.getIfUnique();\n\t\tif (gson != null) {\n\t\t\tbuilder.gson(gson);\n\t\t}\n\t\tbuilder.multiThreaded(this.properties.isMultiThreaded());\n\t\tbuilder.connTimeout(this.properties.getConnectionTimeout())\n\t\t\t\t.readTimeout(this.properties.getReadTimeout());\n\t\tcustomize(builder);\n\t\treturn builder.build();\n\t}\n\n\tprivate void customize(HttpClientConfig.Builder builder) {\n\t\tif (this.builderCustomizers != null) {\n\t\t\tfor (HttpClientConfigBuilderCustomizer customizer : this.builderCustomizers) {\n\t\t\t\tcustomizer.customize(builder);\n\t\t\t}\n\t\t}\n\t}\n```\n\n```java\n@ConfigurationProperties(prefix = \"spring.elasticsearch.jest\")\npublic class JestProperties {\n\n\t/**\n\t * Comma-separated list of the Elasticsearch instances to use.\n\t */\n\tprivate List<String> uris = new ArrayList<String>(\n        \t//默认与本机的9200进行交互 我们只需要在配置文件中配置spring.elasticsearch.jest.urls\n\t\t\tCollections.singletonList(\"http://localhost:9200\"));\n\n\tprivate String username;\n\n\tprivate String password;\n\n\tprivate boolean multiThreaded = true;\n\n\tprivate int connectionTimeout = 3000;\n\n\tprivate int readTimeout = 3000;\n\n\tprivate final Proxy proxy = new Proxy();\n\n\tpublic List<String> getUris() {\n\t\treturn this.uris;\n\t}\n\n\tpublic void setUris(List<String> uris) {\n\t\tthis.uris = uris;\n\t}\n\n\tpublic String getUsername() {\n\t\treturn this.username;\n\t}\n\n\tpublic void setUsername(String username) {\n\t\tthis.username = username;\n\t}\n\n\tpublic String getPassword() {\n\t\treturn this.password;\n\t}\n\n\tpublic void setPassword(String password) {\n\t\tthis.password = password;\n\t}\n\n\tpublic boolean isMultiThreaded() {\n\t\treturn this.multiThreaded;\n\t}\n\n\tpublic void setMultiThreaded(boolean multiThreaded) {\n\t\tthis.multiThreaded = multiThreaded;\n\t}\n\n\tpublic int getConnectionTimeout() {\n\t\treturn this.connectionTimeout;\n\t}\n\n\tpublic void setConnectionTimeout(int connectionTimeout) {\n\t\tthis.connectionTimeout = connectionTimeout;\n\t}\n\n\tpublic int getReadTimeout() {\n\t\treturn this.readTimeout;\n\t}\n\n\tpublic void setReadTimeout(int readTimeout) {\n\t\tthis.readTimeout = readTimeout;\n\t}\n\n\tpublic Proxy getProxy() {\n\t\treturn this.proxy;\n\t}\n\n\tpublic static class Proxy {\n\n\t\tprivate String host;\n\n\t\tprivate Integer port;\n\n\t\tpublic String getHost() {\n\t\t\treturn this.host;\n\t\t}\n\n\t\tpublic void setHost(String host) {\n\t\t\tthis.host = host;\n\t\t}\n\n\t\tpublic Integer getPort() {\n\t\t\treturn this.port;\n\t\t}\n\n\t\tpublic void setPort(Integer port) {\n\t\t\tthis.port = port;\n\t\t}\n\n\t}\n\n}\n\n```\n\n在application.properties中配置\n\n```properties\nspring.elasticsearch.jest=http://192.168.1.110:9200\n```\n\n启动SpringBoot项目\n\n![AqyVjU.png](https://s2.ax1x.com/2019/04/12/AqyVjU.png)\n\n数据连接池已经更换。\n\n### 保存文档\n\n```java\npackage com.hph.elasticsearch;\n\nimport com.hph.elasticsearch.bean.Article;\nimport io.searchbox.client.JestClient;\nimport io.searchbox.core.Index;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\nimport java.io.IOException;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringBootElasticsearchApplicationTests {\n\n    @Autowired\n    JestClient jestClient;\n\n    @Test\n    public void savDocument() {\n        Article article = new Article();\n        article.setId(1);\n        article.setTitle(\"SpringBoot和Elasticsearch集成\");\n        article.setAuthor(\"清风笑丶\");\n        article.setContent(\"关于SpringBoot和Elasticsearch集成的文章\");\n\n        //构架一个索引功能\n        Index index = new Index.Builder(article).index(\"springboot\").type(\"javaweb\").build();\n\n        try {\n            //执行\n            jestClient.execute(index);\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n    }\n}\n```\n\n### 注意\n\nElasticSearch引擎是大小写敏感的，强制性要求索引名和文档类型小写，对于字段名，ElasticSearch引擎会将首字母小写，建议在配置索引，文档类型和字段名时，都使用小写字母。\n\n![AqvB0e.png](https://s2.ax1x.com/2019/04/13/AqvB0e.png)\n\n```java\n   //测试搜索\n    @Test\n    public void search() {\n        String rule = \"{\\n\" +\n                \"    \\\"query\\\" : {\\n\" +\n                \"        \\\"match\\\" : {\\n\" +\n                \"            \\\"content\\\" : \\\"Elasticsearch\\\"\\n\" +\n                \"        }\\n\" +\n                \"    }\\n\" +\n                \"}\";\n\n        //构建搜索功能\n        Search search = new Search.Builder(rule).addIndex(\"springboot\").addType(\"javaweb\").build();\n\n        try {\n            SearchResult searchResult = jestClient.execute(search);\n            System.out.println(searchResult.getJsonString());\n        } catch (IOException e) {\n            e.printStackTrace();\n        }\n\n    }\n```\n\n![AqvhnS.png](https://s2.ax1x.com/2019/04/13/AqvhnS.png)\n\n这里只简要叙述一下Jest的用法更多资料请点击[Jest](https://github.com/searchbox-io/Jest/tree/master/jest))链接\n\n## SpringData\n\n首先我们要引入SPringleData\n\n```xml\n <dependency>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-starter-data-elasticsearch</artifactId>\n</dependency>\n```\n\n配置application.properties\n\n```properties\nspring.elasticsearch.jest.uris=http://192.168.1.110:9200\n\nspring.data.elasticsearch.cluster-name=\"elasticsearch\"\nspring.data.elasticsearch.cluster-nodes=192.168.1.110:9300\n```\n\n![AqxM9I.png](https://s2.ax1x.com/2019/04/13/AqxM9I.png)\n\n运行报错\n\n![Aqx1jf.png](https://s2.ax1x.com/2019/04/13/Aqx1jf.png)\n\n该项目中引入的ES版本为2.4.6因此我们需要更换一下ES的版本。[方法链接](https://github.com/spring-projects/spring-data-elasticsearch)\n\n版本对比\n\n| spring data elasticsearch | elasticsearch |\n| ------------------------- | ------------- |\n| 3.2.x                     | 6.5.0         |\n| 3.1.x                     | 6.2.2         |\n| 3.0.x                     | 5.5.0         |\n| 2.1.x                     | 2.4.0         |\n| 2.0.x                     | 2.2.0         |\n| 1.3.x                     | 1.5.2         |\n\n![AqxruT.png](https://s2.ax1x.com/2019/04/13/AqxruT.png)\n\n如果版本不适配,我们可以升级SpringBoot或者安装合适版本的ES\n\n这里我们安装一下适配版本的ES\n\n![AqztsK.png](https://s2.ax1x.com/2019/04/13/AqztsK.png)\n\n![AqzBid.png](https://s2.ax1x.com/2019/04/13/AqzBid.png)\n\n版本更换过来了\n\n![AqzrRI.png](https://s2.ax1x.com/2019/04/13/AqzrRI.png)\n\n### 数据准备\n\n```java\npackage com.hph.elasticsearch.bean;\n\nimport org.springframework.data.elasticsearch.annotations.Document;\n\n@Document(indexName = \"hphblog\",type = \"javaweb\")\npublic class Blog {\n    private  Integer id;\n    private  String  blogName;\n    private  String author;\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getBlogName() {\n        return blogName;\n    }\n\n    public void setBlogName(String blogName) {\n        this.blogName = blogName;\n    }\n\n    public String getAuthor() {\n        return author;\n    }\n\n    public void setAuthor(String author) {\n        this.author = author;\n    }\n\n    @Override\n    public String toString() {\n        return \"Blog{\" +\n                \"id=\" + id +\n                \", blogName='\" + blogName + '\\'' +\n                \", author='\" + author + '\\'' +\n                '}';\n    }\n}\n\n```\n\n`SpringBootElasticsearchApplicationTests`中添加saveIndex方法然后运行\n\n```java\n@Autowired\nBlogRepository blogRepository;\n\n@Test\npublic void saveIndex() {\n    Blog blog = new Blog();\n    blogRepository.index(blog);\n}\n```\n\n![ALSkwD.png](https://s2.ax1x.com/2019/04/13/ALSkwD.png)\n\n### 存储数据\n\n```java\n    @Test\n    public void saveData(){\n        Blog blog = new Blog();\n        blogRepository.index(blog);\n        blog.setId(1);\n        blog.setBlogName(\"SpringBoot和Elasticsearch的学习\");\n        blog.setAuthor(\"清风笑丶\");\n        blogRepository.index(blog);\n\n    }\n```\n\n运行该方法\n\n数据已经存入。\n\n![ALSNpn.png](https://s2.ax1x.com/2019/04/13/ALSNpn.png)\n\n### 自定义查询\n\n在BlogRepository自定义方法。\n\n```java\npublic interface BlogRepository extends ElasticsearchRepository<Blog,Integer> {\n    public List<Book> findByBlogNameLike (String blogName);\n}\n```\n\n在测试类中\n\n```java\n    @Test\n    public void searchBlogBynName(){\n        for (Blog blog : blogRepository.findByBlogNameLike(\"学\")) {\n            System.out.println(blog);\n\n        }\n    }\n```\n\n![ALSvB8.png](https://s2.ax1x.com/2019/04/13/ALSvB8.png)\n\n想了解更多使用方法可以点击[链接](https://docs.spring.io/spring-data/elasticsearch/docs/3.1.6.RELEASE/reference/html/#elasticsearch.query-methods)来阅读更多使用方法。\n\n\n\n\n\n\n\n\n\n","tags":["Elasticsearch"],"categories":["SpringBoot"]},{"title":"Elasticsearch简介","url":"/2019/04/12/Elasticsearch简介/","content":"\n {{ \"elasticsearch的入门简介\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 简介\n\nElasticsearch (ES)是一个基于Lucene构建的开源、分布式、[RESTful](https://baike.baidu.com/item/RESTful/4406165?fr=aladdin)接口全文搜索引擎Elasticsearch还是一个分布式文档数据库，其中每个字段均是被索引的数据且可被搜索，它能够扩展至数以百计的服务器存储以及处理PB级的数据。它可以在很短的时间内存储、搜索和分析大量的数据。它通常作为具有复杂搜索场景情况下的核心发动机。Elasticsearch就是为高可用和可扩展而生的。可以通过购置性能更强的服务器来完成，称为垂直扩展或者向上扩展（Vertical Scale/Scaling Up)，或增加更多的服务器来完成，称为水平扩展或者向外扩展（Horizontal Scale/Scaling Out)尽管ES能够利用更强劲的硬件，垂直扩展毕竟还是有它的极限。真正的可扩展性来自于水平扩展，通过向集群中添加更多的节点来分担负载，增加可靠性。在大多数数据库中，水平扩展通常都需要你对应用进行一次大的重构来利用更多的节点。而ES天生就是分布式的：它知道如何管理多个节点来完成扩展和实现高可用性。这也意味着你的应用不需要做任何的改动。\n\n详细ElasticSearch内容可以在本站搜索阅读,如果有错,请及时提醒,避免误导他人。\n\n## 准备\n\n安装Elasticsearch\n\n![AqntvF.png](https://s2.ax1x.com/2019/04/12/AqntvF.png)\n\n注意看dockerhub中是否存在latest标签的镜像，因为没有这个镜像耽误了一些时间。\n\n![AquFr4.png](https://s2.ax1x.com/2019/04/12/AquFr4.png)\n\n因为elasticsearch在运行中会默认占用2个G，我们本机的资源够用，这里我们可以通过docker限制docker的资源。\n\n![AquRzT.png](https://s2.ax1x.com/2019/04/12/AquRzT.png)\n\n这里我们 制定了初始化内存和最大堆内存为512MB。暴露外部通信端口9200集群节点通信端口9300。\n\n![AquqW6.png](https://s2.ax1x.com/2019/04/12/AquqW6.png)\n\n启动成功。\n\n![AqKc0H.png](https://s2.ax1x.com/2019/04/12/AqKc0H.png)\n\n为什么Json编程这个样子了，因为我安装了JSON VIEW\n\nElasticsearch 是 *面向文档* 的，意味着它存储整个对象或 *文档_。Elasticsearch 不仅存储文档，而且 _索引*每个文档的内容使之可以被检索。在 Elasticsearch 中，你 对文档进行索引、检索、排序和过滤--而不是对行列数据。这是一种完全不同的思考数据的方式，也是 Elasticsearch 能支持复杂全文检索的原因。\n\nElasticsearch 使用 JavaScript Object Notation 或者 [*JSON*](http://en.wikipedia.org/wiki/Json) 作为文档的序列化格式。JSON 序列化被大多数编程语言所支持，并且已经成为 NoSQL 领域的标准格式。 它简单、简洁、易于阅读。\n\n考虑一下这个 JSON 文档，它代表了一个 user 对象：\n\n```json\n{\n    \"email\":      \"john@smith.com\",\n    \"first_name\": \"John\",\n    \"last_name\":  \"Smith\",\n    \"info\": {\n        \"bio\":         \"Eco-warrior and defender of the weak\",\n        \"age\":         25,\n        \"interests\": [ \"dolphins\", \"whales\" ]\n    },\n    \"join_date\": \"2014/05/01\"\n}\n```\n\n我们受雇于 *Megacorp* 公司，作为 HR 部门新的 *“热爱无人机”* （_\"We love our drones!\"_）激励项目的一部分，我们的任务是为此创建一个雇员目录。该目录应当能培养雇员认同感及支持实时、高效、动态协作，因此有一些业务需求：\n\n- 支持包含多值标签、数值、以及全文本的数据\n- 检索任一雇员的完整信息\n- 允许结构化搜索，比如查询 30 岁以上的员工\n- 允许简单的全文搜索以及较复杂的短语搜索\n- 支持在匹配文档内容中高亮显示搜索片段\n- 支持基于数据创建和管理分析仪表盘\n\n![Aqlg74.png](https://s2.ax1x.com/2019/04/12/Aqlg74.png)\n\n## 数据准备\n\n  我们可以把索引类比为MYSQL中的数据库而类型类比成为数据库中的表，文档则时数据库中的记录。\n\n```json\n{\n    \"first_name\" : \"John\",\n    \"last_name\" :  \"Smith\",\n    \"age\" :        25,\n    \"about\" :      \"I love to go rock climbing\",\n    \"interests\": [ \"sports\", \"music\" ]\n}\n```\n\n![Aq1MbF.png](https://s2.ax1x.com/2019/04/12/Aq1MbF.png)\n\n这里使用的软件为Postman\n\n保存2号员工\n\n```json\n{\n    \"first_name\" :  \"Jane\",\n    \"last_name\" :   \"Smith\",\n    \"age\" :         32,\n    \"about\" :       \"I like to collect rock albums\",\n    \"interests\":  [ \"music\" ]\n}\n```\n\n![Aq1Ygx.png](https://s2.ax1x.com/2019/04/12/Aq1Ygx.png)\n\n```json\n{\n    \"first_name\" :  \"Douglas\",\n    \"last_name\" :   \"Fir\",\n    \"age\" :         35,\n    \"about\":        \"I like to build cabinets\",\n    \"interests\":  [ \"forestry\" ]\n}\n```\n\n![Aq172q.png](https://s2.ax1x.com/2019/04/12/Aq172q.png)\n\n## 检索数据\n\n![Aq1xIJ.png](https://s2.ax1x.com/2019/04/12/Aq1xIJ.png)\n\n如果将GET换成DELETE则是删除。\n\n![Aq3qfA.png](https://s2.ax1x.com/2019/04/12/Aq3qfA.png)\n\n删除后GET的效果\n\n将 HTTP 命令由 PUT 改为 GET 可以用来检索文档，同样的，可以使用 DELETE 命令来删除文档，以及使用 HEAD 指令来检查文档是否存在。如果想更新已存在的文档，只需再次 PUT 。\n\n![Aq8emT.png](https://s2.ax1x.com/2019/04/12/Aq8emT.png)\n\n![AqGSjx.png](https://s2.ax1x.com/2019/04/12/AqGSjx.png)\n\n![AqGMb8.png](https://s2.ax1x.com/2019/04/12/AqGMb8.png)\n\n### 查询规则\n\n```json\n{\n    \"query\" : {\n        \"match\" : {\n            \"last_name\" : \"Smith\"\n        }\n    }\n}\n```\n\n![AqGUK0.png](https://s2.ax1x.com/2019/04/12/AqGUK0.png)\n\n和_search?q=last_name:Smith效果相同\n\n```json\n{\n    \"query\" : {\n        \"bool\": {\n            \"must\": {\n                \"match\" : {\n                    \"last_name\" : \"smith\" \n                }\n            },\n            \"filter\": {\n                \"range\" : {\n                    \"age\" : { \"gt\" : 33 } \n                }\n            }\n        }\n    }\n```\n\n![AqJsw8.png](https://s2.ax1x.com/2019/04/12/AqJsw8.png)\n\n### 全文检索\n\n```json\n{\n    \"query\" : {\n        \"match\" : {\n            \"about\" : \"rock climbing\"\n        }\n    }\n}\n```\n\n![AqJzm6.png](https://s2.ax1x.com/2019/04/12/AqJzm6.png)\n\n### 短语搜索\n\n短语搜索和全文搜索的区别在于短语搜索会完全匹配\n\n```json\n{\n    \"query\" : {\n        \"match_phrase\" : {\n            \"about\" : \"rock climbing\"\n        }\n    }\n}\n```\n\n![AqtZ8J.png](https://s2.ax1x.com/2019/04/12/AqtZ8J.png)\n\n### 高亮搜索\n\n```json\n{\n    \"query\" : {\n        \"match_phrase\" : {\n            \"about\" : \"rock climbing\"\n        }\n    },\n    \"highlight\": {\n        \"fields\" : {\n            \"about\" : {}\n        }\n    }\n}\n```\n\n![Aqt35D.png](https://s2.ax1x.com/2019/04/12/Aqt35D.png)\n\n![Aqt62j.png](https://s2.ax1x.com/2019/04/12/Aqt62j.png)\n\n如何理解高亮搜扫，类似于百度百科搜索的关键词会又`<em>`标签。\n\n\n\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"SpringBoot和RabbitMQ集成","url":"/2019/04/11/SpringBoot和RabbitMQ集成/","content":"\n {{ \"SpringBoot和RabbitMQ的集成\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 步骤\n\n![AH1g2V.png](https://s2.ax1x.com/2019/04/11/AH1g2V.png)\n\n## 自动配置\n\n```java\n\t@Bean\n\t\tpublic CachingConnectionFactory rabbitConnectionFactory(RabbitProperties config)\n\t\t\t\tthrows Exception {\n\t\t\tRabbitConnectionFactoryBean factory = new RabbitConnectionFactoryBean();\n\t\t\tif (config.determineHost() != null) {\n                //设置mq的host地址\n\t\t\t\tfactory.setHost(config.determineHost());\n\t\t\t}\n\t\t\tfactory.setPort(config.determinePort());\n\t\t\tif (config.determineUsername() != null) {\n                //设置mq的username\n\t\t\t\tfactory.setUsername(config.determineUsername());\n\t\t\t}\n\t\t\tif (config.determinePassword() != null) {\n                //设置mq的密码\n\t\t\t\tfactory.setPassword(config.determinePassword());\n\t\t\t}\n\t\t\tif (config.determineVirtualHost() != null) {\n                //是指虚拟主机\n\t\t\t\tfactory.setVirtualHost(config.determineVirtualHost());\n\t\t\t}\n\t\t\tif (config.getRequestedHeartbeat() != null) {\n                //心跳\n                factory.setRequestedHeartbeat(config.getRequestedHeartbeat());\n            }\n\t\t.....\n\t}\n```\n\n```java\n@ConfigurationProperties(prefix = \"spring.rabbitmq\")\npublic class RabbitProperties {\n\t//地址\n   private String host = \"localhost\";\n\t//端口\n   private int port = 5672;\n\t//账号\n   private String username;\n\t//密码\n   private String password;\n\t//SSL配置\n   private final Ssl ssl = new Ssl();\n    //虚拟主机\n   private String virtualHost;\n    //地址\n   private String addresses;\n\n\t//请求心跳超时，以秒为单位; 零，没有。\n   private Integer requestedHeartbeat;\n    \n\t//Publisher Confirms and Returns机制\n   private boolean publisherConfirms;\n    \n   private boolean publisherReturns;\n\t//连接超时时间\n   private Integer connectionTimeout;\n \t//缓存\n   private final Cache cache = new Cache();\n\n \t//监听容器配置\n   private final Listener listener = new Listener();\n\n   private final Template template = new Template();\n\n   private List<Address> parsedAddresses;\n\n   public String getHost() {\n      return this.host;\n   }\n```\n\nRabbitProperties封装了RabbitMQ发送和接收消息。\n\nRabbitTemplate给RabbitMQ发送和接收消息。\n\nAmqpAdmin,RabbitMQ系统管理功能组件。\n\n```java\n@Bean\n \t\t@ConditionalOnSingleCandidate(ConnectionFactory.class)\n\t\t@ConditionalOnMissingBean(RabbitTemplate.class)\n\t\tpublic RabbitTemplate rabbitTemplate(ConnectionFactory connectionFactory) {\n            //生成rabbitTemplate来操作rabbitmq\n\t\t\tRabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);\n\t\t\tMessageConverter messageConverter = this.messageConverter.getIfUnique();\n            //如果messageConverter不为空设置我们自己的messageConverter\n\t\t\tif (messageConverter != null) {\n\t\t\t\trabbitTemplate.setMessageConverter(messageConverter);\n\t\t\t}\n\t\t\trabbitTemplate.setMandatory(determineMandatoryFlag());\n\t\t\tRabbitProperties.Template templateProperties = this.properties.getTemplate();\n\t\t\tRabbitProperties.Retry retryProperties = templateProperties.getRetry();\n\t\t\tif (retryProperties.isEnabled()) {\n\t\t\t\trabbitTemplate.setRetryTemplate(createRetryTemplate(retryProperties));\n\t\t\t}\n\t\t\tif (templateProperties.getReceiveTimeout() != null) {\n\t\t\t\trabbitTemplate.setReceiveTimeout(templateProperties.getReceiveTimeout());\n\t\t\t}\n\t\t\tif (templateProperties.getReplyTimeout() != null) {\n\t\t\t\trabbitTemplate.setReplyTimeout(templateProperties.getReplyTimeout());\n\t\t\t}\n\t\t\treturn rabbitTemplate;\n\t\t}\n\n\t\t@Bean\n\t\t@ConditionalOnSingleCandidate(ConnectionFactory.class)\n\t\t@ConditionalOnProperty(prefix = \"spring.rabbitmq\", name = \"dynamic\",\n\t\t\t\tmatchIfMissing = true)\n\t\t@ConditionalOnMissingBean(AmqpAdmin.class)\n\t\tpublic AmqpAdmin amqpAdmin(ConnectionFactory connectionFactory) {\n\t\t\treturn new RabbitAdmin(connectionFactory);\n\t\t}\n\n\t}\n```\n\n## P2P发送\n\n```java\npackage com.hph.amqp;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.amqp.rabbit.core.RabbitTemplate;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringBootAmqpApplicationTests {\n\n\n    @Autowired\n    RabbitTemplate rabbitTemplate;\n\n    /**\n     * 单播 P2P\n     */\n    @Test\n    public void p2p() {\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"msg\",\"这是第1个消息\");\n        map.put(\"data\", Arrays.asList(\"Hello Rabitmq\",123456, true));\n\t\t//对象默认被序列化以后发送出去\n        rabbitTemplate.convertAndSend(\"exchange.direct\", \"phh.news\",map);\n    }\n\n}\n```\n\n![AHdZeP.png](https://s2.ax1x.com/2019/04/11/AHdZeP.png)\n\n这是因为默认使用的是application/x-java-serialized-object的序列化\n\n## 获取消息\n\n```java\n    @Test\n    public void receive() {\n        Object o = rabbitTemplate.receiveAndConvert(\"hph.news\");\n        System.out.println(o.getClass());\n        System.out.println(o);\n    }\n```\n\n## 转为Json\n\n由于是RabbitTemplate操作Rabbit的在RabbitTemplate中RabbitTemplate为默认的序列化器\n\n```java\nprivate volatile MessageConverter messageConverter = new SimpleMessageConverter();\n```\n\nMessageConverter又一下实现类我们使用的是Jackson2JsonMessageConverter的序列化器\n\n![AHwo34.png](https://s2.ax1x.com/2019/04/11/AHwo34.png)\n\n在设置我们自己的MessageConverter\n\n```java\nif (messageConverter != null) {\n\t\t\t\trabbitTemplate.setMessageConverter(messageConverter);\n\t\t\t}\n```\n\n```java\npackage com.hph.amqp.config;\n\nimport org.springframework.amqp.support.converter.Jackson2JsonMessageConverter;\nimport org.springframework.amqp.support.converter.MessageConverter;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n@Configuration\npublic class MyAMQPConfig {\n    @Bean\n    public MessageConverter messageConverter() {\n        return new Jackson2JsonMessageConverter();\n    }\n}\n```\n\n再次发送消息\n\n![AHB9iT.png](https://s2.ax1x.com/2019/04/11/AHB9iT.png)\n\n## 自定义发送\n\n```java\n  @Test\n    public void sendMessage() {\n        Map<String, Object> map = new HashMap<>();\n        map.put(\"msg\", \"这是第1个消息\");\n        map.put(\"data\", Arrays.asList(\"清风笑丶\",123456,true));\n        rabbitTemplate.convertAndSend(\"exchange.direct\", \"hph.news\", new Person(\"小明\",18));\n    }\n\n```\n\n```java\npackage com.hph.amqp.bean;\n\npublic class Person {\n    private String name;\n    private Integer age;\n\n    public Person() {\n    }\n\n    public Person(String name, Integer age) {\n        this.name = name;\n        this.age = age;\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"name='\" + name + '\\'' +\n                \", age=\" + age +\n                '}';\n    }\n}\n```\n\n![AHBqk6.png](https://s2.ax1x.com/2019/04/11/AHBqk6.png)\n\n## 反序列化\n\n```java\n    @Test\n    public void receive() {\n        Object o = rabbitTemplate.receiveAndConvert(\"hph.news\");\n\n        System.out.println(o.getClass());\n        System.out.println(o);\n    }\n```\n\n![AHcEOe.png](https://s2.ax1x.com/2019/04/11/AHcEOe.png)\n\n## 广播发送\n\n```java\n    @Test\n    public void sendMessages() {\n        rabbitTemplate.convertAndSend(\"exchange.fanout\", \"hph.news\", new Person(\"清风笑丶\",18));\n    }\n```\n\n![AHrePK.png](https://s2.ax1x.com/2019/04/11/AHrePK.png)\n\n## 监听消息队列\n\n```java\npackage com.hph.amqp;\n\nimport org.springframework.amqp.rabbit.annotation.EnableRabbit;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@EnableRabbit //开启基于注解的RabbitMQ的模式\n@SpringBootApplication\npublic class SpringBootAmqpApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringBootAmqpApplication.class, args);\n    }\n}\n```\n\n```java\npackage com.hph.amqp.service;\n\nimport com.hph.amqp.bean.Person;\nimport org.springframework.amqp.rabbit.annotation.RabbitListener;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class PersonService {\n\n    @RabbitListener(queues = \"hph.news\")\n    public void receive(Person person) {\n        System.out.println(\"收到消息\" + person+\"上线\");\n\n    }\n}\n```\n\n启动SpringBoot然后运行sendMessage任务。\n\n![AHc1l8.png](https://s2.ax1x.com/2019/04/11/AHc1l8.png)\n\n```java\n   @RabbitListener(queues = \"hph\")\n    public void receive02(Message message){\n        System.out.println(message.getBody());\n        System.out.println(message.getMessageProperties());\n    }\n}\n```\n\n![AHcWfx.png](https://s2.ax1x.com/2019/04/11/AHcWfx.png)\n\n消息头信息。\n\n## 管理\n\n在SpringBoot中消息队列的管理使用到了amqpAdmin\n\n```java\n\t@ConditionalOnMissingBean(AmqpAdmin.class)\n\t\tpublic AmqpAdmin amqpAdmin(ConnectionFactory connectionFactory) {\n\t\t\treturn new RabbitAdmin(connectionFactory);\n\t\t}\n```\n\n在RabbitAutoConfiguration\n\n![AHgiAs.png](https://s2.ax1x.com/2019/04/11/AHgiAs.png)\n\n```java\npublic class DirectExchange extends AbstractExchange {\n\n   public static final DirectExchange DEFAULT = new DirectExchange(\"\");\n\n\t//设置名字\n   public DirectExchange(String name) {\n      super(name);\n   }\n\t//名字  是否持久化 自动删除\n   public DirectExchange(String name, boolean durable, boolean autoDelete) {\n      super(name, durable, autoDelete);\n   }\n\n   public DirectExchange(String name, boolean durable, boolean autoDelete, Map<String, Object> arguments) {\n      super(name, durable, autoDelete, arguments);\n   }\n\n   @Override\n   public final String getType() {\n      return ExchangeTypes.DIRECT;\n   }\n\n}\n```\n\n![\n\nAHg0UA.png](https://s2.ax1x.com/2019/04/11/AHg0UA.png)\n\n```java\n   @Test\n    public void createExchange(){\n    amqpAdmin.declareExchange(new DirectExchange(\"amqpadmin.exchange\"));\n        System.out.println(\"创建完成\");\n    }\n```\n\n运行该方法。\n\n![AH2Yin.png](https://s2.ax1x.com/2019/04/11/AH2Yin.png)\n\n### 创建exchange\n\n```java\n\tpublic Queue(String name, boolean durable) {\n\t\tthis(name, durable, false, false, null);\n\t}\n\n\tpublic Queue(String name, boolean durable, boolean exclusive, boolean autoDelete) {\n\t\tthis(name, durable, exclusive, autoDelete, null);\n\t}\n\n\tpublic Queue(String name, boolean durable, boolean exclusive, boolean autoDelete, Map<String, Object> arguments) {\n\t\tAssert.notNull(name, \"'name' cannot be null\");\n\t\tthis.name = name;\n\t\tthis.durable = durable;\n\t\tthis.exclusive = exclusive;\n\t\tthis.autoDelete = autoDelete;\n\t\tthis.arguments = arguments;\n\t}\n```\n### 创建Queue\n\n```java\n  @Test\n    public void createQueue() {\n        amqpAdmin.declareQueue(new Queue(\"amqpadmin.queue\", true));\n        System.out.println(\"创建队列成功\");\n    }\n```\n\n![AHRUkd.png](https://s2.ax1x.com/2019/04/11/AHRUkd.png)\n\n\n\n### 绑定exchange\n\n```java\npublic Binding(String destination, DestinationType destinationType, String exchange, String routingKey,\n\t\t\tMap<String, Object> arguments) {\n\t\tthis.destination = destination;\n\t\tthis.destinationType = destinationType;\n\t\tthis.exchange = exchange;\n\t\tthis.routingKey = routingKey;\n\t\tthis.arguments = arguments;\n\t}\n```\n\n之前尚未绑定\n\n![AHR5cV.png](https://s2.ax1x.com/2019/04/11/AHR5cV.png)\n\n```java\n    @Test\n    public void bindExchange() {\n        amqpAdmin.declareBinding(new Binding(\"amqpadmin.queue\", Binding.DestinationType.QUEUE, \"amqpadmin.exchange\", \"amqp.bind\", null));\n    }\n```\n\n![AHRqAJ.png](https://s2.ax1x.com/2019/04/11/AHRqAJ.png)\n\n绑定成功\n\n\n\n\n\n","tags":["RabbitMQ"],"categories":["SpringBoot"]},{"title":"消息队列RabbitMQ","url":"/2019/04/11/SpringBoot和消息队列/","content":"\n {{ \"消息队列RabbitMQ的基本知识\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 消息队列（Message Queue）\n\n消息: 网络中的两台计算机或者两个通讯设备之间传递的数据。例如说：文本、音乐、视频等内容。\n\n队列：一种特殊的线性表（数据元素首尾相接），特殊之处在于只允许在首部删除元素和在尾部追加元素。入队、出队。\n\n消息队列：顾名思义，消息+队列，保存消息的队列。消息的传输过程中的容器；主要提供生产、消费接口供外部调用做数据的存储和获取。\n\n## 消息队列分类\n\nMQ分类：点对点（P2P）、发布订阅（Pub/Sub）\n\n共同点：消息生产者生产消息发送到queue中，然后消息消费者从queue中读取并且消费消息。\n\n不同点：    P2P模型包含：消息队列(Queue)、发送者(Sender)、接收者(Receiver)一个生产者生产的消息只有一个消费者(Consumer)（即一旦被消费，消息就不在消息队列中）。打电话。\n\nPub/Sub包含：消息队列(Queue)、主题（Topic）、发布者（Publisher）、订阅者（Subscriber）\n\n每个消息可以有多个消费者，彼此互不影响。比如我发布一个微博：关注我的人都能够看到。\n\n## 消息队列模式\n\n1. 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）\n\n点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。\n\n2. 发布/订阅模式（一对多，数据生产后，推送给所有订阅者）\n\n发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。\n\n## 消息队列的好处\n\n- 解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。\n\n![A7YSKK.png](https://s2.ax1x.com/2019/04/11/A7YSKK.png)\n\n- 冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。\n- 扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。\n- 灵活性 & 峰值处理能力： 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。\n\n![A7YAPA.png](https://s2.ax1x.com/2019/04/11/A7YAPA.png)\n\n- 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。\n- 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）\n- 缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。\n- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n\n![A7Y9bD.png](https://s2.ax1x.com/2019/04/11/A7Y9bD.png)\n\n\n\nJMS（Java Message Service）JAVA消息服务：基于JVM消息代理的规范。ActiveMQ、HornetMQ是JMS实现\n\nAMQP（Advanced Message Queuing Protocol）高级消息队列协议，也是一个消息代理的规范，兼容JMS RabbitMQ是AMQP的实现\n\n![A7YHQP.png](https://s2.ax1x.com/2019/04/11/A7YHQP.png)\n\n消息队列还有Kafaka也可以在本站中搜索阅读。\n\n### RabbitMQ\n\n### 简介\n\nRabbitMQ是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ服务器是用Erlang语言编写的，而集群和故障转移是构建在开放电信平台框架上的。所有主要的编程语言均有与代理接口通讯的客户端库。\n\n###  核心概念\n\n#### Message\n\n消息，消息是不具名的，它由消息头和消息体组成。消息体是不透明的，而消息头则由一系列的可选属性组成，这些属性包括routing-key（路由键）、priority（相对于其他消息的优先权）、delivery-mode（指出该消息可能需要持久性存储）等。\n\n![A7tW60.png](https://s2.ax1x.com/2019/04/11/A7tW60.png)\n\n#### Publisher\n\n消息的生产者，也是一个向交换器发布消息的客户端应用程序。\n\n#### Exchange\n\n交换器，用来接收生产者发送的消息并将这些消息路由给服务器中的队列。\n\nExchange有4种类型：direct(默认)，fanout, topic, 和headers，不同类型的Exchange转发消息的策略有所区别\n\n#### Queue\n\n消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。\n\n#### Binding\n\n绑定，用于消息队列和交换器之间的关联。一个绑定就是基于路由键将交换器和消息队列连接起来的路由规则，所以可以将交换器理解成一个由绑定构成的路由表。\n\nExchange 和Queue的绑定可以是多对多的关系。\n\n#### Connection\n\n网络连接，比如一个TCP连接。\n\n#### Channel\n\n信道，多路复用连接中的一条独立的双向数据流通道。信道是建立在真实的TCP连接内的虚拟连接，AMQP 命令都是通过信道发出去的，不管是发布消息、订阅队列还是接收消息，这些动作都是通过信道完成。因为对于操作系统来说建立和销毁\nTCP 都是非常昂贵的开销，所以引入了信道的概念，以复用一条TCP 连接。\n\n#### Consumer\n\n消息的消费者，表示一个从消息队列中取得消息的客户端应用程序。\n\n#### Virtual Host\n\n虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个\nvhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / 。\n\n####  Broker\n\n表示消息队列服务器实体\n\n### 运行机制\n\nAMQP 中的消息路由:\n\nAMQP中消息的路由过程和Java开发者熟悉的JMS存在一些差别，AMQP中增加了**Exchange**和**Binding**的角色。生产者把消息发布到 Exchange 上，消息最终到达队列并被消费者接收，而 Binding 决定交换器的消息应该发送到那个队列。\n\n![A7tvnK.png](https://s2.ax1x.com/2019/04/11/A7tvnK.png)\n\n#### Exchange\n\n**Exchange**分发消息时根据类型的不同分发策略有区别，目前共四种类型：**direct**、**fanout**、**topic**、**headers** 。headers 匹配 AMQP 消息的 header 而不是路由键， headers 交换器和 direct 交换器完全一致，但性能差很多，目前几乎用不到了，所以直接看另外三种类型：\n\n##### Direct\n\n![A7NyDK.png](https://s2.ax1x.com/2019/04/11/A7NyDK.png)\n\n消息中的路由键（routingkey）如果和 Binding中的 bindingkey 一致，交换器就将消息发到对应的队列中。路由键与队列名完全匹配，如果一个队列绑定到交换机要求路由键为“dog”，则只转发 routing key 标记为“dog”的消息，不会转发“dog.puppy”，也不会转发“dog.guard”等等。它是完全匹配、单播的模式。\n\n##### Fanout\n\n![A7Nh8A.png](https://s2.ax1x.com/2019/04/11/A7Nh8A.png)\n\n每个发到 fanout 类型交换器的消息都会分到所有绑定的队列上去。fanout交换器不处理路由键，只是简单的将队列绑定到交换器上，每个发送到交换器的消息都会被转发到与该交换器绑定的所有队列上。很像子网广播，每台子网内的主机都获得了一份复制的消息。fanout类型转发消息是最快的。\n\n##### Topic\n\n![A7NL5Q.png](https://s2.ax1x.com/2019/04/11/A7NL5Q.png)\n交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上。它将路由键和绑定键的字符串切分成单词，这些**单词之间用点隔开**。它同样也会识别两个通配符：符号`#`和符号`*`。`#`匹配**0**个或多个单词，`*`匹配一个单词。\n\n## 准备\n\n### Docker安装rabbitmq\n\n![A7a6te.png](https://s2.ax1x.com/2019/04/11/A7a6te.png)\n\n![A7dJDP.png](https://s2.ax1x.com/2019/04/11/A7dJDP.png)\n\n运行成功\n\n### 登录\n\n默认的账号密码都为guest\n\n![A7dDvn.png](https://s2.ax1x.com/2019/04/11/A7dDvn.png)\n\n![A7dgET.png](https://s2.ax1x.com/2019/04/11/A7dgET.png)\n\n![A708Tf.png](https://s2.ax1x.com/2019/04/11/A708Tf.png)\n\n![A704n1.png](https://s2.ax1x.com/2019/04/11/A704n1.png)\n\n![A7BwCD.png](https://s2.ax1x.com/2019/04/11/A7BwCD.png)\n\n![A7DVRe.png](https://s2.ax1x.com/2019/04/11/A7DVRe.png)\n\n![A7DuqI.png](https://s2.ax1x.com/2019/04/11/A7DuqI.png)\n\n![A7D0oV.png](https://s2.ax1x.com/2019/04/11/A7D0oV.png)\n\n### 绑定\n\n![A7DHQH.png](https://s2.ax1x.com/2019/04/11/A7DHQH.png)\n\n![A7DOeI.png](https://s2.ax1x.com/2019/04/11/A7DOeI.png)\n\n![A7sAjH.png](https://s2.ax1x.com/2019/04/11/A7sAjH.png)\n\n### Direct\n\n![A7y9qs.png](https://s2.ax1x.com/2019/04/11/A7y9qs.png)\n\n只有一条匹配到了\n\n![A7yEGT.png](https://s2.ax1x.com/2019/04/11/A7yEGT.png)\n\n点对点模式只有一条消息我们来获取一下。\n\n![A7yJzD.png](https://s2.ax1x.com/2019/04/11/A7yJzD.png)\n\n### Fanount\n\n![A7ywdI.png](https://s2.ax1x.com/2019/04/11/A7ywdI.png)\n\n![A7yrJf.png](https://s2.ax1x.com/2019/04/11/A7yrJf.png)\n\n所有队列都收到消息了\n\n![A7ygyQ.png](https://s2.ax1x.com/2019/04/11/A7ygyQ.png)\n\n### Topic\n\n根据路由键的规则发送\n\n![A7cl8K.png](https://s2.ax1x.com/2019/04/11/A7cl8K.png)\n\n由于hphblog.news只与hph.news 和hphblog.news匹配因此我们可以在hph.new和hphblog.news中匹配到我们所想要匹配的消息。\n\n![A7cGKe.png](https://s2.ax1x.com/2019/04/11/A7cGKe.png)\n\n我们在换成其他的\n\n![A7cgVs.png](https://s2.ax1x.com/2019/04/11/A7cgVs.png)\n\n此时每个队列中的消息增加一条\n\n![A7cR5q.png](https://s2.ax1x.com/2019/04/11/A7cR5q.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["RabbitMQ"],"categories":["SpringBoot"]},{"title":"SpringBoot与Redis缓存","url":"/2019/04/10/SpringBoot与Redis缓存/","content":"\n {{ \"SpringBoot与Redis缓存\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 准备\n\n在Docker安装Redis\n\n![ATwKnP.png](https://s2.ax1x.com/2019/04/10/ATwKnP.png)\n\n![ATw8hQ.png](https://s2.ax1x.com/2019/04/10/ATw8hQ.png)\n\n连接成功\n\n![ATw639.png](https://s2.ax1x.com/2019/04/10/ATw639.png)\n\n对于Redis不熟悉的同学可以在本站搜索Redis的文章阅读。\n\n## 整合Redis\n\n在pom文件中加入\n\n```xml\n <!--引入Redis-->\n<dependency>\n   <groupId>org.springframework.boot</groupId>\n   <artifactId>spring-boot-starter-data-redis</artifactId>\n</dependency>\n```\n\n```java\n@Configuration\n@ConditionalOnClass(RedisOperations.class)\n@EnableConfigurationProperties(RedisProperties.class)\n@Import({ LettuceConnectionConfiguration.class, JedisConnectionConfiguration.class })\npublic class RedisAutoConfiguration {\n\n\t@Bean\n\t@ConditionalOnMissingBean(name = \"redisTemplate\")\n    //简化操作KV 都是对象的\n\tpublic RedisTemplate<Object, Object> redisTemplate(\n\t\t\tRedisConnectionFactory redisConnectionFactory) throws UnknownHostException {\n\t\tRedisTemplate<Object, Object> template = new RedisTemplate<>();\n\t\ttemplate.setConnectionFactory(redisConnectionFactory);\n\t\treturn template;\n\t}\n\n\t@Bean\n\t@ConditionalOnMissingBean\n    //简化操作字符串的\n\tpublic StringRedisTemplate stringRedisTemplate(\n\t\t\tRedisConnectionFactory redisConnectionFactory) throws UnknownHostException {\n\t\tStringRedisTemplate template = new StringRedisTemplate();\n\t\ttemplate.setConnectionFactory(redisConnectionFactory);\n\t\treturn template;\n\t}\n\n}\n```\n\n## 准备\n\n```java\npackage com.hph.cache;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.core.StringRedisTemplate;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringbootCacheApplicationTests {\n\n    @Autowired\n    StringRedisTemplate stringRedisTemplate;\n\n    @Autowired\n    RedisTemplate redisTemplate;\n\n    /**\n     * Redis常用的数据类型\n     * String(字符串)、List(列表)、Set(集合)、Hash(散列)、(有序集合)\n     * String（字符串）、List（列表）、Set（集合）、Hash（散列）、ZSet（有序集合）\n     * stringRedisTemplate.opsForValue()[String（字符串）]\n     * stringRedisTemplate.opsForList()[List（列表）]\n     * stringRedisTemplate.opsForSet()[Set（集合）]\n     * stringRedisTemplate.opsForHash()[Hash（散列）]\n     * stringRedisTemplate.opsForZSet()[ZSet（有序集合）]\n     */\n    @Test\n    public void redis() {\n        stringRedisTemplate.opsForValue().append(\"msg\",\"hello\");\n\n    }\n}\n```\n\n![ATsOk6.png](https://s2.ax1x.com/2019/04/10/ATsOk6.png)\n\n### 读取数据\n\n更新原有数据\n\n![ATyCnA.png](https://s2.ax1x.com/2019/04/10/ATyCnA.png)\n\n```java\n    @Test\n    public void getmesg() {\n        String msg = stringRedisTemplate.opsForValue().get(\"msg\");\n        System.out.println(msg);\n    }\n```\n\n![ATyMBn.png](https://s2.ax1x.com/2019/04/10/ATyMBn.png)\n\n```java\n@Test\npublic void listops(){\n     stringRedisTemplate.opsForList().leftPush(\"mylist\",\"1\");\n     stringRedisTemplate.opsForList().leftPush(\"mylist\",\"2\");\n     stringRedisTemplate.opsForList().leftPush(\"mylist\",\"3\");\n     stringRedisTemplate.opsForList().leftPush(\"mylist\",\"4\");\n}\n```\n\n![ATyRud.png](https://s2.ax1x.com/2019/04/10/ATyRud.png)\n\n```java\n    @Test\n    public void cachObject(){\n        Employee empById = employeeMapper.getEmpById(1);\n        redisTemplate.opsForValue().set(\"emp-001\",empById);\n    }\n```\n\n![AT6K2D.png](https://s2.ax1x.com/2019/04/10/AT6K2D.png)\n\n### 序列化\n\n报错Employee需要序列化。\n\n```java\npublic class Employee implements Serializable \n```\n\n![AT6Dqs.md.png](https://s2.ax1x.com/2019/04/10/AT6Dqs.md.png)\n\nRedisTemplate默认的是JdkSerializationRedisSerializer的序列化器。\n\n```java\n\tif (defaultSerializer == null) {\n\t\t\tdefaultSerializer = new JdkSerializationRedisSerializer(\n\t\t\t\t\tclassLoader != null ? classLoader : this.getClass().getClassLoader());\n\t\t}\n```\n\n我们可以使用RedisSerializer的序列化器设置默认的序列化器。\n\n```java\n\tpublic void setDefaultSerializer(RedisSerializer<?> serializer) {\n\t\tthis.defaultSerializer = serializer;\n\t}\n```\n\n\n\n![ATcs6e.png](https://s2.ax1x.com/2019/04/10/ATcs6e.png)\n\n#### 自定义序列化器\n\n```java\npackage com.hph.cache.config;\n\nimport com.hph.cache.bean.Employee;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;\n\nimport java.net.UnknownHostException;\n\n@Configuration\npublic class MyRedisConfig {\n\n    @Bean\n    public RedisTemplate<Object, Employee> empRedisTemplate(\n            RedisConnectionFactory redisConnectionFactory)\n            throws UnknownHostException {\n        RedisTemplate<Object, Employee> template = new RedisTemplate<Object, Employee>();\n        template.setConnectionFactory(redisConnectionFactory);\n        Jackson2JsonRedisSerializer<Employee> ser = new Jackson2JsonRedisSerializer<Employee>(Employee.class);\n        template.setDefaultSerializer(ser);\n        return template;\n    }\n}\n```\n\n```java\n    @Autowired\n    RedisTemplate<Object, Employee> empRedisTemplate;\n    @Test\n    public void ObjectToJsonRedis(){\n        Employee empById = employeeMapper.getEmpById(1);\n        //默认保存对象使用jdk序列化机制，序列化的数据保存在redis中\n        empRedisTemplate.opsForValue().set(\"emp-001\",empById);\n    }\n```\n\n成功\n\n![ATg63T.png](https://s2.ax1x.com/2019/04/10/ATg63T.png)\n\n好的我们开始运行以下看看使用Redis缓存的效果。\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.CacheEvict;\nimport org.springframework.cache.annotation.CachePut;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.cache.annotation.Caching;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"},keyGenerator = \"myKeyGenerator\")\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n\n    /**\n     * @CachePut即调用方法,又更新缓存数据 修改了数据库的某个数据, 同时更新缓存\n     * 运行时机:先调用目标方法,现将目标方法的结果缓存起来\n     */\n    @CachePut(value = \"emp\", key = \"#result.id \")\n    public Employee updateEmp(Employee employee) {\n        System.out.println(\"updateEmp\" + employee);\n        employeeMapper.updateEmp(employee);\n        return employee;\n    }\n\n    /**\n     * @Cachevict :缓存清除\n     * key: 指定要清除的数据\n     */\n    //缓存默认清除才足在方法执行之后执行;如果出现异常缓存不会被清除\n    @CacheEvict(value = \"emp\", beforeInvocation = true)\n    public void deleteEmp(Integer id) {\n        System.out.println(\"deletEmp\" + id);\n        //  employeeMapper.deleteEmpById(id);\n        int i = 10 / 0;\n    }\n}\n```\n\n###  流程分析\n\n原理：CacheManager==某一个Cache缓存组件来给实际给缓存中存取得数据，比如我们使用Redis缓存，那么SimpleCacheManager就不匹配进而不能使用，从而使用Redis\n\n![ATbUJg.png](https://s2.ax1x.com/2019/04/10/ATbUJg.png)\n\n![ATb6oT.png](https://s2.ax1x.com/2019/04/10/ATb6oT.png)\n\n引入Redis的starter，容器中保存的是RedisCcheManager；\n\nRedisCacheManager帮我们创建RedisCache作为缓存组件，RedisCache通过Redis缓存数据。\n\n ![ATq81J.png](https://s2.ax1x.com/2019/04/10/ATq81J.png)\n\n我们可以看到SQL只执行了一次。\n\n![ATqtn1.png](https://s2.ax1x.com/2019/04/10/ATqtn1.png)\n\nRedis中已经缓存我们查询过的数据。\n\n默认保留数据的K-V都是Object；利用序列化保存可以保存为Json。\n\n引入了redis的stater，cacheManager变为RedisCacheManager；默认创建的RedisCacheManager\n\n```java\n\t@Bean\n\tpublic RedisCacheManager cacheManager(RedisTemplate<Object, Object> redisTemplate) {\n\t\tRedisCacheManager cacheManager = new RedisCacheManager(redisTemplate);\n\t\tcacheManager.setUsePrefix(true);\n\t\tList<String> cacheNames = this.cacheProperties.getCacheNames();\n\t\tif (!cacheNames.isEmpty()) {\n\t\t\tcacheManager.setCacheNames(cacheNames);\n\t\t}\n\t\treturn this.customizerInvoker.customize(cacheManager);\n\t}\n```\n\nRedisCacheManager操作Redis使用的是RedisTemplate默认使用的是JdkSerializationRedisSerializer\n\n```java\n\tif (defaultSerializer == null) {\n\n\t\t\tdefaultSerializer = new JdkSerializationRedisSerializer(\n\t\t\t\t\tclassLoader != null ? classLoader : this.getClass().getClassLoader());\n\t\t}\n```\n\n### 缓存管理器\n\n接下来我们定制以下缓存管理器。\n\n```java\npackage com.hph.cache.config;\n\nimport com.hph.cache.bean.Employee;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.data.redis.cache.RedisCacheManager;\nimport org.springframework.data.redis.connection.RedisConnectionFactory;\nimport org.springframework.data.redis.core.RedisTemplate;\nimport org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer;\n\nimport java.net.UnknownHostException;\n\n@Configuration\npublic class MyRedisConfig {\n    @Bean\n    public RedisTemplate<Object, Employee> empRedisTemplate(\n            RedisConnectionFactory redisConnectionFactory)\n            throws UnknownHostException {\n        RedisTemplate<Object, Employee> template = new RedisTemplate<Object, Employee>();\n        template.setConnectionFactory(redisConnectionFactory);\n        Jackson2JsonRedisSerializer<Employee> ser = new Jackson2JsonRedisSerializer<Employee>(Employee.class);\n        template.setDefaultSerializer(ser);\n        return template;\n    }\n\n    //CacheManagerCustomizers可以定制缓存的一些规则。\n    @Bean\n    public RedisCacheManager  employeeCacheManager(RedisTemplate<Object, Employee> empRedisTemplate){\n        RedisCacheManager cacheManager = new RedisCacheManager(empRedisTemplate);\n        cacheManager.setUsePrefix(true);\n        return cacheManager;\n    }\n\n}\n```\n\n而创建缓存管理器的条件是。\n\n```java\n@AutoConfigureAfter(RedisAutoConfiguration.class)\n@ConditionalOnBean(RedisTemplate.class)\n@ConditionalOnMissingBean(CacheManager.class)  //没有CacheManager的时候创建RedisCacheConfiguration\n@Conditional(CacheCondition.class)\nclass RedisCacheConfiguration {\n```\n\n运行结果。\n\n![ATX81g.png](https://s2.ax1x.com/2019/04/10/ATX81g.png)\n\n我们在创建一个关于Department的缓存查询。\n\n```java\npackage com.hph.cache.bean;\n\npublic class Department {\n\t\n\tprivate Integer id;\n\tprivate String departmentName;\n\t\n\t\n\tpublic Department() {\n\t\tsuper();\n\t\t// TODO Auto-generated constructor stub\n\t}\n\tpublic Department(Integer id, String departmentName) {\n\t\tsuper();\n\t\tthis.id = id;\n\t\tthis.departmentName = departmentName;\n\t}\n\tpublic Integer getId() {\n\t\treturn id;\n\t}\n\tpublic void setId(Integer id) {\n\t\tthis.id = id;\n\t}\n\tpublic String getDepartmentName() {\n\t\treturn departmentName;\n\t}\n\tpublic void setDepartmentName(String departmentName) {\n\t\tthis.departmentName = departmentName;\n\t}\n\t@Override\n\tpublic String toString() {\n\t\treturn \"Department [id=\" + id + \", departmentName=\" + departmentName + \"]\";\n\t}\n\n}\n```\n\n```java\npackage com.hph.cache.mapper;\n\nimport com.hph.cache.bean.Department;\nimport org.apache.ibatis.annotations.Mapper;\nimport org.apache.ibatis.annotations.Select;\n\n@Mapper\npublic interface DeptMapper {\n    @Select(\"SELECT * FROM department WHERE id = #{id}\")\n    Department getDeptById(Integer id);\n}\n```\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Department;\nimport com.hph.cache.mapper.DeptMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class DeptService {\n    @Autowired\n    DeptMapper deptMapper;\n\n    @Cacheable(cacheNames = \"dept\")\n    public Department getDeptById(Integer id) {\n        System.out.println(\"查询部门\" + id);\n        Department department = deptMapper.getDeptById(id);\n\n        return department;\n    }\n}\n\n```\n\n```java\npackage com.hph.cache.controller;\n\nimport com.hph.cache.bean.Department;\nimport com.hph.cache.service.DeptService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class DeptContorller {\n\n    @Autowired\n    DeptService deptService;\n\n    @GetMapping(\"/dept/{id}\")\n    public Department getDept(@PathVariable(\"id\") Integer id) {\n        return deptService.getDeptById(id);\n    }\n}\n```\n\n![ATjaKH.png](https://s2.ax1x.com/2019/04/10/ATjaKH.png)\n\n![ATjBVI.png](https://s2.ax1x.com/2019/04/10/ATjBVI.png)\n\n然而当我们再次请求缓存的时候。\n\n![ATjDat.png](https://s2.ax1x.com/2019/04/10/ATjDat.png)\n\n5个属性要映射，这是因为我们自定义的那个缓存管理器是属于员工的而不是部门，因此字段对应不上去。同时我们也发现了缓存数据第一次可以存入Redis，第二次从缓存中查询就不能够反序列化回来了。这是因为当我们存储dept的json数据时，CacheManager默认使用的是RedisTemplate&lt;Object,Employee&gt;操作Redis，只能将Employee数据反序列化。\n\n我们需要配置以下缓存管理器。\n\n```java\n   @Bean\n    public RedisTemplate<Object, Department> deptRedisTemplate(\n            RedisConnectionFactory redisConnectionFactory)\n            throws UnknownHostException {\n        RedisTemplate<Object, Department> template = new RedisTemplate<Object, Department>();\n        template.setConnectionFactory(redisConnectionFactory);\n        Jackson2JsonRedisSerializer<Department> ser = new Jackson2JsonRedisSerializer<Department>(Department.class);\n        template.setDefaultSerializer(ser);\n        return template;\n    }\n\n    @Bean\n    public RedisCacheManager  deptCacheManager(RedisTemplate<Object, Department> deptRedisTemplate){\n        RedisCacheManager cacheManager = new RedisCacheManager(deptRedisTemplate);\n        cacheManager.setUsePrefix(true);\n        return cacheManager;\n    }\n```\n\n给Employee Dept分别指定缓存类别\n\n```java\n@CacheConfig(cacheNames = \"emp\",cacheManager = \"employeeCacheManager\")\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n    ....\n}\n```\n\n```java\n@Service\npublic class DeptService {\n    @Autowired\n    DeptMapper deptMapper;\n\n    @Cacheable(cacheNames = \"dept\",cacheManager = \"deptCacheManager\")\n    public Department getDeptById(Integer id) {\n        System.out.println(\"查询部门\" + id);\n        Department department = deptMapper.getDeptById(id);\n        return department;\n    }\n}\n```\n\n![ATzTit.png](https://s2.ax1x.com/2019/04/10/ATzTit.png)\n\n在MyRedisConfig中指定一个主类\n\n```java\n    @Primary\n    @Bean\n    public RedisCacheManager  employeeCacheManager(RedisTemplate<Object, Employee> empRedisTemplate){\n        RedisCacheManager cacheManager = new RedisCacheManager(empRedisTemplate);\n        cacheManager.setUsePrefix(true);\n        return cacheManager;\n    }\n```\n\n我们把Rdis的数据清空下\n\n![A79E5R.png](https://s2.ax1x.com/2019/04/10/A79E5R.png)\n\n由于默认指定的是employeeCacheManager我们可以在\n\n```java\n@CacheConfig(cacheNames = \"emp\")EmployeeService上省略。\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n```\n\n在实际开发中不应该将employeeCacheManager作为默认指定，应该将RedisCacheManager作为默认指定。@Primary将某个缓存管理器作为默认的 \n\n另外我们可以选择编码的方式l来配置缓存管理器。\n\n在DeptService中。\n\n```java\n    @Qualifier(\"deptCacheManager\")\n    @Autowired\n    RedisCacheManager deptCacheManager;\n\n    public Department getDeptById(Integer id) {\n        System.out.println(\"查询部门\" + id);\n        Department department = deptMapper.getDeptById(id);\n        //获取某个缓存\n        Cache dept = deptCacheManager.getCache(\"dept\");\n        dept.put(\"缓存操作1\", department);\n        return department;\n    }\n```\n\n![A7Ptu8.png](https://s2.ax1x.com/2019/04/10/A7Ptu8.png)","tags":["Redis"],"categories":["SpringBoot"]},{"title":"SpringBoot和缓存","url":"/2019/04/09/SpringBoot和缓存/","content":"\n {{ \"Spring  Boot的初体验和自动配置的探究 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nJSR是Java Specification Requests的缩写，意思是Java 规范提案。是指向[JCP](https://baike.baidu.com/item/JCP)(Java Community Process)提出新增一个标准化技术规范的正式请求。任何人都可以提交JSR，以向Java平台增添新的API和服务。JSR已成为Java界的一个重要标准。\n\n2012年10月26日JSR规范委员会发布了JSR 107（JCache API）的首个早期草案。自该JSR启动以来，已经过去近12年时间，因此该规范颇为Java社区所诟病，但由于目前对缓存需求越来越多，因此专家组加快了这一进度。\n\nJCache规范定义了一种对Java对象临时在内存中进行缓存的方法，包括对象的创建、共享访问、假脱机（spooling）、失效、各JVM的一致性等，可被用于缓存JSP内最经常读取的数据，如产品目录和价格列表。利用JCACHE，多数查询的反应时间会因为有缓存的数据而加快（内部测试表明反应时间大约快15倍）。\n\n## 内容\n\nJava Caching定义了5个核心接口，分别是**CachingProvider**, **CacheManager**, **Cache**, **Entry** 和 **Expiry**。\n\n•**CachingProvider**定义了创建、配置、获取、管理和控制多个**CacheManager**。一个应用可以在运行期访问多个CachingProvider。\n\n•**CacheManager**定义了创建、配置、获取、管理和控制多个唯一命名的**Cache**，这些Cache存在于CacheManager的上下文中。一个CacheManager仅被一个CachingProvider所拥有。\n\n•**Cache**是一个类似Map的数据结构并临时存储以Key为索引的值。一个Cache仅被一个CacheManager所拥有。\n\n•**Entry**是一个存储在Cache中的key-value对。\n\n•**Expiry** 每一个存储在Cache中的条目有一个定义的有效期。一旦超过这个时间，条目为过期的状态。一旦过期，条目将不可访问、更新和删除。缓存有效期可以通过ExpiryPolicy设置。\n\n![AIpCqA.png](https://s2.ax1x.com/2019/04/09/AIpCqA.png)\n\n\n\n## 重要概念\n\n![AIpUsJ.png](https://s2.ax1x.com/2019/04/09/AIpUsJ.png)\n\n![AIpRLd.png](https://s2.ax1x.com/2019/04/09/AIpRLd.png)\n\n## 环境搭建\n\n![AIpLLj.png](https://s2.ax1x.com/2019/04/09/AIpLLj.png)\n\n![AI9MlD.png](https://s2.ax1x.com/2019/04/09/AI9MlD.png)\n\n### SQL\n\n```sql\n\nSET FOREIGN_KEY_CHECKS=0;\n\n-- ----------------------------\n-- Table structure for department\n-- ----------------------------\nDROP TABLE IF EXISTS `department`;\nCREATE TABLE `department` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `departmentName` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n\n-- ----------------------------\n-- Table structure for employee\n-- ----------------------------\nDROP TABLE IF EXISTS `employee`;\nCREATE TABLE `employee` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `lastName` varchar(255) DEFAULT NULL,\n  `email` varchar(255) DEFAULT NULL,\n  `gender` int(2) DEFAULT NULL,\n  `d_id` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8;\n```\n\n### Bean\n\n```java\npackage com.hph.cache.bean;\n\npublic class Department {\n\t\n\tprivate Integer id;\n\tprivate String departmentName;\n\t\n\t\n\tpublic Department() {\n\t\tsuper();\n\t\t// TODO Auto-generated constructor stub\n\t}\n\tpublic Department(Integer id, String departmentName) {\n\t\tsuper();\n\t\tthis.id = id;\n\t\tthis.departmentName = departmentName;\n\t}\n\tpublic Integer getId() {\n\t\treturn id;\n\t}\n\tpublic void setId(Integer id) {\n\t\tthis.id = id;\n\t}\n\tpublic String getDepartmentName() {\n\t\treturn departmentName;\n\t}\n\tpublic void setDepartmentName(String departmentName) {\n\t\tthis.departmentName = departmentName;\n\t}\n\t@Override\n\tpublic String toString() {\n\t\treturn \"Department [id=\" + id + \", departmentName=\" + departmentName + \"]\";\n\t}\n\n}\n```\n\n```java\npackage com.hph.cache.bean;\n\npublic class Employee {\n\t\n\tprivate Integer id;\n\tprivate String lastName;\n\tprivate String email;\n\tprivate Integer gender; //性别 1男  0女\n\tprivate Integer dId;\n\t\n\t\n\tpublic Employee() {\n\t\tsuper();\n\t}\n\n\t\n\tpublic Employee(Integer id, String lastName, String email, Integer gender, Integer dId) {\n\t\tsuper();\n\t\tthis.id = id;\n\t\tthis.lastName = lastName;\n\t\tthis.email = email;\n\t\tthis.gender = gender;\n\t\tthis.dId = dId;\n\t}\n\t\n\tpublic Integer getId() {\n\t\treturn id;\n\t}\n\tpublic void setId(Integer id) {\n\t\tthis.id = id;\n\t}\n\tpublic String getLastName() {\n\t\treturn lastName;\n\t}\n\tpublic void setLastName(String lastName) {\n\t\tthis.lastName = lastName;\n\t}\n\tpublic String getEmail() {\n\t\treturn email;\n\t}\n\tpublic void setEmail(String email) {\n\t\tthis.email = email;\n\t}\n\tpublic Integer getGender() {\n\t\treturn gender;\n\t}\n\tpublic void setGender(Integer gender) {\n\t\tthis.gender = gender;\n\t}\n\tpublic Integer getdId() {\n\t\treturn dId;\n\t}\n\tpublic void setdId(Integer dId) {\n\t\tthis.dId = dId;\n\t}\n\t@Override\n\tpublic String toString() {\n\t\treturn \"Employee [id=\" + id + \", lastName=\" + lastName + \", email=\" + email + \", gender=\" + gender + \", dId=\"\n\t\t\t\t+ dId + \"]\";\n\t}\n\t\n}\n```\n\n### 整合Mybatis\n\n#### 配置数据源\n\n```properties\nspring.datasource.url=jdbc:mysql://192.168.1.110:3306/spring_cache\nspring.datasource.data-username=root\nspring.datasource.password=123456\nspring.datasource.driver-class-name=com.mysql.jdbc.Driver\n```\n\n#### 数据准备\n\n![AIiSYt.png](https://s2.ax1x.com/2019/04/09/AIiSYt.png)\n\n## 注解版\n\n```java\npackage com.hph.cache.mapper;\n\nimport com.hph.cache.bean.Employee;\nimport org.apache.ibatis.annotations.*;\n\n@Mapper\npublic interface EmployeeMapper {\n\n    @Select(\"SELECT * FROM  employee WHERE id = #{id}\")\n    public Employee getEmpById(Integer id);\n\n    @Update(\"UPDATE employee SET lastName=#{lastName},email=#{email},gender=#{gender},d_id=#{dID} WHERE id = #{id} \")\n    public  void updateEmp(Employee employee);\n\n    @Delete(\"DELETE FROM employee WHERE id=#{id} \")\n    public void deleteEmp(Integer id);\n\n    @Insert(\"INSERT INTO employee(lastName,email,gender,d_id) VALUESE(#{lastName},#{email},#{gender},#{dId}) \")\n    public void insertEmployee(Employee employee);\n}\n```\n\n### 测试\n\n```java\npackage com.hph.cache;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringbootCacheApplicationTests {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    @Test\n    public void getEmpById() {\n        Employee emp = employeeMapper.getEmpById(1);\n        System.out.println(emp);\n    }\n\n}\n```\n\n![AIinf0.png](https://s2.ax1x.com/2019/04/09/AIinf0.png)\n\n![AIifc8.png](https://s2.ax1x.com/2019/04/09/AIifc8.png)\n\n![AIihjS.png](https://s2.ax1x.com/2019/04/09/AIihjS.png)\n\n因为没有开启驼峰命名法，数据库中的字段和JavaBean中的字段未完全对应，在application.properties中配置驼峰命名。\n\n```properties\nspring.datasource.url=jdbc:mysql://192.168.1.110:3306/spring_cache\nspring.datasource.username=root\nspring.datasource.password=123456\n#驼峰命名开启\nmybatis.configuration.map-underscore-to-camel-case=true  \n```\n\n![AIiXcT.png](https://s2.ax1x.com/2019/04/09/AIiXcT.png)\n\n## 使用缓存\n\n添加@EnableCaching的注解\n\n````java\npackage com.hph.cache;\n\nimport org.mybatis.spring.annotation.MapperScan;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cache.annotation.EnableCaching;\n\n@MapperScan(\"com.hph.cache.mapper\")\n@SpringBootApplication\n@EnableCaching  //开启基于注解的缓存\npublic class SpringbootCacheApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootCacheApplication.class, args);\n    }\n\n}\n````\n\n开启DEBUG\n\n```properties\nspring.datasource.url=jdbc:mysql://192.168.1.110:3306/spring_cache\nspring.datasource.username=root\nspring.datasource.password=123456\n#驼峰命名开启\nmybatis.configuration.map-underscore-to-camel-case=true  \n#开启debug\nlogging.level.com.hph.cache.mapper=debug\n```\n\n![AIFyKU.png](https://s2.ax1x.com/2019/04/09/AIFyKU.png)\n\n![AIpQZn.png](https://s2.ax1x.com/2019/04/09/AIpQZn.png)\n\n```java\n\n\npackage org.springframework.cache.annotation;\n\nimport java.lang.annotation.Documented;\nimport java.lang.annotation.ElementType;\nimport java.lang.annotation.Inherited;\nimport java.lang.annotation.Retention;\nimport java.lang.annotation.RetentionPolicy;\nimport java.lang.annotation.Target;\nimport java.util.concurrent.Callable;\n\nimport org.springframework.core.annotation.AliasFor;\n\n@Target({ElementType.METHOD, ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Inherited\n@Documented\npublic @interface Cacheable {\n\n\t@AliasFor(\"cacheNames\")\n\tString[] value() default {};\n\n    //指定缓存的名字  \n    //CacheManager管理多个组件，对缓存的真正的CRUD操作在Cache组件中，每一个缓存操作有自己唯一的名字\n\t@AliasFor(\"value\")\n\tString[] cacheNames() default {};\n\t//缓存数据使用的key可以用它来指定，默认是方法的参数值。\n\tString key() default \"\";\n\n\t//主键key的生成器：可以自己指定key的生成器组件ID \n    //  key/keyGenerator  二者选一者使用\n\tString keyGenerator  () default \"\";\n\n\t//指定缓存管理器\n\tString cacheManager() default \"\";\n\n\n\tString cacheResolver() default \"\";\n\n\t//指定符合条件下才缓存\n\tString condition() default \"\";\n\t\n\t//否定缓存,当unless指定的条件为true,方法的返回值就不会被缓存,可以取到结果进行判断\n\tString unless() default \"\";\n\n\n\tboolean sync() default false;\n\n}\n\n```\n\n| **名字**        | **位置**           | **描述**                                                     | **示例**             |\n| --------------- | ------------------ | ------------------------------------------------------------ | -------------------- |\n| methodName      | root object        | 当前被调用的方法名                                           | #root.methodName     |\n| method          | root object        | 当前被调用的方法                                             | #root.method.name    |\n| target          | root object        | 当前被调用的目标对象                                         | #root.target         |\n| targetClass     | root object        | 当前被调用的目标对象类                                       | #root.targetClass    |\n| args            | root object        | 当前被调用的方法的参数列表                                   | #root.args[0]        |\n| caches          | root object        | 当前方法调用使用的缓存列表（如@Cacheable(value={\"cache1\",   \"cache2\"})），则有两个cache | #root.caches[0].name |\n| *argument name* | evaluation context | 方法参数的名字. 可以直接 #参数名 ，也可以使用 #p0或#a0 的形式，0代表参数的索引； | #iban 、 #a0 、  #p0 |\n| result          | evaluation context | 方法执行后的返回值（仅当方法执行之后的判断有效，如‘unless’，’cache put’的表达式 ’cache evict’的表达式beforeInvocation=false） | #result              |\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //将方法的运行结果进行缓存,如果有相同的数据直接从缓存中获取不用调用方法\n    @Cacheable(cacheNames = {\"emp\"})\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n}\n```\n\n```java\npackage com.hph.cache;\n\nimport org.mybatis.spring.annotation.MapperScan;\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.cache.annotation.EnableCaching;\n\n@MapperScan(\"com.hph.cache.mapper\")\n@SpringBootApplication\n@EnableCaching  //开启基于注解的缓存\npublic class SpringbootCacheApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(SpringbootCacheApplication.class, args);\n    }\n\n}\n```\n\n![Aokec6.png](https://s2.ax1x.com/2019/04/09/Aokec6.png)\n\n![AokdHg.png](https://s2.ax1x.com/2019/04/09/AokdHg.png)\n\n缓存生效了。\n\n### 流程\n\n在CacheAutoConfiguration中\n\n```java\n\tstatic class CacheConfigurationImportSelector implements ImportSelector {\n\n\t\t@Override\n\t\tpublic String[] selectImports(AnnotationMetadata importingClassMetadata) {\n\t\t\tCacheType[] types = CacheType.values();\n\t\t\tString[] imports = new String[types.length];\n\t\t\tfor (int i = 0; i < types.length; i++) {\n\t\t\t\timports[i] = CacheConfigurations.getConfigurationClass(types[i]);\n\t\t\t}\n\t\t\treturn imports;\n\t\t}\n\n\t}\n```\n\n```java\npublic interface ImportSelector {\n\n\tString[] selectImports(AnnotationMetadata importingClassMetadata);\n\n}\n```\n\n我们试着在CacheAutoConfiguration中打断点。看其加载的配置类。\n\n![AoEakQ.png](https://s2.ax1x.com/2019/04/09/AoEakQ.png)\n\n![AoZwMq.png](https://s2.ax1x.com/2019/04/09/AoZwMq.png)\n\n![AoeKkF.png](https://s2.ax1x.com/2019/04/09/AoeKkF.png)\n\n ```java\n//给容器中注册了一个CacheManager;ConcurrentMapCacheManager\n//可以获取和创建ConcurrentMapCache类型的缓存的缓存组件\nclass SimpleCacheConfiguration {\n\t\n\tprivate final CacheProperties cacheProperties;\n\n\tprivate final CacheManagerCustomizers customizerInvoker;\n\n\tSimpleCacheConfiguration(CacheProperties cacheProperties,\n\t\t\tCacheManagerCustomizers customizerInvoker) {\n\t\tthis.cacheProperties = cacheProperties;\n\t\tthis.customizerInvoker = customizerInvoker;\n\t}\n\n\t@Bean\n    //查看ConcurrentMapCacheManager\n\tpublic ConcurrentMapCacheManager cacheManager() {\n\t\tConcurrentMapCacheManager cacheManager = new ConcurrentMapCacheManager();\n\t\tList<String> cacheNames = this.cacheProperties.getCacheNames();\n\t\tif (!cacheNames.isEmpty()) {\n\t\t\tcacheManager.setCacheNames(cacheNames);\n\t\t}\n\t\treturn this.customizerInvoker.customize(cacheManager);\n\t}\n}\n ```\n\n```java\n\tpublic Cache getCache(String name) {\n        //按照名字获取组件\n\t\tCache cache = this.cacheMap.get(name);\n\t\tif (cache == null && this.dynamic) {\n\t\t\tsynchronized (this.cacheMap) {\n\t\t\t\tcache = this.cacheMap.get(name);\n                  //缓存如果为空 我们选择创建一个\n\t\t\t\tif (cache == null) {\n\t\t\t\t\tcache = createConcurrentMapCache(name);\n\t\t\t\t\tthis.cacheMap.put(name, cache);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn cache;\n\t}\n\n\t//创建一个缓存对象\n\tprotected Cache createConcurrentMapCache(String name) {\n\t\tSerializationDelegate actualSerialization = (isStoreByValue() ? this.serialization : null);\n\t\treturn new ConcurrentMapCache(name, new ConcurrentHashMap<>(256),\n\t\t\t\tisAllowNullValues(), actualSerialization);\n\n\t}\n```\n\n```java\n\tprivate final ConcurrentMap<Object, Object> store;\n```\n\n     @Cacheable：\n        1、方法运行之前，先去查询Cache（缓存组件），按照cacheNames指定的名字获取；\n           （CacheManager先获取相应的缓存），第一次获取缓存如果没有Cache组件会自动创建。\n        2、去Cache中查找缓存的内容，使用一个key，默认就是方法的参数；\n           key是按照某种策略生成的；默认是使用keyGenerator生成的，默认使用SimpleKeyGenerator生成key；\n               SimpleKeyGenerator生成key的默认策略；\n                       如果没有参数；key=new SimpleKey()；\n                       如果有一个参数：key=参数的值\n                       如果有多个参数：key=new SimpleKey(params)；\n        3、没有查到缓存就调用目标方法；\n        4、将目标方法返回的结果，放进缓存中\n     \n        @Cacheable标注的方法执行之前先来检查缓存中有没有这个数据，默认按照参数的值作为key去查询缓存，\n        如果没有就运行方法并将结果放入缓存；以后再来调用就可以直接使用缓存中的数据；\n    \n        核心：\n           1）、使用CacheManager【ConcurrentMapCacheManager】按照名字得到Cache【ConcurrentMapCache】组件\n           2）、key使用keyGenerator生成的，默认是SimpleKeyGenerator\n```java\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //将方法的运行结果进行缓存,如果有相同的数据直接从缓存中获取不用调用方法\n    @Cacheable(cacheNames = {\"emp\"},key = \"#root.methodName+'['+#id+']'\")\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n}\n```\n\n![AolZ8S.png](https://s2.ax1x.com/2019/04/09/AolZ8S.png)\n\n### 自定义myKeyGenerator\n\n```java\npackage com.hph.cache.config;\n\nimport org.springframework.cache.interceptor.KeyGenerator;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.lang.reflect.Method;\nimport java.util.Arrays;\n\n@Configuration\npublic class MyCacheConfig {\n\n    @Bean(\"myKeyGenerator\")\n    public KeyGenerator keyGenerator(){\n        return new KeyGenerator(){\n\n            @Override\n            public Object generate(Object target, Method method, Object... params) {\n                return method.getName()+\"[\"+ Arrays.asList(params).toString()+\"]\";\n            }\n        };\n    }\n}\n```\n\n![Ao1pGT.png](https://s2.ax1x.com/2019/04/09/Ao1pGT.png)\n\n注意要关闭`debug=true`\n\n![Ao3E6g.png](https://s2.ax1x.com/2019/04/09/Ao3E6g.png)\n\n```java\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"},keyGenerator = \"myKeyGenerator\",condition = \"#a0>0\",unless = \"#a0==2\")\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n}\n```\n\n![Ao8CDJ.png](https://s2.ax1x.com/2019/04/09/Ao8CDJ.png)\n\n### CachePut\n\n```java\n@Target({ElementType.METHOD, ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Inherited\n@Documented\npublic @interface CachePut {\n//和Cacheable基本相似\n\t@AliasFor(\"cacheNames\")\n\tString[] value() default {};\n\n\t@AliasFor(\"value\")\n\tString[] cacheNames() default {};\n\n\tString key() default \"\";\n\n\tString keyGenerator() default \"\";\n\n\tString cacheManager() default \"\";\n\n\tString cacheResolver() default \"\";\n\n\tString condition() default \"\";\n\n\tString unless() default \"\";\n\n}\n```\n\n#### 实现\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.CachePut;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"}/*, keyGenerator = \"myKeyGenerator\", condition = \"#a0>0\", unless = \"#a0==2\"*/)\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n\n    /**\n     * @CachePut即调用方法,又更新缓存数据 修改了数据库的某个数据, 同时更新缓存\n     *运行时机:先调用目标方法,现将目标方法的结果缓存起来\n     */\n    @CachePut(value = \"emp\")\n    public Employee updateEmp(Employee employee) {\n        System.out.println(\"updateEmp\"+employee);\n        employeeMapper.updateEmp(employee);\n        return employee;\n    }\n}\n```\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.CachePut;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"}/*, keyGenerator = \"myKeyGenerator\", condition = \"#a0>0\", unless = \"#a0==2\"*/)\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n\n    /**\n     * @CachePut即调用方法,又更新缓存数据 修改了数据库的某个数据, 同时更新缓存\n     *运行时机:先调用目标方法,现将目标方法的结果缓存起来\n     */\n    @CachePut(value = \"emp\")\n    public Employee updateEmp(Employee employee) {\n        System.out.println(\"updateEmp\"+employee);\n        employeeMapper.updateEmp(employee);\n        return employee;\n    }\n}\n```\n\n```java\npackage com.hph.cache.controller;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.service.EmployeeService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class EmployeeController {\n    @Autowired\n    EmployeeService employeeService;\n\n    @GetMapping(\"/emp/{id}\")\n    public Employee getEmployee(@PathVariable(\"id\") Integer id) {\n        Employee emp = employeeService.getEmp(id);\n        return emp;\n    }\n\n    @GetMapping(\"/emp\")\n    public Employee update(Employee employee) {\n        Employee emp = employeeService.updateEmp(employee);\n        return emp;\n\n    }\n}\n```\n\n\n\n#### 测试步骤\n\n1. 查询1号员工查到的j结果会放在缓存中。\n\n    key:1  value:LastName=清风丶\n\n![AotWSx.png](https://s2.ax1x.com/2019/04/09/AotWSx.png)\n\n2.以后查询还是之前的结果\n\n3.更新员工信息1号员工信息【LastName=清风丶；email=`qingfeng@gmail.com`】\n\n将方法的返回值也放进了缓存\n\nkey:传入的employee对象  value:返回的employee对象\n\n![AoUSv6.png](https://s2.ax1x.com/2019/04/09/AoUSv6.png) \n\n4.查询员工\n\n![AoaZo4.png](https://s2.ax1x.com/2019/04/09/AoaZo4.png)1号员工没有在缓存中更新。\n\nkey=“#employee。id”使用传入参数员工的id；\n\nkey=“result.id”使用返回后的id\n\n```java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.CachePut;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"}/*, keyGenerator = \"myKeyGenerator\", condition = \"#a0>0\", unless = \"#a0==2\"*/)\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n\n    /**\n     * @CachePut即调用方法,又更新缓存数据 修改了数据库的某个数据, 同时更新缓存\n     *运行时机:先调用目标方法,现将目标方法的结果缓存起来\n     */\n    @CachePut(value = \"emp\",key = \"#result.id \")\n    public Employee updateEmp(Employee employee) {\n        System.out.println(\"updateEmp\"+employee);\n        employeeMapper.updateEmp(employee);\n        return employee;\n    }\n}\n```\n\n![Aow9K0.png](https://s2.ax1x.com/2019/04/09/Aow9K0.png)\n\n![AowFVU.png](https://s2.ax1x.com/2019/04/09/AowFVU.png)\n\n![AowlVO.png](https://s2.ax1x.com/2019/04/09/AowlVO.png)\n\n查找\n\n![AowYRA.png](https://s2.ax1x.com/2019/04/09/AowYRA.png)\n\n已经更新过来了\n\n### CacheEvict\n\nEmployeeService\n\n````java\npackage com.hph.cache.service;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.mapper.EmployeeMapper;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.cache.annotation.CacheEvict;\nimport org.springframework.cache.annotation.CachePut;\nimport org.springframework.cache.annotation.Cacheable;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class EmployeeService {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    //condition = \"#a0>1 第一个参数的值>>1的时候可以进行缓存 除非a0参数是2\n    @Cacheable(cacheNames = {\"emp\"}/*, keyGenerator = \"myKeyGenerator\", condition = \"#a0>0\", unless = \"#a0==2\"*/)\n    public Employee getEmp(Integer id) {\n        System.out.println(\"查询\" + id + \"号员工\");\n        Employee emp = employeeMapper.getEmpById(id);\n        return emp;\n    }\n\n    /**\n     * @CachePut即调用方法,又更新缓存数据 修改了数据库的某个数据, 同时更新缓存\n     *运行时机:先调用目标方法,现将目标方法的结果缓存起来\n     */\n    @CachePut(value = \"emp\",key = \"#result.id \")\n    public Employee updateEmp(Employee employee) {\n        System.out.println(\"updateEmp\"+employee);\n        employeeMapper.updateEmp(employee);\n        return employee;\n    }\n\n    /**\n     * @Cachevict :缓存清除\n     * key: 指定要清除的数据\n     */\n    @CacheEvict(value = \"emp\",key = \"#id\")\n    public void deleteEmp(Integer id){\n        System.out.println(\"deletEmp\"+id);\n        employeeMapper.deleteEmpById(id);\n    }\n\n}\n\n````\n\nEmployeeController\n\n```java\npackage com.hph.cache.controller;\n\nimport com.hph.cache.bean.Employee;\nimport com.hph.cache.service.EmployeeService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class EmployeeController {\n    @Autowired\n    EmployeeService employeeService;\n\n    @GetMapping(\"/emp/{id}\")\n    public Employee getEmployee(@PathVariable(\"id\") Integer id) {\n        Employee emp = employeeService.getEmp(id);\n        return emp;\n    }\n\n    @GetMapping(\"/emp\")\n    public Employee update(Employee employee) {\n        Employee emp = employeeService.updateEmp(employee);\n        return emp;\n    }\n\n    @GetMapping(\"/delemp\")\n    public String deleteEmp(Integer id) {\n        employeeService.deleteEmp(id);\n        return \"删除成功\";\n    }\n}\n```\n\n![Ao0lyq.png](https://s2.ax1x.com/2019/04/09/Ao0lyq.png)\n\n(⊙﹏⊙)出现乱码了 先忽略吧\n\n![Ao08mV.png](https://s2.ax1x.com/2019/04/09/Ao08mV.png)\n\n![Ao0yTO.png](https://s2.ax1x.com/2019/04/09/Ao0yTO.png)\n\n```java\n    //删除缓存中的所有数据\n    @CacheEvict(value = \"emp\",key = \"#id\",allEntries = true)\n    public void deleteEmp(Integer id){\n        System.out.println(\"deletEmp\"+id);\n        employeeMapper.deleteEmpById(id);\n    }\n```\n\n![AoBSBV.png](https://s2.ax1x.com/2019/04/09/AoBSBV.png)\n\n数据被全部清空\n\n![AoBGjI.png](https://s2.ax1x.com/2019/04/09/AoBGjI.png)\n\n```java\n    //缓存默认清除才足在方法执行之后执行;如果出现异常缓存不会被清除\n    @CacheEvict(value = \"emp\")\n    public void deleteEmp(Integer id){\n        System.out.println(\"deletEmp\"+id);\n      //  employeeMapper.deleteEmpById(id);\n        int i = 10/0;\n    }\n```\n\n![AoBRET.png](https://s2.ax1x.com/2019/04/09/AoBRET.png)\n\n![AoBbb6.png](https://s2.ax1x.com/2019/04/09/AoBbb6.png)\n\n2号数据库缓存也被清除掉了\n\n![AoBvPe.png](https://s2.ax1x.com/2019/04/09/AoBvPe.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["JSR107"],"categories":["SpringBoot"]},{"title":"SpringBoot与JPA","url":"/2019/04/09/SpringBoot与JPA/","content":"\n {{ \"SpringBoot与JPA集成\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n## 简介\n\nJPA是Java Persistence API的简称，中文名Java持久层API，是JDK 5.0注解或XML描述对象－关系表的映射关系，并将运行期的实体对象持久化到数据库中。\n\n![A5GTm9.png](https://s2.ax1x.com/2019/04/08/A5GTm9.png)\n\n## 准备\n\n![A5qYx1.png](https://s2.ax1x.com/2019/04/09/A5qYx1.png)\n\n![A5qLLV.png](https://s2.ax1x.com/2019/04/09/A5qLLV.png)\n\n### Maven\n\nMaven的依赖关系\n\n![A5qjdU.png](https://s2.ax1x.com/2019/04/09/A5qjdU.png)\n\n### 目录结构\n\n![A5LlOP.png](https://s2.ax1x.com/2019/04/09/A5LlOP.png)\n\n```java\npackage com.hph.springboot.repository;\n\nimport com.hph.springboot.entity.User;\nimport org.springframework.data.jpa.repository.JpaRepository;\n\n//继承JpaRepository来完成对数据库的操作\npublic interface UserRepository extends JpaRepository<User,Integer> {\n}\n```\n\n```java\npackage com.hph.springboot.entity;\n\n\nimport javax.persistence.*;\n\n//使用JPA注解配置映射关系\n@Entity //告诉JPA这是一个实体类（和数据表映射的类）\n@Table(name = \"tbl_user\") //@Table来指定和哪个数据表对应;如果省略默认表名就是user；\npublic class User {\n\n    @Id //这是一个主键\n    @GeneratedValue(strategy = GenerationType.IDENTITY)//自增主键\n    private Integer id;\n\n    @Column(name = \"last_name\",length = 50) //这是和数据表对应的一个列\n    private String lastName;\n    @Column //省略默认列名就是属性名\n    private String email;\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n}\n```\n\n```java\npackage com.hph.springboot.controller;\n\nimport com.hph.springboot.entity.User;\nimport com.hph.springboot.repository.UserRepository;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class UserController {\n\n    @Autowired\n    UserRepository userRepository;\n\n    @GetMapping(\"/user/{id}\")\n    public User getUser(@PathVariable(\"id\") Integer id){\n        User user = userRepository.findOne(id);\n        return user;\n    }\n\n    @GetMapping(\"/user\")\n    public User insertUser(User user){\n        User save = userRepository.save(user);\n        return save;\n    }\n}\n```\n\n\n```yaml\nspring:\n  datasource:\n    url: jdbc:mysql://192.168.1.110/jpa\n    username: root\n    password: 123456\n    driver-class-name: com.mysql.jdbc.Driver\n  jpa:\n    hibernate:\n      #     更新或者创建数据表结构\n      ddl-auto: update\n    #    控制台显示SQL\n    show-sql: true\n```\n\n## 运行\n\n![A5L5m6.png](https://s2.ax1x.com/2019/04/09/A5L5m6.png)\n\n创建tbl_user表。\n\n![A5O9AS.png](https://s2.ax1x.com/2019/04/09/A5O9AS.png)\n\n## 测试 \n\n### 数据准备\n\n![A5Od4e.png](https://s2.ax1x.com/2019/04/09/A5Od4e.png)\n\n### 查询\n\n![A5OrjI.png](https://s2.ax1x.com/2019/04/09/A5OrjI.png)\n\n","tags":["JPA"],"categories":["SpringBoot"]},{"title":"SpringBoot与Mybatis的集成","url":"/2019/04/08/SpringBoot与Mybatis的集成/","content":"\n {{ \"SpringBoot与Mybatis集成\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 准备\n\n![A5ehtJ.png](https://s2.ax1x.com/2019/04/08/A5ehtJ.png)\n\n![A5eT6x.png](https://s2.ax1x.com/2019/04/08/A5eT6x.png)\n\n### Maven依赖\n\n![A5mDED.png](https://s2.ax1x.com/2019/04/08/A5mDED.png)\n\n### 引入Druid\n\n```xml\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>druid</artifactId>\n    <version>1.1.16</version>\n</dependency>\n```\n\n### 引入配置类\n\n```java\npackage com.hph.springboot.config;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport com.alibaba.druid.support.http.StatViewServlet;\nimport com.alibaba.druid.support.http.WebStatFilter;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.boot.web.servlet.ServletRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.sql.DataSource;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\n\n@Configuration\npublic class DruidConfig {\n\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    @Bean\n    public DataSource druid(){\n        return  new DruidDataSource();\n    }\n\n    //配置Druid的监控\n    //1、配置一个管理后台的Servlet\n    @Bean\n    public ServletRegistrationBean statViewServlet(){\n        ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\");\n        Map<String,String> initParams = new HashMap<>();\n\n        initParams.put(\"loginUsername\",\"admin\");\n        initParams.put(\"loginPassword\",\"123456\");\n        initParams.put(\"allow\",\"\");//默认就是允许所有访问\n        initParams.put(\"deny\",\"192.168.1.110\");\n\n        bean.setInitParameters(initParams);\n        return bean;\n    }\n\n\n    //2、配置一个web监控的filter\n    @Bean\n    public FilterRegistrationBean webStatFilter(){\n        FilterRegistrationBean bean = new FilterRegistrationBean();\n        bean.setFilter(new WebStatFilter());\n\n        Map<String,String> initParams = new HashMap<>();\n        initParams.put(\"exclusions\",\"*.js,*.css,/druid/*\");\n\n        bean.setInitParameters(initParams);\n\n        bean.setUrlPatterns(Arrays.asList(\"/*\"));\n\n        return  bean;\n    }\n}\n```\n\n### 验证可用\n\n![A5ncoF.png](https://s2.ax1x.com/2019/04/08/A5ncoF.png)\n\n![A5uJ61.png](https://s2.ax1x.com/2019/04/08/A5uJ61.png)\n\n### 数据准备\n\n#### SQL文件\n\n```sql\nSET FOREIGN_KEY_CHECKS=0;\n\nDROP TABLE IF EXISTS `department`;\nCREATE TABLE `department` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `departmentName` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;\n```\n\n```sql\nSET FOREIGN_KEY_CHECKS=0;\n\nDROP TABLE IF EXISTS `employee`;\nCREATE TABLE `employee` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `lastName` varchar(255) DEFAULT NULL,\n  `email` varchar(255) DEFAULT NULL,\n  `gender` int(2) DEFAULT NULL,\n  `d_id` int(11) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;\n```\n\n### 配置文件\n\n```yml\nspring:\n  datasource:\n    username: root\n    password: 123456\n    url: jdbc:mysql://192.168.1.110:3306/mybatis\n    driver-class-name: com.mysql.jdbc.Driver\n    type: com.alibaba.druid.pool.DruidDataSource\n\n    initialSize: 5\n    minIdle: 5\n    maxActive: 20\n    maxWait: 60000\n    timeBetweenEvictionRunsMillis: 60000\n    minEvictableIdleTimeMillis: 300000\n    validationQuery: SELECT 1 FROM DUAL\n    testWhileIdle: true\n    testOnBorrow: false\n    testOnReturn: false\n    poolPreparedStatements: true\n    #   配置监控统计拦截的filters，去掉后监控界面sql无法统计，'wall'用于防火墙\n    filters: stat,wall,log4j\n    maxPoolPreparedStatementPerConnectionSize: 20\n    useGlobalDataSourceStat: true\n    connectionProperties: druid.stat.mergeSql=true;druid.stat.slowSqlMillis=500\n    schema:\n      - classpath:/sql/department.sql\n      - classpath:/sql/employee.sql\n```\n\n![A5ubn0.png](https://s2.ax1x.com/2019/04/08/A5ubn0.png)\n\n### 创建Bean\n\n```java\npackage com.hph.springboot.bean;\n\npublic class Employee {\n    private  Integer id;\n    private  String lastName;\n    private  Integer  gender;\n    private String email;\n    private  String dId;\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public String getdId() {\n        return dId;\n    }\n\n    public void setdId(String dId) {\n        this.dId = dId;\n    }\n\n    @Override\n    public String toString() {\n        return \"Employee{\" +\n                \"id=\" + id +\n                \", lastName='\" + lastName + '\\'' +\n                \", gender=\" + gender +\n                \", email='\" + email + '\\'' +\n                \", dId='\" + dId + '\\'' +\n                '}';\n    }\n}\n```\n\n```java\npackage com.hph.springboot.bean;\n\npublic class Department {\n    private  Integer id;\n    private  String departmentName;\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getDepartmentName() {\n        return departmentName;\n    }\n\n    public void setDepartmentName(String departmentName) {\n        this.departmentName = departmentName;\n    }\n}\n```\n\n注意SQL数据表已经生成接下来我们在配置文件中注释掉\n\n## 注解\n\n### Mapper类\n\n```java\npackage com.hph.springboot.Mapper;\n\nimport com.hph.springboot.bean.Department;\nimport org.apache.ibatis.annotations.*;\n\n//指定这是一个操作数据库的Mapper\n@Mapper\npublic interface DepartmentMapper {\n\n    @Select(\"select * from department where id=#{id}\")\n    public Department getDeptById(Integer id);\n\n    @Delete(\"delete from department where id=#{id}\")\n    public int deleteDeptById(Integer id);\n\n    @Insert(\"insert into department(departmentName) values(#{departmentName})\")\n    public int insertDept(Department department);\n\n    @Update(\"update department set departmentName=#{departmentName} where id=#{id}\")\n    public int updateDept(Department department);\n}\n\n```\n\n### Controller\n\n```java\npackage com.hph.springboot.Mapper;\n\nimport com.hph.springboot.bean.Department;\nimport org.apache.ibatis.annotations.*;\n\n//指定这是一个操作数据库的Mapper\n@Mapper\npublic interface DepartmentMapper {\n\n    @Select(\"select * from department where id=#{id}\")\n    public Department getDeptById(Integer id);\n\n    @Delete(\"delete from department where id=#{id}\") \n    public int deleteDeptById(Integer id);\n\t\n    @Insert(\"insert into department(departmentName) values(#{departmentName})\")\n    public int insertDept(Department department);\n\n    @Update(\"update department set departmentName=#{departmentName} where id=#{id}\")\n    public int updateDept(Department department);\n\t}\n}\n```\n\n### 查询\n\n![A5M3s1.png](https://s2.ax1x.com/2019/04/08/A5M3s1.png)\n\n## 插入\n\n![A5Qrp4.png](https://s2.ax1x.com/2019/04/08/A5Qrp4.png)\n\n如何让插入的时候返回主键呢只需要在Insert方法上添加:\n\n```java\n    @Options(useGeneratedKeys = true,keyProperty = \"id\")   //添加该选项\n    @Insert(\"insert into department(department_name) values(#{departmentName})\")\n    public int insertDept(Department department);\n```\n\n![A5Q4hD.png](https://s2.ax1x.com/2019/04/08/A5Q4hD.png)\n\n### 再次查询\n\n![A5Qs1J.png](https://s2.ax1x.com/2019/04/08/A5Qs1J.png)\n\n如果我们修改表的字段departmentName为department_Name\n\n### Mybatis的自动配置\n\n```java\n  @Bean\n  @ConditionalOnMissingBean\n  public SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception {\n    SqlSessionFactoryBean factory = new SqlSessionFactoryBean();\n    factory.setDataSource(dataSource);\n    factory.setVfs(SpringBootVFS.class);\n    if (StringUtils.hasText(this.properties.getConfigLocation())) {\n      factory.setConfigLocation(this.resourceLoader.getResource(this.properties.getConfigLocation()));\n    }\n    Configuration configuration = this.properties.getConfiguration();\n    if (configuration == null && !StringUtils.hasText(this.properties.getConfigLocation())) {\n      configuration = new Configuration();\n    }\n      //驼峰命名 \n    if (configuration != null && !CollectionUtils.isEmpty(this.configurationCustomizers)) {\n      for (ConfigurationCustomizer customizer : this.configurationCustomizers) {\n        customizer.customize(configuration);\n      }\n    }\n    factory.setConfiguration(configuration);\n    if (this.properties.getConfigurationProperties() != null) {\n      factory.setConfigurationProperties(this.properties.getConfigurationProperties());\n    }\n    if (!ObjectUtils.isEmpty(this.interceptors)) {\n      factory.setPlugins(this.interceptors);\n    }\n    if (this.databaseIdProvider != null) {\n      factory.setDatabaseIdProvider(this.databaseIdProvider);\n    }\n    if (StringUtils.hasLength(this.properties.getTypeAliasesPackage())) {\n      factory.setTypeAliasesPackage(this.properties.getTypeAliasesPackage());\n    }\n    if (this.properties.getTypeAliasesSuperType() != null) {\n      factory.setTypeAliasesSuperType(this.properties.getTypeAliasesSuperType());\n    }\n    if (StringUtils.hasLength(this.properties.getTypeHandlersPackage())) {\n      factory.setTypeHandlersPackage(this.properties.getTypeHandlersPackage());\n    }\n    if (!ObjectUtils.isEmpty(this.properties.resolveMapperLocations())) {\n      factory.setMapperLocations(this.properties.resolveMapperLocations());\n    }\n\n    return factory.getObject();\n  }\n```\n\n### 配置类\n\n```java\npackage com.hph.springboot.config;\n\nimport org.apache.ibatis.session.Configuration;\nimport org.mybatis.spring.boot.autoconfigure.ConfigurationCustomizer;\nimport org.springframework.context.annotation.Bean;\n\n@org.springframework.context.annotation.Configuration\npublic class MybatisConfig {\n\n    @Bean\n    public ConfigurationCustomizer configurationCustomizer() {\n        return new ConfigurationCustomizer() {\n            @Override\n            public void customize(Configuration configuration) {\n                configuration.setMapUnderscoreToCamelCase(true);\n            }\n        };\n    }\n}\n```\n\n![A51mIf.png](https://s2.ax1x.com/2019/04/08/A51mIf.png)\n\nQAQ依然可以使用。\n\n小技巧：如果Mapper报下配置的的Mapper类很多一个一个添加可能会有些，麻烦或者遗漏因此我们可以在主类上添加一个注释(“@MapperScan(value = \"com.hph.springboot.Mapper\")”) 指定Mapper扫描的包名。\n\n## XML配置\n\n### mybatis-config.xml\n\n````xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE configuration\n        PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n\n    <settings>\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n    </settings>\n</configuration>\n````\n\n### EmployeeMapper.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper\n        PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<mapper namespace=\"com.hph.springboot.Mapper.EmployeeMapper\">\n   <!--    public Employee getEmpById(Integer id);\n    public void insertEmp(Employee employee);-->\n    <select id=\"getEmpById\" resultType=\"com.hph.springboot.bean.Employee\">\n        SELECT * FROM employee WHERE id=#{id}\n    </select>\n\n    <insert id=\"insertEmp\">\n        INSERT INTO employee(lastName,email,gender,d_id) VALUES (#{lastName},#{email},#{gender},#{dId})\n    </insert>\n</mapper>\n```\n\n### Mapper类\n\n```java\npackage com.hph.springboot.Mapper;\n\nimport com.hph.springboot.bean.Employee;\n\npublic interface EmployeeMapper {\n    public Employee getEmpById(Integer id);\n\n    public void insertEmp(Employee employee);\n}\n```\n\n### Controller\n\n```java\npackage com.hph.springboot.controller;\n\nimport com.hph.springboot.Mapper.EmployeeMapper;\nimport com.hph.springboot.bean.Employee;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.web.bind.annotation.GetMapping;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RestController;\n\n@RestController\npublic class EmpController {\n    @Autowired\n    EmployeeMapper employeeMapper;\n\n    @GetMapping(\"/emp/{id}\")\n    public Employee getEmp(@PathVariable(\"id\") Integer id){\n        return employeeMapper.getEmpById(id);\n    }\n}\n```\n\n在application.yml配置\n\n```yml\nmybatis:\n  config-location: classpath:mybatis/mybatis-config.xml\n  mapper-locations: classpath:mybatis/mapper/*.xml\n```\n\n### 数据准备\n\n![A5GEL9.png](https://s2.ax1x.com/2019/04/08/A5GEL9.png)\n\n### 数据查询\n\n![A5GkM4.png](https://s2.ax1x.com/2019/04/08/A5GkM4.png)\n\n","tags":["Mybatis"],"categories":["SpringBoot"]},{"title":"SpringBoot数据访问","url":"/2019/04/08/SpringBoot数据访问/","content":"\n {{ \"SpringBoot对于Mysql的数据访问和的Druid集成 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 准备\n\n![A4Lxr6.png](https://s2.ax1x.com/2019/04/08/A4Lxr6.png)\n\n![A4OpVO.png](https://s2.ax1x.com/2019/04/08/A4OpVO.png)\n\n在Maven中会多依赖\n\n```xml\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-jdbc</artifactId>\n</dependency>\n\n<dependency>\n\t<groupId>mysql</groupId>\n\t<artifactId>mysql-connector-java</artifactId>\n\t<scope>runtime</scope>\n</dependency>\n```\n\napplication.yml的配置文件关于JDBC的\n```yml\nspring:\n  datasource:\n    username: root\n    password: 123456\n    url: jdbc:mysql://192.168.1.110:3306/jdbc\n    driver-class-name: com.mysql.jdbc.Driver\n```\n\n## 测试\n\n```java\npackage com.hph.springboot;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\nimport javax.sql.DataSource;\nimport java.sql.Connection;\nimport java.sql.SQLException;\n\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class SpringBootDataJdbcApplicationTests {\n\n    @Autowired\n    DataSource dataSource;\n    @Test\n    public void contextLoads() throws SQLException {\n        System.out.println(dataSource.getClass());\n        Connection connection = dataSource.getConnection();\n        System.out.println(connection);\n\n    }\n\n}\n```\n\n![A4O0JJ.png](https://s2.ax1x.com/2019/04/08/A4O0JJ.png)\n\n默认是用org.apache.tomcat.jdbc.pool.DataSource作为数据源；数据源的相关配置都在DataSourceProperties里面；\n\n## 自动配置\n\n```java\n/**\n\t * Tomcat Pool DataSource configuration.\n\t */\n\t@Configuration\n\t@ConditionalOnClass(org.apache.tomcat.jdbc.pool.DataSource.class)\n\t@ConditionalOnMissingBean(DataSource.class)\n\t@ConditionalOnProperty(name = \"spring.datasource.type\",\n\t\t\thavingValue = \"org.apache.tomcat.jdbc.pool.DataSource\", matchIfMissing = true)\n\t//默认\n\tstatic class Tomcat {\n\t\t@Bean\n\t\t@ConfigurationProperties(prefix = \"spring.datasource.tomcat\")\n\t\tpublic org.apache.tomcat.jdbc.pool.DataSource dataSource(\n\t\t\t\tDataSourceProperties properties) {\n\t\t\torg.apache.tomcat.jdbc.pool.DataSource dataSource = createDataSource(\n\t\t\t\t\tproperties, org.apache.tomcat.jdbc.pool.DataSource.class);\n\t\t\tDatabaseDriver databaseDriver = DatabaseDriver\n\t\t\t\t\t.fromJdbcUrl(properties.determineUrl());\n\t\t\tString validationQuery = databaseDriver.getValidationQuery();\n\t\t\tif (validationQuery != null) {\n\t\t\t\tdataSource.setTestOnBorrow(true);\n\t\t\t\tdataSource.setValidationQuery(validationQuery);\n\t\t\t}\n\t\t\treturn dataSource;\n\t\t}\n\t}\n```\n\n1、参考DataSourceConfiguration，根据配置创建数据源，默认使用Tomcat连接池；可以使用spring.datasource.type指定自定义的数据源类型；\n\n2、SpringBoot默认可以支持；\n\n```text\norg.apache.tomcat.jdbc.pool.DataSource    HikariDataSource   BasicDataSource、\n```\n\n3、自定义数据源类型\n\n```java\n/**\n * Generic DataSource configuration.\n */\n@ConditionalOnMissingBean(DataSource.class)\n@ConditionalOnProperty(name = \"spring.datasource.type\")\nstatic class Generic {\n\n   @Bean\n   public DataSource dataSource(DataSourceProperties properties) {\n       //使用DataSourceBuilder创建数据源，利用反射创建响应type的数据源，并且绑定相关属性\n      return properties.initializeDataSourceBuilder().build();\n   }\n}\n```\n\n4、`DataSourceInitializer：ApplicationListener`\n\nrunSchemaScripts();运行建表语句；\n\n```java\n\tprivate void runSchemaScripts() {\n\t\tList<Resource> scripts = getScripts(\"spring.datasource.schema\",\n\t\t\t\tthis.properties.getSchema(), \"schema\");\n\t\tif (!scripts.isEmpty()) {\n\t\t\tString username = this.properties.getSchemaUsername();\n\t\t\tString password = this.properties.getSchemaPassword();\n\t\t\trunScripts(scripts, username, password);\n\t\t\ttry {\n\t\t\t\tthis.applicationContext\n\t\t\t\t\t\t.publishEvent(new DataSourceInitializedEvent(this.dataSource));\n\t\t\t\t// The listener might not be registered yet, so don't rely on it.\n\t\t\t\tif (!this.initialized) {\n\t\t\t\t\trunDataScripts();\n\t\t\t\t\tthis.initialized = true;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (IllegalStateException ex) {\n\t\t\t\tlogger.warn(\"Could not send event to complete DataSource initialization (\"\n\t\t\t\t\t\t+ ex.getMessage() + \")\");\n\t\t\t}\n\t\t}\n\t}\n```\n\nrunDataScripts();运行插入数据的sql语句；\n\n```java\n\tprivate void runDataScripts() {\n\t\tList<Resource> scripts = getScripts(\"spring.datasource.data\",\n\t\t\t\tthis.properties.getData(), \"data\");\n\t\tString username = this.properties.getDataUsername();\n\t\tString password = this.properties.getDataPassword();\n\t\trunScripts(scripts, username, password);\n\t}\n```\n\n```java\n//获取列表\nprivate List<Resource> getScripts(String propertyName, List<String> resources,\n                                  //fallback 就是schema\n\t\t\tString fallback) {\n\t\tif (resources != null) {\n\t\t\treturn getResources(propertyName, resources, true);\n\t\t}\n\t\tString platform = this.properties.getPlatform();\n\t\tList<String> fallbackResources = new ArrayList<String>();\n    \t//如果获不到则从类路径下寻找\".sql\"文件\n\t\tfallbackResources.add(\"classpath*:\" + fallback + \"-\" + platform + \".sql\");\n\t\tfallbackResources.add(\"classpath*:\" + fallback + \".sql\");\n\t\treturn getResources(propertyName, fallbackResources, false);\n\t}\n```\n\n\n\n```reStructuredText\nschema-*.sql、data-*.sql\n默认规则：schema.sql，schema-all.sql；\n可以使用   \n\t  schema:\n      - classpath:department.sql\n      指定位置\n```\n\n```sql\nSET FOREIGN_KEY_CHECKS=0;\n-- ----------------------------\n-- Table structure for department\n-- ----------------------------\nDROP TABLE IF EXISTS `department`;\nCREATE TABLE `department` (\n  `id` int(11) NOT NULL AUTO_INCREMENT,\n  `departmentName` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8;\n```\n\n未启动Run方法之前\n\n![A4vsQs.png](https://s2.ax1x.com/2019/04/08/A4vsQs.png)\n\n![A4vTyR.png](https://s2.ax1x.com/2019/04/08/A4vTyR.png)\n\n![A4vqw6.png](https://s2.ax1x.com/2019/04/08/A4vqw6.png)\n\n如果你想运行指定的sql文件可以在配置文件中指定\n\n![A4xNc9.png](https://s2.ax1x.com/2019/04/08/A4xNc9.png)\n\n执行成功\n\n## JdbcTemplate\n\n操作数据库：自动配置了JdbcTemplate操作数据库\n\n```java\n@Configuration\n@ConditionalOnClass({ DataSource.class, JdbcTemplate.class })\n@ConditionalOnSingleCandidate(DataSource.class)\n@AutoConfigureAfter(DataSourceAutoConfiguration.class)\npublic class JdbcTemplateAutoConfiguration {\n\n\tprivate final DataSource dataSource;\n\n\tpublic JdbcTemplateAutoConfiguration(DataSource dataSource) {\n\t\tthis.dataSource = dataSource;\n\t}\n\n\t@Bean\n\t@Primary\n\t@ConditionalOnMissingBean(JdbcOperations.class)\n\tpublic JdbcTemplate jdbcTemplate() {\n\t\treturn new JdbcTemplate(this.dataSource);\n\t}\n\n\t@Bean\n\t@Primary\n\t@ConditionalOnMissingBean(NamedParameterJdbcOperations.class)\n\tpublic NamedParameterJdbcTemplate namedParameterJdbcTemplate() {\n\t\treturn new NamedParameterJdbcTemplate(this.dataSource);\n\t}\n}\n```\n\n## 数据库准备\n\n![A5pt1g.png](https://s2.ax1x.com/2019/04/08/A5pt1g.png)\n\n ![A5pc3F.png](https://s2.ax1x.com/2019/04/08/A5pc3F.png)\n\n注意要取消application.yml中指定sql的配置,因为这样重新运行表会重新创建,消失。\n\n## 整合Druid数据源\n\n### 简介\n\nDRUID是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、PROXOOL等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池(据说是目前最好的连接池)\n\n### 步骤\n\n在pom文件中加入Druid依赖\n\n```xml\n<dependency>\n    <groupId>com.alibaba</groupId>\n    <artifactId>druid</artifactId>\n    <version>1.1.16</version>\n</dependency>\n```\n\n### 测试\n\n![A5CwF0.png](https://s2.ax1x.com/2019/04/08/A5CwF0.png)\n\n以成功更换\n\nDebug\n\n![A5ChY6.png](https://s2.ax1x.com/2019/04/08/A5ChY6.png)\n\n\n\n发现配置未生效，我们需要编写一个类来实现它。\n\n```java\npackage com.hph.springboot.config;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.sql.DataSource;\n\n@Configuration\npublic class DruidConfig {\n\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    @Bean\n    public DataSource druid(){\n        return  new DruidDataSource();\n\n    }\n}\n```\n\n\n\n![A5CLTI.png](https://s2.ax1x.com/2019/04/08/A5CLTI.png)\n\n### 监控\n\n在`DruidConfig`中\n\n```java\npackage com.hph.springboot.config;\n\nimport com.alibaba.druid.pool.DruidDataSource;\nimport com.alibaba.druid.support.http.StatViewServlet;\nimport com.alibaba.druid.support.http.WebStatFilter;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.boot.web.servlet.ServletRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport javax.sql.DataSource;\nimport java.util.Arrays;\nimport java.util.HashMap;\nimport java.util.Map;\n\n@Configuration\npublic class DruidConfig {\n\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    @Bean\n    public DataSource druid(){\n        return  new DruidDataSource();\n    }\n\n    //配置Druid的监控\n    //1、配置一个管理后台的Servlet\n    @Bean\n    public ServletRegistrationBean statViewServlet(){\n        ServletRegistrationBean bean = new ServletRegistrationBean(new StatViewServlet(), \"/druid/*\");\n        Map<String,String> initParams = new HashMap<>();\n\n        initParams.put(\"loginUsername\",\"admin\");\n        initParams.put(\"loginPassword\",\"123456\");\n        initParams.put(\"allow\",\"\");//默认就是允许所有访问\n        initParams.put(\"deny\",\"192.168.1.110\");\n\n        bean.setInitParameters(initParams);\n        return bean;\n    }\n\n\n    //2、配置一个web监控的filter\n    @Bean\n    public FilterRegistrationBean webStatFilter(){\n        FilterRegistrationBean bean = new FilterRegistrationBean();\n        bean.setFilter(new WebStatFilter());\n\n        Map<String,String> initParams = new HashMap<>();\n        initParams.put(\"exclusions\",\"*.js,*.css,/druid/*\");\n\n        bean.setInitParameters(initParams);\n\n        bean.setUrlPatterns(Arrays.asList(\"/*\"));\n\n        return  bean;\n    }\n}\n\n```\n\n![A5i3rQ.png](https://s2.ax1x.com/2019/04/08/A5i3rQ.png)\n\n![A5iUP0.png](https://s2.ax1x.com/2019/04/08/A5iUP0.png)\n\n![A5id2T.png](https://s2.ax1x.com/2019/04/08/A5id2T.png)\n\n![A5iwxU.png](https://s2.ax1x.com/2019/04/08/A5iwxU.png)\n\nWeb中也可以查看我们JDBC的执行次数。\n\n\n\n\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"DockerFile","url":"/2019/04/07/DockerFile/","content":"\n {{ \"DokerFile相关知识\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nDockerfile是用来构建Docker镜像的构建文件，是由一系列命令和参数构成的脚本。\n\n1：每条保留字指令都必须为大写字母且后面要跟随至少一个参数\n\n2：指令按照从上到下，顺序执行\n\n3：#表示注释\n\n4：每条指令都会创建一个新的镜像层，并对镜像进行提交\n\n## 步骤\n\n1. 编写Dockerfile文件\n2. docker build\n3. docker run\n\n## 用法\n\ndocker build命令从Dockerfile和上下文构建映像。 构建的上下文是指定位置PATH或URL处的文件集。 PATH是本地文件系统上的目录。 URL是Git存储库位置。 递归处理上下文。 因此，PATH包括任何子目录，URL包括存储库及其子模块。\n\n构建由Docker守护程序运行，而不是由CLI运行。 构建过程所做的第一件事是将整个上下文（递归地）发送到守护进程。 在大多数情况下，最好以空目录作为上下文，并将Dockerfile保存在该目录中。 仅添加构建Dockerfile所需的文件。\n\n注意：`不要将根目录/ /用作PATH`，因为它会导致构建将硬盘驱动器的全部内容传输到Docker守护程序。\n\n要在构建上下文中使用文件，Dockerfile引用指令中指定的文件，例如COPY指令。 要提高构建的性能，请通过将.dockerignore文件添加到上下文目录来排除文件和目录。传统意义上上，Dockerfile称为Dockerfile，位于上下文的根目录中。 您可以将-f标志与docker build一起使用，以指向文件系统中任何位置的Dockerfile。\n\n从版本18.09开始，Docker支持一个新的后端，用于执行[moby/buildkit](https://github.com/moby/buildkit)项目提供的构建。与旧的实现相比，BuildKit后端提供了许多好处。例如，BuildKit可以:\n\n- Detect and skip executing unused build stages\n- Parallelize building independent build stages\n- Incrementally transfer only the changed files in your build context between builds\n- Detect and skip transferring unused files in your build context\n- Use external Dockerfile implementations with many new features\n- Avoid side-effects with rest of the API (intermediate images and containers)\n- Prioritize your build cache for automatic pruning\n\n### 保留字指令\n\n![AfrmDS.png](https://s2.ax1x.com/2019/04/07/AfrmDS.png)\n\n更详尽的内容参考[Docker官网指南](https://docs.docker.com/engine/reference/builder/)\n\n## 案例1\n\n定制一个Centos镜像\n\n![AfrYuT.png](https://s2.ax1x.com/2019/04/07/AfrYuT.png)\n\n在centos镜像中默认登录的位置为 跟目录 不支持 vim  不支持 ifconfig。我们可以在自定义镜像文件中添加这些。\n\n```shell\n#Description: test image\nFROM centos:latest\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n # 设置环境变量\nENV mypath /usr/local\n# 设置工作目录\nWORKDIR ${mypath} \n# 运行指令\nRUN yum -y install vim\n# 运行指令\nRUN yum -y install net-tools \n\nEXPOSE 80\n\nCMD echo $mypath\nCMD echo \"success --------OK\"\n#指定bash\nCMD /bin/bash\n```\n\n```shell\ndocker build -f Dockerfile -t mycentos:v1.0 ./\n```\n\n![AfcJJJ.png](https://s2.ax1x.com/2019/04/07/AfcJJJ.png)\n\n倒着加载镜像层顺着执行 。\n\n![AfcUQ1.png](https://s2.ax1x.com/2019/04/07/AfcUQ1.png)\n\n\n\n![AfcbSs.png](https://s2.ax1x.com/2019/04/07/AfcbSs.png)\n\n## 案例2\n\nDockerfile 中可以有多个 CMD 指令，但只有最后一个生效，CMD 会被 docker run 之后的参数替换\n\n![AfRSz9.png](https://s2.ax1x.com/2019/04/07/AfRSz9.png)\n\n![AfRPqx.png](https://s2.ax1x.com/2019/04/07/AfRPqx.png)\n\n```dockerfile\n#Description: test image\nFROM centos:latest\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n#设置指定命令\nRUN yum install -y curl\n\nCMD [\"curl\", \"-s\",\"http://ip.cn\"]\n```\n\n![Af7gpV.png](https://s2.ax1x.com/2019/04/07/Af7gpV.png)\n\n​\t定制一个查询ip的Centos\n\n```dockerfile\n#Description: test image\nFROM centos:latest\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n#设置指定命令\nRUN yum install -y curl\nCMD [\"curl\",\"-s\",\"https://ip.cn\"]\n```\n\n```shell\ndocker build -f ./mydocker/Dockerfile2 -t myip ./\n```\n\n![AfqulV.png](https://s2.ax1x.com/2019/04/07/AfqulV.png)\n\n![AfHhgf.png](https://s2.ax1x.com/2019/04/07/AfHhgf.png)\n\n![AfXhnK.png](https://s2.ax1x.com/2019/04/07/AfXhnK.png)\n\n### 改进\n\n```dockerfile\n#Description: test image\nFROM centos:latest\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n#设置指定命令\nRUN yum install -y curl\nENTRYPOINT [\"curl\",\"-s\",\"https://ip.cn\"]\n```\n\n```shell\n[root@localhost mydocker]# docker build -f Dockerfile3 -t myip2plus .\n```\n\nENTRYPOINT  相当于在命令curl 后面添加了一个参数。\n\n![AfXoAe.png](https://s2.ax1x.com/2019/04/07/AfXoAe.png)\n\n![AfXOjP.png](https://s2.ax1x.com/2019/04/07/AfXOjP.png)\n\nCMD 运行容器时参数会被覆盖。 ENTRYPOINT会被追加。\n\n## 案例3\n\nDocker \n\n```dockerfile\n[root@localhost mydocker]# vim Dockerfile4\n#Description: test image\nFROM centos:latest\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n#设置指定命令\nRUN yum install -y curl\nENTRYPOINT [\"curl\",\"-s\",\"https://ip.cn\"]\n\nONBUILD RUN echo \"father image onbuild----886\"\n```\n\n ![Afj1jx.png](https://s2.ax1x.com/2019/04/07/Afj1jx.png) \n\n```dockerfile\n[root@localhost mydocker]# vim Dockerfile5\n#Description: test image\nFROM myip_father\nMAINTAINER \"bigdataxiaohan <467008580@qq.com>\"\n\n#设置指定命令\nRUN yum install -y curl\nENTRYPOINT [\"curl\",\"-s\",\"https://ip.cn\"]\n```\n\n\n\n![AfjOPJ.png](https://s2.ax1x.com/2019/04/07/AfjOPJ.png)\n\n\n\n## 案例4\n\n自定义Tomcat\n\n![AhksAI.png](https://s2.ax1x.com/2019/04/07/AhksAI.png)\n\n```dockerfile\n[root@localhost tomcat9]# vim Dockerfile\nFROM         centos\n#把宿主机当前上下文的c.txt拷贝到容器/usr/local/路径下\nCOPY HostMachine.txt /usr/local/HostMachine.txt\n#把java与tomcat添加到容器中\nADD jdk-8u201-linux-x64.tar.gz /usr/local/\nADD apache-tomcat-9.0.17.tar.gz /usr/local/\n##安装vim编辑器\nRUN yum -y install vim\n##设置工作访问时候的WORKDIR路径，登录落脚点\nENV MYPATH /usr/local\nWORKDIR $MYPATH\n##配置java与tomcat环境变量\nENV JAVA_HOME /usr/local/jdk1.8.0_201\nENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar\nENV CATALINA_HOME /usr/local/apache-tomcat-9.0.17\nENV CATALINA_BASE /usr/local/apache-tomcat-9.0.17\nENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin\n##容器运行时监听的端口\n#EXPOSE  8080\n##启动时运行tomcat\n## ENTRYPOINT [\"/usr/local/apache-tomcat-9.0.17/bin/startup.sh\" ]\n## CMD [\"/usr/local/apache-tomcat-9.0.17/bin/catalina.sh\",\"run\"]\nCMD /usr/local/apache-tomcat-9.0.17/bin/startup.sh && tail -F /usr/local/apache-tomcat-9.0.17/logs/catalina.out\n```\n\n![AhydxI.png](https://s2.ax1x.com/2019/04/07/AhydxI.png)\n\n![Ahy6Ig.png](https://s2.ax1x.com/2019/04/07/Ahy6Ig.png)\n\n![Ahy4s0.png](https://s2.ax1x.com/2019/04/07/Ahy4s0.png)\n\n验证自定义Tomcat9完成。\n\n![AhyoZT.png](https://s2.ax1x.com/2019/04/07/AhyoZT.png)\n\n## 部署Web\n\n![Ah6tmV.png](https://s2.ax1x.com/2019/04/07/Ah6tmV.png)\n\n```jsp\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n  <head>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n    <title>Insert title here</title>\n  </head>\n  <body>\n    -----------welcome------------\n    <%=\"i am in docker tomcat self Test\"%>\n    <br>\n    <br>\n    <% System.out.println(\"=============docker tomcat self\");%>\n  </body>\n</html>\n```\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xmlns=\"http://java.sun.com/xml/ns/javaee\"\n  xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\"\n  id=\"WebApp_ID\" version=\"2.5\">\n  <display-name>test</display-name>\n</web-app>\n```\n\n![Ah6yOx.png](https://s2.ax1x.com/2019/04/07/Ah6yOx.png)\n\n修改a.jsp\n\n```jsp\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\" pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\" \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n  <head>\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n    <title>Insert title here</title>\n  </head>\n  <body>\n    -----------welcome------------<br/>\n    <%=\"i am in docker tomcat self Test\"%><br/>\n    <%=\"这是我在宿主机中修改后的Docker中Tomcat文件\"%>\n    <br>\n    <br>\n    <% System.out.println(\"=============docker tomcat self\");%>\n  </body>\n</html>\n```\n\n![AhoZdA.png](https://s2.ax1x.com/2019/04/07/AhoZdA.png)\n\n可以在`/mydockerfile/tomcat9/logs/`目录中查看logs文件。\n\n## 图解\n\n![AhoseJ.png](https://s2.ax1x.com/2019/04/07/AhoseJ.png)\n\n\n\n\n\n\n\n","tags":["Docker"],"categories":["容器"]},{"title":"Docker存储卷","url":"/2019/04/06/Docker存储卷/","content":"\n {{ \"Doker的存储卷的相关内容\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n简介\n\nDocker镜像是由多个只读层添加而成，启动容器时，Docker加载值读镜像层并在镜像栈顶部添加一个读写层。\n\n如果运行中的容器修改了现有的一个已经存在的文件，那么该文件将会在读写层下面的只读层复制到读写层，该文件的只读版本仍然存在，只是已经被读写曾中该文件的副本所隐层，这就是我们所说的写时复制，Elasticsearch也是用了写时复制。和这个略有不同。\n\n默认情况下，容器不使用任何 volume，此时，容器的数据被保存在容器之内，它只在容器的生命周期内存在，关闭并重启容器，其数据不会受到影响，但是删除Docker容器，数据将会全部丢失。当然，也可以使用 docker commit 命令将它持久化为一个新的镜像。\n\n![AW0cOe.png](https://s2.ax1x.com/2019/04/06/AW0cOe.png)\n\n## 问题\n\n- 存储于联合联合文件系统中，不易于宿主机访问；\n- 容器间数据共享不便。\n- 删除容器会导致数据全部丢失。\n\n## 卷(volume)\n\n“卷”是容器上的一个或者多个目录，此目录可以绕过联合文件系统，与宿主机上的某目录“绑定(关联)”。\n\nVolume于容器化初始化之时即会创建，由base image提供的卷中的数据会于此期间完成复制。\n\n数据卷可以在容器之间共享和重用。\n\n对数据卷的更改是直接进行的。\n\n更新映像时，不包括对数据卷的更改。\n\n即使删除容器本身，数据卷也会保持\n\nVolume的初衷时独立于容器的生命周期实现数据持久化，因此删除容器的时候既不会删除卷，也不会对哪怕未被引用的卷当作垃圾回收操作。\n\n![AWBmp6.png](https://s2.ax1x.com/2019/04/06/AWBmp6.png)\n\n卷为docker提供了独立于容器的数据管理机制，我们可以把“镜像”看作成为静态的文件，例如“程序”，把卷类比为动态内容，比如“数据”，镜像可以重用，而卷可以共享；\n\n卷实现了“程序(镜像)”和“数据(卷)”分离，以及“程序(镜像)”和“制作镜像的主机”分离，用户制作镜像时无需再考虑镜像运行的容器所在的主机的环境。\n\n![AWgrsP.png](https://s2.ax1x.com/2019/04/06/AWgrsP.png)\n\n考虑到容器应用是需要持久存储数据的，可能是有状态的，如果考虑使用NFS做反向代理是没必要存储数据的，应用可以分为有状态和无状态，有状态是当前这次连接请求处理一定此前的处理是有关联的，无状态是前后处理是没有关联关系的，大多数有状态应用都是数据持久存储的，如mysql,redis有状态应用，在持久存储，如nginx作为反向代理是无状态应用，tomcat可以是有状态的，但是它有可能不需要持久存储数据，因为它的session都是保存在内存中就可以的，会导致节点宕机而丢失。session，如果有必要应该让它持久，这也算是有状态的。\n\n![AWgtaD.png](https://s2.ax1x.com/2019/04/06/AWgtaD.png)\n\n对于Docker来说有两种类型的卷，每种类型都在容器中创业中存在一个挂载点，但其再宿主机上上的位置会有所不同：\n\nBind mount volume(绑定挂载卷)：宿主机上的路径需要人工指定。容器中的路径需要指定。\n\nDokcer Managed volume(Docker管理卷)：Docker守护进程在宿主机文件系统中所拥有的一部分中创建托管卷。自动自动\n\n### Docker管理管卷\n\n```shell\n[root@localhost ~]# docker run --name b2 -it -v /data  busybox\n```\n\n```shell\n[root@localhost ~]# docker inspect b2\n```\n\n![AWHtds.png](https://s2.ax1x.com/2019/04/06/AWHtds.png)\n\n![AWHDQU.png](https://s2.ax1x.com/2019/04/06/AWHDQU.png)\n\n容器中：\n\n![AWHcw9.png](https://s2.ax1x.com/2019/04/06/AWHcw9.png)\n\n![AWHz6S.png](https://s2.ax1x.com/2019/04/06/AWHz6S.png)\n\n![AWbM79.png](https://s2.ax1x.com/2019/04/06/AWbM79.png)\n\n### 绑定挂载卷\n\n```shell\n[root@localhost ~]# docker run --name b2 -it --rm -v /data/volumes/b2:/data  busybox  #运行一个b2名称的busybox的容器 再宿主机上的位置为/data/volumes/b2 再docker容器中的位置为data \n```\n\n```shell\n[root@localhost ~]# docker inspect b2\n```\n\n![AWby1f.png](https://s2.ax1x.com/2019/04/06/AWby1f.png)\n\n宿主机：\n\n![AWbhAs.png](https://s2.ax1x.com/2019/04/06/AWbhAs.png)\n\nDocker：\n\n![AWb5hq.png](https://s2.ax1x.com/2019/04/06/AWb5hq.png)\n\nDocker查询\n\n![AWbvNR.png](https://s2.ax1x.com/2019/04/06/AWbvNR.png)\n\n![AWqsa9.png](https://s2.ax1x.com/2019/04/06/AWqsa9.png)\n\n![AWLGLD.png](https://s2.ax1x.com/2019/04/06/AWLGLD.png)\n\n![AWLLk9.png](https://s2.ax1x.com/2019/04/06/AWLLk9.png)\n\n### 复制卷设置\n\n当我们希望练习比较紧密的容器放置在一起的时候我们可以使用复制卷设置，让他们共享Network、IPC、UTS还可以共享存储卷。比如 Tomcat和Nginx这种的，所以再以后我们可以专门做一个基础卷。\n\n```shell\n[root@localhost ~]# docker run --name infracon -it  -v /data/infracon/volume/:/data/web/html busybox  #创建基础卷\n```\n\n```shell\n[root@localhost b2]# docker run --name nginx --network container:infracon --volumes-from infracon -it busybox          #复制infracon的卷配置\n```\n\n```shell\n[root@localhost ~]# docker inspect infracon\n```\n\n![AWXBqS.png](https://s2.ax1x.com/2019/04/06/AWXBqS.png)\n\n![AWXhrT.png](https://s2.ax1x.com/2019/04/06/AWXhrT.png)\n\n```shell\n[root@localhost ~]# docker inspect nginx\n```\n\n\n\n![AWXrVg.png](https://s2.ax1x.com/2019/04/06/AWXrVg.png)\n\n这样基础支持的容器我们就创建起来。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"],"categories":["容器"]},{"title":"Dokcer网络","url":"/2019/04/06/Dokcer网络/","content":"\n {{ \"Docker的网络的原理\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\nDocker网络功能介绍\n\nDocker的网络实现是利用Linux上的网络命名空间、网桥和虚拟网络设备 ( VETH ）等来实现。 默认情况下， Docker安装完成后会创建一个网桥docker0。 Docker中的网络接口默认都是虚拟的网络接口。Docker 容器网络在本地主机和容器内分别创建个虚拟接口，并让它们彼此连通。对于docker0我们可以把它当成虚拟的交换机或者虚拟的网卡。\n\n![AWUige.png](https://s2.ax1x.com/2019/04/06/AWUige.png)\n\n关于命名空间更加详细的内容可以参考[InfoQ](https://www.infoq.cn/article/docker-kernel-knowledge-namespace-resource-isolation)相关内容。\n\n## 创建容器步骤\n\n1. 创建一对虚拟接口，分别放到本地主机和新容器中。\n2.  本地主机一端桥接到默认的 docker0 或指定网桥上，并具有一个唯一的名字，如veth526a417。\n3.  容器一端放到新容器中，并修改名称为eth0，这个接口只在容器的命名空间可见。\n4.  从网桥可用地址段中获取一个空闲地址IP，分配给容器的eth0，并配置默认路由到桥接网卡 veth526a417。 \n\n![AWUKC8.png](https://s2.ax1x.com/2019/04/06/AWUKC8.png)\n\n5. 完成上述步骤之后， 容器就可以使用eth0虚拟网卡来连接其他容器和其他网络。\n\n\n![AWUEDA.png](https://s2.ax1x.com/2019/04/06/AWUEDA.png)\n\n当Docker 启动时，会自动在主机上创建一个docker0 虚拟网桥，实际上是Linux 的一个bridge，可以理解为一个软件交换机。它会在挂载到它的接口之间进行转发，同时Docker随机分配一个本地未占用的私有网段 （ 在RFCl918中定义 ）中 的一个地址给 docker0接口 。 例如典型的172.17.42.1，掩码为 255.255.0.0。此后启动的容器内的接口也会自动分配一个同一网段 （ 172.17.0.0/16 ）的地址。 当创建一个Docker容器的时候，同时会创建一对veth pair 接口 （ 当数据包发送到一个接口时，另外一个接口也可以收到相同的数据包 ）。 这对接口一端在容器内，即eth0; 另一端在本地，并被挂载到 docker0网桥，名称以 veth开头（ 例如veth526a417） 通过这种方式，主机可以跟容器通信，容器之间也可以相互通信。 此时， Docke 就创建了主机和所有容器之间的一个虚拟共享网络。\n\n![AWaJQe.png](https://s2.ax1x.com/2019/04/06/AWaJQe.png)\n\n由于Docker网络比较复杂,因此本人只学习了其基本原理。\n\n## 案例(Docker安装Tomcat)\n\n```shell\ndocker pull tomcat  #拉取镜像\ndocker images|grep tomcat  #查看镜像\n```\n\n![AWw6Vs.png](https://s2.ax1x.com/2019/04/06/AWw6Vs.png)\n\n![AWwjxO.png](https://s2.ax1x.com/2019/04/06/AWwjxO.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Docker"],"categories":["容器"]},{"title":"Docker的基础命令","url":"/2019/04/05/Docker的基础命令/","content":"\n {{ \"Doker的基础命令\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## docker\n\n```shell\n[root@localhost ~]# docker\n\nUsage:  docker [OPTIONS] COMMAND\n\nA self-sufficient runtime for containers\n\nOptions:\n      --config string      Location of client config files (default \"/root/.docker\")\n  -D, --debug              Enable debug mode\n  -H, --host list          Daemon socket(s) to connect to\n  -l, --log-level string   Set the logging level (\"debug\"|\"info\"|\"warn\"|\"error\"|\"fatal\") (default \"info\")\n      --tls                Use TLS; implied by --tlsverify\n      --tlscacert string   Trust certs signed only by this CA (default \"/root/.docker/ca.pem\")\n      --tlscert string     Path to TLS certificate file (default \"/root/.docker/cert.pem\")\n      --tlskey string      Path to TLS key file (default \"/root/.docker/key.pem\")\n      --tlsverify          Use TLS and verify the remote\n  -v, --version            Print version information and quit\n \nManagement Commands:     #管理命令\n  builder     Manage builds\n  config      Manage Docker configs  #管理配置\n  container   Manage containers      #管理容器 \n  engine      Manage the docker engine   # 管理引擎\n  image       Manage images               #管理插件\n  network     Manage networks\n  node        Manage Swarm nodes            #管理节点\n  plugin      Manage plugins\n  secret      Manage Docker secrets\n  service     Manage services\n  stack       Manage Docker stacks\n  swarm       Manage Swarm\n  system      Manage Docker\n  trust       Manage trust on Docker images\n  volume      Manage volumes\n\n## docker 兼容了新旧的使用格式 建议使用分组管理.\n\nCommands:\n  attach      Attach local standard input, output, and error streams to a running container\n  build       Build an image from a Dockerfile\n  commit      Create a new image from a container's changes\n  cp          Copy files/folders between a container and the local filesystem\n  create      Create a new container\n  diff        Inspect changes to files or directories on a container's filesystem\n  events      Get real time events from the server\n  exec        Run a command in a running container\n  export      Export a container's filesystem as a tar archive\n  history     Show the history of an image\n  images      List images\n  import      Import the contents from a tarball to create a filesystem image\n  info        Display system-wide information\n  inspect     Return low-level information on Docker objects\n  kill        Kill one or more running containers\n  load        Load an image from a tar archive or STDIN\n  login       Log in to a Docker registry\n  logout      Log out from a Docker registry\n  logs        Fetch the logs of a container\n  pause       Pause all processes within one or more containers\n  port        List port mappings or a specific mapping for the container\n  ps          List containers\n  pull        Pull an image or a repository from a registry\n  push        Push an image or a repository to a registry\n  rename      Rename a container\n  restart     Restart one or more containers\n  rm          Remove one or more containers\n  rmi         Remove one or more images\n  run         Run a command in a new container\n  save        Save one or more images to a tar archive (streamed to STDOUT by default)\n  search      Search the Docker Hub for images\n  start       Start one or more stopped containers\n  stats       Display a live stream of container(s) resource usage statistics\n  stop        Stop one or more running containers\n  tag         Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n  top         Display the running processes of a container\n  unpause     Unpause all processes within one or more containers\n  update      Update configuration of one or more containers\n  version     Show the Docker version information\n  wait        Block until one or more containers stop, then print their exit codes\n\nRun 'docker COMMAND --help' for more information on a command.\n```\n\n ## 查看版本\n\n```shell\n[root@localhost ~]# docker version\nClient:              #客户端\n Version:           18.09.4    #docker版本\n API version:       1.39     #Docker接口版本\n Go version:        go1.10.8  # Docker Go版本\n Git commit:        d14af54266\n Built:             Wed Mar 27 18:34:51 2019\n OS/Arch:           linux/amd64  #  系统\n Experimental:      false        # 是否为企业版\n \nServer: Docker Engine - Community   \n Engine:\n  Version:          18.09.4\n  API version:      1.39 (minimum version 1.12)\n  Go version:       go1.10.8\n  Git commit:       d14af54\n  Built:            Wed Mar 27 18:04:46 2019\n  OS/Arch:          linux/amd64\n  Experimental:     false\nYou have new mail in /var/spool/mail/root\n```\n\n## 详细信息\n\n```shell\n[root@localhost ~]# docker info\nContainers: 0   #容器个数\n Running: 0    # 运行\n Paused: 0    # 暂停\n Stopped: 0   # 停止\nImages: 0      # 镜像\nServer Version: 18.09.4  #服务器版本\nStorage Driver: overlay2  #存储驱动后端 很重要  分层构建和联合挂载 需要专门的文件驱动在之前可能是使用的DM 性能奇差\n Backing Filesystem: xfs\n Supports d_type: true\n Native Overlay Diff: true\nLogging Driver: json-file\nCgroup Driver: cgroupfs\nPlugins:         #  插件\n Volume: local         #卷\n Network: bridge host macvlan null overlay  # 网络插件\n Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog  #日志插件\nSwarm: inactive\nRuntimes: runc\nDefault Runtime: runc\nInit Binary: docker-init\ncontainerd version: bb71b10fd8f58240ca47fbb579b9d1028eea7c84\nrunc version: 2b18fe1d885ee5083ef9f0838fee39b62d653e30\ninit version: fec3683\nSecurity Options:  #安全选项\n seccomp     \n  Profile: default  #默认选项\nKernel Version: 3.10.0-957.10.1.el7.x86_64\nOperating System: CentOS Linux 7 (Core)\nOSType: linux\nArchitecture: x86_64\nCPUs: 1\nTotal Memory: 3.683GiB\nName: localhost.localdomain\nID: BUTI:XQW2:K5UQ:SNVB:EC67:GBOV:LHZD:FI2G:UFL2:TU4H:FSCN:PQAU\nDocker Root Dir: /var/lib/docker\nDebug Mode (client): false\nDebug Mode (server): false\nRegistry: https://index.docker.io/v1/\nLabels:\nExperimental: false\nInsecure Registries:\n 127.0.0.0/8\nRegistry Mirrors:\n https://registry.docker-cn.com/  #加速镜像配置\nLive Restore Enabled: false\nProduct License: Community Engine\n```\n\n##  Docker Search\n\n搜索 Docker hub 中的 nginx 镜像\n\n```shell\ndocker search nginx\n```\n\n![ARtYjA.png](https://s2.ax1x.com/2019/04/05/ARtYjA.png)\n\n## Docker pull\n\n多个层级每一层分别下载\n\n![ARtaHP.png](https://s2.ax1x.com/2019/04/05/ARtaHP.png)\n\n## docker images\n\nlast表示时最新的\n\n```shell\n[root@localhost docker]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnginx               latest              2bcb04bdb83f        9 days ago          109MB\n[root@localhost docker]# docker image ls\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nnginx               latest              2bcb04bdb83f        9 days ago          109MB\n```\n\n##  docker 卸载镜像\n\n```shell\n[root@localhost docker]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nbusybox             latest              af2f74c517aa        2 days ago          1.2MB\nnginx               latest              2bcb04bdb83f        9 days ago          109MB\n[root@localhost docker]# docker image rm busybox     #第一种命令\nUntagged: busybox:latest \nUntagged: busybox@sha256:f79f7a10302c402c052973e3fa42be0344ae6453245669783a9e16da3d56d5b4\nDeleted: sha256:af2f74c517aac1d26793a6ed05ff45b299a037e1a9eefeae5eacda133e70a825\nDeleted: sha256:0b97b1c81a3200e9eeb87f17a5d25a50791a16fa08fc41eb94ad15f26516ccea\n[root@localhost docker]# docker rmi nginx      #第二种命令\nUntagged: nginx:latest\nUntagged: nginx@sha256:dabecc7dece2fff98fb00add2f0b525b7cd4a2cacddcc27ea4a15a7922ea47ea\nDeleted: sha256:2bcb04bdb83f7c5dc30f0edaca1609a716bda1c7d2244d4f5fbbdfef33da366c\nDeleted: sha256:dfce9ec5eeabad339cf90fce93b20f179926d5819359141e49e0006a52c066ca\nDeleted: sha256:166d13b0f0cb542034a2aef1c034ee2271e1d6aaee4490f749e72d1c04449c5b\nDeleted: sha256:5dacd731af1b0386ead06c8b1feff9f65d9e0bdfec032d2cd0bc03690698feda\n[root@localhost docker]# docker images\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\n[root@localhost docker]#\n\n```\n\n## Docker 创建一个容器\n\n```shell\n[root@localhost docker]# docker container\n\nUsage:  docker container COMMAND\n\nManage containers\n\nCommands:\n  attach      Attach local standard input, output, and error streams to a running container\n  commit      Create a new image from a container's changes\n  cp          Copy files/folders between a container and the local filesystem\n  create      Create a new container\n  diff        Inspect changes to files or directories on a container's filesystem\n  exec        Run a command in a running container\n  export      Export a container's filesystem as a tar archive\n  inspect     Display detailed information on one or more containers\n  kill        Kill one or more running containers\n  logs        Fetch the logs of a container\n  ls          List containers\n  pause       Pause all processes within one or more containers\n  port        List port mappings or a specific mapping for the container\n  prune       Remove all stopped containers\n  rename      Rename a container\n  restart     Restart one or more containers\n  rm          Remove one or more containers\n  run         Run a command in a new container\n  start       Start one or more stopped containers\n  stats       Display a live stream of container(s) resource usage statistics\n  stop        Stop one or more running containers\n  top         Display the running processes of a container\n  unpause     Unpause all processes within one or more containers\n  update      Update configuration of one or more containers\n  wait        Block until one or more containers stop, then print their exit codes\n\nRun 'docker container COMMAND --help' for more information on a command.\n```\n\n```shell\n#启动一个nginx容器\n[root@localhost ~]# docker run --name nginx-docker1 -d nginx  #-d为后台运行\naaed74bf582677ed39f186a3967f4a4efb11836a164b78f1d0b157fb2f26dcaa\n[root@localhost ~]# docker inspect nginx-docker1             #查看docker绑定地址 为172.17.0.2\n```\n\n![ARU8OA.png](https://s2.ax1x.com/2019/04/05/ARU8OA.png)\n\n![ARUcT0.png](https://s2.ax1x.com/2019/04/05/ARUcT0.png)\n\n### Redis\n\n即使本地没有Redis的镜像我们依然可以安装。\n\n```shell\ndocker run --name Redis -d redis:4-alpine\n```\n\n进入终端\n\n```shell\ndocker exec -it kvsotre /bin/sh     #进入容器中\n```\n\n![ARdMKH.png](https://s2.ax1x.com/2019/04/05/ARdMKH.png)\n\n## 查看日志\n\n```shell\ndocker logs kvstore  #产看kvstore的日志\n```\n\n![ARd3VI.png](https://s2.ax1x.com/2019/04/05/ARd3VI.png)\n\n## 常用命令\n\n![ARddMQ.png](https://s2.ax1x.com/2019/04/05/ARddMQ.png)\n\n## 镜像生成途径\n\n![AR0fER.png](https://s2.ax1x.com/2019/04/05/AR0fER.png)\n\n### 基于容器\n\n1.创建一个busybox的容器\n\n```shell\n[root@localhost ~]# docker run --name b1 -it busybox\n/ # mkdir -p /data/html\n/ # vi /data/html/index.html\n<h1>Busybox httpd Server </h1>\n```\n\n```shell\n[root@localhost ~]# docker commit  -h\nFlag shorthand -h has been deprecated, please use --help\n\nUsage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n\nCreate a new image from a container's changes\n\nOptions:\n  -a, --author string    Author (e.g., \"John Hannibal Smith <hannibal@a-team.com>\")\n  -c, --change list      Apply Dockerfile instruction to the created image\n  -m, --message string   Commit message   \n  -p, --pause            Pause container during commit (default true) #暂停镜像文件\n```\n\n![ARIFLn.png](https://s2.ax1x.com/2019/04/05/ARIFLn.png)\n\n为了引用方便我们可以给镜像打上标签.\n\n```shell\n[root@localhost ~]# docker tag --help\n\nUsage:  docker tag SOURCE_IMAGE[:TAG] TARGET_IMAGE[:TAG]\n\nCreate a tag TARGET_IMAGE that refers to SOURCE_IMAGE\n```\n\n![ARIz01.png](https://s2.ax1x.com/2019/04/05/ARIz01.png)\n\n![ARo11g.png](https://s2.ax1x.com/2019/04/05/ARo11g.png)\n\n ![ARo2Ax.png](https://s2.ax1x.com/2019/04/05/ARo2Ax.png)\n\n```json\ndocker inspect busybox\n\"Cmd\": [\n                \"/bin/sh\",\n                \"-c\",\n                \"#(nop) \",\n                \"CMD [\\\"sh\\\"]\"\n            ],\n```\n\n ![ARTFU0.png](https://s2.ax1x.com/2019/04/05/ARTFU0.png)\n\ndocker运行镜像生成了原先保存的镜像文件。\n\n```shell\n[root@localhost ~]# docker commit -h\nFlag shorthand -h has been deprecated, please use --help\n\nUsage:  docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n\nCreate a new image from a container's changes\n\nOptions:\n  -a, --author string    Author (e.g., \"John Hannibal Smith <hannibal@a-team.com>\")  # 附加作者信息\n  -c, --change list      Apply Dockerfile instruction to the created image  #修改原有基础镜像的命令\n  -m, --message string   Commit message    # 提交信息\n  -p, --pause            Pause container during commit (default true)  # 暂停容器\n```\n\n```shell\ndocker commit -a \"bigdataxiaohan <467008580@qq.com>\" -c 'CMD [\"/bin/httpd\",\"-f\",\"-h\",\"/data/html\"]' -p t1 hphblog/httpd:v0.2\n```\n\n![ARTsG8.png](https://s2.ax1x.com/2019/04/05/ARTsG8.png)\n\n启动基于v0.2版本的服务\n\n![ART2rj.png](https://s2.ax1x.com/2019/04/05/ART2rj.png)\n\n![ARTfZn.png](https://s2.ax1x.com/2019/04/05/ARTfZn.png)\n\ndcoker服务可以直接访问。\n\n由于将docker镜像推导Docker hub 需要相同的组织类名 而我在DockerHub 上创建的仓库为bigdataxiaohan/httpd 我们把镜像更改以下名称\n\n```shell\ndocker tag hphblog/httpd:v0.2 bigdataxiaohan/httd:v0.2\n```\n\n然后登录\n\n```shell\n[root@localhost ~]# docker login --help\n\nUsage:  docker login [OPTIONS] [SERVER]\n\nLog in to a Docker registry\n\nOptions:\n  -p, --password string   Password\n      --password-stdin    Take the password from stdin\n  -u, --username string   Username\n```\n\n![AR7wy4.png](https://s2.ax1x.com/2019/04/05/AR7wy4.png)\n\n网络不是很好建议多尝试几次。\n\n```shell\ndocker tag hphblog/httpd:v0.2 bigdataxiaohan/httpd:v0.2\n```\n\n(⊙﹏⊙) 还是放弃吧  我们可以使用腾讯云的Docker镜像仓库\n\n![ARjd4s.png](https://s2.ax1x.com/2019/04/05/ARjd4s.png)\n\n![ARjhCR.png](https://s2.ax1x.com/2019/04/05/ARjhCR.png)\n\n![ARvla4.png](https://s2.ax1x.com/2019/04/05/ARvla4.png)\n\n哎呀妈呀QAQ 终于push成功了\n\n![ARv8i9.png](https://s2.ax1x.com/2019/04/05/ARv8i9.png)\n\n![ARvJR1.png](https://s2.ax1x.com/2019/04/05/ARvJR1.png)\n\n#### Docker 打包\n\n ```shell\n[root@localhost ~]# docker  save --help\n\nUsage:  docker save [OPTIONS] IMAGE [IMAGE...]\n\nSave one or more images to a tar archive (streamed to STDOUT by default)\n\nOptions:\n  -o, --output string   Write to a file, instead of STDOUT\n ```\n\n![ARvTWn.png](https://s2.ax1x.com/2019/04/05/ARvTWn.png)\n\n#### Dokcer加载\n\n```shell\n[root@localhost ~]# docker load --help\n\nUsage:  docker load [OPTIONS]\n\nLoad an image from a tar archive or STDIN\n\nOptions:\n  -i, --input string   Read from tar archive file, instead of STDIN\n  -q, --quiet          Suppress the load output\n```\n\n![ARx7Ae.png](https://s2.ax1x.com/2019/04/05/ARx7Ae.png)\n\n![ARxL9A.png](https://s2.ax1x.com/2019/04/05/ARxL9A.png)\n\n```shell\ndocker load -i myimages.gz\n```\n\n![ARxjjP.png](https://s2.ax1x.com/2019/04/05/ARxjjP.png)\n\n![ARzSHS.png](https://s2.ax1x.com/2019/04/05/ARzSHS.png)\n\n\n\n","tags":["Docker"],"categories":["容器"]},{"title":"Docker初识与安装","url":"/2019/04/04/Docker初识与安装/","content":"\n {{ \"Doker的基本概念和安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 简介\n\nDocker就是虚拟化的一种轻量级替代技术。Docker的容器技术不依赖任何语言、框架或系统，可以将App变成一种标准化的、可移植的、自管理的组件，并脱离服务器硬件在任何主流系统中开发、调试和运行。\n\n通俗来说Docker是在Linux系统上迅速创建一个容器（类似虚拟机）并在容器上部署和运行应用程序，通过配置文件可以轻松实现应用程序的自动化安装、部署和升级，非常方便。因为使用了容器，所以可以很方便的把生产环境和开发环境分开，互不影响。\n\n![A2IMcj.png](https://s2.ax1x.com/2019/04/04/A2IMcj.png)\n\n## Docker的基本概念\n\nDocker是开发人员和系统管理员使用容器开发，部署和运行应用程序的平台。 使用Linux容器部署应用程序称为容器化。 容器不是新生的概念，但它们可以用于轻松部署应用程序。\n\n容器化越来越受欢迎，因为容器是：\n\n灵活：即使是最复杂的应用也可以容器化化。\n\n轻量级：容器利用并共享主机内核。 \n\n可互换：您可以即时部署更新和升级。 \n\n便携式：您可以在本地构建，部署到云，并在任何地方运行。 \n\n可扩展：您可以增加并自动分发容器副本。 \n\n可堆叠：您可以垂直和即时堆叠服务。\n\n![A2OPmQ.png](https://s2.ax1x.com/2019/04/04/A2OPmQ.png)\n\n### Image\n\n- Docker Image是一个极度精简版的Linux程序运行环境，vi这种基本的工具没有，官网的Java镜像包括的东西更少，除非是镜像叠加方式的， 如Centos+Java7。\n- Docker Image是需要定制化Build的一个“安装包”，包括基础镜像+应用的二进制部署包 \n- Docker Image内不建议有运行期需要修改的配置文件 \n- Dockerfile用来创建一个自定义的image,包含了用户指定的软件依赖等。 当前目录下包含Dockerfile,使用命令build来创建新的image \n- Docker Image的最佳实践之一是尽量重用和使用网上公开的基础镜像 \n\n![ARdOQe.png](https://s2.ax1x.com/2019/04/05/ARdOQe.png)\n\n![ARw9FP.png](https://s2.ax1x.com/2019/04/05/ARw9FP.png)\n\n### Container\n\n- Docker Container是Image的实例，共享内核。 \n- Docker Container里可以运行不同Os的Image，比如Ubuntu的或者 Centos 。\n- Docker Container不建议内部开启一个SSHD服务，1.3版本后新增了 docker exec命令进入容器排查问题。 \n-  Docker Container没有IP地址，通常不会有服务端口暴露，是一个封闭的 “盒子/沙箱”。\n\n通过运行镜像启动容器。 镜像是一个可执行包，包含运行应用程序所需的所有内容代码，库、环境变量和配置文件。 容器是镜像的运行时实例，镜像在执行时在内存中变为什么（有状态的镜像或用户进程）。 您可以使用命令docker ps查看正在运行的容器列表，就像在Linux中一样。\n\n容器和虚拟机：\n\n容器在Linux上本机运行，并与其他容器共享主机的内核。它运行一个独立的进程，不占用任何其他可执行文件的内存，使其轻量级。\n\n相比之下，虚拟机（VM）运行一个完整的“客户”操作系统，通过虚拟机管理程序对主机资源进行虚拟访问。通常，VM提供的环境比大多数应用程序需要的资源更多。\n\n![A2XBKU.png](https://s2.ax1x.com/2019/04/04/A2XBKU.png)\n\n\n\n#### 生命周期\n\n![ARN6PO.png](https://s2.ax1x.com/2019/04/05/ARN6PO.png)\n\n[相关资料](https://blog.csdn.net/u010278923/article/details/78751306)\n\n### Daemon\n\nDocker Daemon是创建和运行Container的Linux守护进程，也是Docker 最主要的核心组件。\n\nDocker Daemon 可以理解为Docker Container的Container 。\n\nDocker Daemon可以绑定本地端口并提供Rest API服务，用来远程访问和控制 。\n\n### Registry/Hub\n\nDocker之所以这么吸引人，除了它的新颖的技术外，围绕官方Registry（Docker Hub）的生态圈也是相当吸引人眼球的地方。在Docker Hub上你可以很轻松下载 到大量已经容器化好的应用镜像，即拉即用。这些镜像中，有些是Docker官方维 护的，更多的是众多开发者自发上传分享的。而且你还可以在Docker Hub中绑定你的代码托管系统（目前支持Github和Bitbucket）配置自动生成镜像功能，这样 Docker Hub会在你代码更新时自动生成对应的Docker镜像。 \n\n![ARwymd.png](https://s2.ax1x.com/2019/04/05/ARwymd.png)\n\n![ARwqkq.png](https://s2.ax1x.com/2019/04/05/ARwqkq.png)\n\n### 组件关系\n\nC/S架构：支持IPV4  IPV6 Unix Socket\n\n![ARe2vD.png](https://s2.ax1x.com/2019/04/04/ARe2vD.png)\n\n容器与镜像是Docker主机上非常重要的部分Docker的images来自于Registry（Docker镜像仓库默认Docket Hub）。\n\n从Docker hub 上pull镜像时默认的时https,docker的镜像仓库在国外，docker-cn加速不是很好，因此我们可是使用阿里云的docker加速。\n\n## 安装Docker\n\n本人使用的是Centos7\n\n### 系统准备\n\n要安装Docker CE，您需要CentOS 7的维护版本。不支持或测试存档版本。 必须启用centos-extras存储库。 默认情况下，此存储库已启用，但如果已将其禁用，则需要重新启用它。 建议使用overlay2存储驱动程序。\n\n### 卸载旧版本\n\n较旧版本的Docker被称为docker或docker-engine。如果已安装这些，请卸载它们以及相关的依赖项。\n\n```shell\nsudo yum remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-engine\n```\n\n\n\n如果yum报告没有安装这些软件包，那就没关系。 保留/ var / lib / docker /的内容，包括图像，容器，卷和网络。 Docker CE包现在称为docker-ce。\n\n### 安装Docker CE\n\n在新主机上首次安装Docker CE之前，需要设置Docker存储库。之后，您可以从存储库安装和更新Docker。\n\n安装所需的包。 yum-utils提供yum-config-manager实用程序，devicemapper存储驱动程序需要device-mapper-persistent-data和lvm2。\n\n```shell\nsudo yum install -y yum-utils device-mapper-persistent-data lvm2\n```\n\n使用以下命令设置稳定存储库。\n\n```shell\nsudo yum-config-manager --add-repo   https://download.docker.com/linux/centos/docker-ce.repo\n```\n\n安装最新版本的Docker CE和containerd，或者安装特定版本。\n\n```shell\nsudo yum install docker-ce docker-ce-cli containerd.io\n```\n\n如果提示接受GPG密钥，请确认指纹符合060A 61C5 1B55 8A7F 742B 77AA C52F EB6B 621E 9F35，如果符合，则接受该指纹。\n\n如果您启用了多个Docker存储库，则无需在yum install或yum update命令中指定版本进行安装或更新，则始终安装尽可能高的版本，这可能不适合您的稳定性需求。\n\n要安装特定版本的Docker CE，请在repo中列出可用版本，然后选择并安装：\n\n```shell\n[root@localhost ~]# yum list docker-ce --showduplicates | sort -r\n * updates: mirrors.aliyun.com\nLoading mirror speeds from cached hostfile\nLoaded plugins: fastestmirror\nInstalled Packages\n * extras: mirrors.tuna.tsinghua.edu.cn\ndocker-ce.x86_64            3:18.09.4-3.el7                    docker-ce-stable\ndocker-ce.x86_64            3:18.09.4-3.el7                    @docker-ce-stable\ndocker-ce.x86_64            3:18.09.3-3.el7                    docker-ce-stable\ndocker-ce.x86_64            3:18.09.2-3.el7                    docker-ce-stable\ndocker-ce.x86_64            3:18.09.1-3.el7                    docker-ce-stable\ndocker-ce.x86_64            3:18.09.0-3.el7                    docker-ce-stable\ndocker-ce.x86_64            18.06.3.ce-3.el7                   docker-ce-stable\ndocker-ce.x86_64            18.06.2.ce-3.el7                   docker-ce-stable\ndocker-ce.x86_64            18.06.1.ce-3.el7                   docker-ce-stable\ndocker-ce.x86_64            18.06.0.ce-3.el7                   docker-ce-stable\ndocker-ce.x86_64            18.03.1.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            18.03.0.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.12.1.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.12.0.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.09.1.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.09.0.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.06.2.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.06.1.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.06.0.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.03.3.ce-1.el7                   docker-ce-stable\ndocker-ce.x86_64            17.03.2.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.03.1.ce-1.el7.centos            docker-ce-stable\ndocker-ce.x86_64            17.03.0.ce-1.el7.centos            docker-ce-stable\n * base: mirrors.aliyun.com\nAvailable Packages\n```\n\n返回的列表取决于启用的存储库，并且特定于您的CentOS版本（在此示例中以.el7后缀表示）\n\n通过其完全限定的包名称安装特定版本，包名称（docker-ce）加上从第一个冒号（:)开始的版本字符串（第2列），直到第一个连字符，用连字符分隔（ - ）。例如 ` docker-ce-18.09.1`\n\n```\nsudo yum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io\n```\n\nDocker已安装但尚未启动。已创建docker组，但未向该组添加任何用户。\n\n由于网络问题下载速度可能会稍微慢一些，可以耐心等待。\n\n### 启动Docker\n\n```shell\n sudo systemctl start docker\n```\n\n### 拉取helloworld\n\n```shell\n[root@localhost ~]# docker pull hello-world\nUsing default tag: latest\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n```\n\n这里出现了错误我们解决下\n\n首先安装\n\n```shell\nyum install bind-utils\n```\n\n```shell\ndig @192.168.1.2 registry-1.docker.io\n```\n\n```shell\n[root@localhost ~]# dig @192.168.1.2 registry-1.docker.io  #此处填写CDN\n\n; <<>> DiG 9.9.4-RedHat-9.9.4-73.el7_6 <<>> @192.168.1.2 registry-1.docker.io\n; (1 server found)\n;; global options: +cmd\n;; Got answer:\n;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 11981\n;; flags: qr rd ra; QUERY: 1, ANSWER: 8, AUTHORITY: 0, ADDITIONAL: 1\n\n;; OPT PSEUDOSECTION:\n; EDNS: version: 0, flags:; MBZ: 0005 , udp: 4096\n;; QUESTION SECTION:\n;registry-1.docker.io.          IN      A\n\n;; ANSWER SECTION:\nregistry-1.docker.io.   5       IN      A       54.175.43.85\nregistry-1.docker.io.   5       IN      A       34.197.189.129\nregistry-1.docker.io.   5       IN      A       34.199.40.84\nregistry-1.docker.io.   5       IN      A       54.210.105.17\nregistry-1.docker.io.   5       IN      A       100.24.246.89\nregistry-1.docker.io.   5       IN      A       34.199.77.19\nregistry-1.docker.io.   5       IN      A       54.88.231.116\nregistry-1.docker.io.   5       IN      A       34.201.196.144\n```\n\n修改/etc/hosts `docker.io`相关的域名解析到其它可用IP\n\n```shell\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\n```\n\n配置加速\n\n```shell\ncurl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://f1361db2.m.daocloud.io\n```\n\n重启docker\n\n```shell\nservice docker restart\n```\n\npull helloworld镜像\n\n```shell\n[root@localhost ~]# docker pull hello-world\nUsing default tag: latest\n\nlatest: Pulling from library/hello-world\n1b930d010525: Pull complete\nDigest: sha256:92c7f9c92844bbbb5d0a101b22f7c2a7949e40f8ea90c8b3bc396879d95e899a\nStatus: Downloaded newer image for hello-world:latest\n```\n\n![ARZgmj.png](https://s2.ax1x.com/2019/04/04/ARZgmj.png)\n\n此命令下载测试映像并在容器中运行它。当容器运行时，它会打印一条信息性消息并退出。\n\n### 便捷安装\n\n```shell\ncurl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh get-docker.sh\n```\n\n## 推荐安装方法(使用清华镜像)\n\n1.下载清华大学Docker CE的repo文件\n\n```shell\ncd /etc/yum.repos.d/\nwget https://mirrors.tuna.tsinghua.edu.cn/docker-ce/linux/centos/docker-ce.repo\n```\n\n2.我们打开文件发现依旧指向的时Docker的地址这里我们修改为清华大学的地址\n\n```shell\n:%s@https://download.docker.com/@https://mirrors.tuna.tsinghua.edu.cn/docker-ce/@\n```\n\n3.安装Docker CE\n\n```shell\nyum install docker-ce\n```\n\n## 配置Docker加速器\n\n1.可以使用docker cn加速不过效果不太理想\n\n```shell\nvim /etc/docker/daemon.json\n```\n\n```json\n\n{            \n     \"registry-mirrors\": [\"https://registry.docker-cn.com\"]    \n }             \n```\n\n2.可以使用阿里云等云服务厂商的加速。\n\n## 卸载Docker CE\n\n卸载Docker包\n\n```shell\nsudo yum remove docker-ce\n```\n\n主机上的镜像，容器，卷或自定义配置文件不会自动删除。要删除所有镜像，容器和卷：\n\n```shell\nsudo rm -rf /var/lib/docker\n```\n\n\n\n## 核心\n\n![A2I1un.png](https://s2.ax1x.com/2019/04/04/A2I1un.png)\n\n### cgroups\n\nLinux系统中经常有个需求就是希望能限制某个或者某些进程的分配资源。于是就出现了cgroups的概念， cgroup就是controller group ，在这个group中，有分配好的特定比例的cpu时间，IO时间，可用内存大小等。 cgroups是将任意进程进行分组化管理的Linux内核功能。最初由google的工程师提出，后来被整合进Linux内核中。 \ncgroups中的 重要概念是“子系统”，也就是资源控制器，每种子系统就是一个资源的分配器，比如cpu子系 统是控制cpu时间分配的。首先挂载子系统，然后才有control group的。比如先挂载memory子系统，然后在 memory子系统中创建一个cgroup节点，在这个节点中，将需要控制的进程id写入，并且将控制的属性写入， 这就完成了内存的资源限制。 \ncgroups 被Linux内核支持，有得天独厚的性能优势，发展势头迅猛。在很多领域可以取代虚拟化技术分割资源。 cgroup默认有诸多资源组，可以限制几乎所有服务器上的资源：cpu mem iops,iobandwide,net,device acess等 \n\n资料： [kernel文档](https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt)  [infoQ相关](https://www.infoq.cn/article/docker-kernel-knowledge-cgroups-resource-isolation)\n\n### LXC\n\nLXC是Linux containers的简称，是一种基于容器的操作系统层级的虚拟化技术。借助于namespace的隔离机制和cgroup限额功能，LXC提供了一套统一的API和工具来建立和管理container。LXC跟其他操作系统层次的虚拟化技术相比，最大的优势在于LXC被整合进内核，不用单独为内核打补丁。\n\nLXC旨在提供一个共享kernel的 OS级虚拟化方法，在执行时不用重复加载Kernel, 且container的kernel与host 共享，因此可以大大加快container的启动过程，并显著减少内存消耗，容器在提供隔离的同时，还通过共享这些资源节省开销，这意味着容器比真正的虚拟化的开销要小得多。 在实际测试中，基于LXC的虚拟化方法的IO和 CPU性能几乎接近 baremetal 的性能。 \n\n虽然容器所使用的这种类型的隔离总的来说非常强大，然而是不是像运行在hypervisor上的虚拟机那么强壮仍具有争议性。如果内核停止，那么所有的容器就会停止运行。 \n\n 性能方面：LXC>>KVM>>XEN \n\n内存利用率：LXC>>KVM>>XEN \n\n隔离程度： XEN>>KVM>>LXC \n\n[baremetal相关](https://blog.csdn.net/BtB5e6Nsu1g511Eg5XEg/article/details/81091459)\n\n### AUFS\n\nAuFS是一个能透明覆盖一或多个现有文件系统的层状文件系统(类比PhotoShop图层)。 支持将不同目录挂载到同一个虚拟文件系统下，可以把不同的目录联合在一起，组成一个单一的目录。这种是一种虚拟的文件系统，文件系统不用格式化，直接挂载即可。 \nDocker最初使用AuFS作为容器的文件系统。当一个进程需要修改一个文件时，AuFS创建该文件的一个副本。 AuFS可以把多层合并成文件系统的单层表示。这个过程称为写入复制（ copy on write ）。 它目前作仍然作为存储后端之一来支持。\n\naufs的竞争产品时overlayfs，后者在3.18版本开始合并到Linux内核；\n\ndocker的分层镜像，除了aufs，docker还支持btrfs，devicemapper和vfs等。在Ubuntu系统下docker默认的时aufs；在Centos7上默认的时devicemapper（ 性能比较差）。\n\n![ARw3y4.png](https://s2.ax1x.com/2019/04/05/ARw3y4.png)\n\n\n\nAuFS允许Docker把某些镜像作为容器的基础。例如，你可能有一个可以作为很多不同容器的基础的CentOS 系统镜像。多亏AuFS，只要一个CentOS镜像的副本就够了，这样既节省了存储和内存，也保证更快速的容器部署。 \n使用AuFS的另一个好处是Docker的版本容器镜像能力。每个新版本都是一个与之前版本的简单差异改动， 有效地保持镜像文件最小化。但，这也意味着你总是要有一个记录该容器从一个版本到另一个版本改动的审计跟踪。\n\n### App打包\n\nLXC的基础上, Docker额外提供的Feature包括：标准统一的 打包部署运行方案 为了最大化重用Image，加快运行速度，减少内存和磁盘 footprint, Docker container运行时所构造的运行环境，实际 上是由具有依赖关系的多个Layer组成的。例如一个apache的运行环境可能是在基础的rootfs image的基础上，叠加了包含例如Emacs等各种工具的image，再叠加包含apache及其相关依赖library的image，这些image由AUFS文件系统加载合并到统一路径中，以只读的方式存在，最后再叠加加载 一层可写的空白的Layer用作记录对当前运行环境所作的修改。 \n有了层级化的Image做基础，理想中，不同的APP就可以既可能的共用底层文件系统，相关依赖工具等，同一个APP的 不同实例也可以实现共用绝大多数数据，进而以copy on write的形式维护自己的那一份修改过的数据等。\n\n![A2HISS.png](https://s2.ax1x.com/2019/04/04/A2HISS.png)\n\n### 生命周期开发模式\n\nDocker正在迅速改变云计算领域的运作规则，并彻底颠覆云技术的发展前景。 从持续集成/持续交付到微服务、开源协作乃至DevOps，Docker一路走来已经给应用程序开发生命周期以及云工程技术实践带来了巨大变革。 \n\n![A2HUd1.png](https://s2.ax1x.com/2019/04/04/A2HUd1.png)\n\n","tags":["Docker"],"categories":["容器"]},{"title":"SpringBoot使用外置的Servlet容器","url":"/2019/04/03/SpringBoot使用外置的Servlet容器/","content":"\n {{ \"Spring  Boot使用外部的Servlet容器支持JSP页面 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 使用\n\n嵌入式Servlet容器：应用打成可执行的jar\n\n优点：简单、便携；\n\n缺点：默认不支持JSP、优化定制比较复杂（使用定制器【ServerProperties、自定义EmbeddedServletContainerCustomizer】，自己编写嵌入式Servlet容器的创建工厂【EmbeddedServletContainerFactory】）；\n\n![AgShnO.png](https://s2.ax1x.com/2019/04/03/AgShnO.png)\n\n注意打包为War包\n\n![AgSLgP.png](https://s2.ax1x.com/2019/04/03/AgSLgP.png)\n\n外置的Servlet容器：外面安装Tomcat---应用war包的方式打包；\n\n编写一个**SpringBootServletInitializer**的子类，并调用configure方法\n\n```java\npublic class ServletInitializer extends SpringBootServletInitializer {\n\n   @Override\n   protected SpringApplicationBuilder configure(SpringApplicationBuilder application) {\n       //传入SpringBoot应用的主程序\n      return application.sources(SpringBoot04WebJspApplication.class);\n   }\n}\n```\n\n启动服务器就可以使用；\n\n但是SpringBoot默认的嵌入容器Tomcat不支持JSP,因此我们需要用外置的Tomcat来打包该项目\n\n![AgmXJf.png](https://s2.ax1x.com/2019/04/03/AgmXJf.png)\n\n![Agnmy4.png](https://s2.ax1x.com/2019/04/03/Agnmy4.png)\n\n![AgnMwR.png](https://s2.ax1x.com/2019/04/03/AgnMwR.png)\n\n![Agnd0A.png](https://s2.ax1x.com/2019/04/03/Agnd0A.png)\n\n## 原理\n\njar包：执行SpringBoot主类的main方法，启动ioc容器，创建嵌入式的Servlet容器；\n\nwar包：启动服务器，**服务器启动SpringBoot应用**【SpringBootServletInitializer】，启动ioc容器；\n\n在servlet3.0中的第8.3 JSP container pluggability章节中\n\n[文档链接](http://note.youdao.com/noteshare?id=9546758239c94cdc156b93f96b8aaf05&sub=1CC193D2303942AD8BDEAEC0A141DABA)\n\nThe ServletContainerInitializer class is looked up via the jar services API. For each application, an instance of the ServletContainerInitializer is created by the container at application startup time. The framework providing an \nimplementation of the ServletContainerInitializer MUST bundle in the META-INF/services directory of the jar file a file called javax.servlet.ServletContainerInitializer, as per the jar services API, that points to the implementation class of the ServletContainerInitializer.In addition to the ServletContainerInitializer we also have an annotation -HandlesTypes. The HandlesTypes annotation on the implementation of the ServletContainerInitializer is used to express interest in classes that may have annotations (type, method or field level annotations) specified in the value of  the HandlesTypes or if it extends / implements one those classes anywhere in the class’ super types. The HandlesTypes annotation is applied irrespective of the setting of metadata-complete.\n\n解读:\n\n​\t1）、服务器启动（web应用启动）会创建当前web应用里面每一个jar包里面ServletContainerInitializer实例：\n\n​\t2）、ServletContainerInitializer的实现放在jar包的META-INF/services文件夹下，有一个名为javax.servlet.ServletContainerInitializer的文件，内容就是ServletContainerInitializer的实现类的全类名\n\n​\t3）、还可以使用@HandlesTypes，在应用启动的时候加载我们感兴趣的类；\n\n## 流程\n\n1）、启动Tomcat\n\n2）、org/springframework/spring-web/5.1.5.RELEASE/spring-web-5.1.5.RELEASE.jar!/META-INF/services/javax.servlet.ServletContainerInitializer：\n\nSpring的web模块里面有这个文件：**org.springframework.web.SpringServletContainerInitializer**\n\n3）、SpringServletContainerInitializer将@HandlesTypes(WebApplicationInitializer.class)标注的所有这个类型的类都传入到onStartup方法的Set<Class<?>>；为这些WebApplicationInitializer类型的类创建实例；\n\n4）、每一个WebApplicationInitializer都调用自己的onStartup；\n\n```java\npublic interface WebApplicationInitializer {\n\tvoid onStartup(ServletContext servletContext) throws ServletException;\n}\n\n```\n\n![AgYWFg.png](https://s2.ax1x.com/2019/04/03/AgYWFg.png)\n\n5）、相当于我们的SpringBootServletInitializer的类会被创建对象，并执行onStartup方法\n\n6）、SpringBootServletInitializer实例执行onStartup的时候会createRootApplicationContext；创建容器\n\n```java\nprotected WebApplicationContext createRootApplicationContext(\n\t\t\tServletContext servletContext) {\n        //1、创建SpringApplicationBuilder\n\t\tSpringApplicationBuilder builder = createSpringApplicationBuilder();\n\t\tbuilder.main(getClass());\n\t\tApplicationContext parent = getExistingRootWebApplicationContext(servletContext);\n\t\tif (parent != null) {\n\t\t\tthis.logger.info(\"Root context already created (using as parent).\");\n\t\t\tservletContext.setAttribute(\n\t\t\t\t\tWebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE, null);\n\t\t\tbuilder.initializers(new ParentContextApplicationContextInitializer(parent));\n\t\t}\n\t\tbuilder.initializers(\n\t\t\t\tnew ServletContextApplicationContextInitializer(servletContext));\n\t\tbuilder.contextClass(AnnotationConfigServletWebServerApplicationContext.class);\t\t    \t    //调用configure方法，子类重写了这个方法，将SpringBoot的主程序类传入了进来\n\t\tbuilder = configure(builder);\n    \n\t\tbuilder.listeners(new WebEnvironmentPropertySourceInitializer(servletContext));\n        //使用builder创建一个Spring应用\n\t\tSpringApplication application = builder.build();\n\t\tif (application.getAllSources().isEmpty() && AnnotationUtils\n\t\t\t\t.findAnnotation(getClass(), Configuration.class) != null) {\n\t\t\tapplication.addPrimarySources(Collections.singleton(getClass()));\n\t\t}\n\t\tAssert.state(!application.getAllSources().isEmpty(),\n\t\t\t\t\"No SpringApplication sources have been defined. Either override the \"\n\t\t\t\t\t\t+ \"configure method or add an @Configuration annotation\");\n\t\t// Ensure error pages are registered\n\t\tif (this.registerErrorPageFilter) {\n\t\t\tapplication.addPrimarySources(\n\t\t\t\t\tCollections.singleton(ErrorPageFilterConfiguration.class));\n\t\t}\n        //启动Spring应用\n\t\treturn run(application);\n\t}\n```\n\nSpring的应用就启动并且创建IOC容器\n\n```java\n\tprotected WebApplicationContext run(SpringApplication application) {\n        //点击进入run方法\n\t\treturn (WebApplicationContext) application.run();\n\t}\n\n\tprivate ApplicationContext getExistingRootWebApplicationContext(\n\t\t\tServletContext servletContext) {\n\t\tObject context = servletContext.getAttribute(\n\t\t\t\tWebApplicationContext.ROOT_WEB_APPLICATION_CONTEXT_ATTRIBUTE);\n\t\tif (context instanceof ApplicationContext) {\n\t\t\treturn (ApplicationContext) context;\n\t\t}\n\t\treturn null;\n\t}\n```\n\n点击进入run方法\n\n```java\n\tpublic ConfigurableApplicationContext run(String... args) {\n\t\tStopWatch stopWatch = new StopWatch();\n\t\tstopWatch.start();\n\t\tConfigurableApplicationContext context = null;\n\t\tCollection<SpringBootExceptionReporter> exceptionReporters = new ArrayList<>();\n\t\tconfigureHeadlessProperty();\n\t\tSpringApplicationRunListeners listeners = getRunListeners(args);\n\t\tlisteners.starting();\n\t\ttry {\n\t\t\tApplicationArguments applicationArguments = new DefaultApplicationArguments(\n\t\t\t\t\targs);\n\t\t\tConfigurableEnvironment environment = prepareEnvironment(listeners,\n\t\t\t\t\tapplicationArguments);\n\t\t\tconfigureIgnoreBeanInfo(environment);\n\t\t\tBanner printedBanner = printBanner(environment);\n\t\t\tcontext = createApplicationContext();\n\t\t\texceptionReporters = getSpringFactoriesInstances(\n\t\t\t\t\tSpringBootExceptionReporter.class,\n\t\t\t\t\tnew Class[] { ConfigurableApplicationContext.class }, context);\n\t\t\tprepareContext(context, environment, listeners, applicationArguments,\n\t\t\t\t\tprintedBanner);\n            \n              //刷新IOC容器\n\t\t\trefreshContext(context);\n\t\t\tafterRefresh(context, applicationArguments);\n\t\t\tstopWatch.stop();\n\t\t\tif (this.logStartupInfo) {\n\t\t\t\tnew StartupInfoLogger(this.mainApplicationClass)\n\t\t\t\t\t\t.logStarted(getApplicationLog(), stopWatch);\n\t\t\t}\n\t\t\tlisteners.started(context);\n\t\t\tcallRunners(context, applicationArguments);\n\t\t}\n\t\tcatch (Throwable ex) {\n\t\t\thandleRunFailure(context, ex, exceptionReporters, listeners);\n\t\t\tthrow new IllegalStateException(ex);\n\t\t}\n\n\t\ttry {\n\t\t\tlisteners.running(context);\n\t\t}\n\t\tcatch (Throwable ex) {\n\t\t\thandleRunFailure(context, ex, exceptionReporters, null);\n\t\t\tthrow new IllegalStateException(ex);\n\t\t}\n\t\treturn context;\n\t}\n\n```\n\n\n\n\n\n\n\n","tags":["JSP"],"categories":["SpringBoot"]},{"title":"SpringBoot配置嵌入式Servlet容器","url":"/2019/04/02/SpringBoot配置嵌入式Servlet容器/","content":"\n {{ \"Spring Boot配置嵌入式Servlet容器的一些思考 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 注册 Servlet  Filter、Listener\n\n当使用嵌入式的Servlet容器（Tomcat、Jetty等）时，我们通过将Servlet、Filter和Listener声明为Spring Bean而达到注册的效果；或者注册ServletRegistrationBean、FilterRegistrationBean 和 ServletListenerRegistrationBean的Bean。Spring Boot默认内嵌的Tomcat为servlet容器。通用的Servlet容器配置都以“server”作为前缀，而Tomcat特有配置都以“server.tomcat”作为前缀。\n\n由于SpringBoot默认是以jar包的方式启动嵌入式的Servlet容器来启动SpringBoot的web应用,没有web.xml文件。\n\n![A6xDl8.png](https://s2.ax1x.com/2019/04/02/A6xDl8.png)\n\n## 问题\n\n如何定制和修改Servlet容器中的相关配置；\n\nSpringBoot能不能支持其他的Servlet容器；\n\n\n\n配置Servler容器\n\n## 配置文件\n\n```properties\n#默认程序端口为8080\nserver.port=8080 \n#用户会话session过期事件,以秒为单位\nserver.session.cookie.comment=60 \n#配配置默认的访问路径,默认为/\nserver.context-path= / \n```\n\n通常的Servlet容器设置为为server.xxx=XXX 而Tomcat的设置则为server.tomcat=XXX\n\n## 源码\n\n```java\n@ConfigurationProperties(prefix = \"server\", ignoreUnknownFields = true)\npublic class ServerProperties\n\t\timplements EmbeddedServletContainerCustomizer, EnvironmentAware, Ordered {\n\tprivate Integer port;\n\n\tprivate InetAddress address;\n\n\tprivate String contextPath;\n\n\tprivate String displayName = \"application\";\n\n\t@NestedConfigurationProperty\n\tprivate ErrorProperties error = new ErrorProperties();\n\n\tprivate String servletPath = \"/\";\n\n\n\tprivate final Map<String, String> contextParameters = new HashMap<String, String>();\n\n\tprivate Boolean useForwardHeaders;\n\n\tprivate String serverHeader;\n\n\tprivate int maxHttpHeaderSize = 0; // bytes\n\n\tprivate int maxHttpPostSize = 0; // bytes\n\n\tprivate Integer connectionTimeout;\n\n\tprivate Session session = new Session();\n\n\t@NestedConfigurationProperty\n\tprivate Ssl ssl;\n\n\t@NestedConfigurationProperty\n\tprivate Compression compression = new Compression();\n\n\t@NestedConfigurationProperty\n\tprivate JspServlet jspServlet;\n\n\tprivate final Tomcat tomcat = new Tomcat();\n\n\tprivate final Jetty jetty = new Jetty();\n\n\tprivate final Undertow undertow = new Undertow();\n\n\tprivate Environment environment;\n```\n\n## 代码配置\n\n```java\n\n    //配置嵌入式的Servlet容器\n    @Bean\n    public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer(){\n        return new EmbeddedServletContainerCustomizer() {\n\n            //定制嵌入式的Servlet容器相关的规则\n            @Override\n            public void customize(ConfigurableEmbeddedServletContainer container) {\n                container.setPort(8083);\n            }\n        };\n    }\n```\n\n启动服务\n\n![Ac90EQ.png](https://s2.ax1x.com/2019/04/02/Ac90EQ.png)\n\n这个两个配置本质上都是差不多的都调用了 embeddedServletContainerCustomizer() 方法在SpringBoot中我们也会遇到很多的xxCustomizer帮助我们进行定制配置。\n\n## 注册三大组件\n\n```java\npackage com.hph.springboot.config;\n\nimport com.hph.springboot.filter.MyFilter;\nimport com.hph.springboot.listener.MyListener;\nimport com.hph.springboot.servlet.MyServlet;\nimport org.springframework.boot.context.embedded.ConfigurableEmbeddedServletContainer;\nimport org.springframework.boot.context.embedded.EmbeddedServletContainerCustomizer;\nimport org.springframework.boot.web.servlet.FilterRegistrationBean;\nimport org.springframework.boot.web.servlet.ServletListenerRegistrationBean;\nimport org.springframework.boot.web.servlet.ServletRegistrationBean;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\nimport java.util.Arrays;\n\n@Configuration\npublic class MyServerConfig {\n\n    //注册三大组件\n    @Bean\n    public ServletRegistrationBean myServlet(){\n        ServletRegistrationBean registrationBean = new ServletRegistrationBean(new MyServlet(),\"/myServlet\");\n        registrationBean.setLoadOnStartup(1);\n        return registrationBean;\n    }\n\n    @Bean\n    public FilterRegistrationBean myFilter(){\n        FilterRegistrationBean registrationBean = new FilterRegistrationBean();\n        registrationBean.setFilter(new MyFilter());\n        registrationBean.setUrlPatterns(Arrays.asList(\"/hello\",\"/myServlet\"));\n        return registrationBean;\n    }\n\n    @Bean\n    public ServletListenerRegistrationBean myListener(){\n        ServletListenerRegistrationBean<MyListener> registrationBean = new ServletListenerRegistrationBean<>(new MyListener());\n        return registrationBean;\n    }\n\n\n    //配置嵌入式的Servlet容器\n    @Bean\n    public EmbeddedServletContainerCustomizer embeddedServletContainerCustomizer(){\n        return new EmbeddedServletContainerCustomizer() {\n\n            //定制嵌入式的Servlet容器相关的规则\n            @Override\n            public void customize(ConfigurableEmbeddedServletContainer container) {\n                container.setPort(8083);\n            }\n        };\n    }\n\n}\n```\n\n点击ServletRegistrationBean进入\n\n```java\n\t\n\t\t//这里的Servlet就是标准的Javax中的Servlet\t\n\tpublic ServletRegistrationBean(Servlet servlet, String... urlMappings) {\n\t\tthis(servlet, true, urlMappings);\n\t}\n\n\t/**\n\t * Create a new {@link ServletRegistrationBean} instance with the specified\n\t * {@link Servlet} and URL mappings.\n\t * @param servlet the servlet being mapped\n\t * @param alwaysMapUrl if omitted URL mappings should be replaced with '/*'\n\t * @param urlMappings the URLs being mapped\n\t */\n\tpublic ServletRegistrationBean(Servlet servlet, boolean alwaysMapUrl,\n\t\t\tString... urlMappings) {\n\t\tAssert.notNull(servlet, \"Servlet must not be null\");\n\t\tAssert.notNull(urlMappings, \"UrlMappings must not be null\");\n\t\tthis.servlet = servlet;\n\t\tthis.alwaysMapUrl = alwaysMapUrl;\n\t\tthis.urlMappings.addAll(Arrays.asList(urlMappings));\n\t}\n```\n\n![AcCrZD.png](https://s2.ax1x.com/2019/04/02/AcCrZD.png)\n\n点击FilterRegistrationBean进入\n\n```java\npublic FilterRegistrationBean() {\n\t}\n\n\t/**\n\t * Create a new {@link FilterRegistrationBean} instance to be registered with the\n\t * specified {@link ServletRegistrationBean}s.\n\t * @param filter the filter to register\n\t * @param servletRegistrationBeans associate {@link ServletRegistrationBean}s\n\t */\n\tpublic FilterRegistrationBean(Filter filter,\n\t\t\tServletRegistrationBean... servletRegistrationBeans) {\n\t\tsuper(servletRegistrationBeans);\n\t\tAssert.notNull(filter, \"Filter must not be null\");\n\t\tthis.filter = filter;\n\t}\n```\n\n![AcPmwD.png](https://s2.ax1x.com/2019/04/02/AcPmwD.png)\n\n```java\npackage com.hph.springboot.servlet;\n\nimport javax.servlet.ServletException;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport java.io.IOException;\n\npublic class MyServlet extends HttpServlet {\n\n    //处理get请求\n    @Override\n    protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {\n        doPost(req,resp);\n    }\n\n    @Override\n    protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException {\n        resp.getWriter().write(\"Hello MyServlet\");\n    }\n}\n```\n\n```java\npackage com.hph.springboot.listener;\n\nimport javax.servlet.ServletContextEvent;\nimport javax.servlet.ServletContextListener;\n\npublic class MyListener implements ServletContextListener {\n    @Override\n    public void contextInitialized(ServletContextEvent sce) {\n        System.out.println(\"contextInitialized...web应用启动\");\n    }\n\n    @Override\n    public void contextDestroyed(ServletContextEvent sce) {\n        System.out.println(\"contextDestroyed...当前web项目销毁\");\n    }\n}\n```\n\n```java\npackage com.hph.springboot.filter;\n\nimport javax.servlet.*;\nimport java.io.IOException;\n\npublic class MyFilter implements Filter {\n\n    @Override\n    public void init(FilterConfig filterConfig) throws ServletException {\n\n    }\n\n    @Override\n    public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException {\n        System.out.println(\"MyFilter process...\");\n        chain.doFilter(request,response);\n\n    }\n\n    @Override\n    public void destroy() {\n\n    }\n}\n\n```\n\nSpringBoot帮我们自动配置SpringMVC的时候，自动注册SpringMVC的前端控制器，DispatcherServlet;\n\n```java\n\t@Bean(name = DEFAULT_DISPATCHER_SERVLET_REGISTRATION_BEAN_NAME)\n\t\t@ConditionalOnBean(value = DispatcherServlet.class, name = DEFAULT_DISPATCHER_SERVLET_BEAN_NAME)\n\t\tpublic ServletRegistrationBean dispatcherServletRegistration(\n\t\t\t\tDispatcherServlet dispatcherServlet) {\n\t\t\tServletRegistrationBean registration = new ServletRegistrationBean(\n\t\t\t\t\tdispatcherServlet, this.serverProperties.getServletMapping());\n            //默认拦截了 所有请求 包括青苔资源,但是不拦截jsp请求\n\t\t\tregistration.setName(DEFAULT_DISPATCHER_SERVLET_BEAN_NAME);\n\t\t\tregistration.setLoadOnStartup(\n\t\t\t\t\tthis.webMvcProperties.getServlet().getLoadOnStartup());\n\t\t\tif (this.multipartConfig != null) {\n\t\t\t\tregistration.setMultipartConfig(this.multipartConfig);\n\t\t\t}\n\t\t\treturn registration;\n\t\t}\n    \n    \n```\n\n## 替换为其他的嵌入式Servlet容器\n\n![AcHPaV.png](https://s2.ax1x.com/2019/04/03/AcHPaV.png)\n\nSpringBoot默认支持\n\n### Tomcat(默认使用)\n\n```properties\n\t\t<!-- 引入web模块 -->\n\t\t<dependency>\n\t\t\t<groupId>org.springframework.boot</groupId>\n\t\t\t<artifactId>spring-boot-starter-web</artifactId>\n\t\t</dependency>\n```\n\n引入web模块时默认的时Tomcat\n\n### Jetty\n\n![Acb40I.png](https://s2.ax1x.com/2019/04/03/Acb40I.png)\n\n只需要修改pom文件即可\n\n### Undertow\n\n![Acq0gg.png](https://s2.ax1x.com/2019/04/03/Acq0gg.png)\n\n###  原理\n\nEmbeddedServletContainerAutoConfiguration:嵌入式的Servlet容器自动配置。\n\n```java\n@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE)\n@Configuration\n@ConditionalOnWebApplication\n@Import(BeanPostProcessorsRegistrar.class)\n//导入BeanPostProcessorsRegistrar：给容器中导入一些组件\n//导入了EmbeddedServletContainerCustomizerBeanPostProcessor：\n//后置处理器：bean初始化前后（创建完对象，还没赋值赋值）执行初始化工作\npublic class EmbeddedServletContainerAutoConfiguration {\n    \n    @Configuration\n\t@ConditionalOnClass({ Servlet.class, Tomcat.class })//判断当前是否引入了Tomcat依赖；\n\t@ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)//判断当前容器没有用户自己定义EmbeddedServletContainerFactory：嵌入式的Servlet容器工厂；作用：创建嵌入式的Servlet容器\n\tpublic static class EmbeddedTomcat {\n\n\t\t@Bean\n\t\tpublic TomcatEmbeddedServletContainerFactory tomcatEmbeddedServletContainerFactory() {\n\t\t\treturn new TomcatEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n    \n    /**\n\t * Nested configuration if Jetty is being used.\n\t */\n\t@Configuration\n\t@ConditionalOnClass({ Servlet.class, Server.class, Loader.class,\n\t\t\tWebAppContext.class })\n\t@ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)\n\tpublic static class EmbeddedJetty {\n\n\t\t@Bean\n\t\tpublic JettyEmbeddedServletContainerFactory jettyEmbeddedServletContainerFactory() {\n\t\t\treturn new JettyEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n\n\t/**\n\t * Nested configuration if Undertow is being used.\n\t */\n\t@Configuration\n\t@ConditionalOnClass({ Servlet.class, Undertow.class, SslClientAuthMode.class })\n\t@ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)\n\tpublic static class EmbeddedUndertow {\n\n\t\t@Bean\n\t\tpublic UndertowEmbeddedServletContainerFactory undertowEmbeddedServletContainerFactory() {\n\t\t\treturn new UndertowEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n\n```\n\n1. EmbeddedServletContainerFactory(嵌入式Servlet容器工厂)\n\n![AcOJtP.png](https://s2.ax1x.com/2019/04/03/AcOJtP.png)\n\n```java\npublic interface EmbeddedServletContainerFactory {\n\n\tEmbeddedServletContainer getEmbeddedServletContainer(\n\t\t\tServletContextInitializer... initializers);\n}\n```\n\n2. EmbeddedServletContainer：（嵌入式的Servlet容器）\n\n![AcOa6g.png](https://s2.ax1x.com/2019/04/03/AcOa6g.png)\n\n在\n\n```java\n\tpublic static class EmbeddedTomcat {\n\n\t\t@Bean\n\t\tpublic TomcatEmbeddedServletContainerFactory tomcatEmbeddedServletContainerFactory() {\n\t\t\treturn new TomcatEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n\n\t//如果是 Jetty被使用  配置文件\n\t@Configuration\n\t@ConditionalOnClass({ Servlet.class, Server.class, Loader.class,\n\t\t\tWebAppContext.class })\n\t@ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)\n\tpublic static class EmbeddedJetty {\n\n\t\t@Bean\n\t\tpublic JettyEmbeddedServletContainerFactory jettyEmbeddedServletContainerFactory() {\n\t\t\treturn new JettyEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n\n\t//如果是undertow配置文件\n\t@Configuration\n\t@ConditionalOnClass({ Servlet.class, Undertow.class, SslClientAuthMode.class })\n\t@ConditionalOnMissingBean(value = EmbeddedServletContainerFactory.class, search = SearchStrategy.CURRENT)\n\tpublic static class EmbeddedUndertow {\n\n\t\t@Bean\n\t\tpublic UndertowEmbeddedServletContainerFactory undertowEmbeddedServletContainerFactory() {\n\t\t\treturn new UndertowEmbeddedServletContainerFactory();\n\t\t}\n\n\t}\n```\n\n以TomcatEmbeddedServletContainerFactory为例分析\n\n```java\n@Override\npublic EmbeddedServletContainer getEmbeddedServletContainer(\n      ServletContextInitializer... initializers) {\n    //创建一个Tomcat\n   Tomcat tomcat = new Tomcat();\n    \n    //配置Tomcat的基本环节\n   File baseDir = (this.baseDirectory != null ? this.baseDirectory\n         : createTempDir(\"tomcat\"));\n   tomcat.setBaseDir(baseDir.getAbsolutePath());\n   Connector connector = new Connector(this.protocol);\n   tomcat.getService().addConnector(connector);\n   customizeConnector(connector);\n   tomcat.setConnector(connector);\n   tomcat.getHost().setAutoDeploy(false);\n   configureEngine(tomcat.getEngine());\n   for (Connector additionalConnector : this.additionalTomcatConnectors) {\n      tomcat.getService().addConnector(additionalConnector);\n   }\n   prepareContext(tomcat.getHost(), initializers);\n    \n    //将配置好的Tomcat传入进去，返回一个EmbeddedServletContainer；并且启动Tomcat服务器\n   return getTomcatEmbeddedServletContainer(tomcat);\n}\n```\n\n```java\n\tprotected TomcatEmbeddedServletContainer getTomcatEmbeddedServletContainer(\n\t\t\tTomcat tomcat) {\n\t\treturn new TomcatEmbeddedServletContainer(tomcat, getPort() >= 0);\n\t}\n```\n\n```java\n\tpublic TomcatEmbeddedServletContainer(Tomcat tomcat, boolean autoStart) {\n\t\tAssert.notNull(tomcat, \"Tomcat Server must not be null\");\n\t\tthis.tomcat = tomcat;\n\t\tthis.autoStart = autoStart;\n\t\tinitialize();\n\t}\n\tprivate void initialize() throws EmbeddedServletContainerException {\n\t\tTomcatEmbeddedServletContainer.logger\n\t\t\t\t.info(\"Tomcat initialized with port(s): \" + getPortsDescription(false));\n\t\tsynchronized (this.monitor) {\n\t\t\ttry {\n\t\t\t\taddInstanceIdToEngineName();\n\t\t\t\ttry {\n\t\t\t\t\t// Remove service connectors to that protocol binding doesn't happen\n\t\t\t\t\t// yet\n\t\t\t\t\tremoveServiceConnectors();\n\n\t\t\t\t\t// Start the server to trigger initialization listeners\n\t\t\t\t\tthis.tomcat.start();\n\n\t\t\t\t\t// We can re-throw failure exception directly in the main thread\n\t\t\t\t\trethrowDeferredStartupExceptions();\n\n\t\t\t\t\tContext context = findContext();\n\t\t\t\t\ttry {\n\t\t\t\t\t\tContextBindings.bindClassLoader(context, getNamingToken(context),\n\t\t\t\t\t\t\t\tgetClass().getClassLoader());\n\t\t\t\t\t}\n\t\t\t\t\tcatch (NamingException ex) {\n\t\t\t\t\t\t// Naming is not enabled. Continue\n\t\t\t\t\t}\n\n\t\t\t\t\t// Unlike Jetty, all Tomcat threads are daemon threads. We create a\n\t\t\t\t\t// blocking non-daemon to stop immediate shutdown\n\t\t\t\t\tstartDaemonAwaitThread();\n\t\t\t\t}\n\t\t\t\tcatch (Exception ex) {\n\t\t\t\t\tcontainerCounter.decrementAndGet();\n\t\t\t\t\tthrow ex;\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (Exception ex) {\n\t\t\t\tthrow new EmbeddedServletContainerException(\n\t\t\t\t\t\t\"Unable to start embedded Tomcat\", ex);\n\t\t\t}\n\t\t}\n\t}\n```\n\n如何对嵌入式容器的配置修改时怎么样生效的。\n\n1. 使用serverProperties  \n2.  EmbeddedServletContainerCustomizer : 定制器帮我们修改了Servlet容器的配置导入BeanPostProcessorsRegistrar：给容器中导入一些组件\n\n```java\n\t\t@Override\n\t\tpublic void registerBeanDefinitions(AnnotationMetadata importingClassMetadata,\n\t\t\t\tBeanDefinitionRegistry registry) {\n\t\t\tif (this.beanFactory == null) {\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tregisterSyntheticBeanIfMissing(registry,\n\t\t\t\t\t\"embeddedServletContainerCustomizerBeanPostProcessor\",\n\t\t\t\t\tEmbeddedServletContainerCustomizerBeanPostProcessor.class);\n\t\t\tregisterSyntheticBeanIfMissing(registry,\n\t\t\t\t\t\"errorPageRegistrarBeanPostProcessor\",\n\t\t\t\t\tErrorPageRegistrarBeanPostProcessor.class);\n\t\t}\n\n```\n\n容器中导入了**EmbeddedServletContainerCustomizerBeanPostProcessor**\n\n```java\n//初始化之前\n@Override\npublic Object postProcessBeforeInitialization(Object bean, String beanName)\n      throws BeansException {\n    //如果当前初始化的是一个ConfigurableEmbeddedServletContainer类型的组件\n   if (bean instanceof ConfigurableEmbeddedServletContainer) {\n       //\n      postProcessBeforeInitialization((ConfigurableEmbeddedServletContainer) bean);\n   }\n   return bean;\n}\n\nprivate void postProcessBeforeInitialization(\n\t\t\tConfigurableEmbeddedServletContainer bean) {\n    //获取所有的定制器，调用每一个定制器的customize方法来给Servlet容器进行属性赋值；\n    for (EmbeddedServletContainerCustomizer customizer : getCustomizers()) {\n        customizer.customize(bean);\n    }\n}\n\nprivate Collection<EmbeddedServletContainerCustomizer> getCustomizers() {\n    if (this.customizers == null) {\n        // Look up does not include the parent context\n        this.customizers = new ArrayList<EmbeddedServletContainerCustomizer>(\n            this.beanFactory\n            //从容器中获取所有这葛类型的组件：EmbeddedServletContainerCustomizer\n            //定制Servlet容器，给容器中可以添加一个EmbeddedServletContainerCustomizer类型的组件\n            .getBeansOfType(EmbeddedServletContainerCustomizer.class,\n                            false, false)\n            .values());\n        Collections.sort(this.customizers, AnnotationAwareOrderComparator.INSTANCE);\n        this.customizers = Collections.unmodifiableList(this.customizers);\n    }\n    return this.customizers;\n}\n//ServerProperties也是定制器\n```\n\n### 步骤\n\n步骤：\n\n1）、SpringBoot根据导入的依赖情况，给容器中添加相应的EmbeddedServletContainerFactory【TomcatEmbeddedServletContainerFactory】\n\n2）、容器中某个组件要创建对象就会惊动后置处理器；EmbeddedServletContainerCustomizerBeanPostProcessor；\n\n只要是嵌入式的Servlet容器工厂，后置处理器就工作；\n\n3）、后置处理器，从容器中获取所有的**EmbeddedServletContainerCustomizer**，调用定制器的定制方法\n\n\n\n\n\n## 嵌入式Servlet容器启动原理\n\n什么时候创建嵌入式的Servlet容器工厂？什么时候获取嵌入式的Servlet容器并启动Tomcat；\n\n获取嵌入式的Servlet容器工厂：\n\n1）、SpringBoot应用启动运行run方法\n\n2）、refreshContext(context);SpringBoot刷新IOC容器【创建IOC容器对象，并初始化容器，创建容器中的每一个组件】；如果是web应用创建**AnnotationConfigEmbeddedWebApplicationContext**，否则：**AnnotationConfigApplicationContext**\n\n3）、refresh(context);**刷新刚才创建好的ioc容器；**\n\n```java\npublic void refresh() throws BeansException, IllegalStateException {\n   synchronized (this.startupShutdownMonitor) {\n      // Prepare this context for refreshing.\n      prepareRefresh();\n\n      // Tell the subclass to refresh the internal bean factory.\n      ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory();\n\n      // Prepare the bean factory for use in this context.\n      prepareBeanFactory(beanFactory);\n\n      try {\n         // Allows post-processing of the bean factory in context subclasses.\n         postProcessBeanFactory(beanFactory);\n\n         // Invoke factory processors registered as beans in the context.\n         invokeBeanFactoryPostProcessors(beanFactory);\n\n         // Register bean processors that intercept bean creation.\n         registerBeanPostProcessors(beanFactory);\n\n         // Initialize message source for this context.\n         initMessageSource();\n\n         // Initialize event multicaster for this context.\n         initApplicationEventMulticaster();\n\n         // Initialize other special beans in specific context subclasses.\n         onRefresh();\n\n         // Check for listener beans and register them.\n         registerListeners();\n\n         // Instantiate all remaining (non-lazy-init) singletons.\n         finishBeanFactoryInitialization(beanFactory);\n\n         // Last step: publish corresponding event.\n         finishRefresh();\n      }\n\n      catch (BeansException ex) {\n         if (logger.isWarnEnabled()) {\n            logger.warn(\"Exception encountered during context initialization - \" +\n                  \"cancelling refresh attempt: \" + ex);\n         }\n\n         // Destroy already created singletons to avoid dangling resources.\n         destroyBeans();\n\n         // Reset 'active' flag.\n         cancelRefresh(ex);\n\n         // Propagate exception to caller.\n         throw ex;\n      }\n\n      finally {\n         // Reset common introspection caches in Spring's core, since we\n         // might not ever need metadata for singleton beans anymore...\n         resetCommonCaches();\n      }\n   }\n}\n```\n\n4）、  onRefresh(); web的ioc容器重写了onRefresh方法\n\n5）、webioc容器会创建嵌入式的Servlet容器；**createEmbeddedServletContainer**();\n\n**6）、获取嵌入式的Servlet容器工厂：**\n\nEmbeddedServletContainerFactory containerFactory = getEmbeddedServletContainerFactory();\n\n​\t从ioc容器中获取EmbeddedServletContainerFactory 组件；**TomcatEmbeddedServletContainerFactory**创建对象，后置处理器一看是这个对象，就获取所有的定制器来先定制Servlet容器的相关配置；\n\n7）、**使用容器工厂获取嵌入式的Servlet容器**：this.embeddedServletContainer = containerFactory      .getEmbeddedServletContainer(getSelfInitializer());\n\n8）、嵌入式的Servlet容器创建对象并启动Servlet容器；\n\n**先启动嵌入式的Servlet容器，再将ioc容器中剩下没有创建出的对象获取出来；**\n\n**IOC容器启动创建嵌入式的Servlet容器**\n\n\n\n\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"SpringBoot之SpringMVC自动配置","url":"/2019/03/29/SpringBoot之SpringMVC自动配置/","content":"\n {{ \"关于SpringBoot中的SpringMVC自动配置的一些思考 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 自动配置\n\nSpring Boot 自动配置好了SpringMVC\n\n以下是SpringBoot对SpringMVC的默认配置:（WebMvcAutoConfiguration）\n\n- Inclusion of `ContentNegotiatingViewResolver` and `BeanNameViewResolver` beans.\n    - 自动配置了ViewResolver（视图解析器：根据方法的返回值得到视图对象（View），视图对象决定如何渲染（转发？重定向？））\n    - ContentNegotiatingViewResolver：组合所有的视图解析器的；\n    - 如何定制：我们可以自己给容器中添加一个视图解析器；自动的将其组合进来；\n- Support for serving static resources, including support for WebJars (see below).静态资源文件夹路径,webjars\n- Static `index.html` support. 静态首页访问\n- Custom `Favicon` support (see below).  favicon.ico\n- 自动注册了 of `Converter`, `GenericConverter`, `Formatter` beans.\n    - Converter：转换器；  public String hello(User user)：类型转换使用Converter\n    - `Formatter`  格式化器；  2019.3.30===Date；\n\n```java\n\t\t@Bean\n\t\t@ConditionalOnProperty(prefix = \"spring.mvc\", name = \"date-format\") //在文件中配置日期格式化的规则\n\t\tpublic Formatter<Date> dateFormatter() {\n\t\t\treturn new DateFormatter(this.mvcProperties.getDateFormat());//日期格式化组件\n\t\t}\n```\n\n**自己添加的格式化器转换器，我们只需要放在容器中即可** \n\n\n\n- Support for **HttpMessageConverters** (see below).\n    - HttpMessageConverter：SpringMVC用来转换Http请求和响应的；User---Json；\n    - HttpMessageConverters是从容器中确定；获取所有的HttpMessageConverter；\n    - 自己给容器中添加HttpMessageConverter，只需要将自己的组件注册容器中（@Bean,@Component）\n\nAutomatic registration of `MessageCodesResolver` (see below).定义错误代码生成规则\n\nAutomatic use of a `ConfigurableWebBindingInitializer` bean (see below).\n\n我们可以配置一个ConfigurableWebBindingInitializer来替换默认的；（添加到容器）\n\n```\n初始化WebDataBinder；\n请求数据=====JavaBean；\n```\n\n**org.springframework.boot.autoconfigure.web：web的所有自动场景；**\n\n If you want to keep Spring Boot MVC features, and you just want to add additional [MVC configuration](https://docs.spring.io/spring/docs/4.3.14.RELEASE/spring-framework-reference/htmlsingle#mvc) (interceptors, formatters, view controllers etc.) you can add your own `@Configuration` class of type `WebMvcConfigurerAdapter`, but **without** `@EnableWebMvc`. If you wish to provide custom instances of `RequestMappingHandlerMapping`, `RequestMappingHandlerAdapter` or `ExceptionHandlerExceptionResolver` you can declare a `WebMvcRegistrationsAdapter` instance providing such components.\n\n  If you want to take complete control of Spring MVC, you can add your own `@Configuration` annotated with `@EnableWebMvc`.\n\n## 修改SprigBoot的默认配置\n\n 在WebMvcAutoConfiguration中springboot如果想要给容器配置一个HiddenHttpMethodFilter\n\n```java\n\t@Bean\n\t@ConditionalOnMissingBean(HiddenHttpMethodFilter.class)   //先判断容器中是否由该组件\n\tpublic OrderedHiddenHttpMethodFilter hiddenHttpMethodFilter() {\n\t\treturn new OrderedHiddenHttpMethodFilter();       //如果没有 new一个新的组件添加到容器中\n\t}\n```\n\n### 修改模式\n\n- SpringBoot在自动配置很多组件的时候，先看容器中有没有用户自己配置的（@Bean、@Component）如果有就用用户配置的，如果没有，才自动配置；如果有些组件可以有多个（ViewResolver）将用户配置的和自己默认的组合起来；\n- 在SpringBoot中会有非常多的xxxConfigurer帮助我们进行扩展配置\n- 在SpringBoot中会有很多的xxxCustomizer帮助我们进行定制配置\n\n## 扩展SpringMVC\n\n在实际的开发过程中,仅仅依靠Springboot的自动配置是远远不够的。在有SpringMVC配置文件的时候，我们可能像下面以配置\n\n```xml\n    <mvc:view-controller path=\"/hello\" view-name=\"success\"/>\n    <mvc:interceptors>\n        <mvc:interceptor>\n            <mvc:mapping path=\"/hello\"/>\n            <bean></bean>\n        </mvc:interceptor>\n    </mvc:interceptors>\n```\n\n在SpringBoot的开发中我们可以编写一个配置类（@Configuration），是WebMvcConfigurerAdapter类型；不能标注@EnableWebMvc;\n\n```java\npackage com.hph.springboot.config;\n\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.ViewControllerRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;\n\n@Configuration\npublic class MyConfig extends WebMvcConfigurerAdapter {\n    @Override\n    public void addViewControllers(ViewControllerRegistry registry) {\n        //super.addViewControllers(registry);\n        //相当于浏览器发送/hello请求,也来到成功页面\n        registry.addViewController(\"/hellomvc\").setViewName(\"success\");\n    }\n}\n```\n\n这样做即保留了所有的自动配置也能用我们扩展的配置\n\n![ABDvb8.png](https://s2.ax1x.com/2019/03/30/ABDvb8.png)\n\n### 原理探究\n\n- WebMvcAutoConfiguration是SpringMVC的自动配置类\n\n- 在做其他自动配置时会导入；@Import(**EnableWebMvcConfiguration**.class)\n- 容器中所有的WebMvcConfigurer都会一起起作用；\n- 我们的配置类也会被调用；\n- 效果：SpringMVC的自动配置和我们的扩展配置都会起作用；\n\n```java\n\tpublic static class EnableWebMvcConfiguration extends DelegatingWebMvcConfiguration {\n\n\t\tprivate final WebMvcProperties mvcProperties;\n\n\t\tprivate final ListableBeanFactory beanFactory;\n\n\t\tprivate final WebMvcRegistrations mvcRegistrations;\n\n\t\tpublic EnableWebMvcConfiguration(\n\t\t\t\tObjectProvider<WebMvcProperties> mvcPropertiesProvider,\n\t\t\t\tObjectProvider<WebMvcRegistrations> mvcRegistrationsProvider,\n\t\t\t\tListableBeanFactory beanFactory) {\n\t\t\tthis.mvcProperties = mvcPropertiesProvider.getIfAvailable();\n\t\t\tthis.mvcRegistrations = mvcRegistrationsProvider.getIfUnique();\n\t\t\tthis.beanFactory = beanFactory;\n\t\t}\n```\n\n```java\n@Configuration\npublic class DelegatingWebMvcConfiguration extends WebMvcConfigurationSupport {\n\n\tprivate final WebMvcConfigurerComposite configurers = new WebMvcConfigurerComposite();\n\n\t//从容器中获取所有的WebMvcConfigurer\n\t@Autowired(required = false)\n\tpublic void setConfigurers(List<WebMvcConfigurer> configurers) {\n\t\tif (!CollectionUtils.isEmpty(configurers)) {\n\t\t\tthis.configurers.addWebMvcConfigurers(configurers);           \n\t\t}\n\t}\n```\n\n### 参考实现\n\n```java\n//一个参考实现；将所有的WebMvcConfigurer相关配置都来一起调用；  \n@Override\npublic void addViewControllers(ViewControllerRegistry registry) {\n\tfor (WebMvcConfigurer delegate : this.delegates) {\n  \t\t\tdelegate.addViewControllers(registry);\n\t }\n}\n```\n\n该方法时是将所有的WebMvcConfigurer相关配置都会一起调用。也就是我们自己写的配置类也会被调用。实现的效果就是：springmvc自身的配置起作用的时候把我们的自定义配置也遍历进来了SpringMVC的自动配置和我们的扩展配置都会起作用，就是我们所说的拓展\n\n## 全面接管 SpringMVC\n\nSpringBoot对SpringMVC的全部配置不在需要了，所有的配置都是我们自己来配置，只需要在配置类中添加一个@EnableWebMvc\n\n```java\npackage com.hph.springboot.config;\n\nimport org.springframework.context.annotation.Configuration;\nimport org.springframework.web.servlet.config.annotation.EnableWebMvc;\nimport org.springframework.web.servlet.config.annotation.ViewControllerRegistry;\nimport org.springframework.web.servlet.config.annotation.WebMvcConfigurerAdapter;\n\n@EnableWebMvc\n@Configuration\npublic class MyConfig extends WebMvcConfigurerAdapter {\n    @Override\n    public void addViewControllers(ViewControllerRegistry registry) {\n        //super.addViewControllers(registry);\n        //相当于浏览器发送/hello请求,也来到成功页面\n        registry.addViewController(\"/hellomvc\").setViewName(\"success\");\n    }\n}\n```\n\n![ABrn54.png](https://s2.ax1x.com/2019/03/30/ABrn54.png)\n\n当添加@EnableWebMvc的时候SpringMVC的自动配置都失效了\n\n### 建议\n\n如果只是做一些简单的功能，不需要那么强大的功能我们可以使用全面接管，\n\n### 原理\n\n为什么@EnableWebMvc自动配置就失效了呢\n\n进入@EnableWebMvc\n\n```java\n@EnableWebMvc\n@Configuration\npublic class MyConfig extends WebMvcConfigurerAdapter {\n    @Override\n    public void addViewControllers(ViewControllerRegistry registry) {\n        //super.addViewControllers(registry);\n        //相当于浏览器发送/hello请求,也来到成功页面\n        registry.addViewController(\"/hellomvc\").setViewName(\"success\");\n    }\n}\n```\n\n```java\n@Retention(RetentionPolicy.RUNTIME)\n@Target(ElementType.TYPE)\n@Documented\n@Import(DelegatingWebMvcConfiguration.class)\npublic @interface EnableWebMvc {\n}\n```\n\n进入WebMvcAutoConfiguration的\n\n```java\n@Configuration\n@ConditionalOnWebApplication\n@ConditionalOnClass({ Servlet.class, DispatcherServlet.class,\n\t\tWebMvcConfigurerAdapter.class })\n//容器中没有这个组件的时候接下来的自动的自动配置来才生效\n@ConditionalOnMissingBean(WebMvcConfigurationSupport.class)\n@AutoConfigureOrder(Ordered.HIGHEST_PRECEDENCE + 10)\n@AutoConfigureAfter({ DispatcherServletAutoConfiguration.class,\n\t\tValidationAutoConfiguration.class })\n```\n\nEnableWebMvc将WebMvcConfigurationSupport组件导入进来了，这个组件只是SpringMVC的基本功能；\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["SpringBoot"]},{"title":"SpringBoot之Thymeleaf","url":"/2019/03/29/SpringBoot之Thymeleaf/","content":"\n {{ \"探究Thymeleaf模板引擎\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nThymeleaf是用于Web和独立环境的现代服务器端Java模板引擎。\n\nThymeleaf的主要目标是将优雅的自然模板带到您的开发工作流程中—HTML能够在浏览器中正确显示，并且可以作为静态原型，从而在开发团队中实现更强大的协作。Thymeleaf能够处理HTML，XML，JavaScript，CSS甚至纯文本。\n\nThymeleaf的主要目标是提供一个优雅和高度可维护的创建模板的方式。 为了实现这一点，它建立在自然模板的概念之上，以不影响模板作为设计原型的方式将其逻辑注入到模板文件中。 这改善了设计沟通，弥合了前端设计和开发人员之间的理解偏差。\n\nThymeleaf也是从一开始就设计(特别是HTML5)允许创建完全验证的模板。\n\n![ABng4H.png](https://s2.ax1x.com/2019/03/29/ABng4H.png)\n\n## 处理模板\n\n- HTML\n- XML\n- TEXT\n- JAVASCRIPT\n- CSS\n- RAW\n\n有两种标记模板模式(HTML和XML)，三种文本模板模式(TEXT，JAVASCRIPT和CSS)和一种无操作模板模式(RAW)。\n\nHTML模板模式将允许任何类型的HTML输入，包括HTML5，HTML4和XHTML。 将不会执行验证或格式检查，并且在输出中尽可能地遵守模板代码/结构。\n\nXML模板模式将允许XML输入。 在这种情况下，代码应该是格式良好的 - 没有未封闭的标签，没有未加引号的属性等等，如果发现格式错误，解析器将会抛出异常。 请注意，将不会执行验证(针对DTD或XML模式)。\n\nTEXT模板模式将允许对非标记性质的模板使用特殊语法。 这种模板的例子可能是文本电子邮件或模板文档。 请注意，HTML或XML模板也可以作为TEXT处理，在这种情况下，它们不会被解析为标记，而每个标记，DOCTYPE，注释等都将被视为纯文本。\n\nJAVASCRIPT模板模式将允许处理Thymeleaf应用程序中的JavaScript文件。这意味着能够像在HTML文件中一样使用JavaScript文件中的模型数据，但是使用特定于JavaScript的集成(例如专门转义或自然脚本)。 JAVASCRIPT模板模式被认为是文本模式，因此使用与TEXT模板模式相同的特殊语法。\n\nCSS模板模式将允许处理Thymeleaf应用程序中涉及的CSS文件。类似于JAVASCRIPT模式，CSS模板模式也是文本模式，并使用TEXT模板模式中的特殊处理语法。\n\nRAW模板模式根本不会处理模板。它意味着用于将未触及的资源(文件，URL响应等)插入正在处理的模板中。例如，可以将HTML格式的外部非受控资源包含在应用程序模板中，从而安全地知道这些资源可能包含的任何Thymeleaf代码都不会被执行。\n\n## 规则\n\n```java\n@ConfigurationProperties(prefix = \"spring.thymeleaf\")\npublic class ThymeleafProperties {\n\n\tprivate static final Charset DEFAULT_ENCODING = Charset.forName(\"UTF-8\");\n\n\tprivate static final MimeType DEFAULT_CONTENT_TYPE = MimeType.valueOf(\"text/html\");\n\n\tpublic static final String DEFAULT_PREFIX = \"classpath:/templates/\";\n\n\tpublic static final String DEFAULT_SUFFIX = \".html\";\n    //只要我们把HTML页面放在classpath:/templates/，thymeleaf就能自动渲染；\n```\n\n## 使用\n\n引入thymeleaf的命名空间\n\n```html\n<html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\">\n```\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Title</title>\n</head>\n<body>\n    <h1>成功！</h1>\n    <!--th:text 将div里面的文本内容设置为 -->\n    <div th:text=\"${hello}\">这是显示欢迎信息</div>\n</body>\n</html>\n```\n\n![ABu6s0.png](https://s2.ax1x.com/2019/03/29/ABu6s0.png)\n\n未经渲染访问\n\n![ABuhi4.png](https://s2.ax1x.com/2019/03/29/ABuhi4.png)\n\n如果使用了模板引擎则使用了后端的数据，前后端更加合用。\n\n## 语法规则\n\nth:text；改变当前元素里面的文本内容；\n\nth：任意html属性；来替换原生属性的值\n\n![ABKkTS.png](https://s2.ax1x.com/2019/03/29/ABKkTS.png)\n\n```properties\nSimple expressions:（表达式语法）\n    Variable Expressions: ${...}：获取变量值；OGNL；\n    \t\t1）、获取对象的属性、调用方法\n    \t\t2）、使用内置的基本对象：\n    \t\t\t#ctx : the context object.\n    \t\t\t#vars: the context variables.\n                #locale : the context locale.\n                #request : (only in Web Contexts) the HttpServletRequest object.\n                #response : (only in Web Contexts) the HttpServletResponse object.\n                #session : (only in Web Contexts) the HttpSession object.\n                #servletContext : (only in Web Contexts) the ServletContext object.\n                \n                ${session.foo}\n            3）、内置的一些工具对象：\n#execInfo : information about the template being processed.\n#messages : methods for obtaining externalized messages inside variables expressions, in the same way as they would be obtained using #{…} syntax.\n#uris : methods for escaping parts of URLs/URIs\n#conversions : methods for executing the configured conversion service (if any).\n#dates : methods for java.util.Date objects: formatting, component extraction, etc.\n#calendars : analogous to #dates , but for java.util.Calendar objects.\n#numbers : methods for formatting numeric objects.\n#strings : methods for String objects: contains, startsWith, prepending/appending, etc.\n#objects : methods for objects in general.\n#bools : methods for boolean evaluation.\n#arrays : methods for arrays.\n#lists : methods for lists.\n#sets : methods for sets.\n#maps : methods for maps.\n#aggregates : methods for creating aggregates on arrays or collections.\n#ids : methods for dealing with id attributes that might be repeated (for example, as a result of an iteration).\n\n    Selection Variable Expressions: *{...}：选择表达式：和${}在功能上是一样；\n    \t补充：配合 th:object=\"${session.user}：\n   <div th:object=\"${session.user}\">\n    <p>Name: <span th:text=\"*{firstName}\">Sebastian</span>.</p>\n    <p>Surname: <span th:text=\"*{lastName}\">Pepper</span>.</p>\n    <p>Nationality: <span th:text=\"*{nationality}\">Saturn</span>.</p>\n    </div>\n    \n    Message Expressions: #{...}：获取国际化内容\n    Link URL Expressions: @{...}：定义URL；\n    \t\t@{/order/process(execId=${execId},execType='FAST')}\n    Fragment Expressions: ~{...}：片段引用表达式\n    \t\t<div th:insert=\"~{commons :: main}\">...</div>\n    \t\t\nLiterals（字面量）\n      Text literals: 'one text' , 'Another one!' ,…\n      Number literals: 0 , 34 , 3.0 , 12.3 ,…\n      Boolean literals: true , false\n      Null literal: null\n      Literal tokens: one , sometext , main ,…\nText operations:（文本操作）\n    String concatenation: +\n    Literal substitutions: |The name is ${name}|\nArithmetic operations:（数学运算）\n    Binary operators: + , - , * , / , %\n    Minus sign (unary operator): -\nBoolean operations:（布尔运算）\n    Binary operators: and , or\n    Boolean negation (unary operator): ! , not\nComparisons and equality:（比较运算）\n    Comparators: > , < , >= , <= ( gt , lt , ge , le )\n    Equality operators: == , != ( eq , ne )\nConditional operators:条件运算（三元运算符）\n    If-then: (if) ? (then)\n    If-then-else: (if) ? (then) : (else)\n    Default: (value) ?: (defaultvalue)\nSpecial tokens:\n    No-Operation: _ \n```\n\n参考文档： http://note.youdao.com/noteshare?id=1cce0eaf21c8f05d667151541cabfc03&sub=75C83C680CC94CF2A73B7D01407A229B\n\n## Demo\n\n```java\npackage com.hph.springboot.controller;\n\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\nimport java.util.Arrays;\nimport java.util.Map;\n\n@Controller\npublic class HelloController {\n\n    @ResponseBody\n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello SpringBoot\";\n    }\n\n    @RequestMapping(\"/success\")\n    public String success(Map<String,Object> map) {\n        map.put(\"hello\",\"<h1>你好 thymeleaf</h1>\");\n        map.put(\"users\", Arrays.asList(\"张三\",\"李四\",\"王五\",\"赵六\"));\n        return \"success\";\n    }\n}\n```\n\nsuccess.html\n\n```html\n<!DOCTYPE html>\n<html lang=\"en\" xmlns:th=\"http://www.thymeleaf.org\">\n<head>\n    <meta charset=\"UTF-8\">\n    <title>Title</title>\n</head>\n<body>\n<h1>成功！</h1>\n<!--th:text 将div里面的文本内容设置为 -->\n<div id=\"div01\" class=\"myDiv\" th:id=\"${hello}\" th:class=\"${hello}\" th:text=\"${hello}\">这是显示欢迎信息</div>\n<hr/>\n<div th:text=\"${hello}\"></div>\n<div th:utext=\"${hello}\"></div>\n<hr/>\n\n<!-- th:each每次遍历都会生成当前这个标签： 3个h4 -->\n<h4 th:text=\"${user}\"  th:each=\"user:${users}\"></h4>\n<hr/>\n<h4>\n    <span th:each=\"user:${users}\"> [[${user}]] </span>\n</h4>\n<form th:action=\"@{/upload}\" method=\"post\" enctype=\"multipart/form-data\">\n    <input type=\"file\" name=\"file\">\n    <input type=\"submit\"/>\n</form>\n\n</body>\n</html>\n```\n\n![ABQmq0.png](https://s2.ax1x.com/2019/03/29/ABQmq0.png)\n\n\n\n\n\n","tags":["Thymeleaf"],"categories":["SpringBoot"]},{"title":"SpringBoot的Web开发","url":"/2019/03/29/SpringBoot的Web开发/","content":"\n {{ \"SpringBoot 静态资源映射探究 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nSpring Boot 提供了 spring-boot-starter-web 为 Web 开发予以支持，spring-boot-starter- web 为我们提供了嵌入的Tomcat以及Spring MVC的依赖。而Web相关的自动配置存储在 spring-boot-autoconfigure.jar 的org.springfiramework.boot.autoconfigure.web\n\n在当前Spring毫无疑问已经成为java后台对象管理标准框架，除了通过IOC能够管理我们的自定义对象的生命周期之外还提供了众多功能繁复的可配置功能模块。但同时带来了复杂的配置项，这对初学者而言简直是一种灾难。于是SpringBoot应运而生，Springboot的出现大大简化了配置，主要表现在消除了web.xml和依赖注入配置的整合，处处遵循规约大于配置的思想，将初学者在繁杂的配置项中解放出来，专注于业务的实现，而不需要去关注太底层的实现。当然，也可以自己手动添加Web.xml，因为对于高端玩家而言，很多时候配置项还是很有必要的。\n\n![ABFqZd.png](https://s2.ax1x.com/2019/03/29/ABFqZd.png)\n\n从这些文件名可以看出：\n\n```\nServerPropertiesAutoConfiguration 和 ServerProperties 自动配置内嵌 Servlet 容器； HttpEncodingAutoConfiguration 和 HttpEncodingProperties 用来自动配置 http 的编码； MultipartAutoConfiguration 和 MultipartProperties 用来自动配置上传文件的属性； JacksonHttpMessageConvertersConfiguration 用来自动配置 mappingJackson2Http MessageConverter 和 mappingJackson2XmlHttpMessage Converter; \nWebMvcAutoConfiguration 和 WebMvcProperties 配置 Spring MVC\n```\n\n## 工程结构\n\n![ABkQeJ.png](https://s2.ax1x.com/2019/03/29/ABkQeJ.png)\n\n```java\npackage com.hph.springboot.controller;\n\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\n@Controller\npublic class HelloController {\n\n    @ResponseBody\n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"Hello SpringBoot\";\n    }\n}\n```\n\n![ABkweH.png](https://s2.ax1x.com/2019/03/29/ABkweH.png)\n\n## 静态资源映射\n\n在springboot中引入静态资源我们可是以用https://www.webjars.org/该网站上的配置,以jquery为例\n\n在pom中添加这样我们就引入了jquery的静态资源\n\n```xml\n        <!--引入jquery-web-->\n        <dependency>\n            <groupId>org.webjars</groupId>\n            <artifactId>jquery</artifactId>\n            <version>3.3.1</version>\n        </dependency>\n```\n\n![ABkq6U.png](https://s2.ax1x.com/2019/03/29/ABkq6U.png)\n\n这样就在Maven中添加了jquery的依赖那么我们该如何访问呢让我们探究以下一种的秘密:\n\n观察WebMvcAutoConfiguration中的addResourceHandlers方法\n\n```java\n@ConfigurationProperties(prefix = \"spring.resources\", ignoreUnknownFields = false)\npublic class ResourceProperties implements ResourceLoaderAware {\n  //可以设置和静态资源有关的参数，缓存时间等\n```\n\n\n\n```java\n\t@Override\n\t\tpublic void addResourceHandlers(ResourceHandlerRegistry registry) {\n\t\t\tif (!this.resourceProperties.isAddMappings()) {\n\t\t\t\tlogger.debug(\"Default resource handling disabled\");\n\t\t\t\treturn;\n\t\t\t}\n\t\t\tInteger cachePeriod = this.resourceProperties.getCachePeriod();\n\t\t\tif (!registry.hasMappingForPattern(\"/webjars/**\")) {\n\t\t\t\tcustomizeResourceHandlerRegistration(registry\n\t\t\t\t\t\t.addResourceHandler(\"/webjars/**\")\n\t\t\t\t\t\t.addResourceLocations(\"classpath:/META-INF/resources/webjars/\")\n\t\t\t\t\t\t.setCachePeriod(cachePeriod));\n\t\t\t}\n\t\t\tString staticPathPattern = this.mvcProperties.getStaticPathPattern();\n \t\t\t//静态资源文件夹映射\n\t\t\tif (!registry.hasMappingForPattern(staticPathPattern)) {\n\t\t\t\tcustomizeResourceHandlerRegistration(\n\t\t\t\t\t\tregistry.addResourceHandler(staticPathPattern)\n\t\t\t\t\t\t\t\t.addResourceLocations(\n\t\t\t\t\t\t\t\t\t\tthis.resourceProperties.getStaticLocations())\n\t\t\t\t\t\t\t\t.setCachePeriod(cachePeriod));\n\t\t\t}\n\t\t}\n        //配置欢迎页映射\n\t\t@Bean\n\t\tpublic WelcomePageHandlerMapping welcomePageHandlerMapping(\n\t\t\t\tResourceProperties resourceProperties) {\n\t\t\treturn new WelcomePageHandlerMapping(resourceProperties.getWelcomePage(),\n\t\t\t\t\tthis.mvcProperties.getStaticPathPattern());\n\t\t}\n\n       //配置喜欢的图标\n\t\t@Configuration\n\t\t@ConditionalOnProperty(value = \"spring.mvc.favicon.enabled\", matchIfMissing = true)\n\t\tpublic static class FaviconConfiguration {\n\n\t\t\tprivate final ResourceProperties resourceProperties;\n\n\t\t\tpublic FaviconConfiguration(ResourceProperties resourceProperties) {\n\t\t\t\tthis.resourceProperties = resourceProperties;\n\t\t\t}\n\n\t\t\t@Bean\n\t\t\tpublic SimpleUrlHandlerMapping faviconHandlerMapping() {\n\t\t\t\tSimpleUrlHandlerMapping mapping = new SimpleUrlHandlerMapping();\n\t\t\t\tmapping.setOrder(Ordered.HIGHEST_PRECEDENCE + 1);\n              \t//所有  **/favicon.ico \n\t\t\t\tmapping.setUrlMap(Collections.singletonMap(\"**/favicon.ico\",\n\t\t\t\t\t\tfaviconRequestHandler()));\n\t\t\t\treturn mapping;\n\t\t\t}\n\n\t\t\t@Bean\n\t\t\tpublic ResourceHttpRequestHandler faviconRequestHandler() {\n\t\t\t\tResourceHttpRequestHandler requestHandler = new ResourceHttpRequestHandler();\n\t\t\t\trequestHandler\n\t\t\t\t\t\t.setLocations(this.resourceProperties.getFaviconLocations());\n\t\t\t\treturn requestHandler;\n\t\t\t}\n\n\t\t}\n\n```\n\n通过分析得知webjars以jar包的方式引入静态资源,因此我们可以访问\n\n![ABAwcT.png](https://s2.ax1x.com/2019/03/29/ABAwcT.png)\n\n 所有 /webjars/** ，都去 classpath:/META-INF/resources/webjars/ 找资源；\n\n\"/**\" 访问当前项目的任何资源，都去（静态资源的文件夹）找映射\n\n```\n\"classpath:/META-INF/resources/\", \n\"classpath:/resources/\",\n\"classpath:/static/\", \n\"classpath:/public/\" \n\"/\"：当前项目的根路径\n```\n\nlocalhost:8080/abc   去静态资源文件夹里面找abc\n\n![ABE4Gq.png](https://s2.ax1x.com/2019/03/29/ABE4Gq.png)\n\n**注意 :** 如果在添加静态资源之后访问静态资源返回ERROR可重启以下IDEA。\n\n欢迎页； 静态资源文件夹下的所有index.html页面；被\"/**\"映射；\n\n![ABVQeg.png](https://s2.ax1x.com/2019/03/29/ABVQeg.png)\n\n所有的 **/favicon.ico  都是在静态资源文件下找；\n\n改变图标 在resources目录下添加favicon.ico文件\n\n![ABVXnS.png](https://s2.ax1x.com/2019/03/29/ABVXnS.png)\n\n\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"SpringBoot的日志框架","url":"/2019/03/28/SpringBoot的日志框架/","content":"\n {{ \"Spring  Boot的日志框架\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\n在项目的开发中，日志是必不可少的一个记录事件的组件，所以也会相应的在项目中实现和构建我们所需要的日志框架。\n\n而市面上常见的日志框架有很多，比如：JCL、SLF4J、Jboss-logging、jUL、log4j、log4j2、logback等等，我们该如何选择呢？\n\n通常情况下，日志是由一个抽象层+实现层的组合来搭建的。\n\n| 日志门面  （日志的抽象层）                                   | 日志实现                                             |\n| ------------------------------------------------------------ | ---------------------------------------------------- |\n| ~~JCL（Jakarta  Commons Logging）~~    SLF4j（Simple  Logging Facade for Java）    **~~jboss-logging~~** | Log4j  JUL（java.util.logging）  Log4j2  **Logback** |\n\nSpringBoot：底层是Spring框架，Spring框架默认是用JCL；\n\nSpringBoot选用 SLF4j和logback；\n\n## SLF4j使用\n\n使用SLF4j   https://www.slf4j.org\n\n在开法过程中,日志记录方法的调用不应该来直接调用日志的实现类，而是调用日志抽象层里面的方法；\n\n```java\n  @Test\n    public static void main(String[] args) {\n        Logger logger = LoggerFactory.getLogger(HelloWorld.class);\n        logger.info(\"Hello World\");\n    }\n```\n\n```log\n20:33:41.673 [main] INFO com.hph.springboot.HelloWorld - Hello World\n```\n\n![AwTUds.png](https://s2.ax1x.com/2019/03/28/AwTUds.png)\n\n每一个日志的实现框架都有自己的配置文件。使用slf4j以后，配置文件还是做成日志实现框架自己本身的配置文件；\n\n## 遗留问题\n\na（slf4j+logback）: Spring（commons-logging）、Hibernate（jboss-logging）、MyBatis、其他框架\n\n统一日志记录，即使是别的框架和如何和我一起统一使用slf4j进行输出呢?\n\n![AwTbeH.png](https://s2.ax1x.com/2019/03/28/AwTbeH.png)\n\n**如何让系统中所有的日志都统一到slf4j；**\n\n**将系统中其他日志框架先排除出去。**\n\n**用中间包来替换原有的日志框架。**\n\n**我们导入slf4j其他的实现。**\n\n## 日志关系\n\n引入springboot-logging\n\n```xml\n<dependency>\n\t<groupId>org.springframework.boot</groupId>\n\t<artifactId>spring-boot-starter-logging</artifactId>\n</dependency>\n```\n\n### 引入maven以来后\n\n![Aw7H3V.png](https://s2.ax1x.com/2019/03/28/Aw7H3V.png)\n\n由上图我们可以知道在SpringBoot底层也是使用slf4j+logback的方式进行日志记录、SpringBoot也把其他的日志都替换成了slf4j；在者中间发生了什么呢？\n\n原来是包被替换掉了\n\n![AwHeUA.png](https://s2.ax1x.com/2019/03/28/AwHeUA.png)\n\n如果我们要引入其他框架？一定要把这个框架的默认日志依赖移除掉，Spring框架用的是commons-logging；\n\n```xml\n\t\t<dependency>\n\t\t\t<groupId>org.springframework</groupId>\n\t\t\t<artifactId>spring-core</artifactId>\n\t\t\t<exclusions>\n\t\t\t\t<exclusion>\n\t\t\t\t\t<groupId>commons-logging</groupId>\n\t\t\t\t\t<artifactId>commons-logging</artifactId>\n\t\t\t\t</exclusion>\n\t\t\t</exclusions>\n\t\t</dependency>\n```\n\nSpringBoot能自动适配所有的日志，而且底层使用slf4j+logback的方式记录日志，引入其他框架的时候，只需要把这个框架依赖的日志框架排除掉即可；\n\n```java\n\n    @Test\n    public  void contextLoads() {\n        //System.out.println();\n        Logger logger = LoggerFactory.getLogger(getClass());\n\n        //日志的级别；\n        //由低到高   trace<debug<info<warn<error\n        //可以调整输出的日志级别；日志就只会在这个级别以以后的高级别生效\n        logger.trace(\"这是trace日志...\");\n        logger.debug(\"这是debug日志...\");\n        //SpringBoot默认给我们使用的是info级别的，没有指定级别的就用SpringBoot默认规定的级别；root级别\n        logger.info(\"这是info日志...\");\n        logger.warn(\"这是warn日志...\");\n        logger.error(\"这是error日志...\");\n    }\n```\n\n```verilog\n2019-03-28 21:15:40.771  INFO 6056 --- [           main] c.h.s.SpringBootLoggingApplicationTests  : 这是info日志...\n2019-03-28 21:15:40.771  WARN 6056 --- [           main] c.h.s.SpringBootLoggingApplicationTests  : 这是warn日志...\n2019-03-28 21:15:40.772 ERROR 6056 --- [           main] c.h.s.SpringBootLoggingApplicationTests  : 这是error日志...\n\n```\n\n| logging.file | logging.path | Example  | Description                        |\n| ------------ | ------------ | -------- | ---------------------------------- |\n| (none)       | (none)       |          | 只在控制台输出                     |\n| 指定文件名   | (none)       | my.log   | 输出日志到my.log文件               |\n| (none)       | 指定目录     | /var/log | 输出到指定目录的 spring.log 文件中 |\n\n给类路径下放上每个日志框架自己的配置文件即可；SpringBoot就不使用他默认配置的了\n\n| Logging System          | Customization                                                |\n| ----------------------- | ------------------------------------------------------------ |\n| Logback                 | logback-spring.xml, logback-spring.groovy, logback.xml or logback.groovy |\n| Log4j2                  | log4j2-spring.xml or log4j2.xml                          |\n| JDK (Java Util Logging) | logging.properties                                        |\n\n### 输出格式\n\n```\n    日志输出格式：\n\t\t%d表示日期时间，\n\t\t%thread表示线程名，\n\t\t%-5level：级别从左显示5个字符宽度\n\t\t%logger{50} 表示logger名字最长50个字符，否则按照句点分割。 \n\t\t%msg：日志消息，\n\t\t%n是换行符\n    -->\n    %d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{50} - %msg%n\n```\n\n### 配置格式\n\n```properties\nlogging.level.com.hph.springboot=trace\n\n#loging.path=\n#不指定路径在当前目录下生成springboot.log日志\n#可以指定完整的路径\nlogging.file=D:/spring-boot.log\n\n#在控制台输入的日志的格式\nlogging.pattern.console=    %d{yyyy-MM-dd} =====> [%thread] %-5level %logger{50} =====> %msg%n\n#指定文件中日志我输入的格式\nlogging.pattern.file=  %d{yyyy-MM-dd} ----------> [%thread] ---------->%-5level ----------> %logger{50} ----------> %msg%n\n```\n\n控制台\n\n![Awq79I.png](https://s2.ax1x.com/2019/03/28/Awq79I.png)\n\n文件\n\n![AwqOu8.png](https://s2.ax1x.com/2019/03/28/AwqOu8.png)\n\nlogback.xml：直接就被日志框架识别了；\n\n### 指定配置\n\n**logback-spring.xml**：日志框架就不直接加载日志的配置项，由SpringBoot解析日志配置，可以使用SpringBoot的高级Profile功能\n\n```xml\n<springProfile name=\"staging\">\n    <!-- configuration to be enabled when the \"staging\" profile is active -->\n  \t可以指定某段配置只在某个环境下生效\n</springProfile>\n\n```\n\n如：\n\n如果使用logback.xml作为日志配置文件，还要使用profile功能，会有以下错误\n\n `no applicable action for [springProfile]`因此我们可以把logback.xml重命名为logback-spring.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\nscan：当此属性设置为true时，配置文件如果发生改变，将会被重新加载，默认值为true。\nscanPeriod：设置监测配置文件是否有修改的时间间隔，如果没有给出时间单位，默认单位是毫秒当scan为true时，此属性生效。默认的时间间隔为1分钟。\ndebug：当此属性设置为true时，将打印出logback内部日志信息，实时查看logback运行状态。默认值为false。\n-->\n<configuration scan=\"false\" scanPeriod=\"60 seconds\" debug=\"false\">\n    <!-- 定义日志的根目录 -->\n    <property name=\"LOG_HOME\" value=\"D:/log\" />\n    <!-- 定义日志文件名称 -->\n    <property name=\"appName\" value=\"hph-springboot\"></property>\n    <!-- ch.qos.logback.core.ConsoleAppender 表示控制台输出 -->\n    <appender name=\"stdout\" class=\"ch.qos.logback.core.ConsoleAppender\">\n        <!--\n        日志输出格式：\n\t\t\t%d表示日期时间，\n\t\t\t%thread表示线程名，\n\t\t\t%-5level：级别从左显示5个字符宽度\n\t\t\t%logger{50} 表示logger名字最长50个字符，否则按照句点分割。 \n\t\t\t%msg：日志消息，\n\t\t\t%n是换行符\n        -->\n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\n            <springProfile name=\"dev\">\n                <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} ----> [%thread] ---> %-5level %logger{50} - %msg%n</pattern>\n            </springProfile>\n            <springProfile name=\"!dev\">\n                <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} ==== [%thread] ==== %-5level %logger{50} - %msg%n</pattern>\n            </springProfile>\n        </layout>\n    </appender>\n\n    <!-- 滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 -->  \n    <appender name=\"appLogAppender\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">\n        <!-- 指定日志文件的名称 -->\n        <file>${LOG_HOME}/${appName}.log</file>\n        <!--\n        当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名\n        TimeBasedRollingPolicy： 最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动。\n        -->\n        <rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\">\n            <!--\n            滚动时产生的文件的存放位置及文件名称 %d{yyyy-MM-dd}：按天进行日志滚动 \n            %i：当文件大小超过maxFileSize时，按照i进行文件滚动\n            -->\n            <fileNamePattern>${LOG_HOME}/${appName}-%d{yyyy-MM-dd}-%i.log</fileNamePattern>\n            <!-- \n            可选节点，控制保留的归档文件的最大数量，超出数量就删除旧文件。假设设置每天滚动，\n            且maxHistory是365，则只保存最近365天的文件，删除之前的旧文件。注意，删除旧文件是，\n            那些为了归档而创建的目录也会被删除。\n            -->\n            <MaxHistory>365</MaxHistory>\n            <!-- \n            当日志文件超过maxFileSize指定的大小是，根据上面提到的%i进行日志文件滚动 注意此处配置SizeBasedTriggeringPolicy是无法实现按文件大小进行滚动的，必须配置timeBasedFileNamingAndTriggeringPolicy\n            -->\n            <timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\">\n                <maxFileSize>100MB</maxFileSize>\n            </timeBasedFileNamingAndTriggeringPolicy>\n        </rollingPolicy>\n        <!-- 日志输出格式： -->     \n        <layout class=\"ch.qos.logback.classic.PatternLayout\">\n            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [ %thread ] - [ %-5level ] [ %logger{50} : %line ] - %msg%n</pattern>\n        </layout>\n    </appender>\n\n    <!-- \n\t\tlogger主要用于存放日志对象，也可以定义日志类型、级别\n\t\tname：表示匹配的logger类型前缀，也就是包的前半部分\n\t\tlevel：要记录的日志级别，包括 TRACE < DEBUG < INFO < WARN < ERROR\n\t\tadditivity：作用在于children-logger是否使用 rootLogger配置的appender进行输出，\n\t\tfalse：表示只用当前logger的appender-ref，true：\n\t\t表示当前logger的appender-ref和rootLogger的appender-ref都有效\n    -->\n    <!-- hibernate logger -->\n    <logger name=\"com.hph\" level=\"debug\" />\n    <!-- Spring framework logger -->\n    <logger name=\"org.springframework\" level=\"debug\" additivity=\"false\"></logger>\n\n\n    <!-- \n    root与logger是父子关系，没有特别定义则默认为root，任何一个类只会和一个logger对应，\n    要么是定义的logger，要么是root，判断的关键在于找到这个logger，然后判断这个logger的appender和level。 \n    -->\n    <root level=\"info\">\n        <appender-ref ref=\"stdout\" />\n        <appender-ref ref=\"appLogAppender\" />\n    </root>\n</configuration> \n```\n\n![AwONeU.png](https://s2.ax1x.com/2019/03/28/AwONeU.png)\n\n## 切换日志框架\n\n```xml\n<dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n            <exclusions>\n                <exclusion>\n                    <artifactId>logback-classic</artifactId>\n                    <groupId>ch.qos.logback</groupId>\n                </exclusion>\n                <exclusion>\n                    <artifactId>log4j-over-slf4j</artifactId>\n                    <groupId>org.slf4j</groupId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n\n        <dependency>\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n        </dependency>\n        <dependency>\n```\n\n```properties\n### set log levels ###\nlog4j.rootLogger = debug ,stdout ,  D ,  E\n\n### 输出到控制台 ###\nlog4j.appender.stdout = org.apache.log4j.ConsoleAppender\nlog4j.appender.stdout.Target = System.out\nlog4j.appender.stdout.layout = org.apache.log4j.PatternLayout\nlog4j.appender.stdout.layout.ConversionPattern =  %d{ABSOLUTE} ======> %5p ==> %c{ 1 }:%L  --- %m%n\n\n#### 输出到日志文件 ###\nlog4j.appender.D = org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.D.File = D:/log/springboot-log4j.log\nlog4j.appender.D.Append = true\nlog4j.appender.D.Threshold = DEBUG ## 输出DEBUG级别以上的日志\nlog4j.appender.D.layout = org.apache.log4j.PatternLayout\nlog4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n\n#\n#### 保存异常信息到单独文件 ###\n#log4j.appender.D = org.apache.log4j.DailyRollingFileAppender\n#log4j.appender.D.File = logs/error.log ## 异常日志文件名\n#log4j.appender.D.Append = true\n#log4j.appender.D.Threshold = ERROR ## 只输出ERROR级别以上的日志!!!\n#log4j.appender.D.layout = org.apache.log4j.PatternLayout\n#log4j.appender.D.layout.ConversionPattern = %-d{yyyy-MM-dd HH:mm:ss}  [ %t:%r ] - [ %p ]  %m%n\n```\n\n### 控制台\n\n![AwjC8A.png](https://s2.ax1x.com/2019/03/28/AwjC8A.png)\n\n### 日志\n\n![AwXjHO.png](https://s2.ax1x.com/2019/03/28/AwXjHO.png)\n\n## 切换为log4j2\n\n```xml\n   <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n            <exclusions>\n                <exclusion>\n                    <artifactId>spring-boot-starter-logging</artifactId>\n                    <groupId>org.springframework.boot</groupId>\n                </exclusion>\n            </exclusions>\n        </dependency>\n\n<dependency>\n  <groupId>org.springframework.boot</groupId>\n  <artifactId>spring-boot-starter-log4j2</artifactId>\n</dependency>\n```\n\n### 配置\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n    Configuration后面的status，这个用于设置log4j2自身内部的信息输出，可以不设置，当设置成trace时，你会看到log4j2内部各种详细输出。\n-->\n<!--\n    monitorInterval：Log4j能够自动检测修改配置 文件和重新配置本身，设置间隔秒数。\n-->\n<configuration status=\"error\" monitorInterval=\"30\">\n    <!--先定义所有的appender-->\n    <appenders>\n        <!--这个输出控制台的配置-->\n        <Console name=\"Console\" target=\"SYSTEM_OUT\">\n            <!--控制台只输出level及以上级别的信息（onMatch），其他的直接拒绝（onMismatch）-->\n            <ThresholdFilter level=\"trace\" onMatch=\"ACCEPT\" onMismatch=\"DENY\"/>\n            <!--这个都知道是输出日志的格式-->\n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS}  ==> %-5level %class{36} %L %M ===> %msg%xEx%n\"/>\n        </Console>\n        <!--文件会打印出所有信息，这个log每次运行程序会自动清空，由append属性决定，这个也挺有用的，适合临时测试用-->\n        <File name=\"log\" fileName=\"D:/log/log4j2-spring.log\" append=\"false\">\n            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} %-5level %class{36} %L %M - %msg%xEx%n\"/>\n        </File>\n        <!-- 这个会打印出所有的信息，每次大小超过size，则这size大小的日志会自动存入按年份-月份建立的文件夹下面并进行压缩，作为存档-->\n        <RollingFile name=\"RollingFile\" fileName=\"D:/logs/app.log\"\n                     filePattern=\"log/$${date:yyyy-MM}/app-%d{MM-dd-yyyy}-%i.log.gz\">\n            <PatternLayout pattern=\"%d{yyyy-MM-dd 'at' HH:mm:ss z} %-5level %class{36} %L %M - %msg%xEx%n\"/>\n            <SizeBasedTriggeringPolicy size=\"50MB\"/>\n            <!-- DefaultRolloverStrategy属性如不设置，则默认为最多同一文件夹下7个文件，这里设置了20 -->\n            <DefaultRolloverStrategy max=\"20\"/>\n        </RollingFile>\n    </appenders>\n    <!--然后定义logger，只有定义了logger并引入的appender，appender才会生效-->\n    <loggers>\n        <!--建立一个默认的root的logger-->\n        <root level=\"trace\">\n            <appender-ref ref=\"RollingFile\"/>\n            <appender-ref ref=\"Console\"/>\n        </root>\n    </loggers>\n</configuration>\n```\n\n![AwvBWj.png](https://s2.ax1x.com/2019/03/28/AwvBWj.png)\n\n\n\n\n\n\n\n\n\n\n\n","tags":["日志框架"],"categories":["SpringBoot"]},{"title":"SpringBoot自动装配探究","url":"/2019/03/28/SpringBoot自动装配探究/","content":"\n {{ \"Spring Boot自动配置的探究 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 参考\n\n[配置文件能配置的属性参照](https://docs.spring.io/spring-boot/docs/1.5.9.RELEASE/reference/htmlsingle/#common-application-properties)\n\n## 原理\n\nSpringBoot启动的时候加载主配置类，开启了自动配置功能  @EnableAutoConfiguration\n\n### 作用\n\n- 利用EnableAutoConfigurationImportSelector给容器中导入一些组件\n- 查看selectImports()方法的内容\n- List &lt;String&gt; configurations = getCandidateConfigurations(annotationMetadata, attributes);获取候选的配置\n\n```java\npackage com.hph.springboot;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n\n@SpringBootApplication\npublic class HelloworldApplication {\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(HelloworldApplication.class, args);\n\t}\n\n}\n\n```\n\n点击@SpringBootApplication 进入\n\n```java\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@SpringBootConfiguration\n@EnableAutoConfiguration\n@ComponentScan(excludeFilters = {\n\t\t@Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class),\n\t\t@Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) })\npublic @interface SpringBootApplication {\n    ...\n}\n```\n\n点击@EnableAutoConfiguration 进入\n\n```java\n@Target(ElementType.TYPE)\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@AutoConfigurationPackage\n@Import(AutoConfigurationImportSelector.class)\npublic @interface EnableAutoConfiguration {\n\n\tString ENABLED_OVERRIDE_PROPERTY = \"spring.boot.enableautoconfiguration\";\n\n\t/**\n\t * Exclude specific auto-configuration classes such that they will never be applied.\n\t * @return the classes to exclude\n\t */\n\tClass<?>[] exclude() default {};\n\n\t/**\n\t * Exclude specific auto-configuration class names such that they will never be\n\t * applied.\n\t * @return the class names to exclude\n\t * @since 1.3.0\n\t */\n\tString[] excludeName() default {};\n\n}\n```\n\n点击AutoConfigurationImportSelector 进入\n\n```java\npublic class AutoConfigurationImportSelector\n\t\timplements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware,\n\t\tBeanFactoryAware, EnvironmentAware, Ordered {\n\n\tprivate static final AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationEntry();\n\n\tprivate static final String[] NO_IMPORTS = {};\n\n\tprivate static final Log logger = LogFactory\n\t\t\t.getLog(AutoConfigurationImportSelector.class);\n\n\tprivate static final String PROPERTY_NAME_AUTOCONFIGURE_EXCLUDE = \"spring.autoconfigure.exclude\";\n\n\tprivate ConfigurableListableBeanFactory beanFactory;\n\n\tprivate Environment environment;\n\n\tprivate ClassLoader beanClassLoader;\n\n\tprivate ResourceLoader resourceLoader;\n\n\t@Override\n\tpublic String[] selectImports(AnnotationMetadata annotationMetadata) {\n\t\tif (!isEnabled(annotationMetadata)) {\n\t\t\treturn NO_IMPORTS;\n\t\t}\n\t\tAutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader\n\t\t\t\t.loadMetadata(this.beanClassLoader);\n\t\tAutoConfigurationEntry autoConfigurationEntry = getAutoConfigurationEntry(\n\t\t\t\tautoConfigurationMetadata, annotationMetadata);\n\t\treturn StringUtils.toStringArray(autoConfigurationEntry.getConfigurations());\n\t}\n  \tprotected AutoConfigurationEntry getAutoConfigurationEntry(\n\t\t\tAutoConfigurationMetadata autoConfigurationMetadata,\n\t\t\tAnnotationMetadata annotationMetadata) {\n\t\tif (!isEnabled(annotationMetadata)) {\n\t\t\treturn EMPTY_ENTRY;\n\t\t}\n\t\tAnnotationAttributes attributes = getAttributes(annotationMetadata);\n\t\tList<String> configurations = getCandidateConfigurations(annotationMetadata,\n\t\t\t\tattributes);\n\t\tconfigurations = removeDuplicates(configurations);\n\t\tSet<String> exclusions = getExclusions(annotationMetadata, attributes);\n\t\tcheckExcludedClasses(configurations, exclusions);\n\t\tconfigurations.removeAll(exclusions);\n\t\tconfigurations = filter(configurations, autoConfigurationMetadata);\n\t\tfireAutoConfigurationImportEvents(configurations, exclusions);\n\t\treturn new AutoConfigurationEntry(configurations, exclusions);\n\t}\n```\n\n点击getCandidateConfigurations 进入AutoConfigurationImportSelector类\n\n```java\nprotected List<String> getCandidateConfigurations(AnnotationMetadata metadata,\n\t\t\tAnnotationAttributes attributes) {\n\t\tList<String> configurations = SpringFactoriesLoader.loadFactoryNames(\n\t\t\t\tgetSpringFactoriesLoaderFactoryClass(), getBeanClassLoader());\n\t\tAssert.notEmpty(configurations,\n\t\t\t\t\"No auto configuration classes found in META-INF/spring.factories. If you \"\n\t\t\t\t\t\t+ \"are using a custom packaging, make sure that file is correct.\");\n\t\treturn configurations;\n\t}\n```\n\n```\nSpringFactoriesLoader.loadFactoryNames()\n扫描所有jar包类路径下  META-INF/spring.factories\n把扫描到的这些文件的内容包装成properties对象\n从properties中获取到EnableAutoConfiguration.class类（类名）对应的值，然后把他们添加在容器中\n```\n\n![AwKCQ0.png](https://s2.ax1x.com/2019/03/28/AwKCQ0.png)\n\n```properties\n# Initializers\norg.springframework.context.ApplicationContextInitializer=\\\norg.springframework.boot.autoconfigure.SharedMetadataReaderFactoryContextInitializer,\\\norg.springframework.boot.autoconfigure.logging.ConditionEvaluationReportLoggingListener\n\n# Application Listeners\norg.springframework.context.ApplicationListener=\\\norg.springframework.boot.autoconfigure.BackgroundPreinitializer\n\n# Auto Configuration Import Listeners\norg.springframework.boot.autoconfigure.AutoConfigurationImportListener=\\\norg.springframework.boot.autoconfigure.condition.ConditionEvaluationReportAutoConfigurationImportListener\n\n# Auto Configuration Import Filters\norg.springframework.boot.autoconfigure.AutoConfigurationImportFilter=\\\norg.springframework.boot.autoconfigure.condition.OnBeanCondition,\\\norg.springframework.boot.autoconfigure.condition.OnClassCondition,\\\norg.springframework.boot.autoconfigure.condition.OnWebApplicationCondition\n\n# Auto Configure\norg.springframework.boot.autoconfigure.EnableAutoConfiguration=\\\norg.springframework.boot.autoconfigure.admin.SpringApplicationAdminJmxAutoConfiguration,\\\norg.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\\norg.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\\norg.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cache.CacheAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cassandra.CassandraAutoConfiguration,\\\norg.springframework.boot.autoconfigure.cloud.CloudServiceConnectorsAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.ConfigurationPropertiesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.MessageSourceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration,\\\norg.springframework.boot.autoconfigure.couchbase.CouchbaseAutoConfiguration,\\\norg.springframework.boot.autoconfigure.dao.PersistenceExceptionTranslationAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.cassandra.CassandraDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.cassandra.CassandraReactiveRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.cassandra.CassandraRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.couchbase.CouchbaseDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.couchbase.CouchbaseReactiveRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.couchbase.CouchbaseRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.elasticsearch.ElasticsearchRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.jdbc.JdbcRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.jpa.JpaRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.ldap.LdapRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.mongo.MongoDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.mongo.MongoReactiveDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.mongo.MongoReactiveRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.mongo.MongoRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.neo4j.Neo4jDataAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.neo4j.Neo4jRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.solr.SolrRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.redis.RedisAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.redis.RedisReactiveAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.redis.RedisRepositoriesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.rest.RepositoryRestMvcAutoConfiguration,\\\norg.springframework.boot.autoconfigure.data.web.SpringDataWebAutoConfiguration,\\\norg.springframework.boot.autoconfigure.elasticsearch.jest.JestAutoConfiguration,\\\norg.springframework.boot.autoconfigure.elasticsearch.rest.RestClientAutoConfiguration,\\\norg.springframework.boot.autoconfigure.flyway.FlywayAutoConfiguration,\\\norg.springframework.boot.autoconfigure.freemarker.FreeMarkerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.gson.GsonAutoConfiguration,\\\norg.springframework.boot.autoconfigure.h2.H2ConsoleAutoConfiguration,\\\norg.springframework.boot.autoconfigure.hateoas.HypermediaAutoConfiguration,\\\norg.springframework.boot.autoconfigure.hazelcast.HazelcastAutoConfiguration,\\\norg.springframework.boot.autoconfigure.hazelcast.HazelcastJpaDependencyAutoConfiguration,\\\norg.springframework.boot.autoconfigure.http.HttpMessageConvertersAutoConfiguration,\\\norg.springframework.boot.autoconfigure.http.codec.CodecsAutoConfiguration,\\\norg.springframework.boot.autoconfigure.influx.InfluxDbAutoConfiguration,\\\norg.springframework.boot.autoconfigure.info.ProjectInfoAutoConfiguration,\\\norg.springframework.boot.autoconfigure.integration.IntegrationAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jackson.JacksonAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jdbc.JdbcTemplateAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jdbc.JndiDataSourceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jdbc.XADataSourceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jms.JmsAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jms.JndiConnectionFactoryAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jms.activemq.ActiveMQAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jms.artemis.ArtemisAutoConfiguration,\\\norg.springframework.boot.autoconfigure.groovy.template.GroovyTemplateAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jersey.JerseyAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jooq.JooqAutoConfiguration,\\\norg.springframework.boot.autoconfigure.jsonb.JsonbAutoConfiguration,\\\norg.springframework.boot.autoconfigure.kafka.KafkaAutoConfiguration,\\\norg.springframework.boot.autoconfigure.ldap.embedded.EmbeddedLdapAutoConfiguration,\\\norg.springframework.boot.autoconfigure.ldap.LdapAutoConfiguration,\\\norg.springframework.boot.autoconfigure.liquibase.LiquibaseAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mail.MailSenderAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mail.MailSenderValidatorAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mongo.embedded.EmbeddedMongoAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mongo.MongoReactiveAutoConfiguration,\\\norg.springframework.boot.autoconfigure.mustache.MustacheAutoConfiguration,\\\norg.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\\norg.springframework.boot.autoconfigure.quartz.QuartzAutoConfiguration,\\\norg.springframework.boot.autoconfigure.reactor.core.ReactorCoreAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.servlet.SecurityAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.servlet.SecurityRequestMatcherProviderAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.servlet.UserDetailsServiceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.servlet.SecurityFilterAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.reactive.ReactiveSecurityAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.reactive.ReactiveUserDetailsServiceAutoConfiguration,\\\norg.springframework.boot.autoconfigure.sendgrid.SendGridAutoConfiguration,\\\norg.springframework.boot.autoconfigure.session.SessionAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.oauth2.client.servlet.OAuth2ClientAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.oauth2.client.reactive.ReactiveOAuth2ClientAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.oauth2.resource.servlet.OAuth2ResourceServerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.security.oauth2.resource.reactive.ReactiveOAuth2ResourceServerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.solr.SolrAutoConfiguration,\\\norg.springframework.boot.autoconfigure.task.TaskExecutionAutoConfiguration,\\\norg.springframework.boot.autoconfigure.task.TaskSchedulingAutoConfiguration,\\\norg.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\\norg.springframework.boot.autoconfigure.transaction.TransactionAutoConfiguration,\\\norg.springframework.boot.autoconfigure.transaction.jta.JtaAutoConfiguration,\\\norg.springframework.boot.autoconfigure.validation.ValidationAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.client.RestTemplateAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.embedded.EmbeddedWebServerFactoryCustomizerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.HttpHandlerAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.ReactiveWebServerFactoryAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.WebFluxAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.error.ErrorWebFluxAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.function.client.ClientHttpConnectorAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.reactive.function.client.WebClientAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.DispatcherServletAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.ServletWebServerFactoryAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.error.ErrorMvcAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.HttpEncodingAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.MultipartAutoConfiguration,\\\norg.springframework.boot.autoconfigure.web.servlet.WebMvcAutoConfiguration,\\\norg.springframework.boot.autoconfigure.websocket.reactive.WebSocketReactiveAutoConfiguration,\\\norg.springframework.boot.autoconfigure.websocket.servlet.WebSocketServletAutoConfiguration,\\\norg.springframework.boot.autoconfigure.websocket.servlet.WebSocketMessagingAutoConfiguration,\\\norg.springframework.boot.autoconfigure.webservices.WebServicesAutoConfiguration,\\\norg.springframework.boot.autoconfigure.webservices.client.WebServiceTemplateAutoConfiguration\n```\n\n每一个这样的  xxxAutoConfiguration类都是容器中的一个组件，都加入到容器中；用他们来做自动配置；\n\n每一个自动配置类进行自动配置功能；\n\n以**HttpEncodingAutoConfiguration（Http编码自动配置）**为例解释自动配置原理；\n\n```java\n@Configuration   //表示这是一个配置类，以前编写的配置文件一样，也可以给容器中添加组件\n@EnableConfigurationProperties(HttpEncodingProperties.class)  //启动指定类的ConfigurationProperties功能；将配置文件中对应的值和HttpEncodingProperties绑定起来；并把HttpEncodingProperties加入到ioc容器中\n\n@ConditionalOnWebApplication //Spring底层@Conditional注解（Spring注解版），根据不同的条件，如果满足指定的条件，整个配置类里面的配置就会生效；    判断当前应用是否是web应用，如果是，当前配置类生效\n\n@ConditionalOnClass(CharacterEncodingFilter.class)  //判断当前项目有没有这个类CharacterEncodingFilter；SpringMVC中进行乱码解决的过滤器；\n\n@ConditionalOnProperty(prefix = \"spring.http.encoding\", value = \"enabled\", matchIfMissing = true)  //判断配置文件中是否存在某个配置  spring.http.encoding.enabled；如果不存在，判断也是成立的\n//即使我们配置文件中不配置pring.http.encoding.enabled=true，也是默认生效的；\npublic class HttpEncodingAutoConfiguration {\n  \n  \t//他已经和SpringBoot的配置文件映射了\n  \tprivate final HttpEncodingProperties properties;\n  \n   //只有一个有参构造器的情况下，参数的值就会从容器中拿\n  \tpublic HttpEncodingAutoConfiguration(HttpEncodingProperties properties) {\n\t\tthis.properties = properties;\n\t}\n  \n    @Bean   //给容器中添加一个组件，这个组件的某些值需要从properties中获取\n\t@ConditionalOnMissingBean(CharacterEncodingFilter.class) //判断容器没有这个组件？\n\tpublic CharacterEncodingFilter characterEncodingFilter() {\n\t\tCharacterEncodingFilter filter = new OrderedCharacterEncodingFilter();\n\t\tfilter.setEncoding(this.properties.getCharset().name());\n\t\tfilter.setForceRequestEncoding(this.properties.shouldForce(Type.REQUEST));\n\t\tfilter.setForceResponseEncoding(this.properties.shouldForce(Type.RESPONSE));\n\t\treturn filter;\n\t}\n```\n\n一但这个配置类生效；这个配置类就会给容器中添加各种组件；这些组件的属性是从对应的properties类中获取的，这些类里面的每一个属性又是和配置文件绑定的；\n\n所有在配置文件中能配置的属性都是在xxxxProperties类中封装者‘；配置文件能配置什么就可以参照某个功能对应的这个属性类\n\n```java\n@ConfigurationProperties(prefix = \"spring.http.encoding\")  //从配置文件中获取指定的值和bean的属性进行绑定\npublic class HttpEncodingProperties {\n\n   public static final Charset DEFAULT_CHARSET = Charset.forName(\"UTF-8\");\n```\n\n\n\n## 精髓\n\n- SpringBoot启动会加载大量的自动配置类\n- 我们看我们需要的功能有没有SpringBoot默认写好的自动配置类；\n- 我们再来看这个自动配置类中到底配置了哪些组件；（只要我们要用的组件有，我们就不需要再来配置了\n- 给容器中自动配置类添加组件的时候，会从properties类中获取某些属性。我们就可以在配置文件中指定这些属性的值；\n\nxxxxAutoConfigurartion：自动配置类；\n\n给容器中添加组件\n\nxxxxProperties:封装配置文件中相关属性；\n\n\n\n## 细节\n\n@Conditional派生注解（Spring注解版原生的@Conditional作用）\n\n作用：必须是@Conditional指定的条件成立，才给容器中添加组件，配置配里面的所有内容才生效；\n\n| @Conditional扩展注解            | 作用（判断是否满足当前指定条件）                 |\n| ------------------------------- | ------------------------------------------------ |\n| @ConditionalOnJava              | 系统的java版本是否符合要求                       |\n| @ConditionalOnBean              | 容器中存在指定Bean；                             |\n| @ConditionalOnMissingBean       | 容器中不存在指定Bean；                           |\n| @ConditionalOnExpression        | 满足SpEL表达式指定                               |\n| @ConditionalOnClass             | 系统中有指定的类                                 |\n| @ConditionalOnMissingClass      | 系统中没有指定的类                               |\n| @ConditionalOnSingleCandidate   | 容器中只有一个指定的Bean，或者这个Bean是首选Bean |\n| @ConditionalOnProperty          | 系统中指定的属性是否有指定的值                   |\n| @ConditionalOnResource          | 类路径下是否存在指定资源文件                     |\n| @ConditionalOnWebApplication    | 当前是web环境                                    |\n| @ConditionalOnNotWebApplication | 当前不是web环境                                  |\n| @ConditionalOnJndi              | JNDI存在指定项                                   |\n\n**自动配置类必须在一定的条件下才能生效；**\n\n## DEBUG\n\n我们可以通过启用 `debug=true`属性；来让控制台打印自动配置报告，这样我们就可以知道哪些自动配置类生效；\n\n```verilog\n=========================\nAUTO-CONFIGURATION REPORT\n=========================\n\n\nPositive matches:（自动配置类启用的）\n-----------------\n\n   DispatcherServletAutoConfiguration matched:\n      - @ConditionalOnClass found required class 'org.springframework.web.servlet.DispatcherServlet'; @ConditionalOnMissingClass did not find unwanted class (OnClassCondition)\n      - @ConditionalOnWebApplication (required) found StandardServletEnvironment (OnWebApplicationCondition)\n        \n    \nNegative matches:（没有启动，没有匹配成功的自动配置类）\n-----------------\n\n   ActiveMQAutoConfiguration:\n      Did not match:\n         - @ConditionalOnClass did not find required classes 'javax.jms.ConnectionFactory', 'org.apache.activemq.ActiveMQConnectionFactory' (OnClassCondition)\n\n   AopAutoConfiguration:\n      Did not match:\n         - @ConditionalOnClass did not find required classes 'org.aspectj.lang.annotation.Aspect', 'org.aspectj.lang.reflect.Advice' (OnClassCondition)\n```\n\n\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"SpringBoot的配置文件","url":"/2019/03/25/SpringBoot的配置文件/","content":"\n {{ \"Spring  配置文件的相关问题 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 全局配置文件\n\n- application.properties \n- application.yml\n\n## yml简介\n\nyml是YAML（YAML Ain't Markup Language）语言的文件，以数据为中心，比json、xml等更适合做配置文件。\n\nhttp://www.yaml.org/ 参考语法规\n\n### 数据结构\n\n- 对象：键值对的集合\n-  数组：一组按次序排列的值 \n- 字面量：单个的、不可再分的值\n\n#### 对象\n\n对象的一组键值对，使用冒号分隔。如：username: admin \n\n冒号后面跟空格来分开键值； \n\n{k: v}是行内写法\n\n```yaml\nperson:\n  last-name: lisi\n  age: 18\n```\n\n#### 数组\n\n 一组连词线（-）开头的行，构成一个数组，[]为行内写法 – 数组，对象可以组合使\n\n```yaml\n  list:\n    - lisi\n    - zhaoliu\n```\n\n#### 复合结构\n\n &nbsp; &nbsp;以上写法的任意组合都是可以\n\n#### 字面量\n\n &nbsp; &nbsp;数字、字符串、布尔、日期 \n\n &nbsp; &nbsp;字符串 \n\n &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;默认不使用引号\n\n &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;可以使用单引号或者双引号，单引号会转义特殊字符 \n\n &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;字符串可以写成多行，从第二行开始，必须有一个单空格缩进。换行符会被转为空格。\n\n#### 文档\n\n &nbsp; &nbsp;多个文档用 - - - 隔\n\n## 测试\n\n### Person\n\n```java\npackage com.hph.springboot.bean;\n\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\n\n@Component\n@ConfigurationProperties(prefix = \"person\")\npublic class Person {\n    /**\n     * <bean class = \"Person\">\n     *     <property name=\"LastName\" value=\"字面量/${key}从环境变量获取值配置文件获取值/#{SpEL}\"> </property>\n     *  </bean>\n     */\n    private  String lastName;\n    private  Integer age;\n    private  Boolean boss;\n    private Date birth;\n\n    private Map<String,Object> maps;\n    private List<Object> list;\n    private  Dog dog;\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n\n    public Boolean getBoss() {\n        return boss;\n    }\n\n    public void setBoss(Boolean boss) {\n        this.boss = boss;\n    }\n\n    public Date getBirth() {\n        return birth;\n    }\n\n    public void setBirth(Date birth) {\n        this.birth = birth;\n    }\n\n    public Map<String, Object> getMaps() {\n        return maps;\n    }\n\n    public void setMaps(Map<String, Object> maps) {\n        this.maps = maps;\n    }\n\n    public List<Object> getList() {\n        return list;\n    }\n\n    public void setList(List<Object> list) {\n        this.list = list;\n    }\n\n    public Dog getDog() {\n        return dog;\n    }\n\n    public void setDog(Dog dog) {\n        this.dog = dog;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"lastName='\" + lastName + '\\'' +\n                \", age=\" + age +\n                \", boss=\" + boss +\n                \", birth=\" + birth +\n                \", maps=\" + maps +\n                \", list=\" + list +\n                \", dog=\" + dog +\n                '}';\n    }\n}\n\n```\n\n### Dog\n\n```java\npackage com.hph.springboot.bean;\n\npublic class Dog {\n    private  String name;\n    private  Integer age;\n\n    @Override\n    public String toString() {\n        return \"Dog{\" +\n                \"name='\" + name + '\\'' +\n                \", age=\" + age +\n                '}';\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n}\n\n```\n\n### application.yml\n\n```yaml\nperson:\n  last-name: 李四\n  age: 18\n  boss: false\n  birth: 2000/12/12\n  maps: {key1: value1,key2: 12}\n  list:\n    - lisi\n    - zhaoliu\n  dog:\n    name: 狗狗\n    age: 2\n```\n\n### Test\n\n```java\npackage com.hph.springboot;\n\nimport com.hph.springboot.bean.Person;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * SpirngBoot单元测试;\n * 可以在测试期间很方便的类似编码一样进行自动注入等.\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class HelloworldApplicationTests {\n\n    @Autowired\n    Person person;\n\n    @Test\n    public void contextLoads() {\n        System.out.println(person);\n    }\n\n}\n```\n\n```tex\nPerson{lastName='李四', age=18, boss=false, birth=Tue Dec 12 00:00:00 CST 2000, maps={key1=value1, key2=12}, list=[lisi, zhaoliu], dog=Dog{name='狗狗', age=2}}\n```\n\n## 注解获取值对比\n\n @Value获取值和@ConfigurationProperties获取值比较\n\n|                      | @ConfigurationProperties | @Value     |\n| -------------------- | ------------------------ | ---------- |\n| 功能                 | 批量注入配置文件中的属性 | 一个个指定 |\n| 松散绑定（松散语法） | 支持                     | 不支持     |\n| SpEL                 | 不支持                   | 支持       |\n| JSR303数据校验       | 支持                     | 不支持     |\n| 复杂类型封装         | 支持                     | 不支持     |\n\n只是在某个业务逻辑中需要获取一下配置文件中的某项值，使用@Value；\n\n专门编写了一个javaBean来和配置文件进行映射，直接使用@ConfigurationProperties；\n\n## 属性命名规则\n\n- person.firstName：使用标准方式 \n- person.first-name：大写用 \n- person.first_name：大写用_\n- PERSON_FIRST_NAME\n\n## 配置文件注入值数据校验\n\n```java\npackage com.hph.springboot.bean;\n\nimport org.springframework.beans.factory.annotation.Value;\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\nimport org.springframework.validation.annotation.Validated;\n\nimport javax.validation.constraints.Email;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\n\n@Component\n@ConfigurationProperties(prefix = \"person\")\n@Validated\npublic class Person {\n\n    /**\n     * <bean class=\"Person\">\n     * <property name=\"lastName\" value=\"字面量/${key}从环境变量、配置文件中获取值/#{SpEL}\"></property>\n     * <bean/>\n     */\n\n    //lastName必须是邮箱格式\n    @Email\n    private String lastName;\n    @Value(\"#{11-2}\")\n    private Integer age;\n    @Value(\"True\")\n    private Boolean boss;\n    private Date birth;\n\n    private Map<String, Object> maps;\n    private List<Object> list;\n    private Dog dog;\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n\n    public Boolean getBoss() {\n        return boss;\n    }\n\n    public void setBoss(Boolean boss) {\n        this.boss = boss;\n    }\n\n    public Date getBirth() {\n        return birth;\n    }\n\n    public void setBirth(Date birth) {\n        this.birth = birth;\n    }\n\n    public Map<String, Object> getMaps() {\n        return maps;\n    }\n\n    public void setMaps(Map<String, Object> maps) {\n        this.maps = maps;\n    }\n\n    public List<Object> getList() {\n        return list;\n    }\n\n    public void setList(List<Object> list) {\n        this.list = list;\n    }\n\n    public Dog getDog() {\n        return dog;\n    }\n\n    public void setDog(Dog dog) {\n        this.dog = dog;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"lastName='\" + lastName + '\\'' +\n                \", age=\" + age +\n                \", boss=\" + boss +\n                \", birth=\" + birth +\n                \", maps=\" + maps +\n                \", list=\" + list +\n                \", dog=\" + dog +\n                '}';\n    }\n}\n```\n\n![AtbEtK.png](https://s2.ax1x.com/2019/03/25/AtbEtK.png)\n\n\n\n## 加载指定的配置文件\n\n### person.properties\n\n```properties\nperson.last-name=李四\nperson.age=18\nperson.birth=2019/3/25\nperson.boss=false\nperson.maps.k1=v1\nperson.maps.k2=v2\nperson.maps.k3=v3\nperson.maps.k4=14\nperson.lista,b,c,d\nperson.dog.name=dog\nperson.dog.age=15\n```\n\n### Person\n\n```java\npackage com.hph.springboot.bean;\n\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.context.annotation.PropertySource;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\n\n@PropertySource(value = {\"classpath:person.properties\"})\n@Component\n@ConfigurationProperties(prefix = \"person\")\npublic class Person {\n\n    /**\n     * <bean class=\"Person\">\n     * <property name=\"lastName\" value=\"字面量/${key}从环境变量、配置文件中获取值/#{SpEL}\"></property>\n     * <bean/>\n     */\n\n    private String lastName;\n    private Integer age;\n    private Boolean boss;\n    private Date birth;\n\n    private Map<String, Object> maps;\n    private List<Object> list;\n    private Dog dog;\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n\n    public Boolean getBoss() {\n        return boss;\n    }\n\n    public void setBoss(Boolean boss) {\n        this.boss = boss;\n    }\n\n    public Date getBirth() {\n        return birth;\n    }\n\n    public void setBirth(Date birth) {\n        this.birth = birth;\n    }\n\n    public Map<String, Object> getMaps() {\n        return maps;\n    }\n\n    public void setMaps(Map<String, Object> maps) {\n        this.maps = maps;\n    }\n\n    public List<Object> getList() {\n        return list;\n    }\n\n    public void setList(List<Object> list) {\n        this.list = list;\n    }\n\n    public Dog getDog() {\n        return dog;\n    }\n\n    public void setDog(Dog dog) {\n        this.dog = dog;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"lastName='\" + lastName + '\\'' +\n                \", age=\" + age +\n                \", boss=\" + boss +\n                \", birth=\" + birth +\n                \", maps=\" + maps +\n                \", list=\" + list +\n                \", dog=\" + dog +\n                '}';\n    }\n}\n```\n\n### Test\n\n```java\npackage com.hph.springboot;\n\nimport com.hph.springboot.bean.Person;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * SpirngBoot单元测试;\n * 可以在测试期间很方便的类似编码一样进行自动注入等.\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class HelloworldApplicationTests {\n\n    @Autowired\n    Person person;\n\n    @Test\n    public void contextLoads() {\n        System.out.println(person);\n    }\n\n}\n\n```\n\n```\nPerson{lastName='李四', age=18, boss=false, birth=Tue Dec 12 00:00:00 CST 2000, maps={key1=value1, key2=12, k1=v1, k4=14, k3=v3, k2=v2}, list=[lisi, zhaoliu], dog=Dog{name='狗狗', age=2}}\n```\n\n## 加载指定的Spring配置文件\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\">\n    <bean id=\"helloService\" class=\"com.hph.springboot.Service.HelloWorldService\"></bean>\n</beans>\n```\n\n```java\npackage com.hph.springboot;\n\nimport com.hph.springboot.bean.Person;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * SpirngBoot单元测试;\n * 可以在测试期间很方便的类似编码一样进行自动注入等.\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class HelloworldApplicationTests {\n\n    @Autowired\n    ApplicationContext ioc;\n\n    @Test\n    public void testHelloService() {\n        boolean b = ioc.containsBean(\"helloService\");\n        System.out.println(b);\n\n    }\n\n}\n\n```\n\nSpring Boot里面没有Spring的配置文件，我们自己编写的配置文件，也不能自动识别；\n\n@**ImportResource**：导入Spring的配置文件，让配置文件里面的内容生效；\n\n想让Spring的配置文件生效，加载进来；@**ImportResource**标注在一个配置类上\n\n```java\npackage com.hph.springboot;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\nimport org.springframework.context.annotation.ImportResource;\n\n//引入本地配置文件\n@ImportResource(locations = {\"classpath:beans.xml\"})\n@SpringBootApplication\npublic class HelloworldApplication {\n\n   public static void main(String[] args) {\n      SpringApplication.run(HelloworldApplication.class, args);\n   }\n\n}\n```\n\n结果为True\n\n## 配置类（全注解）\n\nSpringBoot推荐给容器中添加组件的方式；推荐使用全注解的方式\n\n1、配置类**@Configuration**------>Spring配置文件\n\n2、使用**@Bean**给容器中添加组件\n\n```java\npackage com.hph.springboot.Service;\n\npublic class HelloService {\n}\n```\n\n```java\npackage com.hph.springboot.config;\n\nimport com.hph.springboot.Service.HelloService;\nimport org.springframework.context.annotation.Bean;\nimport org.springframework.context.annotation.Configuration;\n\n/**\n * @Configuration :指明当前类为配置类,就是来替代之前的spring配置文件\n */\n@Configuration\npublic class MyAppConfig {\n\n    //将方法的发挥值添加到容器中 容器中这个组件默认的id就是方法\n    @Bean\n    public HelloService helloService() {\n        return new HelloService();\n\n    }\n}\n\n```\n\n```java\npackage com.hph.springboot;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n\n@SpringBootApplication\npublic class HelloworldApplication {\n\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(HelloworldApplication.class, args);\n\t}\n\n}\n\n```\n\n```java\npackage com.hph.springboot;\n\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * SpirngBoot单元测试;\n * 可以在测试期间很方便的类似编码一样进行自动注入等.\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class HelloworldApplicationTests {\n\n\n    @Autowired\n    ApplicationContext ioc;\n\n    @Test\n    public void testHelloService() {\n        boolean b = ioc.containsBean(\"helloService\");\n        System.out.println(\"配置类@Bean给容器中添加组件了...\");\n        System.out.println(b);\n\n    }\n}\n```\n\n![AtLzp8.png](https://s2.ax1x.com/2019/03/25/AtLzp8.png)\n\n## 配置文件占位符\n\n### application.properties\n\n```properties\nperson.last-name=张三${random.uuid}\nperson.age=${random.int}\nperson.birth=2001/12/26\nperson.boss=false\nperson.maps.k1=v1\nperson.maps.k2=集合\nperson.list=a,b,c,d,e,f\nperson.dog.name=${person.last-name}_狗狗\nperson.dog.age=3\n```\n\n### Person\n\n```java\npackage com.hph.springboot.bean;\n\nimport org.springframework.boot.context.properties.ConfigurationProperties;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Map;\n\n@Component\n@ConfigurationProperties(prefix = \"person\")\npublic class Person {\n\n    /**\n     * <bean class=\"Person\">\n     * <property name=\"lastName\" value=\"字面量/${key}从环境变量、配置文件中获取值/#{SpEL}\"></property>\n     * <bean/>\n     */\n\n    private String lastName;\n    private Integer age;\n    private Boolean boss;\n    private Date birth;\n\n    private Map<String, Object> maps;\n    private List<Object> list;\n    private Dog dog;\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n\n    public Boolean getBoss() {\n        return boss;\n    }\n\n    public void setBoss(Boolean boss) {\n        this.boss = boss;\n    }\n\n    public Date getBirth() {\n        return birth;\n    }\n\n    public void setBirth(Date birth) {\n        this.birth = birth;\n    }\n\n    public Map<String, Object> getMaps() {\n        return maps;\n    }\n\n    public void setMaps(Map<String, Object> maps) {\n        this.maps = maps;\n    }\n\n    public List<Object> getList() {\n        return list;\n    }\n\n    public void setList(List<Object> list) {\n        this.list = list;\n    }\n\n    public Dog getDog() {\n        return dog;\n    }\n\n    public void setDog(Dog dog) {\n        this.dog = dog;\n    }\n\n    @Override\n    public String toString() {\n        return \"Person{\" +\n                \"lastName='\" + lastName + '\\'' +\n                \", age=\" + age +\n                \", boss=\" + boss +\n                \", birth=\" + birth +\n                \", maps=\" + maps +\n                \", list=\" + list +\n                \", dog=\" + dog +\n                '}';\n    }\n}\n```\n\n### Dog\n\n```java\npackage com.hph.springboot.bean;\n\npublic class Dog {\n    private  String name;\n    private  Integer age;\n\n    @Override\n    public String toString() {\n        return \"Dog{\" +\n                \"name='\" + name + '\\'' +\n                \", age=\" + age +\n                '}';\n    }\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public Integer getAge() {\n        return age;\n    }\n\n    public void setAge(Integer age) {\n        this.age = age;\n    }\n}\n```\n\n### Test\n\n```java\npackage com.hph.springboot;\n\nimport com.hph.springboot.bean.Person;\nimport org.junit.Test;\nimport org.junit.runner.RunWith;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.boot.test.context.SpringBootTest;\nimport org.springframework.test.context.junit4.SpringRunner;\n\n/**\n * SpirngBoot单元测试;\n * 可以在测试期间很方便的类似编码一样进行自动注入等.\n */\n@RunWith(SpringRunner.class)\n@SpringBootTest\npublic class HelloworldApplicationTests {\n\n    @Autowired\n    Person person;\n\n    @Test\n    public void contextLoads() {\n        System.out.println(person);\n    }\n}\n\n```\n\n### 结果\n\n```\nPerson{lastName='张三0563bb3c-32a3-4905-96f6-5b98f95e3f91', age=952443557, boss=false, birth=Wed Dec 26 00:00:00 CST 2001, maps={k1=v1, k2=集合}, list=[a, b, c, d, e, f], dog=Dog{name='张三68aa9e66-24d1-4134-be1a-935c598fe59c_狗狗', age=3}}\n```\n\n## 多环境支持\n\n我们在主配置文件编写的时候，文件名可以是   application-{profile}.properties或者是yml配置文件 默认使用application.properties的配置；\n\n```yaml\n#默认8081\nserver:\n  port: 8081\nspring:\n  profiles:\n    active: prod #启动dev环境\n\n---\nserver:\n  port: 8080\nspring:\n  profiles: dev\n---\nserver:\n  port: 80\nspring:\n  profiles: prod #指定属于哪个环境\n```\n\n### 激活指定profile\n\n- 在配置文件中指定  spring.profiles.active=dev\n-  命令行：java -jar spring-boot-02-config-0.0.1-SNAPSHOT.jar --spring.profiles.active=dev；可以直接在测试的时候，配置传入命令行参数\n- 虚拟机参数；-Dspring.profiles.active=dev\n\n## 配置文件加载\n\nspringboot 启动会扫描以下位置的application.properties或者application.yml文件作为Spring boot的默认配置文件\n\n–file:./config/\n\n–file:./\n\n–classpath:/config/\n\n–classpath:/\n\n优先级由高到底，高优先级的配置会覆盖低优先级的配置；\n\nSpringBoot会从这四个位置全部加载主配置文件；**互补配置**；\n\n我们还可以通过`spring.config.location`来改变默认的配置文件位置\n\n**项目打包好以后，我们可以使用命令行参数的形式，启动项目的时候来指定配置文件的新位置；指定配置文件和默认加载的这些配置文件共同起作用形成互补配置；**\n\njava -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --spring.config.location=G:/application.properties\n\n## 外部配置加载顺序\n\n**SpringBoot也可以从以下位置加载配置； 优先级从高到低；高优先级的配置覆盖低优先级的配置，所有的配置会形成互补配置**\n\n**1.命令行参数**\n\n所有的配置都可以在命令行上进行指定\n\njava -jar spring-boot-02-config-02-0.0.1-SNAPSHOT.jar --server.port=8087  --server.context-path=/abc\n\n多个配置用空格分开； --配置项=值\n\n2.来自java:comp/env的JNDI属性\n\n3.Java系统属性（System.getProperties()）\n\n4.操作系统环境变量\n\n5.RandomValuePropertySource配置的random.*属性值\n\n**由jar包外向jar包内进行寻找；**\n\n**优先加载带profile**\n\n**6.jar包外部的application-{profile}.properties或application.yml(带spring.profile)配置文件**\n\n**7.jar包内部的application-{profile}.properties或application.yml(带spring.profile)配置文件**\n\n**再来加载不带profile**\n\n**8.jar包外部的application.properties或application.yml(不带spring.profile)配置文件**\n\n**9.jar包内部的application.properties或application.yml(不带spring.profile)配置文件**\n\n10.@Configuration注解类上的@PropertySource\n\n11.通过SpringApplication.setDefaultProperties指定的默认属性\n\n所有支持的配置加载来源；\n\n[参考官方文档](https://docs.spring.io/spring-boot/docs/1.5.9.RELEASE/reference/htmlsingle/#boot-features-external-config)\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"SpringBoot初识","url":"/2019/03/25/SpringBoot初识/","content":"\n {{ \"Spring  Boot的初体验和自动配置的探究 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nSpring Boot是由Pivotal团队提供的全新框架，其设计目的是用来简化新Spring应用的初始搭建以及开发过程。该框架使用了特定的方式来进行配置，从而使开发人员不再需要定义样板化的配置。通过这种方式，Spring Boot致力于在蓬勃发展的快速应用开发领域(rapid application development)成为领导者。\n\n## 原因\n\n随着动态语言的流行（Ruby、Groovy、Scala、Node.js)，Java的幵发显得格外的笨重：繁多的配置、低下的开发效率、复杂的部署流程以及第三方技术集成难度大。在上述环境下，Spring Boot应运而生。它使用“习惯优于配置”（项目中存在大量的配置，此外还内置一个习惯性的配置，让你无须手动进行配置）的理念让你的项目快速运行起来。使用Spring Boot很容易创建一个独立运行(运行jar,内嵌Servlet容器）、准生产级别的基于Spring框架的项目，使用Spring Boot你可以不用或者只需要很少的Spring配置。\n\n## 特点\n\n创建独立的Spring应用程序:Spring Boot可以以jar包的形式独立运行，运行一个Spring Boot项目只需通过java -jar xx.jar来运行。\n\n嵌入的Tomcat，无需部署WAR文件:Spring Boot可选择内嵌Tomcat、Jetty或者Undertow ,这样我们无须以war包形式部署项目。\n\n简化Maven配置:Spring提供了一系列的starter pom来简化Maven的依赖加载，spring-boot-starter-web\n\n自动配置Spring:Spring Boot会根据在类路径中的jar包、类，为jar包里的类自动配置Bean，.这样会极大地减少我们要使用的配置。当然,Spring Boot只是考虑了大多数的开发场景，并不是所有的场景，若在实际开发中我们需要自动配置Bean，而Spring Boot没有提供支持，则可以自定义自动配置\n\n提供生产就绪型功能，如指标，健康检查和外部配置:Spring Boot提供基于http、ssh、telnet对运行时的项目进行监控\n\n绝对没有代码生成并且对XML也没有配置要求:Spring Boot的神奇的不是借助于代码生成来实现的，而是通过条件注解来实现的，这是 Spring 4.x提供的新特性，Spring 4.x提倡使用Java配置和注解配置组合，而Spring Boot不需要任何xml配置即可实现Spring的所有配置.\n\n## 单体应用\n\n![AtK9tx.png](https://s2.ax1x.com/2019/03/25/AtK9tx.png)\n\n## 微服务\n\n![AtKZBd.png](https://s2.ax1x.com/2019/03/25/AtKZBd.png)\n\n[微服务文档](https://martinfowler.com/articles/microservices.html#MicroservicesAndSoa)\n\n通信(Http)\n\n![AtK1gS.png](https://s2.ax1x.com/2019/03/25/AtK1gS.png)\n\n![AtKNEn.png](https://s2.ax1x.com/2019/03/25/AtKNEn.png)\n\n## 优点\n\n- 快速创建独立运行的Spring项目以及与主流框架集成 \n- 使用嵌入式的Servlet容器，应用无需打成WAR包 \n- starters自动依赖与版本控制 \n- 大量的自动配置，简化开发，也可修改默认值 \n- 无需配置XML，无代码生成，开箱即用 \n- 准生产环境的运行时应用监控 – 与云计算的天然集成\n\n## 缺点\n\n- 将现有或传统的Spring Framework项目转换为Spring Boot应用程序是一个非常困难和耗时的过程。它仅适用于全新Spring项目；\n- 集成度较高，使用过程中不太容易了解底层；\n\n## HelloWorld\n\n初始化项目\n\n![AtuYY6.png](https://s2.ax1x.com/2019/03/25/AtuYY6.png)\n\n2.设置\n\n![AtualD.png](https://s2.ax1x.com/2019/03/25/AtualD.png)\n\n3.选中web组件\n\n![Atud6e.png](https://s2.ax1x.com/2019/03/25/Atud6e.png)\n\n3.下一步\n\n![AtuwOH.png](https://s2.ax1x.com/2019/03/25/AtuwOH.png)\n\n4.项目结构\n\n![AtufXQ.png](https://s2.ax1x.com/2019/03/25/AtufXQ.png)\n\n添加ava类\n\n```java\npackage com.hph.springboot.controller;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\n\n@Controller\npublic class HelloController {\n    @RequestMapping(\"/helloworld\")\n    public  @ResponseBody String hello(){\n        return \"Hello Spring Boot \";\n\n    }\n}\n```\n\n![AtuOcF.png](https://s2.ax1x.com/2019/03/25/AtuOcF.png)\n\n启动服务\n\n![AtuxB9.png](https://s2.ax1x.com/2019/03/25/AtuxB9.png)\n\n## 部署项目\n\n如果没有SpringBoot的Maven插件 则需要在pom.xml加入\n\n```xml\n    <build>\n        <plugins>\n            <plugin>\n                <groupId>org.springframework.boot</groupId>\n                <artifactId>spring-boot-maven-plugin</artifactId>\n            </plugin>\n        </plugins>\n    </build>\n```\n\n\n\n### 打包\n\n![AtKR4x.png](https://s2.ax1x.com/2019/03/25/AtKR4x.png)\n\n### 测试\n\n![AtKTDH.png](https://s2.ax1x.com/2019/03/25/AtKTDH.png)\n\n## 分析\n\n为什么我们什么也没有配置 就起来了一个JavaEE项目呢.\n\n### 父项目\n\n父项目在传统开发中做依赖管理,在SpringBoot中\n\n```xml\n  <parent>\n        <groupId>org.springframework.boot</groupId>\n        <artifactId>spring-boot-starter-parent</artifactId>\n        <version>2.1.3.RELEASE</version>\n        <relativePath/>\n\n   \n<!--Spring Boot Dependencies管理应用里的所有依赖版本-->\n  <parent>\n    <groupId>org.springframework.boot</groupId>\n    <artifactId>spring-boot-dependencies</artifactId>\n    <version>2.1.3.RELEASE</version>\n    <relativePath>../spring-boot-dependencies</relativePath>\n  </parent>\n```\n\nSpringBoot的版本的仲裁中心：\n\n所有的版本依赖都在 dependencies 里面管理（如果没有在dependenciesl里面的依赖要声明版本号）\n\n### 导入依赖\n\n```xml\n    <dependency>\n            <groupId>org.springframework.boot</groupId>\n            <artifactId>spring-boot-starter-web</artifactId>\n        </dependency>\n```\n\nspring-boot-starter-web:\n\nspring-boot-starter-web:spring-boot场景启动器，点进去发现springboot场景启帮我们导入了webmo模块正常运行的所依赖的组件。\n\nSpring Boot将所有的功能场景都抽取出来，做成一个个的starters（启动器），只需要在项目里面引入这些starter相关场景的所有依赖都会导入进来。要用什么功能就导入什么场景的启动器\n\n```xml\n<dependencies>\n    <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-starter</artifactId>\n      <version>2.1.3.RELEASE</version>\n      <scope>compile</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-starter-json</artifactId>\n      <version>2.1.3.RELEASE</version>\n      <scope>compile</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.springframework.boot</groupId>\n      <artifactId>spring-boot-starter-tomcat</artifactId>\n      <version>2.1.3.RELEASE</version>\n      <scope>compile</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.hibernate.validator</groupId>\n      <artifactId>hibernate-validator</artifactId>\n      <version>6.0.14.Final</version>\n      <scope>compile</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.springframework</groupId>\n      <artifactId>spring-web</artifactId>\n      <version>5.1.5.RELEASE</version>\n      <scope>compile</scope>\n    </dependency>\n    <dependency>\n      <groupId>org.springframework</groupId>\n      <artifactId>spring-webmvc</artifactId>\n      <version>5.1.5.RELEASE</version>\n      <scope>compile</scope>\n    </dependency>\n  </dependencies>\n```\n\n### 主程序\n\n```java\npackage com.hph.springboot;\n\nimport org.springframework.boot.SpringApplication;\nimport org.springframework.boot.autoconfigure.SpringBootApplication;\n\n@SpringBootApplication\npublic class HelloworldApplication {\n\n    public static void main(String[] args) {\n        SpringApplication.run(HelloworldApplication.class, args);\n    }\n\n}\n\n```\n\n## 自动配置\n\n如果我们注释掉主程序类启动则会\n\n![AtQalT.png](https://s2.ax1x.com/2019/03/25/AtQalT.png)\n\n```java\n@Target({ElementType.TYPE})\n@Retention(RetentionPolicy.RUNTIME)\n@Documented\n@Inherited\n@SpringBootConfiguration\n@EnableAutoConfiguration\n@ComponentScan(\n    excludeFilters = {@Filter(\n    type = FilterType.CUSTOM,\n    classes = {TypeExcludeFilter.class}\n), @Filter(\n    type = FilterType.CUSTOM,\n    classes = {AutoConfigurationExcludeFilter.class}\n)}\n)\npublic @interface SpringBootApplication {\n    @AliasFor(\n        annotation = EnableAutoConfiguration.class\n    )\n    Class<?>[] exclude() default {};\n\n    @AliasFor(\n        annotation = EnableAutoConfiguration.class\n    )\n    String[] excludeName() default {};\n\n    @AliasFor(\n        annotation = ComponentScan.class,\n        attribute = \"basePackages\"\n    )\n    String[] scanBasePackages() default {};\n\n    @AliasFor(\n        annotation = ComponentScan.class,\n        attribute = \"basePackageClasses\"\n    )\n    Class<?>[] scanBasePackageClasses() default {};\n}\n\n```\n\n注解源码主要组合了©Configuration、@EnableAutoConfiguration、@ComponentScan ；\n\n![AtQq9P.png](https://s2.ax1x.com/2019/03/25/AtQq9P.png)\n\n@SpringBootApplication :Spring Boot应用标注在某个类上说明这个类是SpringBoot的主配置类，SpringBoot就应该运行这个类的main方法启动SpringBoot应用\n\n@**SpringBootConfiguration**:Spring Boot的配置类:标注在某个类上，表示这是一个Spring Boot的配置类；\n\n@**Configuration**:配置类上来标注这个注解；配置类 -----  配置文件；配置类也是容器中的一个组件；@Component\n\n@**EnableAutoConfiguration**：开启自动配置功能；以前我们需要配置的东西，Spring Boot帮我们自动配置；@**EnableAutoConfiguration**告诉SpringBoot开启自动配置功能；这样自动配置才能生效；\n\n```java\n@AutoConfigurationPackage\n@Import({AutoConfigurationImportSelector.class})\npublic @interface EnableAutoConfiguration {\n```\n\n@AutoConfigurationPackage自动配置包\n\n@Import({AutoConfigurationImportSelector.class})\nSpring的底层注解@import，给容器导入了一个组件；组件由AutoConfigurationPackages.Registrar.class；\n\n将主配置类（@SpringBootApplication标注的类）的所在包及下面所有子包里面的所有组件扫描到Spring容器 \n@**Import**(EnableAutoConfigurationImportSelector.class)；\n\n给容器中导入组件：\n\n**EnableAutoConfigurationImportSelector**：导入哪些组件的选择器；\n\n将所有需要导入的组件以全类名的方式返回；这些组件就会被添加到容器中；\n\n会给容器中导入非常多的自动配置类（xxxAutoConfiguration）；就是给容器中导入这个场景需要的所有组件，并配置好这些组件；\n\nSpring Boot在启动的时候从类路径下的META-INF/spring.factories中获取EnableAutoConfiguration指定的值，将这些值作为自动配置类导入到容器中，自动配置类就生效，帮我们进行自动配置工作；以前我们需要自己配置的东西，自动配置类都帮我们；\n\n\n\n","tags":["SpringBoot"],"categories":["SpringBoot"]},{"title":"SSM集成","url":"/2019/03/24/SSM集成/","content":"\n {{ \"SpringMVC, Spring, Mybatis  框架集\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nSSM（Spring+SpringMVC+MyBatis）框架集由Spring、MyBatis两个开源框架整合而成（SpringMVC是Spring中的部分内容）。常作为数据源较简单的web项目的框架。\n\n### Spring\n　　Spring就像是整个项目中装配bean的大工厂，在配置文件中可以指定使用特定的参数去调用实体类的构造方法来实例化对象。也可以称之为项目中的粘合剂。\n　　Spring的核心思想是IoC（控制反转），即不再需要程序员去显式地`new`一个对象，而是让Spring框架帮你来完成这一切。\n### SpringMVC\n　　SpringMVC在项目中拦截用户请求，它的核心Servlet即DispatcherServlet承担中介或是前台这样的职责，将用户请求通过HandlerMapping去匹配Controller，Controller就是具体对应请求所执行的操作。SpringMVC相当于SSH框架中struts。\n### mybatis\n　　mybatis是对jdbc的封装，它让数据库底层操作变的透明。mybatis的操作都是围绕一个sqlSessionFactory实例展开的。mybatis通过配置文件关联到各实体类的Mapper文件，Mapper文件中配置了每个类对数据库所需进行的sql语句映射。在每次与数据库交互时，通过sqlSessionFactory拿到一个sqlSession，再执行sql命令。\n\n页面发送请求给控制器，控制器调用业务层处理逻辑，逻辑层向持久层发送请求，持久层与数据库交互，后将结果返回给业务层，业务层将处理逻辑发送给控制器，控制器再调用视图展现数据\n\n## 创建项目\n\n首先我们先创建Maven项目\n\n![AY8qTH.png](https://s2.ax1x.com/2019/03/24/AY8qTH.png)\n\n![AYGS6f.png](https://s2.ax1x.com/2019/03/24/AYGS6f.png)\n\n![AYGP0g.png](https://s2.ax1x.com/2019/03/24/AYGP0g.png)\n\n项目结构如下\n\n![AYGkkj.png](https://s2.ax1x.com/2019/03/24/AYGkkj.png)\n\n### Maven依赖\n\n```xml\n<dependencies>                                                              \n    <dependency>                                                            \n        <groupId>javax.servlet</groupId>                        \n        <artifactId>servlet-api</artifactId>                          \n        <version>2.5</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>javax.servlet.jsp</groupId>                        \n        <artifactId>jsp-api</artifactId>                       \n        <version>2.1.3-b06</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-core</artifactId>                          \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-context</artifactId>                       \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-jdbc</artifactId>                          \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-orm</artifactId>                           \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-web</artifactId>                           \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-webmvc</artifactId>                        \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>com.mchange</groupId>                                \n        <artifactId>c3p0</artifactId>                                 \n        <version>0.9.2</version>                                      \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>cglib</groupId>                                      \n        <artifactId>cglib</artifactId>                                \n        <version>2.2</version>                                        \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.aspectj</groupId>                                \n        <artifactId>aspectjweaver</artifactId>                        \n        <version>1.6.8</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <!-- Spring整合MyBatis -->                                              \n    <!-- MyBatis中延迟加载需要使用Cglib -->                                 \n    <!-- https://mvnrepository.com/artifact/org.mybatis/mybatis -->         \n    <dependency>                                                            \n        <groupId>org.mybatis</groupId>                                \n        <artifactId>mybatis</artifactId>                              \n        <version>3.2.8</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.mybatis</groupId>                                \n        <artifactId>mybatis-spring</artifactId>                       \n        <version>1.2.2</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <!-- 控制日志输出：结合log4j -->                                        \n    <dependency>                                                            \n        <groupId>log4j</groupId>                                      \n        <artifactId>log4j</artifactId>                                \n        <version>1.2.17</version>                                     \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.slf4j</groupId>                                  \n        <artifactId>slf4j-api</artifactId>                            \n        <version>1.7.7</version>                                      \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.slf4j</groupId>                                  \n        <artifactId>slf4j-log4j12</artifactId>                        \n        <version>1.7.7</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>mysql</groupId>                                      \n        <artifactId>mysql-connector-java</artifactId>                 \n        <version>5.1.37</version>                                     \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>jstl</groupId>                                       \n        <artifactId>jstl</artifactId>                                 \n        <version>1.2</version>                                        \n    </dependency>                                                           \n                                                                                  \n    <!-- ********其他****************************** -->                     \n                                                                                  \n    <!-- Ehcache二级缓存 -->                                                \n    <dependency>                                                            \n        <groupId>net.sf.ehcache</groupId>                             \n        <artifactId>ehcache</artifactId>                              \n        <version>1.6.2</version>                                      \n    </dependency>                                                           \n                                                                                  \n                                                                                  \n    <!-- 石英调度 - 开始 -->                                                \n    <dependency>                                                            \n        <groupId>org.quartz-scheduler</groupId>                       \n        <artifactId>quartz</artifactId>                               \n        <version>1.8.5</version>                                      \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>org.springframework</groupId>                        \n        <artifactId>spring-context-support</artifactId>               \n        <version>4.0.0.RELEASE</version>                              \n    </dependency>                                                           \n    <dependency>                                                            \n        <groupId>commons-collections</groupId>                        \n        <artifactId>commons-collections</artifactId>                  \n        <version>3.1</version>                                        \n    </dependency>                                                           \n    <!-- 石英调度 - 结束 -->                                                \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.codehaus.jackson</groupId>                       \n        <artifactId>jackson-mapper-asl</artifactId>                   \n        <version>1.9.2</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.apache.poi</groupId>                             \n        <artifactId>poi</artifactId>                                  \n        <version>3.9</version>                                        \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.jfree</groupId>                                  \n        <artifactId>jfreechart</artifactId>                           \n        <version>1.0.19</version>                                     \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>commons-fileupload</groupId>                         \n        <artifactId>commons-fileupload</artifactId>                   \n        <version>1.3.1</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.freemarker</groupId>                             \n        <artifactId>freemarker</artifactId>                           \n        <version>2.3.19</version>                                     \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.activiti</groupId>                               \n        <artifactId>activiti-engine</artifactId>                      \n        <version>5.15.1</version>                                     \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.activiti</groupId>                               \n        <artifactId>activiti-spring</artifactId>                      \n        <version>5.15.1</version>                                     \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.apache.commons</groupId>                         \n        <artifactId>commons-email</artifactId>                        \n        <version>1.3.1</version>                                      \n    </dependency>                                                           \n                                                                                  \n    <dependency>                                                            \n        <groupId>org.activiti</groupId>                               \n        <artifactId>activiti-explorer</artifactId>                    \n        <version>5.15.1</version>                                     \n        <exclusions>                                                        \n            <exclusion>                                                     \n                <artifactId>groovy-all</artifactId>                   \n                <groupId>org.codehaus.groovy</groupId>                \n            </exclusion>                                                    \n        </exclusions>                                                       \n    </dependency>                                                           \n</dependencies>                            \n```\n\n### 集成Spring框架\n\n在`web.xml`文件中增加配置信息集成`Spring`框架\n\n```xml\n  <context-param>\n        <param-name>contextConfigLocation</param-name>\n        <param-value>classpath*:spring/spring-*.xml</param-value>\n    </context-param>\n    <listener>\n        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n    </listener>\n```\n\n`Spring`环境构建时需要读取`web`应用的初始化参数`contextConfigLocation`, 从`classpath`中读取配置文件`spring/spring-*.xml` 因此需要`resources目录中增加`spring`文件夹，并在其中增加`spring-context.xml`配置文件。\n\n`Spring`框架的核心是构建对象，整合对象之间的关系（`IOC`）及扩展对象功能（`AOP`），所以需要在`spring-context.xml`配置文件中增加业务对象扫描的相关配置。扫描后由`Spring`框架进行管理和组合。\n\n```xml\n    <context:component-scan base-package=\"com.hph.*\" >\n        <context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/>\n    </context:component-scan>\n```\n\n扫描配置中为什么要排除`Controller`注解Controller注解的的作用是声明控制器（处理器）类。从数据流转的角度，这个类应该是由`SpringMVC`框架进行管理和组织的，所以不需要由`Spring`框架扫描。\n\n### 集成SpringMVC框架\n\n`SpringMVC`框架用于处理系统中数据的流转及控制操作。（从哪里来，到哪里去）\n\n集成`SpringMVC`框架，需要在`web.xml`文件中增加配置信息\n\n```xml\n <servlet>\n        <servlet-name>springmvc</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            <param-value>classpath:spring/springmvc-context.xml</param-value>\n        </init-param>\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>springmvc</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n```\n\n`SpringMVC`环境构建时需要读取`servlet`初始化参数`init-param`, 从`classpath`中读取配置文件`spring/springmvc-context.xml`\n\n`SpringMVC`框架的核心是处理数据的流转，所以需要在`springmvc-context.xml`配置文件中增加控制器对象（`Controller`）扫描的相关配置。扫描后由`SpringMVC`框架进行管理和组合。\n\n```xml\n    <context:component-scan base-package=\"com.hph.*\" use-default-filters=\"false\" >\n        <context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/>\n    </context:component-scan>\n```\n\n静态资源如何不被`SpringMVC`框架进行拦截:在配置文件中即可\n\n```xml\n<mvc:default-servlet-handler/>\n<mvc:annotation-driven />\n```\n\n在实际的项目中静态资源不会和动态资源放在一起，也就意味着不会放置在服务器中，所以这些配置可以省略。\n\n如果`SpringMVC`框架数据处理为页面跳转，那么需要配置相应的视图解析器`ViewResolver`。\n\n```\n   <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\" >\n        <property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/>\n        <property name=\"prefix\" value=\"/WEB-INF/jsp/\"/>\n        <property name=\"suffix\" value=\".jsp\"/>\n    </bean>\n```\n\n如果有多个解释器?\n\n`SpringMVC`框架中允许存在多个视图解析器，框架会按照配置声明顺序，依次进行解析。`SpringMVC`框架中配置多个视图解析器时，如果将`InternalResourceViewResolver`解析器配置在前，那么即使找不到视图，框架也不会继续解析，直接发生`404`错误，所以必须将`InternalResourceViewResolver`解析器放置在最后。\n\n如果`SpringMVC`框架数据处理为响应`JSON`字符串，那么为了浏览器方便对响应的字符串进行处理，需要明确字符串的类型及编码方式。\n\n\n\n```xml\n <bean class=\"org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter\" >\n        <property name=\"messageConverters\" >\n            <list>\n                <bean class=\"org.springframework.http.converter.json.MappingJacksonHttpMessageConverter\" >\n                    <property name=\"supportedMediaTypes\" >\n                        <list>\n                            <value>application/json;charset=UTF-8</value>\n                        </list>\n                    </property>\n                </bean>\n            </list>\n        </property>\n    </bean>\n```\n\n**如果增加了<mvc:annotation-driven />标签，上面面=的配置可省略。**\n\n如果项目中含有文件上传业务，还需要增加文件上传解析器`MultipartResolver`\n\n```xml\n    <bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\" p:defaultEncoding=\"UTF-8\" >\n        <property name=\"maxUploadSize\" value=\"2097152\"/>\n        <property name=\"resolveLazily\" value=\"true\"/>\n    </bean>\n```\n\n### 集成Mybatis框架\n\n`Mybatis`框架主要处理业务和数据库之间的数据交互，所以创建对象和管理对象生命周期的职责可以委托`Spring`框架完成。如：创建`Mybatis`核心对象。\n\n```xml\n    <bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\" >\n        <property name=\"configLocation\" value=\"classpath:mybatis/config.xml\" />\n        <property name=\"dataSource\" ref=\"dataSource\" />\n        <property name=\"mapperLocations\" >\n            <list>\n                <value>classpath*:mybatis/mapper-*.xml</value>\n            </list>\n        </property>\n    </bean>\n    ...\n    <bean id=\"mapperScannerConfigurer\" class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\" >\n        <property name=\"basePackage\" value=\"com.atguigu.atcrowdfunding.**.dao\" />\n    </bean>\n```\n\n`Mybatis`核心对象就需要依赖于数据库连接池（`C3P0`）,所以在`Spring`配置文件中增加相应的配置\n\n```xml\n   <bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\" >\n        <property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"/>\n        <property name=\"jdbcUrl\" value=\"jdbc:mysql://localhost:3306/ssm?rewriteBatchedStatements=true&amp;useUnicode=true&amp;characterEncoding=utf8\"/>\n        <property name=\"user\" value=\"root\"/>\n        <property name=\"password\" value=\"123456\"/>\n    </bean>\n```\n\n集成`Mybatis`框架时同时还需要增加核心配置文件`mybatis/config.xml`\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <typeAliases>\n        \n    </typeAliases>\n</configuration>\n```\n\n`SQL`映射文件`mybatis/mapper-SQL.xml`。\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<mapper namespace=\"xxx.XXDao\" >\n\n</mapper>\n```\n\n为了保证数据操作的一致性，必须在程序中增加事务处理。`Spring`框架采用声明式事务，通过`AOP`的方式将事务增加到业务中。所以需要在`Spring`配置文件中增加相关配置。\n\n测试前，需要在数据库中增加ssm库及t_user表。\n\n```sql\nCREATE DATABASE `ssm`;\nCREATE TABLE `t_user` (\n  `id` int(11) NOT NULL AUTO_INCREMENT\n  PRIMARY KEY (`id`)\n) ENGINE=InnoDB AUTO_INCREMENT=5 DEFAULT CHARSET=utf8;\n```\n\n## 最终结构图\n\n![AYwDpQ.png](https://s2.ax1x.com/2019/03/24/AYwDpQ.png)\n\n## 最终配置文件\n\n### web.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app version=\"2.4\"\n         xmlns=\"http://java.sun.com/xml/ns/j2ee\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd\">\n  <context-param>\n    <param-name>contextConfigLocation</param-name>\n    <param-value>classpath*:spring/spring-*.xml</param-value>\n  </context-param>\n  <listener>\n    <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n  </listener>\n  <servlet>\n    <servlet-name>springmvc</servlet-name>\n    <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n    <init-param>\n      <param-name>contextConfigLocation</param-name>\n      <param-value>classpath:spring/springmvc-context.xml</param-value>\n    </init-param>\n    <load-on-startup>1</load-on-startup>\n  </servlet>\n  <servlet-mapping>\n    <servlet-name>springmvc</servlet-name>\n    <url-pattern>/</url-pattern>\n  </servlet-mapping>\n</web-app>\n        \n```\n\n### srping-context.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\" xmlns:tx=\"http://www.springframework.org/schema/tx\"\n       xmlns:aop=\"http://www.springframework.org/schema/aop\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd\">\n    <context:component-scan base-package=\"com.hph.*\">\n        <context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/>\n    </context:component-scan>\n\n    <bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\">\n        <property name=\"configLocation\" value=\"classpath:mybatis/config.xml\"/>\n        <property name=\"dataSource\" ref=\"dataSource\"/>\n        <property name=\"mapperLocations\">\n            <list>\n                <value>classpath*:mybatis/mapper-*.xml</value>\n            </list>\n        </property>\n    </bean>\n    <bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\">\n        <property name=\"driverClass\" value=\"com.mysql.jdbc.Driver\"/>\n        <property name=\"jdbcUrl\"\n                  value=\"jdbc:mysql://localhost:3306/ssm?rewriteBatchedStatements=true&amp;useUnicode=true&amp;characterEncoding=utf8\"/>\n        <property name=\"user\" value=\"root\"/>\n        <property name=\"password\" value=\"123456\"/>\n    </bean>\n\n    <bean id=\"mapperScannerConfigurer\" class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\">\n        <property name=\"basePackage\" value=\"com.hph.**.dao\"/>\n    </bean>\n    <bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\">\n        <property name=\"dataSource\" ref=\"dataSource\"/>\n    </bean>\n    <tx:advice id=\"transactionAdvice\" transaction-manager=\"transactionManager\">\n        <tx:attributes>\n            <tx:method name=\"*\" propagation=\"REQUIRED\" isolation=\"DEFAULT\" rollback-for=\"java.lang.Exception\"/>\n            <tx:method name=\"query*\" read-only=\"true\"/>\n        </tx:attributes>\n    </tx:advice>\n    <aop:config>\n        <aop:advisor advice-ref=\"transactionAdvice\" pointcut=\"execution(* com.hph..*Service.*(..))\"/>\n    </aop:config>\n</beans>\n```\n\n\n\n### spring-mvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:p=\"http://www.springframework.org/schema/p\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd\n        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n        http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\">\n\n    <context:component-scan base-package=\"com.hph.*\" use-default-filters=\"false\">\n        <context:include-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"/>\n    </context:component-scan>\n    <mvc:default-servlet-handler/>\n    <mvc:annotation-driven/>\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"viewClass\" value=\"org.springframework.web.servlet.view.JstlView\"/>\n        <property name=\"prefix\" value=\"/WEB-INF/jsp/\"/>\n        <property name=\"suffix\" value=\".jsp\"/>\n    </bean>\n    <bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\"\n          p:defaultEncoding=\"UTF-8\">\n        <property name=\"maxUploadSize\" value=\"2097152\"/>\n        <property name=\"resolveLazily\" value=\"true\"/>\n    </bean>\n\n</beans>\n```\n\n### config(mybatis)\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE configuration PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<configuration>\n    <typeAliases>\n\n    </typeAliases>\n</configuration>\n```\n\n### mapper-SQL.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<mapper namespace=\"xxx.XXDao\" >\n\n</mapper>\n```\n\n### User\n\n```java\npackage com.hph.ssm.controller;\n\nimport com.hph.ssm.bean.User;\nimport com.hph.ssm.service.UserService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n@Controller\n@RequestMapping(\"/user\")\npublic class UserController {\n\n    @Autowired\n    private UserService userService;\n\n    @RequestMapping(\"/index\")\n    public String index() {\n\n        return \"user/index\";\n    }\n\n    @ResponseBody\n    @RequestMapping(\"/json\")\n    public Object json() {\n        Map map = new HashMap();\n        map.put(\"username\", \"张三\");\n        return map;\n    }\n\n    @ResponseBody\n    @RequestMapping(\"/sql\")\n    public Object sql() {\n        List<User> users = userService.queryAll();\n        return users;\n    }\n}\n```\n\n### controller\n\n```java\npackage com.hph.ssm.controller;\n\nimport com.hph.ssm.bean.User;\nimport com.hph.ssm.service.UserService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.ResponseBody;\n\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\n@Controller\n@RequestMapping(\"/user\")\npublic class UserController {\n\n    @Autowired\n    private UserService userService;\n\n    @RequestMapping(\"/index\")\n    public String index() {\n\n        return \"user/index\";\n    }\n\n    @ResponseBody\n    @RequestMapping(\"/json\")\n    public Object json() {\n        Map map = new HashMap();\n        map.put(\"username\", \"张三\");\n        return map;\n    }\n\n    @ResponseBody\n    @RequestMapping(\"/sql\")\n    public Object sql() {\n        List<User> users = userService.queryAll();\n        return users;\n    }\n\n    @ResponseBody\n    @RequestMapping(\"/aop\")\n    public Object aop() {\n        User user = userService.queryById();\n        return user;\n    }\n\n}\n```\n\n### UserDao\n\n```java\npackage com.hph.ssm.dao;\n\nimport com.hph.ssm.bean.User;\nimport org.apache.ibatis.annotations.Select;\n\nimport java.util.List;\n\npublic interface UserDao {\n    @Select(\"select * from t_user where id = 1\")\n    public User queryById();\n\n    @Select(\"select * from t_user  \")\n    List<User>  queryAll();\n}\n```\n\n### UserService\n\n```java\npackage com.hph.ssm.service;\n\nimport com.hph.ssm.bean.User;\n\nimport java.util.List;\n\npublic interface UserService {\n    public User queryById();\n\n    List<User> queryAll();\n}\n```\n\n### UserServiceImpl\n\n```java\npackage com.hph.ssm.service.impl;\n\nimport com.hph.ssm.bean.User;\nimport com.hph.ssm.dao.UserDao;\nimport com.hph.ssm.service.UserService;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\nimport java.util.List;\n\n@Service\npublic class UserServiceImpl implements UserService {\n    @Autowired\n    private UserDao userDao;\n\n    public User queryById() {\n        return userDao.queryById();\n    }\n\n    public List<User> queryAll() {\n        return userDao.queryAll();\n    }\n\n}\n```\n\n![AYwoc9.png](https://s2.ax1x.com/2019/03/24/AYwoc9.png)\n\n配置Tomcat将项目发送服务器\n\n## 测试\n\n### SpringMVC\n\n如果访问成功，说明项目中`SpringMVC`框架集成成功。\n\n![AYwb0x.png](https://s2.ax1x.com/2019/03/24/AYwb0x.png)\n\nSpringMVC框架集成成功\n\n### Spring框架(IOC功能)\n\n![AYwXtO.png](https://s2.ax1x.com/2019/03/24/AYwXtO.png)\n\nSpring框架（`IOC`功能）集成成功。\n\n### Spring框架(AOP功能)\n\n![AY0e3Q.png](https://s2.ax1x.com/2019/03/24/AY0e3Q.png)\n\n`Spring`框架（`AOP`功能）集成成功\n\n### Mybatis\n\n![AY084U.png](https://s2.ax1x.com/2019/03/24/AY084U.png)\n\n![AY03NT.png](https://s2.ax1x.com/2019/03/24/AY03NT.png)\n\n\n\n## 模拟生产环境\n\nWeb软件开发中，由于开发阶段不同，系统环境主要分为：开发环境，测试环境，生产环境。\n\n将系统部署到生产环境后，经常会听开发人员这么说：这个`Bug`不应该呀，我们在测试时没问题呀，怎么到了生产环境就不行了呢!!!\n\n其实之所以会出现这种情况，很大程度上是因为我们在项目的不同阶段时，对应用系统的访问方式不一样所造成的。\n\n一般在开发，测试阶段时，我们都会以本地服务器地址`http://localhost:8080/project`访问系统，但是在生产环境中，我们的访问方式发生了变化：`http://com.xxxxx/`，由于环境的不同，导致访问方式的变化，那么就会产生很多之前没有的问题。\n\n**如果能将开发，测试，生产环境的访问方式保持一致的话，那么就可以提前发现客户在使用时所发生的问题，将这些问题提前解决，还是非常不错的。**\n\n- 将`Tomcat`服务器的默认`HTTP`监听端口号：`8080`修改为`80`\n- 将项目中`.settings`目录下的配置文件`org.eclipse.wst.common.component`中的`context-root`属性修改为`/(斜杠)`(Eclipse)\n- IDEA直接在Tomcat服务器中更改Deployment中的Application context为 /\n- 修改系统主机文件`c:\\Windows\\System32\\drivers\\etc\\hosts`增加`IP`地址和域名的解析关系：`127.0.0.1 www.hph.com`\n\n![AY068e.png](https://s2.ax1x.com/2019/03/24/AY068e.png)\n\n## 参考资料\n\n尚硅谷SSM教学视频\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SSM"],"categories":["JavaWeb"]},{"title":"SSM整合","url":"/2019/03/19/SSM整合/","content":"\n {{ \"IDEA下SSM整合基本流程\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n\n\n","tags":["SSM"],"categories":["Java"]},{"title":"Mybatis之动态SQL","url":"/2019/03/19/Mybatis之动态SQL/","content":"\n {{ \"Mybatis 动态SQL的使用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n## 简介\n\n- 动态SQL是MyBatis强大特性之一。极大的简化我们拼装SQL的操作\n- 动态SQL元素和使用 JSTL 或其他类似基于 XML 的文本处理器相似\n- MyBatis 采用功能强大的基于 OGNL 的表达式来简化操作\n    - If\n    - choose (when, otherwise)\n    - trim (where, set)\n    - foreach\n\nOGNL（ Object Graph Navigation Language ）对象图导航语言，这是一种强大的表达式语言，通过它可以非常方便的来操作对象属性。 类似于我们的EL，SpEL等\n\n| 功能             | 参数                                             |\n| ---------------- | ------------------------------------------------ |\n| 访问对象属性     | person.name                                      |\n| 调用方法         | person.getName()                                 |\n| person.getName() | @java.lang.Math@PI  @java.util.UUID@randomUUID() |\n| 调用构造方法     | new<br/>com.atguigu.bean.Person(‘admin’).name    |\n| 运算符           | +,-*,/,%                                         |\n| 逻辑运算符       | in,not in,>,>=,<,<=,==,!=                        |\n\n**注意：xml中特殊符号如”,>,<等这些都需要使用转义字符**\n\n## 项目结构\n\n## 数据准备\n\n![Amx8Z4.png](https://s2.ax1x.com/2019/03/18/Amx8Z4.png)\n\n![AmxZIs.png](https://s2.ax1x.com/2019/03/18/AmxZIs.png)\n\n## 项目结构\n\n![Amxyod.png](https://s2.ax1x.com/2019/03/18/Amxyod.png)\n\n### Department\n\n```java\npackage com.hph.mybatis.beans;\n\nimport java.util.List;\n\npublic class Department {\n\n    private Integer id;\n    private String departmentName ;\n\n    private List<Employee> emps ;\n\n\n    public List<Employee> getEmps() {\n        return emps;\n    }\n    public void setEmps(List<Employee> emps) {\n        this.emps = emps;\n    }\n    public Integer getId() {\n        return id;\n    }\n    public void setId(Integer id) {\n        this.id = id;\n    }\n    public String getDepartmentName() {\n        return departmentName;\n    }\n    public void setDepartmentName(String departmentName) {\n        this.departmentName = departmentName;\n    }\n    @Override\n    public String toString() {\n        return \"Department [id=\" + id + \", departmentName=\" + departmentName + \"]\";\n    }\n}\n```\n\n### Employee\n\n```java\npackage com.hph.mybatis.beans;\n\npublic class Employee {\n\n    private Integer id;\n    private String lastName;\n    private String email;\n    private Integer gender;\n\n    private Department dept;\n\n    public void setDept(Department dept) {\n        this.dept = dept;\n    }\n\n    public Department getDept() {\n        return dept;\n    }\n\n    public Employee() {\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender, Department dept) {\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n        this.dept = dept;\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender) {\n        super();\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n    }\n\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n\n    @Override\n    public String toString() {\n        return \"Employee [id=\" + id + \", lastName=\" + lastName + \", email=\" + email + \", gender=\" + gender + \"]\";\n    }\n\n}\n```\n\n### EmployeeMapperDynamicSQL\n\n```java\npackage com.hph.mybatis.dao;\n\nimport com.hph.mybatis.beans.Employee;\nimport org.apache.ibatis.annotations.Param;\n\nimport java.util.List;\n\npublic interface EmployeeMapperDynamicSQL {\n\n    public List<Employee> getEmpsByConditionIfWhere(Employee condition);\n\n    public List<Employee> getEmpsByConditionTrim(Employee condition);\n\n    public void updateEmpByConitionSet(Employee condition);\n\n    public  List<Employee> getEmpsByConditionChoose(Employee condition);\n\n    public List<Employee> getEmpsByIds(@Param(\"ids\") List<Integer> ids);\n\n    //批量操作: 删除 修改 添加\n    public void addEmps(@Param(\"emps\") List<Employee> emps);\n}\n```\n\n### EmployeeMapperDynamicSQL.xml\n\n```java\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper\n        PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n\n<mapper namespace=\"com.hph.mybatis.dao.EmployeeMapperDynamicSQL\">\n    <!-- public List<Employee>  getEmpsByConditionIfWhere(Employee Condition); -->\n    <select id=\"getEmpsByConditionIfWhere\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select id, last_name, email, gender\n        from tbl_employee\n        <!-- where 1=1 -->\n        <where> <!-- 在SQL语句中提供WHERE关键字，  并且要解决第一个出现的and 或者是 or的问题 -->\n            <if test=\"id!=null\">\n                and id = #{id }\n            </if>\n            <if test=\"lastName!=null&amp;&amp;lastName!=&quot;&quot;\">\n                and last_name = #{lastName}\n            </if>\n            <if test=\"email!=null and email.trim()!=''\">\n                and email = #{email}\n            </if>\n            <if test=\"gender==0 or gender==1\">\n                and gender = #{gender}\n            </if>\n        </where>\n    </select>\n    <select id=\"getEmpsByConditionTrim\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select id, last_name, email, gender\n        from tbl_employee\n        <!--\n        prefix: 添加一个前缀\n        pprefixOverrides:覆盖/去掉一个前缀\n        ssuffxi:添加一个后缀\n        suffixOverrides:覆盖/去掉一个后缀\n         -->\n\n        <trim prefix=\"where\" suffixOverrides=\"and|or\">\n            <if test=\"id!=null\">\n                id = #{id } and\n            </if>\n            <if test=\"lastName!=null&amp;&amp;lastName!=&quot;&quot;\">\n                last_name = #{lastName} and\n            </if>\n            <if test=\"email!=null and email.trim()!=''\">\n                email = #{email} or\n            </if>\n            <if test=\"gender==0 or gender==1\">\n                gender = #{gender}\n            </if>\n        </trim>\n    </select>\n    <!--public void updateEmpByConitionSet(Employee condition);-->\n\n    <update id=\"updateEmpByConitionSet\">\n        update tbl_employee\n        <set>\n            <if test=\"lastName!=null\">\n                last_name = #{lastName},\n            </if>\n            <if test=\"email!=null\">\n                email = #{email},\n            </if>\n            <if test=\"gender==0 or gender==1\">\n                gender= #{gender},\n            </if>\n        </set>\n        where id = #{id}\n    </update>\n    <!--    public void updateEmpByConitionSet(Employee condition);-->\n    <select id=\"getEmpsByConditionChoose\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select id,last_name,email,gender\n        from tbl_employee\n        where\n        <choose>\n            <when test=\"id!=null\">\n                id=#{id}\n            </when>\n            <when test=\"lastName!=null\">\n                last_name=#{lastName}\n            </when>\n            <when test=\"email!=null\">\n                email = #{email}\n            </when>\n            <otherwise>\n                gender = 0\n            </otherwise>\n        </choose>\n    </select>\n\n    <!--    public List<Employee> getEmpsByIds(@Param(\"ids\")List<Integer> ids);-->\n    <select id=\"getEmpsByIds\" resultType=\"com.hph.mybatis.beans.Employee\">\n        <!--\n        foreach:\n            collection:指定要迭代的几乎额\n            item:当前集合中迭代出的元素\n            open:指定一个开始字符\n            close:指定一个结束字符\n            separtor:元素与元素之间的分隔符\n        -->\n        select id,last_name,email,gender from tbl_employee\n        where id in\n        <foreach collection=\"ids\" item=\"currId\" open=\"(\" close=\")\" separator=\",\">\n            #{currId}\n        </foreach>\n    </select>\n\n    <!--    public void addEmps(@Param(\"emps\") List<Employee> emps)\n    添加:insert into tbl_employee(x,x,x) values(?,?,?),(?,?<?),(?,?,?)\n    删除:delete from tbl_employee where id in (?,?,?)\n    修改:update tbl_employee set last_name = #{lastName }...where id  = #{id}}\n         update tbl_employee set last_name = #{lastName }...where id  = #{id}}\n         update tbl_employee set last_name = #{lastName }...where id  = #{id}}\n         默认情况下,JDBCb允许将多条分号SQL通过;平日你改成一个字符串\n\n    ;-->\n\n    <insert id=\"addEmps\">\n        insert into tbl_employee(last_name, email,gender ) values\n        <foreach collection=\"emps\" item=\"emp\" separator=\",\">\n            (#{emp.lastName},#{emp.email},#{emp.gender})\n        </foreach>\n    </insert>\n\n</mapper>\n```\n\n\n\n### TestMybatisDynamicSQL\n\n```java\npackage com.hph.mybatis.test;\n\nimport com.hph.mybatis.beans.Employee;\nimport com.hph.mybatis.dao.EmployeeMapperDynamicSQL;\nimport org.apache.ibatis.io.Resources;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.apache.ibatis.session.SqlSessionFactoryBuilder;\nimport org.junit.Test;\n\nimport java.io.InputStream;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TestMybatisDynamicSQL {\n\n    public SqlSessionFactory getSqlSessionFactory() throws Exception {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        return sqlSessionFactory;\n    }\n\n    @Test\n    public void testIf() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession();\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n         /*   condition.setId(5);\n            condition.setLastName(\"清风_笑丶\");*/\n            //condition.setEmail(\"qfx@sina.com\");\n            //condition.setGender(1);\n            List<Employee> emps = mapper.getEmpsByConditionIfWhere(condition);\n            System.out.println(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testTrim() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession();\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            condition.setId(5);\n            condition.setLastName(\"清风_笑丶\");\n            condition.setEmail(\"qfx@gmail.com\");\n            //condition.setGender(1);\n            List<Employee> emps = mapper.getEmpsByConditionTrim(condition);\n            System.out.println(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testSet() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            condition.setId(5);\n            condition.setLastName(\"清风_笑丶testSet\");\n            condition.setEmail(\"qf_x@qq.com\");\n//         condition.setGender(1);\n            mapper.updateEmpByConitionSet(condition);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testChoose() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            condition.setId(1);\n            condition.setLastName(\"清风_笑丶testChoose\");\n            condition.setEmail(\"qf_xtestChoose@gmail.com\");\n            //condition.setGender(1);\n            List<Employee> emps = mapper.getEmpsByConditionChoose(condition);\n            System.out.println(emps);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testForeach() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            List<Integer> ids = new ArrayList<Integer>();\n            ids.add(5);\n            ids.add(6);\n            ids.add(7);\n            List<Employee> emps = mapper.getEmpsByIds(ids);\n\n            System.out.println(emps);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testBatch() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            List<Employee> emps = new ArrayList<Employee>();\n            emps.add(new Employee(null, \"清风笑_testBatch1\", \"qfx_testBatch1@sina.com\", 1));\n            emps.add(new Employee(null, \"清风笑_testBatch2\", \"qfx_testBatch2@sina.com\", 0));\n            emps.add(new Employee(null, \"清风笑_testBatch3\", \"qfx_testBatch3@sina.com\", 1));\n\n            mapper.addEmps(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n}\n```\n\n## if  where\n\n If用于完成简单的判断.\n\nWhere用于解决SQL语句中where关键字以及条件中第一个and或者or的问题 \n\n```xml\n  <!-- public List<Employee>  getEmpsByConditionIfWhere(Employee Condition); -->\n    <select id=\"getEmpsByConditionIfWhere\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select id, last_name, email, gender\n        from tbl_employee\n        <!-- where 1=1 -->\n        <where> <!-- 在SQL语句中提供WHERE关键字，  并且要解决第一个出现的and 或者是 or的问题 -->\n            <if test=\"id!=null\">\n                and id = #{id }\n            </if>\n            <if test=\"lastName!=null&amp;&amp;lastName!=&quot;&quot;\">\n                and last_name = #{lastName}\n            </if>\n            <if test=\"email!=null and email.trim()!=''\">\n                and email = #{email}\n            </if>\n            <if test=\"gender==0 or gender==1\">\n                and gender = #{gender}\n            </if>\n        </where>\n    </select>\n```\n\n```java\n   public void testIf() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession();\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            List<Employee> emps = mapper.getEmpsByConditionIfWhere(condition);\n            System.out.println(emps);\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![Anokv9.png](https://s2.ax1x.com/2019/03/19/Anokv9.png)\n\n```java\n   @Test\n    public void testIf() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession();\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            condition.setId(1001);\n            List<Employee> emps = mapper.getEmpsByConditionIfWhere(condition);\n            System.out.println(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![AnotVP.png](https://s2.ax1x.com/2019/03/19/AnotVP.png)\n\n## trim\n\n```xml\n    <select id=\"getEmpsByConditionTrim\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select id, last_name, email, gender\n        from tbl_employee\n        <!--\n        prefix: 添加一个前缀\n        pprefixOverrides:覆盖/去掉一个前缀\n        ssuffxi:添加一个后缀\n        suffixOverrides:覆盖/去掉一个后缀\n         -->\n\n        <trim prefix=\"where\" suffixOverrides=\"and|or\">\n            <if test=\"id!=null\">\n                id = #{id } and\n            </if>\n            <if test=\"lastName!=null&amp;&amp;lastName!=&quot;&quot;\">\n                last_name = #{lastName} and\n            </if>\n            <if test=\"email!=null and email.trim()!=''\">\n                email = #{email} or\n            </if>\n            <if test=\"gender==0 or gender==1\">\n                gender = #{gender}\n            </if>\n        </trim>\n    </select>\n```\n\n\n\n```java\n  @Test\n    public void testTrim() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession();\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            List<Employee> emps = mapper.getEmpsByConditionTrim(condition);\n            System.out.println(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![AnocV0.png](https://s2.ax1x.com/2019/03/19/AnocV0.png)\n\n##  set \n\nset 主要是用于解决修改操作中SQL语句中可能多出逗号的问题.\n\n```xml\n<update id=\"updateEmpByConitionSet\">\n    update tbl_employee\n    <set>\n        <if test=\"lastName!=null\">\n            last_name = #{lastName},\n        </if>\n        <if test=\"email!=null\">\n            email = #{email},\n        </if>\n        <if test=\"gender==0 or gender==1\">\n            gender= #{gender},\n        </if>\n    </set>\n    where id = #{id}\n</update>\n```\n\n\n\n```java\n    @Test\n    public void testSet() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            condition.setId(1001);\n            condition.setLastName(\"清风笑丶\");\n            condition.setEmail(\"qf_x@gmail.com\");\n            mapper.updateEmpByConitionSet(condition);\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![AnTKZq.png](https://s2.ax1x.com/2019/03/19/AnTKZq.png)\n\n![AnTQoV.png](https://s2.ax1x.com/2019/03/19/AnTQoV.png)\n\n\n##  choose(when、otherwise)\n\n\nchoose 主要是用于分支判断，类似于java中的switch case,只会满足所有分支中的一个\n\n```xml\n<!--    public void updateEmpByConitionSet(Employee condition);-->\n<select id=\"getEmpsByConditionChoose\" resultType=\"com.hph.mybatis.beans.Employee\">\n    select id,last_name,email,gender\n    from tbl_employee\n    where\n    <choose>\n        <when test=\"id!=null\">\n            id=#{id}\n        </when>\n        <when test=\"lastName!=null\">\n            last_name=#{lastName}\n        </when>\n        <when test=\"email!=null\">\n            email = #{email}\n        </when>\n        <otherwise>\n            gender = 0\n        </otherwise>\n    </choose>\n</select>\n```\n\n```java\n @Test\n    public void testChoose() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            \n            List<Employee> emps = mapper.getEmpsByConditionChoose(condition);\n            System.out.println(emps);\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![An7b9O.png](https://s2.ax1x.com/2019/03/19/An7b9O.png)\n\n## foreach\n\nforeach 主要用户循环迭代\n\n&nbsp;&nbsp;&nbsp;&nbsp;collection: 要迭代的集合\n\n&nbsp;&nbsp;&nbsp;&nbsp;item: 当前从集合中迭代出的元素\n\n&nbsp;&nbsp;&nbsp;&nbsp;open: 开始字符\n\n&nbsp;&nbsp;&nbsp;&nbsp;close:结束字符\n\n&nbsp;&nbsp;&nbsp;&nbsp;separator: 元素与元素之间的分隔符\n\n&nbsp;&nbsp;&nbsp;&nbsp;index:\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迭代的是List集合: index表示的当前元素的下标\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;迭代的Map集合:  index表示的当前元素的key\n\n```xml\n    <!--    public List<Employee> getEmpsByIds(@Param(\"ids\")List<Integer> ids);-->\n    <select id=\"getEmpsByIds\" resultType=\"com.hph.mybatis.beans.Employee\">\n        <!--\n        foreach:\n            collection:指定要迭代的几乎额\n            item:当前集合中迭代出的元素\n            open:指定一个开始字符\n            close:指定一个结束字符\n            separtor:元素与元素之间的分隔符\n        -->\n        select id,last_name,email,gender from tbl_employee  where id in\n        <foreach collection=\"ids\" item=\"currId\" open=\"(\" close=\")\" separator=\",\">\n            #{currId}\n        </foreach>\n    </select>\n```\n\n```java\n    @Test\n    public void testForeach() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            Employee condition = new Employee();\n            List<Integer> ids = new ArrayList<Integer>();\n            ids.add(1001);\n            ids.add(1002);\n            List<Employee> emps = mapper.getEmpsByIds(ids);\n\n            System.out.println(emps);\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![AnHorj.png](https://s2.ax1x.com/2019/03/19/AnHorj.png)\n\n```xml\n<!--\n    //批量操作: 删除 修改 添加\n    public void addEmps(@Param(\"emps\") List<Employee> emps);-->\n<insert id=\"addEmps\">\n    insert into tbl_employee(last_name, email,gender ) values\n    <foreach collection=\"emps\" item=\"emp\" separator=\",\">\n        (#{emp.lastName},#{emp.email},#{emp.gender})\n    </foreach>\n</insert>\n```\n\n```java\n  @Test\n    public void testBatch() throws Exception {\n        SqlSessionFactory ssf = getSqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapperDynamicSQL mapper = session.getMapper(EmployeeMapperDynamicSQL.class);\n            List<Employee> emps = new ArrayList<Employee>();\n            emps.add(new Employee(null, \"清风笑_testBatch1\", \"qfx_1@sina.com\", 1));\n            emps.add(new Employee(null, \"清风笑_testBatch2\", \"qfx_2@sina.com\", 0));\n            emps.add(new Employee(null, \"清风笑_testBatch3\", \"qfx_3@sina.com\", 1));\n\n            mapper.addEmps(emps);\n\n        } finally {\n            session.close();\n        }\n    }\n```\n\n![AnbnLd.png](https://s2.ax1x.com/2019/03/19/AnbnLd.png)\n\n## sql\n\n sql 标签是用于抽取可重用的sql片段，将相同的，使用频繁的SQL片段抽取出来，单独定义，方便多次引用.\n\n抽取SQL\n\n```xml\n<sql id=\"selectSQL\">\n\t\tselect id , last_name, email ,gender from tbl_employee\n</sql> \n\n```\n\n引用SQL:\n\n```xml\n<include refid=\"selectSQL\"></include>\n```\n\n","tags":["Mybatis"],"categories":["JavaWeb"]},{"title":"Mybatis的resultMap自定义映射","url":"/2019/03/19/Mybatis的resultMap自定义映射/","content":"\n {{ \"Mybatis 自定义映射和分布查询\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 自定义映射\n\n- 自定义resultMap，实现高级结果集映射\n- id ：用于完成主键值的映射\n- result ：用于完成普通列的映射\n- association ：一个复杂的类型关联;许多结果将包成这种类型\n- collection ： 复杂类型的集\n\n## 项目结构\n\n## 数据准备\n\n![Amx8Z4.png](https://s2.ax1x.com/2019/03/18/Amx8Z4.png)\n\n![AmxZIs.png](https://s2.ax1x.com/2019/03/18/AmxZIs.png)\n\n## 项目结构\n\n![Amxyod.png](https://s2.ax1x.com/2019/03/18/Amxyod.png)\n\n### Department\n\n```java\npackage com.hph.mybatis.beans;\n\nimport java.util.List;\n\npublic class Department {\n\n    private Integer id;\n    private String departmentName ;\n\n    private List<Employee> emps ;\n\n\n    public List<Employee> getEmps() {\n        return emps;\n    }\n    public void setEmps(List<Employee> emps) {\n        this.emps = emps;\n    }\n    public Integer getId() {\n        return id;\n    }\n    public void setId(Integer id) {\n        this.id = id;\n    }\n    public String getDepartmentName() {\n        return departmentName;\n    }\n    public void setDepartmentName(String departmentName) {\n        this.departmentName = departmentName;\n    }\n    @Override\n    public String toString() {\n        return \"Department [id=\" + id + \", departmentName=\" + departmentName + \"]\";\n    }\n}\n```\n\n### Employee\n\n```java\npackage com.hph.mybatis.beans;\n\npublic class Employee {\n\n    private Integer id;\n    private String lastName;\n    private String email;\n    private Integer gender;\n\n    private Department dept;\n\n    public void setDept(Department dept) {\n        this.dept = dept;\n    }\n\n    public Department getDept() {\n        return dept;\n    }\n\n    public Employee() {\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender, Department dept) {\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n        this.dept = dept;\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender) {\n        super();\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n    }\n\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n\n    @Override\n    public String toString() {\n        return \"Employee [id=\" + id + \", lastName=\" + lastName + \", email=\" + email + \", gender=\" + gender + \"]\";\n    }\n\n}\n```\n\n### EmployeeMapperResultMap\n\n```java\npackage com.hph.mybatis.dao;\n\nimport com.hph.mybatis.beans.Employee;\n\nimport java.util.List;\n\npublic interface EmployeeMapperResultMap {\n\n    public Employee getEmployeeById(Integer id);\n\n    public Employee getEmpAndDept(Integer id);\n\n    public  Employee getEmpAndDeptStep(Integer id);\n\n    public List<Employee> getEmpsByDid(Integer did);\n\n}\n\n```\n\n### EmployeeMapperResultMap.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE mapper\n        PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n\n<mapper namespace=\"com.hph.mybatis.dao.EmployeeMapperResultMap\">\n    <!-- 自定义映射\n        type: 最终结果集封装的类型\n        <id>: 完成主键列的映射\n            column: 指定结果集的列名\n            property:指定对象的属性名\n        <result>:完成普通列的映射\n     -->\n    <select id=\"getEmployeeById\" resultMap=\"MyEmp\">\n\t\tselect id ,last_name, email,gender from tbl_employee where id = #{id}\n\t</select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"MyEmp\">\n        <id column=\"id\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n    </resultMap>\n\n    <!--\n\t\t需求: 查询员工对象， 并且查询员工所在 的部门信息.\n\t -->\n    <!-- public Employee getEmpAndDept(Integer id ); -->\n    <select id=\"getEmpAndDept\" resultMap=\"myEmpAndDept\">\n        SELECT e.id eid ,  e.last_name, e.email,e.gender  , d.id did , d.dept_name\n\t    FROM  tbl_employee  e  , tbl_dept  d\n\t    WHERE e.d_id = d.id  AND e.id = #{id}\n    </select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"myEmpAndDept\">\n        <id column=\"eid\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n        <!-- 级联的方式 -->\n        <result column=\"did\" property=\"dept.id\"/>\n        <result column=\"dept_name\" property=\"dept.departmentName\"/>\n    </resultMap>\n    <!--\n        association 使用分步查询:\n        需求:  查询员工信息并且查询员工所在的部门信息.\n              1. 先根据员工的id查询员工信息\n              2. 使用外键 d_id查询部门信息\n     -->\n    <!--public  Employee getEmpAndDeptStep(Integer id);-->\n    <select id=\"getEmpAndDeptStep\" resultMap=\"myEmpAndDeptStep\">\n\t  \tselect id, last_name, email,gender ,d_id  from tbl_employee where id = #{id}\n\t  </select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"myEmpAndDeptStep\">\n        <id column=\"id\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n        <!-- 分步查询 -->\n        <association property=\"dept\"\n                     select=\"com.hph.mybatis.dao.DepartmentMapperResultMap.getDeptById\"\n                     column=\"{did=d_id}\" fetchType=\"eager\">\n        </association>\n    </resultMap>\n    <!-- association 分步查询使用延迟加载/懒加载:\n            在全局配置文件中加上两个settings设置:\n            <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n          <setting name=\"aggressiveLazyLoading\" value=\"false\"/>\n     -->\n    <!-- public List<Employee>  getEmpsByDid(Integer did ); -->\n    <select id=\"getEmpsByDid\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select  id,last_name,email,gender from  tbl_employee where  d_id = #{did}\n    </select>\n</mapper>\n```\n\n### TestMapbatisResultMap\n\n```java\npackage com.hph.mybatis.test;\n\nimport com.hph.mybatis.beans.Department;\nimport com.hph.mybatis.beans.Employee;\nimport com.hph.mybatis.dao.DepartmentMapperResultMap;\nimport com.hph.mybatis.dao.EmployeeMapperResultMap;\nimport org.apache.ibatis.io.Resources;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.apache.ibatis.session.SqlSessionFactoryBuilder;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.io.InputStream;\n\npublic class TestMapbatisResultMap {\n\n    public SqlSessionFactory getsqlSessionFactory() throws IOException {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        return sqlSessionFactory;\n    }\n\n    @Test\n    public void testResultMap() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession();\n        try {\n            EmployeeMapperResultMap mapper = session.getMapper(EmployeeMapperResultMap.class);\n            Employee employee = mapper.getEmployeeById(1001);\n            System.out.println(employee);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testResultMapCascade() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession();\n        try {\n            EmployeeMapperResultMap mapper = session.getMapper(EmployeeMapperResultMap.class);\n\n            Employee employee = mapper.getEmpAndDept(1001);\n            System.out.println(employee);\n            System.out.println(employee.getDept());\n\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testResultMapAssociation() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession();\n        try {\n                EmployeeMapperResultMap mapper = session.getMapper(EmployeeMapperResultMap.class);\n            Employee employee = mapper.getEmpAndDeptStep(1001);\n            System.out.println(employee);\n            System.out.println(employee.getDept());\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testResultMapCollectionStep() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession();\n        try {\n            DepartmentMapperResultMap mapper = session.getMapper(DepartmentMapperResultMap.class);\n            Department dept = mapper.getDeptAndEmpsStep(1);\n            System.out.println(dept.getDepartmentName());\n            System.out.println(dept.getEmps());\n        }finally {\n            session.close();\n        }\n    }\n}\n```\n\n\n\n## association\n\n###  级联\n\n```xml\n    <!-- 自定义映射\n        type: 最终结果集封装的类型\n        <id>: 完成主键列的映射\n            column: 指定结果集的列名\n            property:指定对象的属性名\n        <result>:完成普通列的映射\n     -->\n    <select id=\"getEmployeeById\" resultMap=\"MyEmp\">\n\t\tselect id ,last_name, email,gender from tbl_employee where id = #{id}\n\t</select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"MyEmp\">\n        <id column=\"id\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n    </resultMap>\n```\n\ntestResultMap结果\n\n![An2foQ.png](https://s2.ax1x.com/2019/03/19/An2foQ.png)\n\n### Association\n\n```xml\n    <select id=\"getEmpAndDept\" resultMap=\"myEmpAndDept\">\n          SELECT e.id eid ,  e.last_name, e.email,e.gender  , d.id did , d.dept_name\n\t\t FROM  tbl_employee  e  , tbl_dept  d\n\t\t WHERE e.d_id = d.id  AND e.id = #{id}\n    </select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"myEmpAndDept\">\n        <id column=\"eid\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n        <!-- 级联的方式 -->\n        <result column=\"did\" property=\"dept.id\"/>\n        <result column=\"dept_name\" property=\"dept.departmentName\"/>\n    </resultMap>\n```\n\ngetEmpAndDept的结果\n\n![AnRPOK.png](https://s2.ax1x.com/2019/03/19/AnRPOK.png)\n\n#### 分步查询\n\n实际的开发中，对于每个实体类都应该有具体的增删改查方法，也就是DAO层， 因此对于查询员工信息并且将对应的部门信息也查询出来的需求，就可以通过分步的方式完成查询。\n\n①   先通过员工的id查询员工信息\n\n②   再通过查询出来的员工信息中的外键(部门id)查询对应的部门信息. \n\n```xml\n<!--public  Employee getEmpAndDeptStep(Integer id);-->\n    <select id=\"getEmpAndDeptStep\" resultMap=\"myEmpAndDeptStep\">\n\t  \tselect id, last_name, email,gender ,d_id  from tbl_employee where id = #{id}\n\t  </select>\n    <resultMap type=\"com.hph.mybatis.beans.Employee\" id=\"myEmpAndDeptStep\">\n        <id column=\"id\" property=\"id\"/>\n        <result column=\"last_name\" property=\"lastName\"/>\n        <result column=\"email\" property=\"email\"/>\n        <result column=\"gender\" property=\"gender\"/>\n        <!-- 分步查询 -->\n        <association property=\"dept\"\n                     select=\"com.hph.mybatis.dao.DepartmentMapperResultMap.getDeptById\"\n                     column=\"{did=d_id}\" fetchType=\"eager\">\n        </association>\n    </resultMap>\n    <!-- association 分步查询使用延迟加载/懒加载:\n            在全局配置文件中加上两个settings设置:\n            <setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n          <setting name=\"aggressiveLazyLoading\" value=\"false\"/>\n     -->\n    <!-- public List<Employee>  getEmpsByDid(Integer did ); -->\n    <select id=\"getEmpsByDid\" resultType=\"com.hph.mybatis.beans.Employee\">\n        select  id,last_name,email,gender from  tbl_employee where  d_id = #{did}\n    </select>\n```\n\n #### 延迟加载\n\n在分步查询的基础上，可以使用延迟加载来提升查询的效率，只需要在全局(mybatis-config.xml)的Settings中进行如下的配置:\n\n```xml\n<!-- 开启延迟加载 -->\n<setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n<!-- 设置加载的数据是按需还是全部 -->\n<setting name=\"aggressiveLazyLoading\" value=\"false\"/>\n```\n\n## collection\n\nPOJO中的属性可能会是一个集合对象,我们可以使用联合查询，并以级联属性的方式封装对象.使用collection标签定义对象的封装规则\n\n```xml\n    <select id=\"getDeptAndEmpsById\" resultMap=\"myDeptAndEmps\">\n\t\tSELECT d.id did ,d.dept_name, e.id eid, e.last_name, e.email,e.gender\n\t\tFROM tbl_dept d  LEFT OUTER JOIN  tbl_employee  e\n\t\tON d.id = e.d_id  WHERE d.id = #{id}\n\t</select>\n    <resultMap type=\"com.hph.mybatis.beans.Department\" id=\"myDeptAndEmps\">\n        <id column=\"did\" property=\"id\"/>\n        <result column=\"dept_name\" property=\"departmentName\"/>\n        <!--\n            collection: 完成集合类型的联合属性的映射\n                property: 指定联合属性\n                ofType: 指定集合中元素的类型\n         -->\n        <collection property=\"emps\" ofType=\"com.hph.mybatis.beans.Employee\" >\n            <id column=\"eid\" property=\"id\"/>\n            <result column=\"last_name\" property=\"lastName\"/>\n            <result column=\"email\" property=\"email\"/>\n            <result column=\"gender\" property=\"gender\"/>\n        </collection>\n    </resultMap>\n```\n\n![AnRPOK.png](https://s2.ax1x.com/2019/03/19/AnRPOK.png)\n\n### 分步查询\n\n  实际的开发中，对于每个实体类都应该有具体的增删改查方法，也就是DAO层， 因此对于查询部门信息并且将对应的所有的员工信息也查询出来的需求，就可以通过分步的方式完成查询。\n\n①   先通过部门的id查询部门信息\n\n②   再通过部门id作为员工的外键查询对应的部门信息. \n\n```xml\n<select id=\"getDeptAndEmpsStep\" resultMap=\"myDeptAndEmpsStep\">\n\t \tselect id , dept_name from tbl_dept where id = #{id}\n\t </select>\n\n    <resultMap  id=\"myDeptAndEmpsStep\" type=\"com.hph.mybatis.beans.Department\">\n        <id column=\"id\" property=\"id\"/>\n        <result column=\"dept_name\" property=\"departmentName\"/>\n        <collection property=\"emps\"\n                    select=\"com.hph.mybatis.dao.EmployeeMapperResultMap.getEmpsByDid\"\n                    column=\"id\">\n        </collection>\n    </resultMap>\n```\n\n![Anh0nP.png](https://s2.ax1x.com/2019/03/19/Anh0nP.png)\n\n## 分步查询多列值的传递\n\n如果分步查询时，需要传递给调用的查询中多个参数，则需要将多个参数封装成Map来进行传递，语法如下: {k1=v1, k2=v2....}\n\n在所调用的查询方，取值时就要参考Map的取值方式，需要严格的按照封装map时所用的key来取值. \n\ncolumn=\"{key1=column1,key2=column2}\"\n\n##  fetchType属性\n\n 在&lt;association&gt; 和&lt;collection&gt;标签中都可以设置fetchType，指定本次查询是否要使用延迟加载。默认为 fetchType=”lazy” ,如果本次的查询不想使用延迟加载，则可设置为fetchType=”eager”.\n\nfetchType可以灵活的设置查询是否需要使用延迟加载，而不需要因为某个查询不想使用延迟加载将全局的延迟加载设置关闭..\n\n\n\n\n\n\n\n","tags":["Mybatis"],"categories":["JavaWeb"]},{"title":"MyBatis的CURD","url":"/2019/03/18/MyBatis的CURD/","content":"\n {{ \"Mybatis 数据库增删改查操作\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 数据准备\n\n![Amx8Z4.png](https://s2.ax1x.com/2019/03/18/Amx8Z4.png)\n\n![AmxZIs.png](https://s2.ax1x.com/2019/03/18/AmxZIs.png)\n\n## 项目结构\n\n![Amxyod.png](https://s2.ax1x.com/2019/03/18/Amxyod.png)\n\n### Department\n\n```java\npackage com.hph.mybatis.beans;\n\nimport java.util.List;\n\npublic class Department {\n\n    private Integer id;\n    private String departmentName ;\n\n    private List<Employee> emps ;\n\n\n    public List<Employee> getEmps() {\n        return emps;\n    }\n    public void setEmps(List<Employee> emps) {\n        this.emps = emps;\n    }\n    public Integer getId() {\n        return id;\n    }\n    public void setId(Integer id) {\n        this.id = id;\n    }\n    public String getDepartmentName() {\n        return departmentName;\n    }\n    public void setDepartmentName(String departmentName) {\n        this.departmentName = departmentName;\n    }\n    @Override\n    public String toString() {\n        return \"Department [id=\" + id + \", departmentName=\" + departmentName + \"]\";\n    }\n}\n```\n\n### Employee\n\n```java\npackage com.hph.mybatis.beans;\n\npublic class Employee {\n\n    private Integer id;\n    private String lastName;\n    private String email;\n    private Integer gender;\n\n    private Department dept;\n\n    public void setDept(Department dept) {\n        this.dept = dept;\n    }\n\n    public Department getDept() {\n        return dept;\n    }\n\n    public Employee() {\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender, Department dept) {\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n        this.dept = dept;\n    }\n\n    public Employee(Integer id, String lastName, String email, Integer gender) {\n        super();\n        this.id = id;\n        this.lastName = lastName;\n        this.email = email;\n        this.gender = gender;\n    }\n\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n\n    @Override\n    public String toString() {\n        return \"Employee [id=\" + id + \", lastName=\" + lastName + \", email=\" + email + \", gender=\" + gender + \"]\";\n    }\n\n}\n```\n\n### EmployeeMapper\n\n```java\n\nimport com.hph.mybatis.beans.Employee;\nimport org.apache.ibatis.annotations.MapKey;\nimport org.apache.ibatis.annotations.Param;\n\nimport java.util.List;\nimport java.util.Map;\n\npublic interface EmployeeMapper {\n    //根据id查询员工\n    public Employee getEmployeeById(Integer id);\n\n    //添加一个新的员工\n    public void addEmployee(Employee employee);\n\n    //修改一个员工\n    public void updateEmployee(Employee employee);\n\n    //删除一个员工\n    public Integer deleteEmployeeById(Integer id);\n\n    //查询对象通过两个参数\n    public Employee getEmployeeByIdAndLastName(@Param(\"id\") Integer id, @Param(\"lastName\") String lastNmae);\n\n    //查询一个集合\n    public Employee getEmployeeByMap(Map<String, Object> map);\n\n    //查询多个数据返回一个对象的集合\n    public List<Employee>  getEmps();\n\n    //查询单条数据返回一个Map\n    public Map<String,Object> getEmployeeByIdReturnMap(Integer id);\n\n    //查询多条数据返回一个Map\n    @MapKey(\"id\")  //指定使用对象那个属性作为Map的key\n    public Map<Integer,Employee> getEmpsRetrunMap();\n\n}\n```\n\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE mapper  PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<!--配置SQL映射-->\n<mapper namespace=\"com.hph.mybatis.dao.EmployeeMapper\">\n    \n    <select id=\"selectEmployee\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name , email, gender from tbl_employee where id = #{id}\n    </select>\n    <!--public Employee getEmployeeById(Integer id);-->\n    <select id=\"getEmployeeById\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name , email, gender from tbl_employee where id = #{id}\n    </select>\n    <!--public void addEmployee(Employee employee);\n  parameterType:指定ca参数类型,可以省略不写\n  keyProperty:指定用对象的那个属性ba保存mybatisf返回的主键值\n    -->\n    <!--告诉mybatis:需要使用主键自增的方式-->\n    <insert id=\"addEmployee\" useGeneratedKeys=\"true\" keyProperty=\"id\">\n  insert  into  tbl_employee(last_name,email,gender) values (#{lastName},#{email},#{gender})\n    </insert>\n\n    <update id=\"updateEmployee\">\n\tupdate tbl_employee set\n\t\tlast_name = #{lastName},\n\t\temail = #{email},\n\t\tgender = #{gender}\n\t\twhere id = #{id}\n    </update>\n\n    <delete id=\"deleteEmployeeById\">\n        delete  from  tbl_employee where id = #{id}\n    </delete>\n    <select id=\"getEmployeeByIdAndLastName\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name lastName, email, gender from tbl_employee where id = #{id} and last_name = #{lastName}\n    </select>\n    <select id=\"getEmployeeByMap\" resultType=\"com.hph.mybatis.beans.Employee\">\n\n\t\tselect id, last_name , email, gender from ${tableName} where id = ${id} and last_name = #{ln}\n    </select>\n    <!--public List<Employee>  getEmps()\n    resultType结果集的封装类型\n    ;-->\n    <select id=\"getEmps\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name , email, gender from tbl_employee\n    </select>\n    <select id=\"getEmployeeByIdReturnMap\" resultType=\"java.util.Map\">\n\n\t\tselect id, last_name , email, gender from tbl_employee where  id = #{id}\n\n    </select>\n    <select id=\"getEmpsRetrunMap\" resultType=\"java.util.Map\">\n\t\tselect id, last_name , email, gender from tbl_employee\n    </select>\n\n</mapper>\n```\n\n###  EmployeeMapper.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE mapper  PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<!--配置SQL映射-->\n<mapper namespace=\"com.hph.mybatis.dao.EmployeeMapper\">\n\n    <!--public Employee getEmployeeById(Integer id);-->\n    <select id=\"getEmployeeById\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name , email, gender from tbl_employee where id = #{id}\n    </select>\n    <!--public void addEmployee(Employee employee);\n  parameterType:指定ca参数类型,可以省略不写\n  keyProperty:指定用对象的那个属性ba保存mybatisf返回的主键值\n    -->\n    <!--告诉mybatis:需要使用主键自增的方式-->\n    <insert id=\"addEmployee\" useGeneratedKeys=\"true\" keyProperty=\"id\">\n  insert  into  tbl_employee(last_name,email,gender) values (#{lastName},#{email},#{gender})\n    </insert>\n\n\n    <!--public void updateEmployee(Employee employee);-->\n    <update id=\"updateEmployee\">\n\tupdate tbl_employee set\n\t\tlast_name = #{lastName},\n\t\temail = #{email},\n\t\tgender = #{gender}\n\t\twhere id = #{id}\n    </update>\n\n    <!--public Integer deleteEmployeeById(Integer id);-->\n    <delete id=\"deleteEmployeeById\">\n        delete  from  tbl_employee where id = #{id}\n    </delete>\n    <!--    public Employee getEmployeeByIdAndLastName(@Param(\"id\") Integer id, @Param(\"lastName\") String lastNmae);\n-->\n    <select id=\"getEmployeeByIdAndLastName\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name lastName, email, gender from tbl_employee where id = #{id} and last_name = #{lastName}\n    </select>\n    <!--public Employee getEmployeeByMap(Map<String, Object> map);-->\n    <select id=\"getEmployeeByMap\" resultType=\"com.hph.mybatis.beans.Employee\">\n\n\t\tselect id, last_name , email, gender from ${tableName} where id = ${id} and last_name = #{ln}\n    </select>\n    <!--public List<Employee>  getEmps()\n    resultType结果集的封装类型\n    ;-->\n    <!--public List<Employee>  getEmps();-->\n    <select id=\"getEmps\" resultType=\"com.hph.mybatis.beans.Employee\">\n\t\tselect id, last_name , email, gender from tbl_employee\n    </select>\n    <!--public Map<String,Object> getEmployeeByIdReturnMap(Integer id);-->\n    <select id=\"getEmployeeByIdReturnMap\" resultType=\"java.util.Map\">\n\n\t\tselect id, last_name , email, gender from tbl_employee where  id = #{id}\n\n    </select>\n    <!--public Map<Integer,Employee> getEmpsRetrunMap();-->\n    <select id=\"getEmpsRetrunMap\" resultType=\"java.util.Map\">\n\t\tselect id, last_name , email, gender from tbl_employee\n    </select>\n</mapper>\n```\n\n### TestMybatisMapper\n\n```java\npackage com.hph.mybatis.test;\n\nimport com.hph.mybatis.beans.Employee;\nimport com.hph.mybatis.dao.EmployeeMapper;\nimport org.apache.ibatis.io.Resources;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.apache.ibatis.session.SqlSessionFactoryBuilder;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.sql.SQLException;\nimport java.util.HashMap;\nimport java.util.Map;\n\npublic class TestMybatisMapper {\n    @Test\n    public void testSqlsessionFactory() throws IOException {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        System.out.println(sqlSessionFactory);\n        SqlSession session = sqlSessionFactory.openSession();\n        System.out.println(session);\n        try {\n            Employee employee = session.selectOne(\"selectEmployee\", 5);\n            System.out.println(employee);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testHelloWorldMapper() throws IOException {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        SqlSession session = sqlSessionFactory.openSession();\n        try {\n            //Mapper接口 dao接口\n            EmployeeMapper dao = session.getMapper(EmployeeMapper.class);\n            Employee employee = dao.getEmployeeById(1001);\n            System.out.println(employee);\n        } finally {\n            session.close();\n        }\n    }\n\n    public SqlSessionFactory getsqlSessionFactory() throws IOException {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        return sqlSessionFactory;\n    }\n\n    @Test\n    public void testCRUD() throws IOException, SQLException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            //获取代理实现类对象 mapper接口的代理实现类对象\n            EmployeeMapper mapper = session.getMapper(EmployeeMapper.class);\n            //添加\n            Employee employee = new Employee(null, \"清风笑_Test\", \"qfx@qq.com\", 1);\n            mapper.addEmployee(employee);\n            System.out.println(\"返回的键值\" + employee.getId());\n\n            /**\n             *      Connection conn = null;\n             *             PreparedStatement ps = conn.prepareStatement(\"sql\", PreparedStatement.RETURN_GENERATED_KEYS);\n             *\n             *             ps.executeUpdate();\n             *             ps.getGeneratedKeys();\n             *             jdbc获取新插入的组件的数值\n             */\n           //修改\n            Employee employee1 = new Employee(null,\"清风_笑丶\", \"qfx@gmail.com\", 1);\n\n            mapper.updateEmployee(employee1);\n            //删除\n         //   Integer count = mapper.deleteEmployeeById(1001);\n           // System.out.println(count);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void testParameter() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapper mapper = session.getMapper(EmployeeMapper.class);\n            //   Employee employee = mapper.getEmployeeByIdAndLastName(5, \"清风_笑丶\");\n            Map<String, Object> map = new HashMap<String, Object>();\n            map.put(\"id\", \"5\");\n            map.put(\"ln\", \"清风_笑丶\");\n            map.put(\"tableName\", \"tbl_employee\");\n\n            Employee employee = mapper.getEmployeeByMap(map);\n            System.out.println(employee);\n        } finally {\n            session.close();\n        }\n    }\n\n    @Test\n    public void tetSelect() throws IOException {\n        SqlSessionFactory ssf = getsqlSessionFactory();\n        SqlSession session = ssf.openSession(true);\n        try {\n            EmployeeMapper mapper = session.getMapper(EmployeeMapper.class);\n            //List<Employee> emps = mapper.getEmps();\n            //System.out.println(emps);\n            //Map<String, Object> map = mapper.getEmployeeByIdReturnMap(5);\n            //System.out.println(map);\n            Map<Integer, Employee> map = mapper.getEmpsRetrunMap();\n            System.out.println(map);\n        } finally {\n            session.close();\n        }\n    }\n}\n```\n\n## 结果\n\ntestHelloWorldMapper \n\n![AmzojK.png](https://s2.ax1x.com/2019/03/18/AmzojK.png)\n\naddEmployee\n\n添加数据\n\n![AnpOSI.png](https://s2.ax1x.com/2019/03/18/AnpOSI.png)\n\n![AnpbYd.png](https://s2.ax1x.com/2019/03/18/AnpbYd.png)\n\n修改数据\n\n![AnPBOP.png](https://s2.ax1x.com/2019/03/18/AnPBOP.png)\n\n![AnP2Wj.png](https://s2.ax1x.com/2019/03/18/AnP2Wj.png)\n\n删除数据\n\n![AnPfln.png](https://s2.ax1x.com/2019/03/18/AnPfln.png)\n\n![AnPhyq.png](https://s2.ax1x.com/2019/03/18/AnPhyq.png)\n\ntestParameter\n\n![AnFHz9.png](https://s2.ax1x.com/2019/03/18/AnFHz9.png)\n\ntetSelect\n\n无参数\n\n![AnkFsI.png](https://s2.ax1x.com/2019/03/18/AnkFsI.png)\n\n有参数\n\n![AnkKzj.png](https://s2.ax1x.com/2019/03/18/AnkKzj.png)\n\n##  主键生成方式、获取主键值\n\n### 主键生成方式\n\n支持主键自增，例如MySQL数据库\n\n不支持主键自增，例如Oracle数据库\n\n### 获取主键值\n\n数据库支持自动生成主键的字段（比如 MySQL 和 SQL Server），则可以设置 useGeneratedKeys=”true”，然后再把 keyProperty设置到目标属性上。\n\n```xml\n<insert id=\"insertEmployee\" \t\nparameterType=\"com.hph.mybatis.beans.Employee\"  \n\t\t\tdatabaseId=\"mysql\"\n\t\t\tuseGeneratedKeys=\"true\"\n\t\t\tkeyProperty=\"id\">\n\t\tinsert into tbl_employee(last_name,email,gender) values(#{lastName},#{email},#{gender})\n</insert>\n```\n\n### 参数传递\n\n#### 参数传递的方式\n\n- 单个参数: 可以接受基本类型，对象类型。这种情况MyBatis可直接使用这个参数，不需要经过任 何处理。\n- 多个参数: 任意多个参数，都会被MyBatis重新包装成一个Map传入。Map的key是param1，param2，或者0，1…，值就是参数的值\n- 命名参数: 为参数使用@Param起一个名字，MyBatis就会将这些参数封装进map中，key就是我们自己指定的名\n- POJO: 当这些参数属于我们业务POJO时，直接传递POJO\n- Map : 我们也可以封装多个参数为map，直接传递\n- Collection/Array : 会被MyBatis封装成一个map传入, Collection对应的key是collection,Array对应的key是array. 如果确定是List集合，key还可以是list.\n\n#### 参数传递源码\n\n```java\npublic Object getNamedParams(Object[] args) {\n    final int paramCount = names.size();\n    if (args == null || paramCount == 0) {\n      return null;\n    } else if (!hasParamAnnotation && paramCount == 1) {\n      return args[names.firstKey()];\n    } else {\n      final Map<String, Object> param = new ParamMap<Object>();\n      int i = 0;\n      for (Map.Entry<Integer, String> entry : names.entrySet()) {\n        param.put(entry.getValue(), args[entry.getKey()]);\n        // add generic param names (param1, param2, ...)\n        final String genericParamName = GENERIC_NAME_PREFIX + String.valueOf(i + 1);\n        // ensure not to overwrite parameter named with @Param\n        if (!names.containsValue(genericParamName)) {\n          param.put(genericParamName, args[entry.getKey()]);\n        }\n        i++;\n      }\n      return param;\n    }\n  }\n```\n\n### 参数处理\n\n参数位置支持的属性: javaType、jdbcType、mode、numericScale、resultMap、typeHandler、jdbcTypeName、expression\n\n实际上通常被设置的是：可能为空的列名指定 jdbcType ,例如:\n\n```xml\ninsert into orcl_employee(id,last_name,email,gender) values(employee_seq.nextval,#{lastName, ,jdbcType=NULL },#{email},#{gender})可以在全局配置文件指定为null 也可以像上面这面这样单独配置\n```\n\n### 参数获取方式\n\n```xml\n#{key}：可取普通类型、POJO类型、多个参数、集合类型\n获取参数的值，预编译到SQL中。安全。Preparedstatement\n${key}：可取单个普通类型、POJO类型、多个参数、集合类型\n注意：取单个普通类型的参数，${}中不能随便写 必须用_parameter  _parameter是Mybatis的内置参数获取参数的值，拼接到SQL中。有SQL注入问题。Statement ORDER BY ${name}\n\n原则：能用#{}取值就优先使用#{} 解决不了的使用${}\n\t\te.g.原生的JDBC不支持占位符的地方 就可以使用${}\n\t\te.g.Select column1,column2…from 表 where 条件 group by 组表示 having 条件 \n\t\t\t\torder by 排序字段 desc/asc  limit X，X；\n```\n\n###  select查询的几种情况\n\n查询单行数据返回单个对象\n\n```java\npublic Employee getEmployeeById(Integer id );\n```\n\n\n查询多行数据返回对象的集合\n\n```java\npublic List<Employee> getAllEmps();\n```\n\n查询单行数据返回Map集合\n\n```java\npublic Map<String,Object> getEmployeeByIdReturnMap(Integer id );\n```\n\n\n查询多行数据返回Map集合\n\n```java\n@MapKey(\"id\") // 指定使用对象的哪个属性来充当map的key(对象的属性，而不是数据库的列)\npublic Map<Integer,Employee>  getAllEmpsReturnMap();\n```\n\n## resultType自动映射\n\n- autoMappingBehavior默认是PARTIAL，开启自动映射的功能。唯一的要求是列名和javaBean属性名一致\n- 如果autoMappingBehavior设置为null则会取消自动映射\n- 数据库字段命名规范，POJO属性符合驼峰命名法，如A_COLUMNaColumn，我们可以开启自动驼峰命名规则映射功能，mapUnderscoreToCamelCase=true\n\n缺点：多表查询 完成不了\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Mybatis"],"categories":["JavaWeb"]},{"title":"MyBatis全局配置文件和映射文件","url":"/2019/03/18/MyBatis全局配置文件和映射文件/","content":"\n {{ \"Mybatis文件的全局配置和映射文件\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n##  配置文件\n\nMyBatis 的配置文件包含了影响 MyBatis 行为的设置（settings）和属性（properties）信息。\n\n### 配置文件结构\n\n```properties\nconfiguration 配置 \nproperties 属性\nsettings 设置\ntypeAliases 类型命名\ntypeHandlers 类型处理器\nobjectFactory 对象工厂\nplugins 插件\nenvironments 环境 \n\tenvironment 环境变量 \n\t\ttransactionManager 事务管理器\n\t\tdataSource 数据源\ndatabaseIdProvider 数据库厂商标识\nmappers 映射器\n```\n\n### properties属性\n\n可外部配置且可动态替换的，既可以在典型的Java 属性文件中配置，也可以通过 properties 元素的子元素来配置\n\n```xml\n<properties>\n     <property name=\"driver\" value=\"com.mysql.jdbc.Driver\" />\n     <property name=\"url\" \n             value=\"jdbc:mysql://58.87.70.124:3306/test_mybatis\" />\n     <property name=\"username\" value=\"root\" />\n     <property name=\"password\" value=\"123456\" />\n </properties>\n```\n\n你可以创建一个资源文件，名为jdbc.properties的文件,将四个连接字符串的数据在资源文件中通过键值 对(key=value)的方式放置，不要任何符号，一条占一行.\n\nmybastis-conf.xml\n\n````xml\n<!-- \n\t\tproperties: 引入外部的属性文件\n\t\t\tresource: 从类路径下引入属性文件 \n\t\t\turl:  引入网络路径或者是磁盘路径下的属性文件\n-->\n<properties resource=\"db.properties\" ></properties>\n    <environments default=\"development\">\n        <!-- 具体的环境 -->\n        <environment id=\"development\">\n            <transactionManager type=\"JDBC\"/>\n            <dataSource type=\"POOLED\">\n                <property name=\"driver\" value=\"${jdbc.driver}\"/>\n                <property name=\"url\" value=\"${jdbc.url}\"/>\n                <property name=\"username\" value=\"${jdbc.username}\"/>\n                <property name=\"password\" value=\"${jdbc.password}\"/>\n            </dataSource>\n        </environment>\n    </environments>\n````\n\n### jdbc.propertis\n\n```properties\njdbc.driver=com.mysql.jdbc.Driver\njdbc.url=jdbc:mysql://58.87.70.124:3306/test_mybatis\njdbc.username=root\njdbc.password=123456\n```\n\n### settings设置\n\n MyBatis 中极为重要的调整设置，它们会改变 MyBatis 的运行时行为。\n\n```xml\n<settings>\n<setting name=\"cacheEnabled\" value=\"true\"/>\n<setting name=\"lazyLoadingEnabled\" value=\"true\"/>\n<setting name=\"multipleResultSetsEnabled\" value=\"true\"/>\n<setting name=\"useColumnLabel\" value=\"true\"/>\n<setting name=\"useGeneratedKeys\" value=\"false\"/>\n<setting name=\"autoMappingBehavior\" value=\"PARTIAL\"/>\n<setting name=\"autoMappingUnknownColumnBehavior\" value=\"WARNING\"/>\n<setting name=\"defaultExecutorType\" value=\"SIMPLE\"/>\n<setting name=\"defaultStatementTimeout\" value=\"25\"/>\n<setting name=\"defaultFetchSize\" value=\"100\"/>\n<setting name=\"safeRowBoundsEnabled\" value=\"false\"/>\n<setting name=\"mapUnderscoreToCamelCase\" value=\"false\"/>\n<setting name=\"localCacheScope\" value=\"SESSION\"/>\n<setting name=\"jdbcTypeForNull\" value=\"OTHER\"/>\n<setting name=\"lazyLoadTriggerMethods\"\n           value=\"equals,clone,hashCode,toString\"/>\n</settings>\n\n```\n\n![AmG9eS.png](https://s2.ax1x.com/2019/03/18/AmG9eS.png)\n\n![AmG9eS.png](https://s2.ax1x.com/2019/03/18/AmG9eS.png)\n\n[原图片链接](https://blog.csdn.net/fageweiketang/article/details/80767532)\n\n### typeAliases\n\n\n类型别名是为 Java 类型设置一个短的名字，可以方便我们引用某个类。\n\n```xml\n    <typeAliases>\n    \t <typeAlias type=\"com.hph.mybatis.beans.Employee\" alias=\"employee\"/>\n    </typeAliases>\n```\n\n\n类很多的情况下，可以批量设置别名这个包下的每一个类创建一个默认的别名，就是简单类名小写\n\n```xml\n    <typeAliases>\n        <!--  <typeAlias type=\"com.hph.mybatis.beans.Employee\" alias=\"employee\"/> -->\n        <package name=\"com.hph.mybatis.beans\"/>\n    </typeAliases>\n```\n\n\nMyBatis已经取好的别名\n\n![AmGsSI.png](https://s2.ax1x.com/2019/03/18/AmGsSI.png)\n\n###  environments\n\n- MyBatis可以配置多种环境，比如开发、测试和生产环境需要有不同的配置\n- 每种环境使用一个environment标签进行配置并指定唯一标识符\n- 可以通过environments标签中的default属性指定一个环境的标识符来快速的切换环境\n- environment-指定具体环境\n- id：指定当前环境的唯一标识  transactionManager、和dataSource都必须有\n\n```xml\n<environments default=\"oracle\">\n\t\t<environment id=\"mysql\">\n\t\t\t<transactionManager type=\"JDBC\" />\n\t\t\t<dataSource type=\"POOLED\">\n\t\t\t\t<property name=\"driver\" value=\"${jdbc.driver}\" />\n\t\t\t\t<property name=\"url\" value=\"${jdbc.url}\" />\n\t\t\t\t<property name=\"username\" value=\"${jdbc.username}\" />\n\t\t\t\t<property name=\"password\" value=\"${jdbc.password}\" />\n\t\t\t</dataSource>\n\t\t</environment>\n\t\t <environment id=\"oracle\">\n\t\t\t<transactionManager type=\"JDBC\"/>\t\n\t\t\t<dataSource type=\"POOLED\">\n\t\t\t\t<property name=\"driver\" value=\"${orcl.driver}\" />\n\t\t\t\t<property name=\"url\" value=\"${orcl.url}\" />\n\t\t\t\t<property name=\"username\" value=\"${orcl.username}\" />\n\t\t\t\t<property name=\"password\" value=\"${orcl.password}\" />\n\t\t\t</dataSource>\n\t\t</environment> \n\t</environments>\n```\n\n### transactionManager\n\n#### type参数\n\n JDBC：使用JDBC的提交和回滚设置，依赖于从数据源得到的连接来管理事务范围。 JdbcTransactionFactory\n\nMANAGED：不提交或回滚一个连接、让容器来管理事务的整个生命周期（比如JEE应用服务器的上下文）。 ManagedTransactionFactory\n\n自定义：实现TransactionFactory接口，type=全类名/别名\n\n#### dataSource\n\nUNPOOLED：不使用连接池， UnpooledDataSourceFactory\n\nPOOLED：使用连接池， PooledDataSourceFactory\n\nJNDI： 在EJB 或应用服务器这类容器中查找指定的数据源\n\n实际开发中我们使用Spring管理数据源，并进行事务控制的配置来覆盖上述配置\n\n### mappers\n\n 在mybatis初始化的时候，mybatis需要引入的Mapper映射文件.\n\nmapper逐个注册SQL映射文件\n\n&nbsp; &nbsp; &nbsp; &nbsp; resource : 引入类路径下的文件 \n\n&nbsp; &nbsp; &nbsp; &nbsp; url :   引入网络路径或者是磁盘路径下的文件\n\n&nbsp; &nbsp; &nbsp; &nbsp; class :    引入Mapper接口.\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 有SQL映射文件 , 要求Mapper接口与 SQL映射文件同名同位置. \n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 没有SQL映射文件 , 使用注解在接口的方法上写SQL语句.\n\n```xml\n<mappers>\n\t\t<mapper resource=\"EmployeeMapper.xml\" />\n\t\t<mapper class=\"com.hph.mybatis.dao.EmployeeMapper\"/>\n\t\t<package name=\"com.atguigu.mybatis.dao\"/>\n</mappers>\n```\n\n```xml\n    <mappers>\n        <package name=\"com.hph.mybatis.dao\"></package>\n    </mappers>\n```\n\n#### 注意\n\n如果是使用的IDEA的话  IDEA 默认不扫描src/main/java 目录会出现org.apache.ibatis.binding.BindingException: Invalid bound statement (not found):  [方法解决链接](https://blog.csdn.net/iteye_14811/article/details/82673913)\n\n### MyBatis映射文件\n\nMyBatis的真正强大在于它的映射语句，也是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 就是针对SQL 构建的，并且比普通的方法做的更好。\n\nSQL 映射文件有很少的几个顶级元素（按照它们应该被定义的顺序）：\n\n &nbsp; &nbsp; &nbsp; &nbsp;cache – 给定命名空间的缓存配置。\n\n &nbsp; &nbsp; &nbsp; &nbsp;cache-ref – 其他命名空间缓存配置的引用。\n\n &nbsp; &nbsp; &nbsp; &nbsp;resultMap – 是最复杂也是最强大的元素，用来描述如何从数据库结果集中来加对象。\n\n &nbsp; &nbsp; &nbsp; &nbsp;parameterMap – 已废弃！老式风格的参数映射。内联参数是首选,这个元素可能在将来被移除。\n\n &nbsp; &nbsp; &nbsp; &nbsp;sql – 可被其他语句引用的可重用语句块。\n\n &nbsp; &nbsp; &nbsp; &nbsp;insert – 映射插入语句\n\n &nbsp; &nbsp; &nbsp; &nbsp;update – 映射更新语句\n\n  &nbsp; &nbsp;&nbsp; &nbsp;delete – 映射删除语句\n\n &nbsp; &nbsp; &nbsp; &nbsp;select – 映射查询语\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Mybatis"],"categories":["JavaWeb"]},{"title":"Mybatis入门","url":"/2019/03/16/Mybatis入门/","content":"\n {{ \"Mybatis  基本介绍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n## 简介\n\nMyBatis是Apache的一个开源项目iBatis, 2010年6月这个项目由Apache Software      Foundation 迁移到了Google Code，随着开发团队转投Google Code旗下， iBatis3.x   正式更名为MyBatis ，代码于2013年11月迁移到Github\n\niBatis一词来源于“internet”和“abatis”的组合，是一个基于Java的持久层框架。 iBatis  提供的持久层框架包括SQL Maps和Data Access Objects（DAO）\n\n## 特点\n\n- MyBatis 是支持定制化 SQL、存储过程以及高级映射的优秀的持久层框架\n- MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集\n- MyBatis可以使用简单的XML或注解用于配置和原始映射，将接口和Java的POJO（Plain Old Java Objects，普通的Java对象）映射成数据库中的记录\n- 其是一个半自动ORM（Object Relation Mapping对象关系映射）框架   Hibernant是全自动的\n\n## 案例\n\n### 结构\n\n#### pom.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">\n    <modelVersion>4.0.0</modelVersion>\n\n    <groupId>com.hph</groupId>\n    <artifactId>Mybatis</artifactId>\n    <version>1.0-SNAPSHOT</version>\n    <dependencies>\n        <dependency>\n            <!--mybatis版本-->\n            <groupId>org.mybatis</groupId>\n            <artifactId>mybatis</artifactId>\n            <version>3.4.1</version>\n        </dependency>\n        <dependency>\n            <!--log4j版本-->\n            <groupId>org.slf4j</groupId>\n            <artifactId>slf4j-log4j12</artifactId>\n            <version>1.7.25</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <!--mysql驱动-->\n            <groupId>mysql</groupId>\n            <artifactId>mysql-connector-java</artifactId>\n            <version>5.1.37</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>4.12</version>\n            <scope>compile</scope>\n        </dependency>\n    </dependencies>\n</project>\n```\n\n#### Employee\n\n```java\npackage com.hph.mybatis.beans;\n\npublic class Employee {\n    private Integer id;\n    private String lastName;\n    private String email;\n    private Integer gender;\n\n\n    @Override\n    public String toString() {\n        return \"Employee{\" +\n                \"id=\" + id +\n                \", lastName='\" + lastName + '\\'' +\n                \", email='\" + email + '\\'' +\n                \", gender=\" + gender +\n                '}';\n    }\n\n    public Integer getId() {\n        return id;\n    }\n\n    public void setId(Integer id) {\n        this.id = id;\n    }\n\n    public String getLastName() {\n        return lastName;\n    }\n\n    public void setLastName(String lastName) {\n        this.lastName = lastName;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n}\n\n```\n\n#### EmployeeDao\n\n```java\npackage com.hph.mybatis.dao;\n\nimport com.hph.mybatis.beans.Employee;\n\npublic interface EmployeeDao {\n    public Employee getEmployeeById(Integer id);\n}\n```\n\n#### TestMybatis\n\n```java\npackage com.hph.mybatis.test;\n\nimport com.hph.mybatis.beans.Employee;\nimport org.apache.ibatis.io.Resources;\nimport org.apache.ibatis.session.SqlSession;\nimport org.apache.ibatis.session.SqlSessionFactory;\nimport org.apache.ibatis.session.SqlSessionFactoryBuilder;\nimport org.junit.Test;\n\nimport java.io.IOException;\nimport java.io.InputStream;\n\npublic class TestMybatis {\n    @Test\n    public void testSqlsessionFactory() throws IOException {\n        String resource = \"mybatis-config.xml\";\n        InputStream inputStream = Resources.getResourceAsStream(resource);\n        SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream);\n        System.out.println(sqlSessionFactory);\n        SqlSession session = sqlSessionFactory.openSession();\n        System.out.println(session);\n        try {\n            Employee employee = session.selectOne(\"suibian.selectEmployee\", 1001);\n            System.out.println(employee);\n        } finally {\n            session.close();\n        }\n    }\n```\n\n#### db.protertis\n\n```properties\njdbc.driver=com.mysql.jdbc.Driver\njdbc.url=jdbc:mysql://58.87.70.124:3306/test_mybatis\njdbc.username=root\njdbc.password=123456\n```\n\n#### EmployeeMapper.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?> <!DOCTYPE mapper  PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\">\n<!--配置SQL映射-->\n<mapper namespace=\"suibian\">\n    <select id=\"selectEmployee\" resultType=\"com.hph.mybatis.beans.Employee\">\n    select * from tbl_employee where id = #{id}  </select>\n</mapper>\n```\n\n#### mybatis-config.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<!DOCTYPE configuration\n        PUBLIC \"-//mybatis.org//DTD Config 3.0//EN\"\n        \"http://mybatis.org/dtd/mybatis-3-config.dtd\">\n<!-- 配置 -->\n<configuration>\n    <properties resource=\"db.properties\">\n    </properties>\n    <settings>\n        <!-- 映射下划线到驼峰命名 -->\n        <setting name=\"mapUnderscoreToCamelCase\" value=\"true\"/>\n    </settings>\n    <typeAliases>\n        <!--  <typeAlias type=\"com.hph.mybatis.beans.Employee\" alias=\"employee\"/> -->\n        <package name=\"com.hph.mybatis.beans\"/>\n    </typeAliases>\n    <environments default=\"development\">\n        <!-- 具体的环境 -->\n        <environment id=\"development\">\n            <transactionManager type=\"JDBC\"/>\n            <dataSource type=\"POOLED\">\n                <property name=\"driver\" value=\"${jdbc.driver}\"/>\n                <property name=\"url\" value=\"${jdbc.url}\"/>\n                <property name=\"username\" value=\"${jdbc.username}\"/>\n                <property name=\"password\" value=\"${jdbc.password}\"/>\n            </dataSource>\n        </environment>\n    </environments>\n    <mappers>\n        <mapper resource=\"EmployeeMapper.xml\"></mapper>\n    </mappers>\n</configuration>\n```\n\n![AVj2qA.png](https://s2.ax1x.com/2019/03/16/AVj2qA.png)\n\n\n\n\n\n\n\n\n\n","tags":["Mybatis"],"categories":["JavaWeb"]},{"title":"Spring和SpringMVC整合","url":"/2019/03/13/Spring和SpringMVC整合/","content":"\n {{ \"Spring和SpringMVC整合出现的问题\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n## 原因\n\nSpringMVC就运行在Spring环境之下，为什么还要整合呢？SpringMVC和Spring都有IOC容器，是不是都需要保留呢？\n\n通常情况下，类似于数据源，事务，整合其他框架都是放在spring的配置文件中（而不是放在SpringMVC的配置文件中）,实际上放入Spring配置文件对应的IOC容器中的还有Service和Dao.而SpringMVC也搞自己的一个IOC容器，在SpringMVC的容器中只配置自己的Handler(Controller)信息。所以，两者的整合是十分有必要的，SpringMVC负责接受页面发送来的请求，Spring框架则负责整理中间需求逻辑，对数据库发送操作请求，对数据库的操作目前则先使用Spring框架中的JdbcTemplate进行处理。\n\n## 目录结构\n\n![AkUV8P.png](https://s2.ax1x.com/2019/03/13/AkUV8P.png)\n\n### 详细结构\n\n#### applicationContext.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\">\n    <!--spring的配置文件-->\n    <bean id=\"person\" class=\"com.hph.ss.beans.Person\">\n        <property name=\"name\" value=\"Spring+SpringMVC\"></property>\n    </bean>\n    <!-- 组件扫描 -->\n <context:component-scan base-package=\"com.hph.ss\"></context:component-scan>\n</beans>\n```\n\n#### Person\n\n```java\npackage com.hph.ss.beans;\n\npublic class Person {\n    private String name;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n    public void sayHello() {\n        System.out.println(\"My name is \" + name);\n    }\n\n}\n```\n\n#### UserDao\n\n```java\npackage com.hph.ss.dao;\n\nimport org.springframework.stereotype.Repository;\n\n@Repository\npublic class UserDao {\n    public UserDao(){\n        System.out.println(\"UserDao....\");\n    }\n    public void hello() {\n        System.out.println(\"UserDao  hello.....\");\n    }\n}\n\n```\n\n#### UserHandler\n\n```java\npackage com.hph.ss.handler;\n\nimport org.springframework.stereotype.Controller;\n\n@Controller\npublic class UserHandler {\n\n    public UserHandler() {\n        System.out.println(\"UserHandler.......\");\n    }\n}\n```\n\n#### MyServletContextlistener\n\n```java\npackage com.hph.ss.listerner;\n\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\nimport javax.servlet.ServletContext;\nimport javax.servlet.ServletContextEvent;\nimport javax.servlet.ServletContextListener;\n\npublic class MyServletContextlistener implements ServletContextListener {\n    /**\n     * 当监听到ServletContext被创建,则执行该方法\n     */\n    public void contextInitialized(ServletContextEvent sce) {\n        //1.创建SpringIOC容器对象\n        ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\");\n\n        //2.将SpringIOC容器对象绑定到ServletContext中\n        ServletContext sc = sce.getServletContext();\n\n        sc.setAttribute(\"applicationContext\", ctx);\n    }\n\n    public void contextDestroyed(ServletContextEvent sce) {\n\n    }\n}\n\n```\n\n#### UserService\n\n```java\npackage com.hph.ss.service;\n\nimport com.hph.ss.dao.UserDao;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\n@Service\npublic class UserService {\n\t\n\t@Autowired\n\tprivate UserDao userDao  ;\n\t\n\tpublic UserService() {\n\t\tSystem.out.println(\"UserService ......\");\n\t}\n\t\n\tpublic void hello() {\n\t\tuserDao.hello();\n\t}\n}\n```\n\n#### HelloServlet\n\n```java\npackage com.hph.ss.servlet;\n\nimport com.hph.ss.beans.Person;\nimport org.springframework.context.ApplicationContext;\n\nimport javax.servlet.ServletContext;\nimport javax.servlet.ServletException;\nimport javax.servlet.annotation.WebServlet;\nimport javax.servlet.http.HttpServlet;\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\nimport java.io.IOException;\n\n@WebServlet(name = \"HelloServlet\")\npublic class HelloServlet extends HttpServlet {\n\n    protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n\n    }\n\n    protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException {\n        //访问到SpringIOC容器中的person对象.\n        //从ServletContext对象中获取SpringIOC容器对象\n        ServletContext sc = getServletContext();\n\n        ApplicationContext ctx = (ApplicationContext) sc.getAttribute(\"applicationContext\");\n\n        Person person = ctx.getBean(\"person\", Person.class);\n        person.sayHello();\n\n    }\n}\n```\n\n#### springmvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\">\n    <!--组件扫描-->\n    <context:component-scan base-package=\"com.hph.ss\n\"></context:component-scan>\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    <!--处理静态资源-->\n    <mvc:default-servlet-handler/>\n    <mvc:annotation-driven/>\n</beans>\n```\n\n### web\n\n#### index.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n  <head>\n    <title>$Title$</title>\n  </head>\n  <body>\n  <a href=\"hello\">Hello Springmvc</a>\n  </body>\n</html>\n```\n\n#### web. xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\"\n         version=\"4.0\">\n    <!-- 初始化SpringIOC容器的监听器 -->\n    <context-param>\n        <param-name>contextConfigLocation</param-name>\n        <param-value>classpath:applicationContext.xml</param-value>\n    </context-param>\n\n    <listener>\n        <listener-class>org.springframework.web.context.ContextLoaderListener</listener-class>\n    </listener>\n\n    <!-- Springmvc的前端控制器 -->\n    <!-- The front controller of this Spring Web application, responsible for handling all application requests -->\n    <servlet>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            <param-value>classpath:springmvc.xml</param-value>\n        </init-param>\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <!-- Map all requests to the DispatcherServlet for handling -->\n    <servlet-mapping>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n</web-app>\n```\n\n\n\n### 问题\n\n![AkwIi9.png](https://s2.ax1x.com/2019/03/13/AkwIi9.png)\n\n原因:\n\n#### 问题描述\n\nSpring集成SpringMVC启动后同一个bean注入了两次\n\n#### 原因分析\n\nSping+SpringMVC的框架中，IoC容器的加载过程：\n\n1. 基本上Web容器(如Tomcat)先加载ContextLoaderListener，然后生成一个IoC容器。\n2. 然后再实例化DispatchServlet时候会加载对应的配置文件，再次生成Controller相关的IoC容器。\n\n关于上面两个容器关系：\n\nContextLoaderListener中创建ApplicationContext主要用于整个Web应用程序需要共享的一些组件，比如DAO，数据库的ConnectionFactory等。而由**DispatcherServlet**创建的ApplicationContext主要用于和该Servlet相关的一些组件，比如Controller、ViewResovler等。\n\n对于作用范围而言，在DispatcherServlet中可以引用由ContextLoaderListener所创建的ApplicationContext，而反过来不行。\n\n### 解决方法\n\n#### 方法1\n\nspringmvc.xml\n\n```xml\n<!--组件扫描-->\n<context:component-scan base-package=\"com.hph.ss.handler\"></context:component-scan>\n```\n\napplicationContext.xml\n\n```xml\n<context:component-scan base-package=\"com.hph.ss.dao,com.hph.ss.service\"></context:component-scan>\n```\n\n![Ak0MQ0.png](https://s2.ax1x.com/2019/03/13/Ak0MQ0.png)\n\n#### 方法2\n\nspringmvc.xml\n\n```\n   <!--组件扫描-->\n    <context:component-scan base-package=\"com.hph.ss\" use-default-filters=\"false\">\n        <context:include-filter type=\"annotation\"\n                                expression=\"org.springframework.stereotype.Controller\"></context:include-filter>\n    </context:component-scan>\n```\n\napplicationContext.xml\n\n```xml\n    <!-- 组件扫描 -->\n <context:component-scan base-package=\"com.hph.ss\">\n     <context:exclude-filter type=\"annotation\" expression=\"org.springframework.stereotype.Controller\"></context:exclude-filter>\n </context:component-scan>\n```\n\n![Akyk5D.png](https://s2.ax1x.com/2019/03/13/Akyk5D.png)\n\n##  关系\n\nservlet:代表的的容器为spring-mvc的子容器，而DispatcherServlet 是前端控制器，该容器专门为前端监听请求的时候所用，就是说当接收到url请求的时候会引用springmvc容器内的对象来处理。\n\ncontext-param:代表的容器是spring本身的容器，spring-mvc可以理解为一个继承自该容器的子容器，spring容器是最顶层的父类容器，跟java的继承原理一样，子容器能使用父类的对象，但是父容器不能使用子类的对象\n\n初始化的顺序也是父类容器优先级高，当服务器解析web.xml的时候由于listener监听的原因，会优先初始化spring容器，之后才初始化spring-mvc容器。\n\n 在 Spring MVC 配置文件中引用业务层的 Bean\n\n多个 Spring IOC 容器之间可以设置为父子关系，以实现良好的解耦。\n\nSpring MVC WEB 层容器可作为 “业务层” Spring 容器的子容器：即 WEB 层容器可以引用业务层容器的 Bean，而业务层容器却访问不到 WEB 层容器的 Bean\n\n![Ak2eFf.png](https://s2.ax1x.com/2019/03/13/Ak2eFf.png)\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["JavaWeb"]},{"title":"SpringMV工作流程分析","url":"/2019/03/12/SpringMV工作流程分析/","content":"\n {{ \"SpringMVC 运行流程 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 运行流程图解\n\n![AirNFJ.png](https://s2.ax1x.com/2019/03/12/AirNFJ.png)\n\n\n\n(1）用户向服务器发送请求，请求被SpringMVC 前端控制器 DispatcherServlet捕获；\n\n(2）DispatcherServlet对请求URL进行解析，得到请求资源标识符（URI）:判断请求URI对应的映射\n\n​\t①     不存在：\n\n​\t\t  再判断是否配置了mvc:default-servlet-handler：\n\n​\t\t  如果没配置，则控制台报映射查找不到，客户端展示404错误\n\n​\t\t  如果有配置，则执行目标资源（一般为静态资源，如：JS,CSS,HTML）\n\n​\t②     存在：\n\n 执行下面流程\n\n(3）  根据该URI，调用HandlerMapping获得该Handler配置的所有相关的对象（包括Handler对象以及Handler对象对应的拦截器），最后以HandlerExecutionChain对象的形式返回；\n\n(4）   DispatcherServlet 根据获得的Handler，选择一个合适的HandlerAdapter。\n\n(5）   如果成功获得HandlerAdapter后，此时将开始执行拦截器的preHandler(...)方法【正向】\n\n(6）  提取Request中的模型数据，填充Handler入参，开始执行Handler（Controller)方法，处理请求。在填充Handler的入参过程中，根据你的配置，Spring将帮你做一些额外的工作：\n\n①     HttpMessageConveter： 将请求消息（如Json、xml等数据）转换成一个对象，将对象转换为指定的响应信息\n\n②     数据转换：对请求消息进行数据转换。如String转换成Integer、Double等\n\n③     数据根式化：对请求消息进行数据格式化。 如将字符串转换成格式化数字或格式化日期等\n\n④     数据验证： 验证数据的有效性（长度、格式等），验证结果存储到BindingResult或Error中\n\n(7）  Handler执行完成后，向DispatcherServlet 返回一个ModelAndView对象；\n\n(8）  此时将开始执行拦截器的postHandle(...)方法【逆向】\n\n(9）    根据返回的ModelAndView（此时会判断是否存在异常：如果存在异常，则执行HandlerExceptionResolver进行异常处理）选择一个适合的ViewResolver（必须是已经注册到Spring容器中的ViewResolver)返回给DispatcherServlet，根据Model和View，来渲染视图\n\n(10）  在返回给客户端时需要执行拦截器的AfterCompletion方法【逆向】\n\n(11）  将渲染结果返回给客户端\n\n## 代码\n\n### web.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd\">\n\n    <!--组件扫描-->\n    <context:component-scan base-package=\"com.hph.springmvc\"></context:component-scan>\n    <!--视图解析器-->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n</beans>\n```\n\n### SpringmvcHandler\n\n```java\npackage com.hph.springmvc.handler;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.RequestMapping;\n\n@Controller\npublic class SpringmvcHandler {\n    @RequestMapping(\"/hello\")\n    public String hello() {\n        return \"success\";\n    }\n}\n\n```\n\n### index.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n  <head>\n    <title>流程分析</title>\n  </head>\n  <body>\n  <a href=\"hello\">Hello SpringMVC</a>\n  </body>\n</html>\n\n```\n\n### success.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>成功页面</title>\n</head>\n<body>\n<h1>Success Page</h1>\n</body>\n</html>\n```\n\n![AiyQbT.png](https://s2.ax1x.com/2019/03/12/AiyQbT.png)\n\nspringmvc.xml配置文件加上\n\n```xml\n    <!--处理静态资源-->\n    <mvc:default-servlet-handler/>\n    <mvc:annotation-driven/>\n```\n\n![AiyRMt.png](https://s2.ax1x.com/2019/03/12/AiyRMt.png)\n\n\n\n\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["JavaWeb"]},{"title":"SpringMVC处理Json、文件上传、拦截器","url":"/2019/03/11/SpringMVC进阶/","content":"\n {{ \"SpringMVC处理Json、文件上传、拦截器 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 处理JSON\n\n### 链接\n\nhttp://repo1.maven.org/maven2/com/fasterxml/jackson/core/\n\n### 步骤\n\n编写目标方法，使其返回 JSON 对应的对象或集合\n\n```java\n@ResponseBody  //SpringMVC对JSON的支持\n@RequestMapping(\"/testJSON\")\npublic Collection<Employee> testJSON(){\nreturn employeeDao.getAll();\n}\n```\n\n### idex.jsp\n\n```html\n<%@ page language=\"java\" contentType=\"text/html; charset=UTF-8\"\n    pageEncoding=\"UTF-8\"%>\n<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"\n \"http://www.w3.org/TR/html4/loose.dtd\">\n<html>\n<head>\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n<title>Insert title here</title>\n<script type=\"text/javascript\" src=\"scripts/jquery-1.9.1.min.js\"></script>\n \n</head>\n<body>\n<a id=\"testJSON\" href=\"testJSON\">testJSON</a>\n</body>\n</html>\n</body>\n</html>\n```\n\n### 原理\n\nHttpMessageConverter&lt;T&gt;:\nHttpMessageConverter&lt;T&gt;是 Spring3.0 新添加的一个接口，负责将请求信息转换为一个对象（类型为T），将对象（类型为T）输出为响应信息\n\nHttpMessageConverter&lt;T&gt;接口定义的方法：\n\n Boolean canRead(Class&lt;?&gt; clazz,MediaType mediaType): 指定转换器可以读取的对象类型，即转换器是否可将请求信息转换为 clazz 类型的对象，同时指定支持 MIME 类型(text/html,applaiction/json等)\n\nBoolean canWrite(Class<?> clazz,MediaType mediaType):指定转换器是否可将 clazz 类型的对象写到响应流中，响应流支持的媒体类型在MediaType 中定义。\n\n List&lt;MediaType&gt;getSupportMediaTypes()：该转换器支持的媒体类型。\n\nT read(Class&lt;? extends &lt;T&gt; clazz,**HttpInputMessage** inputMessage)：将请求信息流转换为 T 类型的对象。\n\nvoid write(T t,MediaType contnetType,**HttpOutputMessgae** outputMessage):将T类型的对象写到响应流中，同时指定相应的媒体类型为 contentType。\n\n```java\npackage org.springframework.http;\n \nimport java.io.IOException;\nimport java.io.InputStream;\n \npublic interface HttpInputMessage extends HttpMessage {\n \nInputStream getBody() throws IOException;\n \n}\n\n\npackage org.springframework.http;\n \nimport java.io.IOException;\nimport java.io.OutputStream;\n \npublic interface HttpOutputMessage extends HttpMessage {\n \nOutputStream getBody() throws IOException;\n \n}\n\n```\n\n\n\n![ACcLDK.png](https://s2.ax1x.com/2019/03/11/ACcLDK.png)\n\n![ACg1bT.png](https://s2.ax1x.com/2019/03/11/ACg1bT.png)\n\nDispatcherServlet 默认装配 RequestMappingHandlerAdapter ，而 RequestMappingHandlerAdapter 默认装配如下 HttpMessageConverter：\n\n![ACgJ54.png](https://s2.ax1x.com/2019/03/11/ACgJ54.png)\n\n 加入 jackson jar 包后， RequestMappingHandlerAdapter  装配的 HttpMessageConverter  如下：\n\n![ACgNG9.png](https://s2.ax1x.com/2019/03/11/ACgNG9.png)\n\n默认情况下数组长度是6个；增加了jackson的包，后多个一个MappingJackson2HttpMessageConverte\n\n## 文件上传\n\nSpring MVC 为文件上传提供了直接的支持，这种支持是通过即插即用的 **MultipartResolver** 实现的。 \n\nSpring 用 **Jakarta Commons FileUpload** 技术实现了一个 MultipartResolver 实现类：**CommonsMultipartResolver**   \n\nSpring MVC上下文中默认没有装配MultipartResovler，因此默认情况下不能处理文件的上传工作，如果想使用 Spring 的文件上传功能，需现在上下文中配置 MultipartResolver\n\n配置MultipartResolver defaultEncoding: 必须和用户JSP 的 pageEncoding 属性一致，以便正确解析表单的内容,为了让 **CommonsMultipartResolver** 正确工作，必须先将 Jakarta Commons FileUpload 及 Jakarta Commons io 的类包添加到类路径下。\n\n### 案例\n\n#### SpringFileHandler\n\n```java\n  /**\n     * 文件的上传\n     * 上传的原理:将本地文件上传到服务器端\n     */\n    @RequestMapping(\"/upload\")\n    public void testUploadFile(@RequestParam(\"desc\") String desc, @RequestParam(\"uploadFile\") MultipartFile uploadFile, HttpSession session) throws IOException {\n        //获取到上传文件的名字\n        String uploadFileName = uploadFile.getOriginalFilename();\n        //获取输入流\n        InputStream in = uploadFile.getInputStream();\n        //获取服务器端的uploads的真实路径\n        ServletContext sc = session.getServletContext();\n        String realPath = sc.getRealPath(\"uploads\");\n        System.out.println(realPath);\n        File targetFile = new File(realPath + \"/\" + uploadFileName);\n        FileOutputStream os = new FileOutputStream(targetFile);\n        //写文件\n        int i;\n        while ((i = in.read()) != -1) {\n            os.write(i);\n        }\n        in.close();\n        os.close();\n        System.out.println(uploadFileName + \"上传成功!\");\n    }\n```\n\n#### springmvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd\n\t\thttp://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n\t\thttp://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\">\n\n    <!-- 组件扫描 -->\n    <context:component-scan base-package=\"com.hph.springmvc\"></context:component-scan>\n\n    <!-- 视图解析器 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    <mvc:default-servlet-handler/>\n    <mvc:annotation-driven/>\n    <!--配置文件上传 必须配置ie-->\n    <bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\">\n        <!-- 保证与上传表单所在的Jsp页面的编码一致. -->\n        <property name=\"defaultEncoding\" value=\"utf-8\"></property>\n        <property name=\"maxUploadSize\" value=\"10485760\"></property>\n    </bean>\n</beans>\n\n```\n\n#### web.xml配置\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://java.sun.com/xml/ns/javaee\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" id=\"WebApp_ID\" version=\"2.5\">\n  \n  <!-- 字符编码过滤器 -->\n  <filter>\n  \t<filter-name>CharacterEncodingFilter</filter-name>\n  \t<filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n  \t<init-param>\n  \t\t<param-name>encoding</param-name>\n  \t\t<param-value>UTF-8</param-value>\n  \t</init-param>\n  </filter>\n  <filter-mapping>\n  \t<filter-name>CharacterEncodingFilter</filter-name>\n  \t<url-pattern>/*</url-pattern>\n  </filter-mapping>\n  \n  <!-- REST 过滤器 -->\n  <filter>\n  \t<filter-name>HiddenHttpMethodFilter</filter-name>\n  \t<filter-class>org.springframework.web.filter.HiddenHttpMethodFilter</filter-class>\n  </filter>\n  <filter-mapping>\n  \t<filter-name>HiddenHttpMethodFilter</filter-name>\n  \t<url-pattern>/*</url-pattern>\n  </filter-mapping>\n  \n <!-- 前端控制器 -->\n\t<servlet>\n\t\t<servlet-name>springDispatcherServlet</servlet-name>\n\t\t<servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n\t\t<init-param>\n\t\t\t<param-name>contextConfigLocation</param-name>\n\t\t\t<param-value>classpath:springmvc.xml</param-value>\n\t\t</init-param>\n\t\t<load-on-startup>1</load-on-startup>\n\t</servlet>\n\n\t<servlet-mapping>\n\t\t<servlet-name>springDispatcherServlet</servlet-name>\n\t\t<url-pattern>/</url-pattern>\n\t</servlet-mapping>\n</web-app>\n```\n\n#### index.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>FileTest</title>\n</head>\n<body>\n\n<form action=\"upload\" method=\"post\" enctype=\"multipart/form-data\">\n    上传文件:<input type=\"file\" name=\"uploadFile\">\n    <br>\n    文件描述:<input type=\"text\" name=\"desc\">\n    <br>\n    <input type=\"submit\" value=\"上传\">\n</form>\n</body>\n</html>\n```\n\n### 测试\n\n![APYOX9.png](https://s2.ax1x.com/2019/03/12/APYOX9.png)\n\n![APYjmR.png](https://s2.ax1x.com/2019/03/12/APYjmR.png)\n\n![APYv01.png](https://s2.ax1x.com/2019/03/12/APYv01.png)\n\n![APYxTx.png](https://s2.ax1x.com/2019/03/12/APYxTx.png)![APtptK.png](https://s2.ax1x.com/2019/03/12/APtptK.png)\n\n成功上传到服务器端\n\n## 多文件上传\n\n### SpringFileHandler\n\n```java\n /**\n     * 上传多个文件\n     */\n    @RequestMapping(value = \"/manyFileUpload\", method = RequestMethod.POST)\n    public String manyFileUpload(@RequestParam(\"files\") MultipartFile[] file, HttpSession session) throws IOException {\n        //获取服务器端的uploads的真实路径\n        ServletContext sc = session.getServletContext();\n        String realPath = sc.getRealPath(\"uploads\");\n        for (MultipartFile multipartFile : file) {\n            if (!multipartFile.isEmpty()) {\n                multipartFile.transferTo(new File(realPath + \"/\" + multipartFile.getOriginalFilename()));\n            }\n        }\n        return \"success\";\n    }\n\n```\n\n### index.jps\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>FileTest</title>\n</head>\n<body>\n\n<form action=\"upload\" method=\"post\" enctype=\"multipart/form-data\">\n    上传文件:<input type=\"file\" name=\"uploadFile\">\n    <br>\n    文件描述:<input type=\"text\" name=\"desc\">\n    <br>\n    <input type=\"submit\" value=\"上传\">\n</form>\n<br>\n<h2>上传多个文件 实例</h2>\n<form action=\"manyFileUpload\" method=\"post\"  enctype=\"multipart/form-data\">\n    <p>选择文件:<input type=\"file\" name=\"files\"></p>\n    <p>选择文件:<input type=\"file\" name=\"files\"></p>\n    <p><input type=\"submit\" value=\"提交\"></p>\n</form>\n</body>\n</html>\n```\n\n### 测试\n\n![APfIM9.png](https://s2.ax1x.com/2019/03/12/APfIM9.png)\n\n![APhAJS.png](https://s2.ax1x.com/2019/03/12/APhAJS.png)\n\n![APhERg.png](https://s2.ax1x.com/2019/03/12/APhERg.png)\n\n## 文件下载\n\n### 数据准备\n\n![APt9fO.png](https://s2.ax1x.com/2019/03/12/APt9fO.png)\n\n###  SpringFileHandler\n\n```java\n /**\n     * 使用HttpMessageConveter完成下载功能\n     * 支持 @RequestBody @ResponsBody Httpentity ResponseEntity\n     * <p>\n     * 下载的原理:   将服务器端的文件已流的形式写到客户端\n     * ResponseEntity:将要在的文件数据,以及一些响应信息封装到ResponseEntity对象中,浏览器通过解析发送回去的解析发送回去的响应数据就可以进行下载操作\n     */\n    @RequestMapping(\"/download\")\n    public ResponseEntity<byte[]> testDownload(HttpSession session) throws Exception {\n        //将要下载的文件读取成一个z字节数组\n        byte[] imgs;\n\n        ServletContext sc = session.getServletContext();\n        InputStream in = sc.getResourceAsStream(\"image/test.png\");\n        imgs = new byte[in.available()];\n        in.read(imgs);\n        //响应数据,以及响应头信息封装到ResponseEntity中\n        /**\n         * 参数:\n         *  1.要发送给浏览器的数据\n         *  2.设置响应头\n         *  3.设置响应码\n         */\n        HttpHeaders headers = new HttpHeaders();\n        headers.add(\"Content-Disposition\", \"attachment;filename=test.png\");\n        HttpStatus statusCode = HttpStatus.OK;  //200\n        ResponseEntity<byte[]> re = new ResponseEntity<byte[]>(imgs, headers, statusCode);\n\n        return re;\n\n    }\n\n```\n\n## 拦截器\n\nSpring MVC也可以使用拦截器对请求进行拦截处理，用户可以自定义拦截器来实现特定的功能，自定义的拦截器必须实现HandlerInterceptor接口\n\n**preHandle**()：这个方法在业务处理器处理请求之前被调用，在该方法中对用户请求 request 进行处理。如果程序员决定该拦截器对请求进行拦截处理后还要调用其他的拦截器，或者是业务处理器去进行处理，则返回true；如果程序员决定不需要再调用其他的组件去处理请求，则返回fals。\n\n**postHandle**()：这个方法在业务处理器处理完请求后，但是DispatcherServlet 向客户端返回响应前被调用，在该方法中对用户请求request进行处理。\n\n**afterCompletion**()：这个方法**在** DispatcherServlet 完全处理完请求后被调用，可以在该方法中进行一些资源清理的操作。\n\n```java\npackage com.hph.springmvc.interceptor;\n\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n/**\n * 自定义拦截器\n */\n@Component\npublic class MyFirstInterceptor implements HandlerInterceptor {\n    @Override\n    public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception {\n        System.out.println(\"MyFirstInterceptor preHandler\");\n        return true;\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {\n        System.out.println(\"MyFirstInterceptor  postHandle\");\n\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {\n        System.out.println(\"MyFirstInterceptor afterCompletion\");\n    }\n}\n\n```\n\n### 案例\n\n####  springmvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd\n\t\thttp://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n\t\thttp://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\">\n\n    <!-- 组件扫描 -->\n    <context:component-scan base-package=\"com.hph.springmvc\"></context:component-scan>\n\n    <!-- 视图解析器 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n    <mvc:default-servlet-handler/>\n    <mvc:annotation-driven/>\n    <!--配置文件上传 必须配置ie-->\n    <bean id=\"multipartResolver\" class=\"org.springframework.web.multipart.commons.CommonsMultipartResolver\">\n        <!-- 保证与上传表单所在的Jsp页面的编码一致. -->\n        <property name=\"defaultEncoding\" value=\"utf-8\"></property>\n        <property name=\"maxUploadSize\" value=\"10485760\"></property>\n    </bean>\n    <!--配置拦截器-->\n    <mvc:interceptors>\n        <!--1.拦截所有的请求-->\n        <bean class=\"com.hph.springmvc.interceptor.MyFirstInterceptor\"></bean>\n    </mvc:interceptors>\n</beans>\n\n```\n\n#### MyFirstInterceptor\n\n```java\npackage com.hph.springmvc.interceptor;\n\nimport org.springframework.stereotype.Component;\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\n/**\n * 自定义拦截器\n */\n@Component\npublic class MyFirstInterceptor implements HandlerInterceptor {\n    @Override\n    public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception {\n        System.out.println(\"MyFirstInterceptor preHandler\");\n        return true;\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {\n        System.out.println(\"MyFirstInterceptor  postHandle\");\n\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {\n        System.out.println(\"MyFirstInterceptor afterCompletion\");\n    }\n}\n\n```\n\n### 测试\n\n![AP7nL4.png](https://s2.ax1x.com/2019/03/12/AP7nL4.png)\n\n![APIvkt.png](https://s2.ax1x.com/2019/03/12/APIvkt.png)\n\n### 多个拦截器配置\n\n#### MyFirstInterceptor\n\n```java\npackage com.hph.springmvc.interceptor;\n\n        import org.springframework.stereotype.Component;\n        import org.springframework.web.servlet.HandlerInterceptor;\n        import org.springframework.web.servlet.ModelAndView;\n\n        import javax.servlet.http.HttpServletRequest;\n        import javax.servlet.http.HttpServletResponse;\n\n/**\n * 自定义拦截器\n */\n@Component\npublic class MyFirstInterceptor implements HandlerInterceptor {\n    @Override\n    public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception {\n        System.out.println(\"MyFirstInterceptor preHandler\");\n        return true;\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {\n        System.out.println(\"MyFirstInterceptor  postHandle\");\n\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {\n        System.out.println(\"MyFirstInterceptor afterCompletion\");\n    }\n}\n\n```\n\n\n\n#### MySecondInterceptor\n\n```java\npackage com.hph.springmvc.interceptor;\n\nimport org.springframework.web.servlet.HandlerInterceptor;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport javax.servlet.http.HttpServletRequest;\nimport javax.servlet.http.HttpServletResponse;\n\t\npublic class MySecondInterceptor implements HandlerInterceptor {\n    @Override\n    public boolean preHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o) throws Exception {\n        System.out.println(\"[MySecondInterceptor preHandler]\");\n\n        return false;\n    }\n\n    @Override\n    public void postHandle(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, ModelAndView modelAndView) throws Exception {\n        System.out.println(\"[MySecondInterceptor  postHandle]\");\n\n    }\n\n    @Override\n    public void afterCompletion(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, Object o, Exception e) throws Exception {\n        System.out.println(\"[MySecondInterceptor afterCompletion]\");\n\n    }\n}\n\n```\n\n\n\n![AiGC24.png](https://s2.ax1x.com/2019/03/12/AiGC24.png)\n\n![AicPAS.png](https://s2.ax1x.com/2019/03/12/AicPAS.png)\n\n![AiGAq1.png](https://s2.ax1x.com/2019/03/12/AiGAq1.png)\n\n![Ai65Ox.png](https://s2.ax1x.com/2019/03/12/Ai65Ox.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["JavaWeb"]},{"title":"SpringMVC处理请求或响应数据","url":"/2019/03/07/Spring处理请求或响应数据/","content":"\n {{ \"SpringMVC处理请求数据或响应数据\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n## 请求处理方法签名\n\n1. Spring MVC 通过分析处理方法的签名，HTTP请求信息绑定到处理方法的相应人参中。\n2. Spring MVC 对控制器处理方法签名的限制是很宽松的，几乎可以按喜欢的任何方式对方法进行签名。 \n3. 必要时可以对方法及方法入参标注相应的注解（ @PathVariable 、@RequestParam、@RequestHeader 等）、\n4. Spring MVC 框架会将 HTTP 请求的信息绑定到相应的方法入参中，并根据方法的返回值类型做出相应的后续处理。\n\n### @RequestParam注解\n\n1. 在处理方法入参处使用 @RequestParam 可以把请求参数传递给请求方法\n2. value：参数名\n3. required：是否必须。默认为 true, 表示请求参数中必须包含对应的参数，若不存在，将抛出异常\n4. defaultValue: 默认值，当没有传递参数时使用该值\n\n```java\n/**\n * @RequestParam 注解用于映射请求参数\n *         value 用于映射请求参数名称\n *         required 用于设置请求参数是否必须的\n *         defaultValue 设置默认值，当没有传递参数时使用该值\n */\n   \n\t@RequestMapping(\"/testRequestParm\")\n    public String testRequestParm(@RequestParam(\"username\") String username, @RequestParam(value = \"age\", required = false, defaultValue = \"0\") int age) {\n        System.out.println(username + \",\" + age);\n        return \"success\";\n    }\n```\n\n```html\n<a href=\"springmvc/testRequestParm?username=Tom&age=22\">testRequestParm</a>\n```\n\n### @RequestHeader 注解\n\n1. 使用 @RequestHeader 绑定请求报头的属性值\n2. 请求头包含了若干个属性，服务器可据此获知客户端的信息，**通过** **@RequestHeader** **即可将请求头中的属性值绑定到处理方法的入参中** \n\n```java\n   @RequestMapping(\"/testRequestHeader\")\n    public String testRequestHeader(@RequestHeader(\"Accept-Language\") String accpetLanguage) {\n\n        System.out.println(\"accept-Language:\" + accpetLanguage);\n        return \"success\";\n    }\n```\n\n```html\n<a href=\"springmvc/testRequestHeader\">Test RequestHeader</a>\n```\n\n###  @CookieValue 注解\n\n1. 使用 @CookieValue 绑定请求中的 Cookie 值\n\n2. **@CookieValue** 可让处理方法入参绑定某个 Cookie 值\n\n```java\n    @RequestMapping(\"/testCookieValue\")\n    public String testCookieValue(@CookieValue(\"JSESSIONID\") String sessionId) {\n        System.out.println(\"sessionid:\" + sessionId);\n        return \"success\";\n    }\n```\n\n```html\n<a href=\"springmvc/testCookieValue\">Test CookieValue</a>\n```\n\n### 使用POJO作为参数\n\n1. 使用 POJO 对象绑定请求参数值\n2. Spring MVC **会按请求参数名和 POJO** **属性名进行自动匹配，自动为该对象填充属性值**。**支持级联属性**。如：dept.deptId、dept.address.tel 等\n\n```java\npackage com.hph.springmvc.beans;\n\npublic class Address {\n    private String province;\n    private String city;\n\n    @Override\n    public String toString() {\n        return \"Address{\" +\n                \"province='\" + province + '\\'' +\n                \", city='\" + city + '\\'' +\n                '}';\n    }\n\n    public String getProvince() {\n        return province;\n    }\n\n    public void setProvince(String province) {\n        this.province = province;\n    }\n\n    public String getCity() {\n        return city;\n    }\n\n    public void setCity(String city) {\n        this.city = city;\n    }\n}\n\n```\n\n```java\npackage com.hph.springmvc.beans;\n\npublic class User {\n    private String username;\n    private String password;\n    private String email;\n    private Integer gender;\n    private  Address address;\n\n    @Override\n    public String toString() {\n        return \"User{\" +\n                \"username='\" + username + '\\'' +\n                \", password='\" + password + '\\'' +\n                \", email='\" + email + '\\'' +\n                \", gender=\" + gender +\n                \", address=\" + address +\n                '}';\n    }\n\n    public String getUsername() {\n        return username;\n    }\n\n    public void setUsername(String username) {\n        this.username = username;\n    }\n\n    public String getPassword() {\n        return password;\n    }\n\n    public void setPassword(String password) {\n        this.password = password;\n    }\n\n    public String getEmail() {\n        return email;\n    }\n\n    public void setEmail(String email) {\n        this.email = email;\n    }\n\n    public Integer getGender() {\n        return gender;\n    }\n\n    public void setGender(Integer gender) {\n        this.gender = gender;\n    }\n\n    public Address getAddress() {\n        return address;\n    }\n\n    public void setAddress(Address address) {\n        this.address = address;\n    }\n}\n```\n\n 如果中文有乱码，需要配置字符编码过滤器，且配置其他过滤器之前，如（HiddenHttpMethodFilter），否则不起作用。\n\n```xml\n    <!-- 配置编码方式过滤器,注意一点:要配置在所有过滤器的前面 -->\n    <filter>\n        <filter-name>CharacterEncodingFilter</filter-name>\n        <filter-class>org.springframework.web.filter.CharacterEncodingFilter</filter-class>\n        <init-param>\n            <param-name>encoding</param-name>\n            <param-value>utf-8</param-value>\n        </init-param>\n    </filter>\n    <filter-mapping>\n```\n\n### 使用Servlet原生API作为参数\n\n- HttpServletRequest\n\n- HttpServletResponse\n- HttpSession\n- **java.security.Principal**\n- **Locale**\n- InputStream\n- OutputStream\n- Reader\n- Writer\n\n```java\n    @RequestMapping(\"/testServletAPI\")\n    public void testServletAPI(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { //需要导入servlet-api.jar\n        System.out.println(\"request :\" + request);\n        System.out.println(\"response : \" + response);\n        //转发\n        //   request.getRequestDispatcher(\"/WEB-INF/views/success.jsp\").forward(request, response);\n        //重定向 将数据写给客户端\n      //  response.sendRedirect(\"http://www.baidu.com\");\n\n        response.getWriter().println(\"Hello World\");\n    }\n```\n\n```html\n<a href=\"springmvc/testServletAPI\">Test Servlet API</a>\n```\n\n## 处理响应数据\n\n### SpringMVC 输出模型数据概述\n\n **ModelAndView**: 处理方法返回值类型为 ModelAndView 时, 方法体即可通过该对象添加模型数据\n\n **Map** **及 Model**: 入参为 org.springframework.ui.Model、org.springframework.ui.ModelMap 或 java.uti.Map 时，处理方法返回时，Map 中的数据会自动添加到模型中。\n\n#### ModelAndView\n\n控制器处理方法的返回值如果为 ModelAndView, 则其既包含视图信息，也包含模型数据信息。\n\n**添加模型数据:**\n\nMoelAndView addObject(String attributeName, Object attributeValue)\n\nModelAndView addAllObject(Map<String, ?> modelMap)\n\n**设置视图:**\n\nvoid setView(View view)\n\nvoid setViewName(String viewName)\n\n#### 处理模型数据之 Map\n\nSpring MVC 在内部使用了一个 org.springframework.ui.Model 接口存储模型数据\n\n具体使用步骤\n\n**Spring MVC** **在调用方法前会创建一个隐含的模型对象作为模型数据的存储容器**。\n\n**如果方法的入参为 Map** **或 Model** **类型**，Spring MVC 会将隐含模型的引用传递给这些入参。\n\n在方法体内，开发者可以通过这个入参对象访问到模型中的所有数据，也可以向模型中添加**新的属性数据**\n\n![ASMOAA.png](https://s2.ax1x.com/2019/03/09/ASMOAA.png)\n\n## 视图解析器\n\n不论控制器返回一个String,ModelAndView,View都会转换为ModelAndView对象，由视图解析器解析视图，然后，进行页面的跳转。 \n\n![ASQSc8.png](https://s2.ax1x.com/2019/03/09/ASQSc8.png)\n\n### 视图和视图解析器\n\n请求处理方法执行完成后，最终返回一个 ModelAndView 对象。对于那些返回 String，View 或 ModeMap 等类型的处理方法，**Spring MVC** **也会在内部将它们装配成一个 ModelAndView** **对象**，它包含了逻辑名和模型对象的视图。\n\nSpring MVC 借助**视图解析器**（**ViewResolver**）得到最终的视图对象（View），最终的视图可以是 JSP ，也可能是 Excel、JFreeChart等各种表现形式的视图。\n\n对于最终究竟采取何种视图对象对模型数据进行渲染，处理器并不关心，处理器工作重点聚焦在生产模型数据的工作上，从而实现 MVC 的充分解耦。\n\n### 视图\n\n**视图**的作用是渲染模型数据，将模型里的数据以某种形式呈现给客户,主要就是完成转发或者是重定向的操作.\n\n为了实现视图模型和具体实现技术的解耦，Spring 在 org.springframework.web.servlet 包中定义了一个高度抽象的 **View** 接口：\n\n![ASQpjS.png](https://s2.ax1x.com/2019/03/09/ASQpjS.png)\n\n**视图对象由视图解析器负责实例化**。由于视图是**无状态**的，所以他们不会有**线程安全**的问题\n\n![ASQCng.png](https://s2.ax1x.com/2019/03/09/ASQCng.png)\n\n### JstlView\n\n若项目中使用了JSTL，则SpringMVC 会自动把视图由InternalResourceView转为 **JstlView** \n\n1）    若希望直接响应通过 SpringMVC 渲染的页面，可以使用 **mvc:view-controller** 标签实现,不经过Handler直接跳转页面但是会导致RequestMapping映射失效,因此需要加上 annotation-driven的配置\n\n```xml\n<mvc:view-controller path=\"testViewContorller\" view-name=\"success\"></mvc:view-controller>\n```\n\n## 视图解析器\n\nSpringMVC 为逻辑视图名的解析提供了不同的策略，可以在 Spring WEB 上下文中**配置一种或多种解析策略**，**并指定他们之间的先后顺序**。每一种映射策略对应一个具体的视图解析器实现类。\n\n视图解析器的作用比较单一：将逻辑视图解析为一个具体的视图对象。\n\n所有的视图解析器都必须实现 ViewResolver 接口：\n\n```java\npublic interface ViewResolver {\n\n\t/**\n\t * Resolve the given view by name.\n\t * <p>Note: To allow for ViewResolver chaining, a ViewResolver should\n\t * return {@code null} if a view with the given name is not defined in it.\n\t * However, this is not required: Some ViewResolvers will always attempt\n\t * to build View objects with the given name, unable to return {@code null}\n\t * (rather throwing an exception when View creation failed).\n\t * @param viewName name of the view to resolve\n\t * @param locale Locale in which to resolve the view.\n\t * ViewResolvers that support internationalization should respect this.\n\t * @return the View object, or {@code null} if not found\n\t * (optional, to allow for ViewResolver chaining)\n\t * @throws Exception if the view cannot be resolved\n\t * (typically in case of problems creating an actual View object)\n\t */\n\tView resolveViewName(String viewName, Locale locale) throws Exception;\n\n}\n```\n\n### 常用的视图解析器实现类\n\n![ASQkAs.png](https://s2.ax1x.com/2019/03/09/ASQkAs.png)\n\n### mvc:view-controller标签\n\n```xml\n    <mvc:view-controller path=\"testViewContorller\" view-name=\"success\"></mvc:view-controller>\n\t<mvc:annotation-driven></mvc:annotation-driven>\n```\n\n### 重定向\n\n①   一般情况下，控制器方法返回字符串类型的值会被当成逻辑视图名处理\n\n②   如果返回的字符串中带 **forward:** **或** **redirect:** 前缀时，SpringMVC 会对他们进行特殊处理：将 forward: 和 redirect: 当成指示符，其后的字符串作为 URL 来处理\n\n③   redirect:success.jsp：会完成一个到 success.jsp 的重定向的操作\n\n④   forward:success.jsp：会完成一个到 success.jsp 的转发操作\n\n## 代码实例\n\n### 实例结构\n\n![ASQQHJ.png](https://s2.ax1x.com/2019/03/09/ASQQHJ.png)\n\n### SpringMVCHandler\n\n```java\npackage com.hph.springmvc.handler;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.ui.Model;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.servlet.ModelAndView;\n\nimport java.util.Date;\nimport java.util.Map;\n\n@Controller\npublic class SpringMVCHandler {\n    /**\n     * 重定向\n     */\n    @RequestMapping(\"/testRedirectView\")\n    public String testRedirctView() {\n\n        return \"redirect:/ok.jsp\";\n    }\n\n    /**\n     * 视图 View\n     */\n    @RequestMapping(\"/testView\")\n    public String testView() {\n        return \"success\";\n    }\n\n    /**\n     * Model\n     */\n    @RequestMapping(\"/testModel\")\n    public String testModel(Model model) {\n        //模型数据:loginMsg =用户名或者密码错误\n        model.addAttribute(\"loginMsg\", \"用户名或者密码错误\");\n        return \"success\";\n    }\n\n    /**\n     * Map\n     * SpringMVC会把Map中的模型数据存放到request与对象中\n     * SpringMVC会在调完请求处理方法后不管方法的返回是什么类型都会处理成一个ModeAndView(参考DispathcherServlet)\n     */\n    @RequestMapping(\"/testMap\")\n    public String testMap(Map<String, Object> map) {\n        //模型数据 password=123456\n        map.put(\"password\", \"123456\");\n        return \"success\";\n    }\n\n\n    @RequestMapping(\"/testModelAndView\")\n    public ModelAndView testModelAndView() {\n        System.out.println(\"testModelAndView\");\n        String viewName = \"success\";\n        ModelAndView mv = new ModelAndView(viewName);\n        mv.addObject(\"time\", new Date().toString()); //实质上存放到request域中\n        return mv;\n    }\n}\n\n```\n\n### springmvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:mvc=\"http://www.springframework.org/schema/mvc\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc-4.0.xsd\n\t\thttp://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n\t\thttp://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\">\n\n    <!-- 组件扫描 -->\n    <context:component-scan base-package=\"com.hph.springmvc.handler\"></context:component-scan>\n\n    <!-- 视图解析器 -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n\n        <!--配置视图解析器的优先级-->\n        <property name=\"order\" value=\"100\"></property>\n    </bean>\n    <!--不经过Handler直接跳转页面 但是会导致RequestMapping映射失效,因此需要加上 annotation-driven的配置 -->\n    <!---->\n    <mvc:view-controller path=\"testViewContorller\" view-name=\"success\"></mvc:view-controller>\n    <mvc:annotation-driven></mvc:annotation-driven>\n</beans>\n```\n\n### web.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\"\n         version=\"4.0\">\n    <servlet>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            <param-value>classpath:springmvc.xml</param-value>\n        </init-param>\n        <!--\n        load-on-startup:设置DispathcerServlet\n            Servlet的创建时机:\n                   请求到达以后创建:\n                   服务器启动即创建:\n        -->\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <!--任何请求都会进去 对于JSP请求 不会处理-->\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n</web-app>\n```\n\n### index.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>$Title$</title>\n</head>\n<body>\n<a href=\"testRedirectView\">Test RedirectView</a>\n<br>\n<a href=\"testViewContorller\">Test ViewController</a>\n<br>\n<a href=\"testView\">Test View</a>\n<br>\n<a href=\"testModel\">Test Model</a>\n<br>\n<!--测试 ModelAndView 作为处理返回结果 -->\n<a href=\"testModelAndView\">testModelAndView</a>\n<br>\n<a href=\"testMap\">testMap</a>\n</body>\n</html>\n\n```\n\n### ok.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>OK</title>\n</head>\n<body>\n<h1>OK Page</h1>\n</body>\n</html>\n```\n\n### success.jsp\n\n```html\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>Title</title>\n</head>\n<body>\n<h1>Success Page</h1>\ntime:${requestScope.time}\n<br/>\npassword: ${requestScope.password}\n<br/>\nloginMsg:${requestScope.loginMsg}\n</body>\n</html>\n```\n\n### 测试\n\n![ASQ841.png](https://s2.ax1x.com/2019/03/09/ASQ841.png)\n\n![ASQJ9x.png](https://s2.ax1x.com/2019/03/09/ASQJ9x.png)\n\n![ASQY36.png](https://s2.ax1x.com/2019/03/09/ASQY36.png)\n\n![ASQauD.png](https://s2.ax1x.com/2019/03/09/ASQauD.png)\n\n![ASQdDe.png](https://s2.ax1x.com/2019/03/09/ASQdDe.png)\n\n![ASQdDe.png](https://s2.ax1x.com/2019/03/09/ASQdDe.png)\n\n![ASQwHH.png](https://s2.ax1x.com/2019/03/09/ASQwHH.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["JavaWeb"]},{"title":"SpringMVC概述","url":"/2019/03/06/SpringMVC概述/","content":"\n {{ \"Spring的基本概述和REST简单介绍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Spring MVC简介\n\nSpring MVC属于SpringFrameWork的后续产品，已经融合在Spring Web Flow里面。轻量级的、基于MVC的Web层应用框架。偏前端而不是基于业务逻辑层。Spring框架的一个后续产品 Spring 框架提供了构建 Web 应用程序的全功能 MVC 模块。使用Spring可插入的 MVC 架构，从而在使用Spring进行WEB开发时，可以选择使用Spring的Spring MVC框架或集成其他MVC开发框架，如[Struts](https://baike.baidu.com/item/Struts/485073)1(现在一般不用)，[Struts 2](https://baike.baidu.com/item/Struts%202/2187934)(一般老项目使用)等。\n\nSpring MVC 通过一套 MVC 注解，让 POJO 成为处理请求的控制器，而无须实现任何接口。支持 REST 风格的 URL 请求。采用了松散耦合可插拔组件结构，比其他 MVC 框架更具扩展性和灵活性。\n\n![kqlqrq.png](https://s2.ax1x.com/2019/03/02/kqlqrq.png)\n\n## 功能\n\n- 天生与Spring框架集成，如：(IOC,AOP)\n- 支持Restful风格\n-  进行更简洁的Web层开发\n- 支持灵活的URL到页面控制器的映射\n-  非常容易与其他视图技术集成(JSP HTML)，如:Velocity、FreeMarke(模板技术:商品页面页面相同)等等\n- 因为模型数据不存放在特定的API里，而是放在一个Model里(Map数据结构实现，因此很容易被其他框架使用)\n- 非常灵活的数据验证、格式化和数据绑定机制、能使用任何对象进行数据绑定，不必实现特定框架的API.\n- 更加简单、强大的异常处理\n- 对静态资源的支持\n-  支持灵活的本地化、主题等解析\n\n## 使用  \n\n SpringMVC将Web层进行了职责解耦，基于请求-响应模型\n\n常用的组件\n\n| 组件                     | 功能                                                         |\n| ------------------------ | ------------------------------------------------------------ |\n| DispatcherServlet        | 前端控制器                                                   |\n| Controller               | 处理器/页面控制器，做的是MVC中的C的事情，但控制逻辑转移到前端控制器了，用于对请求进行处理 |\n| HandlerMapping           | 请求映射到处理器，找谁来处理，如果映射成功返回一个HandlerExecutionChain对象（包含一个Handler处理器(页面控制器)对象、多个HandlerInterceptor拦截器对象） |\n| View<br/>Resolver        | 视图解析器，找谁来处理返回的页面。把逻辑视图解析为具体的View,进行这种策略模式，很容易更换其他视图技术    如InternalResourceViewResolver将逻辑视图名映射为JSP视图 |\n| LocalResolver            | 本地化、国际化                                               |\n| MultipartResolver        | 文件上传解析器                                               |\n| HandlerExceptionResolver | 异常处理器                                                   |\n\n## 环境配置\n\n![kvdX6I.png](https://s2.ax1x.com/2019/03/06/kvdX6I.png)\n\n目录结构如下\n\n![kvwd4e.png](https://s2.ax1x.com/2019/03/06/kvwd4e.png)\n\n配置Tomcat\n\n![kvwB3d.png](https://s2.ax1x.com/2019/03/06/kvwB3d.png)\n\n这里出现了问题我们进行修复以下\n\n![kvwRUS.png](https://s2.ax1x.com/2019/03/06/kvwRUS.png)\n\n点击第二个选项\n\n![kvwv8J.png](https://s2.ax1x.com/2019/03/06/kvwv8J.png)\n\n基本完成修复\n\n运行Tomcat\n\n![kv0Cb6.png](https://s2.ax1x.com/2019/03/06/kv0Cb6.png)\n\n环境配置完成\n\n## HelloWorld\n\n![kvsapF.png](https://s2.ax1x.com/2019/03/06/kvsapF.png)\n\n### web.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\"\n         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n         xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\"\n         version=\"4.0\">\n    <servlet>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>\n        <init-param>\n            <param-name>contextConfigLocation</param-name>\n            <param-value>classpath:springmvc.xml</param-value>\n        </init-param>\n        <!--\n        load-on-startup:设置DispathcerServlet\n            Servlet的创建时机:\n                   请求到达以后创建:\n                   服务器启动即创建:\n        -->\n        <load-on-startup>1</load-on-startup>\n    </servlet>\n    <servlet-mapping>\n        <servlet-name>springDispatcherServlet</servlet-name>\n        <!--任何请求都会进去 对于JSP请求 不会处理-->\n        <url-pattern>/</url-pattern>\n    </servlet-mapping>\n</web-app>\n```\n\n### springmvc.xml\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\">\n    <!--组件扫描-->\n    <context:component-scan base-package=\"com.hph.springmvc\"></context:component-scan>\n\n    <!--视图解析器：\n           工作机制：prefix +请求方式的返回值 +suffix = 物理视图路径\n                   WEB-INF/views/success.jsp\n             WEB-INF:是服务器内部路径，不能从浏览器访问该l路径下的资源，但是可以内部转发进行访问\n\n    -->\n    <bean class=\"org.springframework.web.servlet.view.InternalResourceViewResolver\">\n        <property name=\"prefix\" value=\"/WEB-INF/views/\"></property>\n        <property name=\"suffix\" value=\".jsp\"></property>\n    </bean>\n</beans>\n```\n\n### index.jsp\n\n```jsp\n<%--\n  Created by IntelliJ IDEA.\n  User: Schindler\n  Date: 2019/3/6\n  Time: 20:28\n  To change this template use File | Settings | File Templates.\n--%>\n<%@ page contentType=\"text/html;charset=UTF-8\" language=\"java\" %>\n<html>\n<head>\n    <title>Title</title>\n</head>\n<body>\n<a href=\"springmvc/testPathVariable/admin/1001\">Test PathVariable</a>\n<br>\n<a href=\"springmvc/testRequestMappingParamsAndHeaders?username=Tom&age=22\">Test RequestMapping Parms Headers</a>\n<br/>\n<a href=\"springmvc/testRequestMapping\">Hello SpringMVC</a>\n<br/>\n<a href=\"springmvc/testMethord\">TestMethord</a>\n\n</body>\n</html>\n\n```\n\n### SpringMVCHandler\n\n```java\npackage com.hph.springmvc.helloWorld;\n\nimport org.springframework.stereotype.Controller;\nimport org.springframework.web.bind.annotation.PathVariable;\nimport org.springframework.web.bind.annotation.RequestMapping;\nimport org.springframework.web.bind.annotation.RequestMethod;\n\n@Controller\n@RequestMapping(\"/springmvc\")\npublic class SpringMVCHandler {\n    /**\n     * 带占位符的URL\n     * 浏览器： http://localhost/SpringMVC//testPathVariable/admin/1001\n     */\n    @RequestMapping(value = \"/testPathVariable/{name}/{id}\")\n    public String testPathVariable(@PathVariable(\"name\") String name, @PathVariable(\"id\") Integer id) {\n        System.out.println(name + \":\" + id);\n        return \"success\";\n\n    }\n\n    /**\n     * @RequestMapping 映射请求参数 以及 请求头信息\n     * params : name=Tom&age=22\n     * headers\n     */\n    @RequestMapping(value = \"/testRequestMappingParamsAndHeaders\", params = {\"username\", \"age=22\"}, headers = {\"Accept-Language\"})\n    public String testRequestMappingParamsAndHeaders() {\n        return \"success\";\n    }\n\n    @RequestMapping(value = \"/testMethord\", method = RequestMethod.POST)\n    public String testMethord() {\n        System.out.println(\"testMethord...\");\n        return \"success\";\n    }\n\n    @RequestMapping(\"/testRequestMapping\")\n    public String testRequestMapping() {\n        return \"success\";\n    }\n}\n\n\n```\n\n### 运行结果\n\n![kvyv8O.png](https://s2.ax1x.com/2019/03/06/kvyv8O.png)\n\n![kvLmbq.png](https://s2.ax1x.com/2019/03/07/kvLmbq.png)\n\n![kvLvJU.png](https://s2.ax1x.com/2019/03/07/kvLvJU.png)\n\n### 运行分析\n\n![kvyuAx.png](https://s2.ax1x.com/2019/03/06/kvyuAx.png)\n\n![kvyrvQ.png](https://s2.ax1x.com/2019/03/06/kvyrvQ.png)\n\n### 基本步骤\n\n①    客户端请求提交到**DispatcherServlet**\n\n②    由DispatcherServlet控制器查询一个或多个**HandlerMapping**，找到处理请求的**Controller**\n\n③    DispatcherServlet将请求提交到Controller（也称为Handler）\n\n④    Controller调用业务逻辑处理后，返回**ModelAndView**\n\n⑤    DispatcherServlet查询一个或多个**ViewResoler**视图解析器，找到ModelAndView指定的视图\n\n⑥    视图负责将结果显示到客户端\n\n## @RequestMapping\n\n- SpringMVC使用@RequestMapping注解为控制器指定可以处理哪些 URL 请求\n- 在控制器的**类定义及方法定义处**都可标注 @RequestMapping\n    -  **标记在类上**：提供初步的请求映射信息。相对于  WEB 应用的根目录\n    - **标记在方法上**：提供进一步的细分映射信息。相对于标记在类上的 URL。\n- 若类上未标注 @RequestMapping，则方法处标记的 URL 相对于 WEB 应用的根目录 \n-  作用：DispatcherServlet 截获请求后，就通过控制器上 @RequestMapping 提供的映射信息确定请求所对应的处理方法。 \n\n```java\nTarget({ElementType.METHOD, ElementType.TYPE})\t//Target来标注当前注解标注的位置   方法 类\n@Retention(RetentionPolicy.RUNTIME)\t\t\t\n@Documented\n@Mapping\npublic @interface RequestMapping {\n    String name() default \"\";\t\t\t\t  //默认可以省略value \n\n    @AliasFor(\"path\")\n    String[] value() default {};\n\n    @AliasFor(\"value\")\n    String[] path() default {};\n\n    RequestMethod[] method() default {};\t//映射请求方式  GET, HEAD, POST, PUT, PATCH, DELETE, OPTIONS, TRACE\n\n    String[] params() default {};\n\n    String[] headers() default {};\n\n    String[] consumes() default {};\n\n    String[] produces() default {};\n}\n\n```\n\n## REST\n\n### 资料链接\n\n理解本真的REST架构风格: <http://kb.cnblogs.com/page/186516/> \n\nREST: <http://www.infoq.com/cn/articles/rest-introduction>\n\n### web.xml\n\n```xml\n    <!--配置REST 过滤器 HiddenHttpMethod-->\n    <filter>\n        <filter-name>HiddenHttpMethodFilter</filter-name>\n        <filter-class>org.springframework.web.filter.HiddenHttpMethodFilter</filter-class>\n    </filter>\n    <filter-mapping>\n        <filter-name>HiddenHttpMethodFilter</filter-name>\n        <url-pattern>/*</url-pattern>\n    </filter-mapping>\n```\n\n### SpringMVCHandler\n\n```java\n @RequestMapping(value=\"/testRESTGet/{id}\",method=RequestMethod.GET)\n    public String testRESTGet(@PathVariable(value=\"id\") Integer id){\n        System.out.println(\"testRESTGet id=\"+id);\n        return \"success\";\n    }\n\n    @RequestMapping(value=\"/testRESTPost\",method=RequestMethod.POST)\n    public String testRESTPost(){\n        System.out.println(\"testRESTPost\");\n        return \"success\";\n    }\n\n    @RequestMapping(value=\"/testRESTPut/{id}\",method=RequestMethod.PUT)\n    public String testRESTPut(@PathVariable(\"id\") Integer id){\n        System.out.println(\"testRESTPut id=\"+id);\n        return \"success\";\n    }\n\n    @RequestMapping(value=\"/testRESTDelete/{id}\",method=RequestMethod.DELETE)\n    public String testRESTDelete(@PathVariable(\"id\") Integer id){\n        System.out.println(\"testRESTDelete id=\"+id);\n        return \"success\";\n    }\n```\n\n\n\n![kx9Kaj.png](https://s2.ax1x.com/2019/03/07/kx9Kaj.png)\n\n\n\n\n\n\n\n","tags":["SpringMVC"],"categories":["JavaWeb"]},{"title":"Spring声明式事务","url":"/2019/03/05/Spring事务概述/","content":"\n {{ \"Spring的基本概述和开发\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n## 事务概述\n\n在JavaEE企业级开发的应用领域，为了保证数据的**完整性**和**一致性**，必须引入数据库事务的概念，所以事务管理是企业级应用程序开发中必不可少的技术。\n\n事务就是一组由于逻辑上紧密关联而合并成一个整体(工作单元)的多个数据库操作，这些操作**要么都执行**，**要么都不执行**。 \n\n### ACID:\n\n- 原子性(atomicity)：“原子”的本意是**“不可再分”**，事务的原子性表现为一个事务中涉及到的多个操作在逻辑上缺一不可。事务的原子性要求事务中的所有操作要么都执行，要么都不执行。 \n- 一致性(consistency)：“一致”指的是数据的一致，具体是指：所有数据都处于**满足业务规则的一致性状态 **。一致性原则要求：一个事务中不管涉及到多少个操作，都必须保证事务执行之前数据是正确的，事务执行之后数据仍然是正确的。如果一个事务在执行的过程中，其中某一个或某几个操作失败了，则必须将其他所有操作撤销，将数据恢复到事务执行之前的状态，这就是**回滚**。\n- 隔离性(isolation)：在应用程序实际运行过程中，事务往往是并发执行的，所以很有可能有许多事务同时处理相同的数据，因此每个事务都应该与其他事务隔离开来，防止数据损坏。隔离性原则要求多个事务在**并发执行过程中不会互相干扰。**\n- 持久性(durability)：持久性原则要求事务执行完成后，对数据的修改**永久的保存**下来，不会因各种系统错误或其他意外情况而受到影响。通常情况下，事务对数据的修改应该被写入到持久化存储器中。\n\n## 编程式事务管理\n\n使用原生的JDBC API进行事务管理:\n\n ①获取数据库连接Connection对象\n\n②取消事务的自动提交\n\n③执行操作\n\n④正常完成操作时手动提交事务\n\n⑤执行失败时回滚事务\n\n⑥关闭相关资源\n\n## 声明式事务\n\n大多数情况下声明式事务比编程式事务管理更好：它将事务管理代码从业务方法中分离出来，以声明的方式来实现事务管理。\n\n事务管理代码的固定模式作为一种横切关注点，可以通过AOP方法模块化，进而借助Spring AOP框架实现声明式事务管理。\n\nSpring在不同的事务管理API之上定义了一个**抽象层**，通过**配置**的方式使其生效，从而让应用程序开发人员**不必了解事务管理API的底层实现细节，就可以使用Spring的事务管理机制。\n\nSpring既支持编程式事务管理，也支持声明式的事务管理。\n\n## Spring提供的事务管理器\n\nSpring从不同的事务管理API中抽象出了一整套事务管理机制，让事务管理代码从特定的事务技术中独立出来。开发人员通过配置的方式进行事务管理，而不必了解其底层是如何实现的。\n\nSpring的核心事务管理抽象是PlatformTransactionManager。它为事务管理封装了一组独立于技术的方法。无论使用Spring的哪种事务管理策略(编程式或声明式)，事务管理器都是必须的。\n\n事务管理器可以以普通的bean的形式声明在Spring IOC容器中。\n\n## 事务管理器的主要实现\n\nDataSourceTransactionManager：在应用程序中只需要处理一个数据源，而且通过JDBC存取。\n\nJtaTransactionManager：在JavaEE应用服务器上用JTA(Java Transaction API)进行事务管理\n\nHibernateTransactionManager：用Hibernate框架存取数据库\n\n## 前置准备\n\n![kvpSII.png](https://s2.ax1x.com/2019/03/06/kvpSII.png)\n\n### Dao准备\n\n```java\npackage com.hph.spring.thing.annotation.dao;\n\npublic interface BookShopDao {\n    //根据书号查询的书的价格\n    public int findPriceByISbn(String isbn);\n\n    //更新书的库存\n    public void updateStock(String isbn);\n\n    //更新用户的月\n    public void updateUserAccount(String username, Integer price);\n\n}\n```\n\n### Daoimpl\n```java\npackage com.hph.spring.thing.annotation.daoimp;\n\nimport com.hph.spring.thing.annotation.dao.BookShopDao;\nimport com.hph.spring.thing.annotation.exception.BookStockException;\nimport com.hph.spring.thing.annotation.exception.UserAccountException;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.stereotype.Repository;\n\n@Repository\npublic class BookShopDaoImpl implements BookShopDao {\n    @Autowired\n    private JdbcTemplate jdbcTemplate;\n\n\n    @Override\n    public int findPriceByISbn(String isbn) {\n        String sql = \"select price from book where isbn = ?\";\n\n        return jdbcTemplate.queryForObject(sql, Integer.class, isbn);\n    }\n\n    @Override\n    public void updateStock(String isbn) {\n        //判断库存是否足够\n        String sql = \"select stock from book_stock where isbn = ?\";\n        Integer stock = jdbcTemplate.queryForObject(sql, Integer.class, isbn);\n        if (stock <= 0) {\n            throw new BookStockException(\"库存不足\");\n        }\n        sql = \"update book_stock set stock =stock -1 where isbn =?\";\n        jdbcTemplate.update(sql, isbn);\n\n    }\n\n    @Override\n    public void updateUserAccount(String username, Integer price) {\n        //判断余额是否足够\n        String sql = \"select balance from account where username = ?\";\n        Integer balance = jdbcTemplate.queryForObject(sql, Integer.class, username);\n        if (balance < price) {\n            throw new UserAccountException(\"余额不足\");\n        }\n        sql = \"update  account set balance = balance - ? where  username = ?\";\n\n        jdbcTemplate.update(sql, price, username);\n    }\n}\n\n```\n### 自定义异常\n\n```java\n\n//自定义库存异常\npublic class BookStockException extends RuntimeException {\n    public BookStockException() {\n    }\n\n    public BookStockException(String message) {\n        super(message);\n    }\n\n    public BookStockException(String message, Throwable cause) {\n        super(message, cause);\n    }\n\n    public BookStockException(Throwable cause) {\n        super(cause);\n    }\n\n    public BookStockException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) {\n        super(message, cause, enableSuppression, writableStackTrace);\n    }\n}\n```\n\n```java\npackage com.hph.spring.thing.annotation.exception;\n\npublic class UserAccountException extends RuntimeException {\n    public UserAccountException() {\n    }\n\n    public UserAccountException(String message) {\n        super(message);\n    }\n\n    public UserAccountException(String message, Throwable cause) {\n        super(message, cause);\n    }\n\n    public UserAccountException(Throwable cause) {\n        super(cause);\n    }\n\n    public UserAccountException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) {\n        super(message, cause, enableSuppression, writableStackTrace);\n    }\n}\n\n```\n\n### Service层\n\n```java\npublic interface BookShopService {\n    public void buyBook(String username, String isbn);\n\n}\n```\n\n```java\npackage com.hph.spring.thing.annotation.service;\n\nimport com.hph.spring.thing.annotation.dao.BookShopDao;\nimport com.hph.spring.thing.annotation.exception.UserAccountException;\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\nimport org.springframework.transaction.annotation.Isolation;\nimport org.springframework.transaction.annotation.Propagation;\nimport org.springframework.transaction.annotation.Transactional;\n\n\n@Transactional   //当前类中所有的方法都起作用\n@Service\npublic class BookShopServiceImpl implements BookShopService {\n    @Autowired\n    private BookShopDao bookShopDao;\n\n    @Transactional(propagation = Propagation.REQUIRES_NEW,isolation = Isolation.READ_COMMITTED,noRollbackFor = {UserAccountException.class})//只对当前的方法起作用\n    public void buyBook(String username, String isbn) {\n        Integer price = bookShopDao.findPriceByISbn(isbn);\n        bookShopDao.updateStock(isbn);\n        bookShopDao.updateUserAccount(username, price);\n    }\n}\n```\n\n```java\npackage com.hph.spring.thing.annotation.service;\n\nimport java.util.List;\n\npublic interface Cashier {\n    public void checkOut(String username, List<String> isbn);\n}\n```\n\n```java\npackage com.hph.spring.thing.annotation.service;\n\nimport java.util.List;\n\npublic interface Cashier {\n    public void checkOut(String username, List<String> isbn);\n}\n```\n\n```java\npackage com.hph.spring.thing.annotation.service;\n\nimport org.springframework.beans.factory.annotation.Autowired;\nimport org.springframework.stereotype.Service;\n\nimport java.util.List;\n\n@Service\npublic class CashierImpl implements Cashier {\n    @Autowired\n    private BookShopService bookShopService;\n\n    public void checkOut(String username, List<String> isbns) {\n        for (String isbn : isbns) {\n            bookShopService.buyBook(username, isbn);\n        }\n    }\n}\n```\n\n### Test层\n\n```java\npackage com.hph.spring.thing.annotation.test;\n\nimport com.hph.spring.thing.annotation.dao.BookShopDao;\nimport com.hph.spring.thing.annotation.service.BookShopService;\nimport com.hph.spring.thing.annotation.service.Cashier;\nimport com.hph.spring.thing.annotation.service.CashierImpl;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TestTransaction {\n\n    private BookShopDao bookShopDao;\n    private BookShopService bookShopService;\n    private Cashier cashier;\n\n    @Before\n    public void init() {\n        ApplicationContext ctx = new ClassPathXmlApplicationContext(\"spring-thing.xml\");\n        bookShopDao = ctx.getBean(\"bookShopDaoImpl\", BookShopDao.class);\n        bookShopService = ctx.getBean(\"bookShopServiceImpl\", BookShopService.class);\n        System.out.println(bookShopService.getClass().getName());\n        cashier = ctx.getBean(\"cashierImpl\", CashierImpl.class);\n\n    }\n\n    @Test\n    public void testThing() {\n        bookShopService.buyBook(\"Tom\", \"1001\");\n    }\n\n    @Test\n    public void testCheckOut() {\n        List<String> isbns = new ArrayList<>();\n        isbns.add(\"1001\");\n        isbns.add(\"1002\");\n\n        cashier.checkOut(\"Tom\", isbns);\n    }\n}\n```\n\n## 数据库表\n\n```sql\nCREATE TABLE book (\n  isbn VARCHAR (50) PRIMARY KEY,\n  book_name VARCHAR (100),\n  price INT\n) ;\n\nCREATE TABLE book_stock (\n  isbn VARCHAR (50) PRIMARY KEY,\n  stock INT,\n) ;\n\nCREATE TABLE account (\n  username VARCHAR (50) PRIMARY KEY,\n  balance INT,\n) ;\n\nINSERT INTO account (`username`,`balance`) VALUES ('Tom',300);\n\nINSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('1001','BigData',100);\nINSERT INTO book (`isbn`,`book_name`,`price`) VALUES ('1002',Java,70);\n\nINSERT INTO book_stock (`isbn`,`stock`) VALUES ('1002',10);\n```\n\n## 配置文件准备\n\n```xml\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n       xmlns:context=\"http://www.springframework.org/schema/context\"\n       xmlns:tx=\"http://www.springframework.org/schema/tx\"\n       xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\n\t\thttp://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.0.xsd\n\t\thttp://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-4.0.xsd\">\n    <!--扫描基包-->\n    <context:component-scan base-package=\"com.hph.spring.thing.annotation\"></context:component-scan>\n    <!--数据源配置-->\n    <context:property-placeholder location=\"classpath:db.properties\"></context:property-placeholder>\n    <bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\">\n        <property name=\"driverClass\" value=\"${jdbc.driver}\"></property>\n        <property name=\"jdbcUrl\" value=\"${jdbc.url}\"></property>\n        <property name=\"user\" value=\"${jdbc.username}\"></property>\n        <property name=\"password\" value=\"${jdbc.password}\"></property>\n        <property name=\"initialPoolSize\" value=\"${initialPoolSize}\"/>\n        <property name=\"minPoolSize\" value=\"${minPoolSize}\"/>\n        <property name=\"maxPoolSize\" value=\"${maxPoolSize}\"/>\n        <property name=\"acquireIncrement\" value=\"${acquireIncrement}\"/>\n        <property name=\"maxStatements\" value=\"${maxStatements}\"/>\n        <property name=\"maxStatementsPerConnection\"\n                  value=\"${maxStatementsPerConnection}\"/>\n    </bean>\n    <!--JdbcTemplate-->\n    <bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\">\n        <property name=\"dataSource\" ref=\"dataSource\"></property>\n    </bean>\n\n    <!--NameParameterJdbcTemplate-->\n    <bean id=\"namedParameterJdbcTemplate\" class=\"org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate\">\n        <constructor-arg ref=\"dataSource\"></constructor-arg>\n    </bean>\n    <!--事务管理器-->\n    <bean id=\"dataSourceTransactionManager\"\n          class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\">\n        <property name=\"dataSource\" ref=\"dataSource\"></property>\n    </bean>\n    <!-- 开启事务注解\n        transaction-manager 用来指定事务管理器， 如果事务管理器的id值 是 transactionManager，\n                                                       可以省略不进行指定。\n    -->\n    <tx:annotation-driven transaction-manager=\"dataSourceTransactionManager\"/>\n\n</beans>\n```\n\n## 事务的传播行为\n\n事务的传播行为在数据库种不存在,是Spring在TransactionDefinition接口中规定了7种类型的*事务传播行为*,当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。\n\n![kv8UAA.png](https://s2.ax1x.com/2019/03/06/kv8UAA.png)\n\n事务传播属性可以在@Transactional注解的propagation属性中定义。\n\n### 测试关系\n\n![kvGCHH.png](https://s2.ax1x.com/2019/03/06/kvGCHH.png)\n\n\n\n### REQUIRED传播行为\n\n当bookService的purchase()方法被另一个事务方法checkout()调用时，它默认会在现有的事务内运行。这个默认的传播行为就是REQUIRED。因此在checkout()方法的开始和终止边界内只有一个事务。这个事务只在checkout()方法结束的时候被提交，结果用户一本书都买不了。\n\n![kvGYvT.png](https://s2.ax1x.com/2019/03/06/kvGYvT.png)\n\n\n\n###  REQUIRES_NEW传播行为\n\n表示该方法必须启动一个新事务，并在自己的事务内运行。如果有事务在运行，就应该先挂起它。\n\n![kvGwVJ.png](https://s2.ax1x.com/2019/03/06/kvGwVJ.png)\n\n\n\n## 事务的隔离级别\n\n### 问题\n\n​         假设现在有两个事务：Transaction01和Transaction02并发执行。\n\n 脏读：\n\n​        ①Transaction01将某条记录的AGE值从20修改为30。\n\n​         ②Transaction02读取了Transaction01更新后的值：30。\n\n​         ③Transaction01回滚，AGE值恢复到了20。\n\n​         ④Transaction02读取到的30就是一个无效的值。\n\n​    不可重复读：\n\n​         ①Transaction01读取了AGE值为20。\n\n​         ②Transaction02将AGE值修改为30。\n\n​         ③Transaction01再次读取AGE值为30，和第一次读取不一致。\n\n​    幻读：\n\n​         ①Transaction01读取了STUDENT表中的一部分数据。\n\n​         ②Transaction02向STUDENT表中插入了新的行。\n\n​         ③Transaction01读取了STUDENT表时，多出了一些行。\n\n### 隔离级别\n\n数据库系统必须具有隔离并发运行各个事务的能力，使它们不会相互影响，避免各种并发问题。**一个事务与其他事务隔离的程度称为隔离级别**。SQL标准中规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。\n\n **读未提交**：READ UNCOMMITTED \t允许Transaction01读取Transaction02未提交的修改。\n\n **读已提交**：READ COMMITTED\t      要求Transaction01只能读取Transaction02已提交的修改。\n\n**可重复读**：REPEATABLE READ\t      确保Transaction01可以多次从一个字段中读取到相同的值，即Transaction01执行期间禁止其它事务对这个字段进行更新。\n\n**串行化**：SERIALIZABLE\t\t            确保Transaction01可以多次从一个表中读取到相同的行，在Transaction01执行期间，禁止其它事务对这个表进行添加、更新、删除操作。可以避免任何并发问题，但性能十分低下。\n\n\n\n|                  | 脏读 | 不可重复读 | 幻读 |\n| ---------------- | ---- | ---------- | ---- |\n| READ UNCOMMITTED | 有   | 有         | 有   |\n| READ COMMITTED   | 无   | 有         | 有   |\n| REPEATABLE READ  | 无   | 无         | 有   |\n| SERIALIZABLE     | 无   | 无         | 无   |\n\n|                  | Oracle  | MySQL   |\n| ---------------- | ------- | ------- |\n| READ UNCOMMITTED | ×       | √       |\n| READ COMMITTED   | √(默认) | √       |\n| REPEATABLE READ  | ×       | √(默认) |\n| SERIALIZABLE     | √       | √       |\n\n### 在Spring中指定事务隔离级别\n\n用@Transactional注解声明式地管理事务时可以在@Transactional的isolation属性中设置隔离级别\n\n### 触发事务回滚的异常\n\n默认情况：捕获到RuntimeException或Error时回滚，而捕获到编译时异常不回滚。\n\n#### 设置注解\n\n​     rollbackFor属性：指定遇到时必须进行回滚的异常类型，可以为多个\n\n​      noRollbackFor属性：指定遇到时不回滚的异常类型，可以为多个\n\n```java\n    @Transactional(propagation = Propagation.REQUIRES_NEW,isolation = Isolation.READ_COMMITTED,noRollbackFor = {UserAccountException.class})//只对当前的方法起作用\n    public void buyBook(String username, String isbn) {\n        Integer price = bookShopDao.findPriceByISbn(isbn);\n        bookShopDao.updateStock(isbn);\n        bookShopDao.updateUserAccount(username, price);\n    }\n}\n\n```\n\n### 事务的超时和只读属性\n\n由于事务可以在行和表上获得锁，因此长事务会占用资源，并对整体性能产生影响。\n\n如果一个事务只读取数据但不做修改，数据库引擎可以对这个事务进行优化。超时事务属性：事务在强制回滚之前可以保持多久。这样可以防止长期运行的事务占用资源。只读事务属性: 表示这个事务只读取数据但不更新数据, 这样可以帮助数据库引擎优化事务。\n\n```java\n\n    @Transactional(propagation = Propagation.REQUIRES_NEW,isolation = Isolation.READ_COMMITTED,noRollbackFor = {UserAccountException.class},readOnly = true,timeout = 30)//只对当前的方法起作用\n    public void buyBook(String username, String isbn) {\n        Integer price = bookShopDao.findPriceByISbn(isbn);\n        bookShopDao.updateStock(isbn);\n        bookShopDao.updateUserAccount(username, price);\n    }\n```\n\n## 参考资料\n\n尚硅谷Spring相关课程和文档资料\n\n","tags":["Spring"],"categories":["JavaWeb"]},{"title":"JdbcTemplate","url":"/2019/03/05/JdbcTemplate/","content":"\n{{ \"JdbcTemplate\"}}： <Excerpt in index | 首页摘要><!-- more -->\n\n### 概述\n\n​         为了使JDBC更加易于使用，Spring在JDBC API上定义了一个抽象层，以此建立一个JDBC存取框架。 作为Spring JDBC框架的核心，JDBC模板的设计目的是为不同类型的JDBC操作提供模板方法，通过这种方式，可以在尽可能保留灵活性的情况下，将数据库存取的工作量降到最低。 可以将Spring的JdbcTemplate看作是一个小型的轻量级持久化层框架，和我们之前使用过的DBUtils风格非常接近。\n\n### jdbc.properties\n\n```properties\njdbc.username=root\t//此处请带前缀否则会导致${username}为系统用户名\njdbc.password=123456\njdbc.url=jdbc:mysql://localhost:3306/bigdata\njdbc.driver=com.mysql.jdbc.Driver\ninitialPoolSize=30\nminPoolSize=10\nmaxPoolSize=100\nacquireIncrement=5\nmaxStatements=1000\nmaxStatementsPerConnection=10\n```\n\n### spring配置文件\n\n```xml\n    <bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\">\n        <property name=\"driverClass\" value=\"${jdbc.driver}\"></property>\n        <property name=\"jdbcUrl\" value=\"${jdbc.url}\"></property>\n        <property name=\"user\" value=\"${jdbc.username}\"></property>\n        <property name=\"password\" value=\"${jdbc.password}\"></property>\n        <property name=\"initialPoolSize\" value=\"${initialPoolSize}\"/>\n        <property name=\"minPoolSize\" value=\"${minPoolSize}\"/>\n        <property name=\"maxPoolSize\" value=\"${maxPoolSize}\"/>\n        <property name=\"acquireIncrement\" value=\"${acquireIncrement}\"/>\n        <property name=\"maxStatements\" value=\"${maxStatements}\"/>\n        <property name=\"maxStatementsPerConnection\"\n                  value=\"${maxStatementsPerConnection}\"/>\n    </bean>\n    <!--JdbcTemplate-->\n    <bean id=\"jdbcTemplate\" class=\"org.springframework.jdbc.core.JdbcTemplate\">\n        <property name=\"dataSource\" ref=\"dataSource\"></property>\n    </bean>\n```\n\n### 持久化操作\n\n**增删改: **JdbcTemplate.update(String, Object...)\n\n**批量增删改: **JdbcTemplate.batchUpdate(String, List<Object[]>) \n\nObject[]封装了SQL语句每一次执行时所需要的参数\n\nList集合封装了SQL语句多次执行时的所有参数\n\n**查询单行 **JdbcTemplate.queryForObject(String, RowMapper&lt;Department&gt;, Object...)\n\n**查询多行:**JdbcTemplate.query(String, RowMapper&lt;Department&gt;, Object...)  RowMapper对象依然可以使用BeanPropertyRowMapper\n\n**查询单一值:**JdbcTemplate.queryForObject(String, Class, Object...)\n\n### 前置准备\n\n```java\n\npublic class Employee {\n\t\n\tprivate Integer id ; \n\tprivate String lastName; \n\tprivate String email ;\n\tprivate Integer gender;\n\t\n\tpublic Employee() {\n\t\t// TODO Auto-generated constructor stub\n\t}\n\t\n\tpublic Employee(Integer id, String lastName, String email, Integer gender) {\n\t\tsuper();\n\t\tthis.id = id;\n\t\tthis.lastName = lastName;\n\t\tthis.email = email;\n\t\tthis.gender = gender;\n\t}\n\tpublic Integer getId() {\n\t\treturn id;\n\t}\n\tpublic void setId(Integer id) {\n\t\tthis.id = id;\n\t}\n\tpublic String getLastName() {\n\t\treturn lastName;\n\t}\n\tpublic void setLastName(String lastName) {\n\t\tthis.lastName = lastName;\n\t}\n\tpublic String getEmail() {\n\t\treturn email;\n\t}\n\tpublic void setEmail(String email) {\n\t\tthis.email = email;\n\t}\n\tpublic Integer getGender() {\n\t\treturn gender;\n\t}\n\tpublic void setGender(Integer gender) {\n\t\tthis.gender = gender;\n\t}\n\t@Override\n\tpublic String toString() {\n\t\treturn \"Employee [id=\" + id + \", lastName=\" + lastName + \", email=\" + email + \", gender=\" + gender + \"]\";\n\t} \n\t\n}\n```\n\n### 方法实现\n\n```java\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\nimport org.springframework.jdbc.core.BeanPropertyRowMapper;\nimport org.springframework.jdbc.core.JdbcTemplate;\nimport org.springframework.jdbc.core.RowMapper;\nimport org.springframework.jdbc.core.namedparam.BeanPropertySqlParameterSource;\nimport org.springframework.jdbc.core.namedparam.NamedParameterJdbcTemplate;\nimport org.springframework.jdbc.core.namedparam.SqlParameterSource;\n\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\npublic class TestJdbc {\n\n    private JdbcTemplate jdbcTemplate;\n\n    private NamedParameterJdbcTemplate npjt;\n\t//前置操作初始化\n    @Before\n    public void init() {\n        ApplicationContext ctx =\n                new ClassPathXmlApplicationContext(\"spring-jdbc.xml\");\n\n        jdbcTemplate = ctx.getBean(\"jdbcTemplate\", JdbcTemplate.class);\n\n        npjt = ctx.getBean(\"namedParameterJdbcTemplate\", NamedParameterJdbcTemplate.class);\n\n    }\n\n    /**\n     * update():  增删改操作\n     */\n    @Test\n    public void testUpdate() {\n        String sql = \"insert into tbl_employee(last_name,email,gender) value(?,?,?)\";\n\n        //jdbcTemplate.update(sql, \"运慧\",\"yh@atguigu.com\",1);\n        jdbcTemplate.update(sql, new Object[]{\"QFX\", \"QFX@atguigu.com\", 1});\n    }\n\n    /**\n     * batchUpdate(): 批量增删改\n     * 作业: 批量删  修改\n     */\n    @Test\n    public void testBatchUpdate() {\n        String sql = \"insert into tbl_employee(last_name,email,gender) value(?,?,?)\";\n        List<Object[]> batchArgs = new ArrayList<Object[]>();\n        batchArgs.add(new Object[]{\"zsf\", \"zsf@sina.com\", 1});\n        batchArgs.add(new Object[]{\"zwj\", \"zwj@sina.com\", 1});\n        batchArgs.add(new Object[]{\"sqs\", \"sqs@sina.com\", 1});\n\n        jdbcTemplate.batchUpdate(sql, batchArgs);\n    }\n\n\n    /**\n     * queryForObject():\n     * 1. 查询单行数据 返回一个对象\n     * 2. 查询单值 返回单个值\n     */\n    @Test\n    public void testQueryForObjectReturnObject() {\n        String sql = \"select id,last_name,email,gender from tbl_employee where id = ?\";\n        //rowMapper: 行映射  将结果集的一条数据映射成具体的一个java对象.\n        RowMapper<Employee> rowMapper = new BeanPropertyRowMapper<>(Employee.class);\n\n        Employee employee = jdbcTemplate.queryForObject(sql, rowMapper, 1001);\n        System.out.println(employee);\n    }\n\n    @Test\n    public void testQueryForObjectReturnValue() {\n        String sql = \"select count(id) from tbl_employee\";\n\n        Integer result = jdbcTemplate.queryForObject(sql, Integer.class);\n        System.out.println(result);\n    }\n\n    /**\n     * query(): 查询多条数据返回多个对象的集合.\n     */\n\n    @Test\n    public void testQuery() {\n        String sql = \"select id,last_name,email,gender from tbl_employee\";\n        RowMapper<Employee> rowMapper = new BeanPropertyRowMapper<>(Employee.class);\n\n        List<Employee> emps = jdbcTemplate.query(sql, rowMapper);\n        System.out.println(emps);\n    }\n\n\n    /**\n     * 测试具名参数模板类\n     */\n\n    @Test\n    public void testNpjt() {\n        String sql = \"insert into tbl_employee(last_name,email,gender) values(:ln,:em,:ge)\";\n        Map<String, Object> paramMap = new HashMap<>();\n\n        paramMap.put(\"ln\", \"Jerry\");\n        paramMap.put(\"em\", \"jerry@sina.com\");\n        paramMap.put(\"ge\", 0);\n\n\n        npjt.update(sql, paramMap);\n    }\n\n\n    @Test\n    public void testNpjtObject() {\n        //模拟Service层 直接传递给Dao层一个具体的  对象\n        Employee employee = new Employee(null, \"张无忌\", \"zwj@sina.com\", 1);\n\n        //在dao的插入方法中:\n        String sql = \"insert into tbl_employee(last_name,email,gender) values(:lastName,:email,:gender)\";\n\n        SqlParameterSource paramSource = new BeanPropertySqlParameterSource(employee);\n\n        npjt.update(sql, paramSource);\n\n    }\n\n\n}\n```\n\n\n\n\n\n\n\n\n\n","tags":["Spring"],"categories":["JavaWeb"]},{"title":"AOP概述","url":"/2019/03/03/AOP概述/","content":"\n {{ \"Aop  和 AspectJ的基本介绍和应用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## AOP简介\n\nAOP(Aspect-Oriented Programming，**面向切面编程**)：是一种新的方法论，是对传统 OOP(Object-Oriented Programming，面向对象编程)的补充。 AOP编程操作的主要对象是切面(aspect)，而切面**模块化横切关注点**。在应用AOP编程时，仍然需要定义公共功能，但可以明确的定义这个功能应用在哪里，以什么方式应用，并且不必修改受影响的类。这样一来横切关注点就被模块化到特殊的类里——这样的类我们通常称之为“切面”。 \n\nAOP的好处： 每个事物逻辑位于一个位置，代码不分散，便于维护和升级业务模块更简洁，只包含核心业务代码\n\n![kL6929.png](https://s2.ax1x.com/2019/03/03/kL6929.png)\n\n## AOP术语\n\n 横切关注点：从每个方法中抽取出来的同一类非核心业务。\n\n切面(Aspect)：封装横切关注点信息的类，每个关注点体现为一个通知方法。\n\n通知(Advice)：切面必须要完成的各个具体工作\n\n目标(Target)：被通知的对象\n\n代理(Proxy)：向目标对象应用通知之后创建的代理对象\n\n连接点(Joinpoint)：横切关注点在程序代码中的具体体现，对应程序执行的某个特定位置。例如：类某个方法调用前、调用后、方法捕获到异常后等。\n\n在应用程序中可以使用横纵两个坐标来定位一个具体的连接点：\n\n![kL6u2d.png](https://s2.ax1x.com/2019/03/03/kL6u2d.png)\n\n切入点：定位连接点的方式。每个类的方法中都包含多个连接点，所以连接点是类中客观存在的事物。如果把连接点看作数据库中的记录，那么切入点就是查询条件——AOP可以通过切入点定位到特定的连接点。切点通过org.springframework.aop.Pointcut 接口进行描述，它使用类和方法作为连接点的查询条件。\n\n![kL6QKI.png](https://s2.ax1x.com/2019/03/03/kL6QKI.png)\n\n##  AspectJ\n\nAspectJ：Java社区里最完整最流行的AOP框架。在Spring2.0以上版本中，可以使用基于AspectJ注解或基于XML配置的AOP。\n\nAspectJ是AOP的Java实现版本，定义了AOP的语法，可以说是对Java的一个扩展。相对于Java，AspectJ引入了join point(连接点)的概念，同时引入三个新的结构，pointcut(切点)， advice(通知)，inter-type declaration(跨类型声明)以及aspect。其中pointcut和advice是AspectJ中动态额部分，用来指定在什么条件下切断执行，以及采取什么动作来实现切面操作。这里的pointcut就是用来定义什么情况下进行横切，而advice则是指横切情况下我们需要做什么操作，pointcut和advice会动态的影响程序的运行流程。从某种角度上说，pointcut(切点)和我们平时用IDE调试程序时打的断点很类似，当程序执行到我们打的断点的地方的时候(运行到满足我们定义的pointcut的语句的时候，也就是join point连接点)，\n\n&lt;aop:aspectj-autoproxy&gt; ：当Spring IOC容器侦测到bean配置文件中的&lt;aop:aspectj-autoproxy&gt;元素时，会自动为与AspectJ切面匹配的bean创建代理\n\n用AspectJ注解声明切面\n\n- 要在Spring中声明AspectJ切面，只需要在IOC容器中将切面声明为bean实例。\n- 当在Spring IOC容器中初始化AspectJ切面之后，Spring IOC容器就会为那些与 AspectJ切面相匹配的bean创建代理。\n-  在AspectJ注解中，切面只是一个带有@Aspect注解的Java类，它往往要包含很多通知。\n- 通知是标注有某种注解的简单的Java方法。\n- AspectJ支持5种类型的通知注解：\n\n@Before：前置通知，在方法执行之前执行\n\n @After：后置通知，在方法执行之后执行\n\n@AfterRunning：返回通知，在方法返回结果之后执行\n\n@AfterThrowing：异常通知，在方法抛出异常之后执行\n\n@Around：环绕通知，围绕着方法执行\n\n## 切入点表达式\n\n需要用到的包：链接：https://pan.baidu.com/s/1gmEusK9hXAaJvcB9_6XZPg  提取码：m3i9 \n\n通过**表达式的方式**定位**一个或多个**具体的连接点。\n\n语法细节:\n\n```\nexecution([权限修饰符] [返回值类型] [简单类名/全类名] [方法名]([参数列表]))\n```\n\n### 准备\n\n```java\npackage com.hph.spring.aspectJ.annotation;\n\n\npublic interface ArithmeticCalculator {\n\t\n\tpublic int add(int i ,int j );\n\t\n\tpublic int sub(int i, int j );\n\t\n\tpublic int mul(int i ,int j );\n\t\n\tpublic int div(int i, int j );\n}\n\n```\n\n```java\npackage com.hph.spring.aspectJ.annotation;\n\n\nimport org.springframework.stereotype.Component;\n\n@Component \t//托管给Spring\npublic class ArithmeticCalculatorImpl implements ArithmeticCalculator {\n\n    @Override\n    public int add(int i, int j) {\n        int result = i + j;\n        return result;\n    }\n\n    @Override\n    public int sub(int i, int j) {\n        int result = i - j;\n        return result;\n    }\n\n    @Override\n    public int mul(int i, int j) {\n        int result = i * j;\n        return result;\n    }\n\n    @Override\n    public int div(int i, int j) {\n        int result = i / j;\n        return result;\n    }\n\n}\t\n```\n\n### 通知\n\n概述:  在具体的连接点上要执行的操作。 一个切面可以包括一个或者多个通知。通知所使用的注解的值往往是切入点表达式。\n\n**前置通知 **：在方法执行之前执行的通知     使用@Before注解\n\n```java\npackage com.hph.spring.aspectJ.annotation;\n\n\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.annotation.After;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Arrays;\n\n/**\n * 日志切面\n */\n@Component\n@Aspect\npublic class LoggingAspect {\n    // 前置通知:再目标方法(连接点)执行之前执行\n    @Before(\"execution(public  int com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.add(int ,int ))\")\n    public void beforeMethod(JoinPoint joinPoint) {\n        //获取方法的参数\n        Object[] args = joinPoint.getArgs();\n        //方法的名字\n        String metodName = joinPoint.getSignature().getName();\n        System.out.println(\"LoggingAspct ==> The method\" + metodName + \"   begin with \" + Arrays.asList(args));\n    }\n\n}\n\n```\n\n```java\npackage com.hph.spring.aspectJ.annotation;\n\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\npublic class Main {\n    public static void main(String[] args) {\n\n        ApplicationContext ctx = new ClassPathXmlApplicationContext(\"spring-aspectJ_annotation.xml\");\n        ArithmeticCalculator ac = ctx.getBean(\"arithmeticCalculatorImpl\", ArithmeticCalculator.class);\n        int result = ac.add(1, 1);\n        System.out.println(result + \"\\n代理对象: \" + ac.getClass().getName());\n    }\n}\n```\n\n\n\n**后置通知 **：后置通知：后置通知是在连接点完成之后执行的，即连接点返回结果或者抛出异常的时候   使用@After注解\n\n```java\n\n    /**\n     * 后置通知:目标方法之后执行:不管目标方法是否抛出异常都会执行 不会获取到方法的结果\n     */\n    //  *: 任意修饰符 任意返回值类型           * 任意方法 ..:任意参数列表\n    @After(\"execution(* com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.*(..))\")\n    //连接点对象\n    public void afterMethod(JoinPoint joinPoint) {\n        //方法名字\n        String methodName = joinPoint.getSignature().getName();\n\n        System.out.println(\"LoggingAspct ==> method \" + methodName + \"ends\");\n    }\n```\n\n**返回通知  **：无论连接点是正常返回还是抛出异常，后置通知都会执行。如果只想在连接点返回的时候记录日志，应使用返回通知代替后置通知。使用@AfterReturning注解,在返回通知中访问连接点的返回值\n\n```java\n/**\n     * 返回通知:再目标方法正常执行结束后执行 可以获取方法的返回值\n     * returing 的值 一定要和 形参 的值一致\n     */\n    @AfterReturning(value = \"execution(* com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.*(..))\", returning = \"result\")\n    public void afterReturningMethod(JoinPoint joinPoint, Object result) {\n        //方法名字\n        String methodname = joinPoint.getSignature().getName();\n        System.out.println(\"返回通知 LoggingAspct ==> The method \" + methodname + \"end with:\" + result);\n    }\n```\n\n在返回通知中，只要将returning属性添加到@AfterReturning注解中，就可以访问连接点的返回值。该属性的值即为用来传入返回值的参数名称\n\n必须在通知方法的签名中添加一个同名参数。在运行时Spring AOP会通过这个参数传递返回值原始的切点表达式需要出现在pointcut属性中\n\n**异常通知**：只在连接点抛出异常时才执行异常通知将throwing属性添加到@AfterThrowing注解中，也可以访问连接点抛出的异常。Throwable是所有错误和异常类的顶级父类，所以在异常通知方法可以捕获到任何错误和异常。\n\n```java\n    /**\n     * 异常通知:再目标方法抛出异常后执行\n     * 获取方法的异常: 通过throwing 来指定一个名字,必须要与一个参数名一致.\n     * 可以通过形参中异常的类型才会执行异常通知\n     */\n    @AfterThrowing(value = \"execution(* com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.*(..))\", throwing = \"ex\")\n    //如果指定的异常类型与抛出异常类型不匹配则会抛出\n    public void afterThrowingMethod(JoinPoint joinPoint, ArithmeticException ex) {\n        //获取方法名称\n        String methodname = joinPoint.getSignature().getName();\n        System.out.println(\"异常通知: LoggingAspect ==>t Thew method\" + methodname + \"occurs Exception \" + ex);\n    }\n```\n\n如果只对某种特殊的异常类型感兴趣，可以将参数声明为其他异常的参数类型。然后通知就只在抛出这个类型及其子类的异常时才被执行\n\n**环绕通知**：环绕通知是所有通知类型中功能最为强大的，能够全面地控制连接点，甚至可以控制是否执行连接点。 \n\n```java\n  /**\n     * 环绕通知:环绕着目标方法执行 可以理解为前置 后置 返回 异常 通知的结合体\n     */\n    //对于环绕通知必须声明一个Object的目标方法执行\n    @Around(\"execution(* com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.*.*(..))\")\n    public Object arroundMethod(ProceedingJoinPoint pjp) {\n\n        //执行目标方法\n        try {\n            //前置通知\n            Object result = pjp.proceed();\n            //返回通知\n            return result;\n        } catch (Throwable e) {\n            //异常通知\n            e.printStackTrace();\n        } finally {\n            //后置通知\n        }\n        return null;\n    }\n}\n```\n\n\n\n对于环绕通知来说，连接点的参数类型必须是ProceedingJoinPoint。它是 JoinPoint的子接口，允许控制何时执行，是否执行连接点。\n\n在环绕通知中需要明确调用ProceedingJoinPoint的proceed()方法来执行被代理的方法。如果忘记这样做就会导致通知被执行了，但目标方法没有被执行。\n\n```java\n@Around(\"execution(* com.hph.spring.aspectJ.annotation.ArithmeticCalculatorImpl.*(..))\")\n    public Object arroundMethod(ProceedingJoinPoint pjp) {\n        //执行目标方法\n        try {\n            //前置通知\n            Object[] args = pjp.getArgs();\n            String metodName = pjp.getSignature().getName();\n            System.out.println(\"Arround前置通知: LoggingAspct ==> The method\" + metodName + \"   begin with \" + Arrays.asList(args));\n            Object result = pjp.proceed();\n            //返回通知\n            String methodname = pjp.getSignature().getName();\n            System.out.println(\"Arround返回通知 LoggingAspct ==> The method \" + methodname + \"end with:\" + result);\n        } catch (Throwable e) {\n            //异常通知\n            e.printStackTrace();\n            String methodname = pjp.getSignature().getName();\n            System.out.println(\"Arround异常通知: LoggingAspect ==>t Thew method\" + methodname + \"occurs Exception \" + e);\n        } finally {\n            //后置\n            String methodName = pjp.getSignature().getName();\n            System.out.println(\"Arround后置通知: LoggingAspct ==> method \" + methodName + \"ends\");\n        }\n        return null;\n    }\n}\n```\n\n\n\n**注意：环绕通知的方法需要返回目标方法执行之后的结果，即调用 joinPoint.proceed();的返回值，否则会出现空指针异常。**\n\n### 重用切入点\n\n在编写AspectJ切面时，可以直接在通知注解中书写切入点表达式。但同一个切点表达式可能会在多个通知中重复出现。\n\n在AspectJ切面中，可以通过@Pointcut注解将一个切入点声明成简单的方法。切入点的方法体通常是空的，因为将切入点定义与应用程序逻辑混在一起是不合理的。\n\n切入点方法的访问控制符同时也控制着这个切入点的可见性。如果切入点要在多个切面中共用，最好将它们集中在一个公共的类中。在这种情况下，它们必须被声明为public。在引入这个切入点时，必须将类名也包括在内。如果类没有与这个切面放在同一个包中，还必须包含包名。\n\n其他通知可以通过方法名称引入该切入点\n\n```java\n@Pointcut(\"execution(* *.*(..))\")\nprivate void loggingOperation() {\n}\n\n@Before(\"loggingOperation()\")\npublic void logBefore(JoinPoint joinPoint)  {\n    //业务逻辑\n    Object[] args = joinPoint.getArgs();\n    //方法的名字\n    String metodName = joinPoint.getSignature().getName();\n    System.out.println(\"重用切入点前置通知: LoggingAspct ==> The method\" + metodName + \"   begin with \" + Arrays.asList(args));\n}\n\n@AfterReturning(pointcut = \"loggingOperation()\",returning = \"result\")\npublic void logAfterReturing(JoinPoint joinPoint,Object result ) {\n    String methodname = joinPoint.getSignature().getName();\n    System.out.println(\"重用切入点返回通知 LoggingAspct ==> The method \" + methodname + \"end with:\" + result);\n}\n@AfterThrowing(pointcut = \"loggingOperation()\",throwing = \"e\")\npublic void logAfterThrowing(JoinPoint joinPoint,ArithmeticException e) {\n    //获取方法名称\n    String methodname = joinPoint.getSignature().getName();\n    System.out.println(\"重用切入点返回通知:异常通知: LoggingAspect ==>t Thew method\" + methodname + \"occurs Exception \" + e);\n}\n```\n\n###  指定切面的优先级\n\n```java\nimport org.aspectj.lang.JoinPoint;\nimport org.aspectj.lang.annotation.Aspect;\nimport org.aspectj.lang.annotation.Before;\nimport org.springframework.core.annotation.Order;\nimport org.springframework.stereotype.Component;\n\nimport java.util.Arrays;\n\n@Component\n@Aspect\n@Order(0) //设置切面的优先级  0-2147483647 数字越小优先级越高\npublic class ValidationAspect {\n    @Before(\"execution(* com.hph.spring.aspectJ.annotation.*.*(..))\")\n    public void beforeMethod(JoinPoint joinPoint) {\n        String methodName = joinPoint.getSignature().getName();\n        Object[] args = joinPoint.getArgs();\n        System.out.println(\"优先级问题ValidationAspcet  ===>The method\" + methodName + \"begin with \" + Arrays.asList(args));\n    }\n}\n```\n\n\n\n### XML配置\n\n切入点使用&lt;aop:pointcut&gt;元素声明。\n\n切入点必须定义在&lt;aop:aspect&gt;元素下，或者直接定义在<aop:config&gt;元素下。 ① 定义在&lt;aop:aspect&gt;元素下：只对当前切面有效② 定义在&lt;aop:config&gt;元素下：对所有切面都有效\n\n基于XML的AOP配置不允许在切入点表达式中用名称引用其他切入点。\n\n```xml\n    <!--目标对象-->\n    <bean id=\"arithmeticCalculatorImpl\" class=\"com.hph.spring.aspectJ.xml.ArithmeticCalculatorImpl\"></bean>\n\n    <!--切面-->\n    <bean id=\"loggingAspect\" class=\"com.hph.spring.aspectJ.xml.LoggingAspect\"></bean>\n    <bean id=\"validationAspect\" class=\"com.hph.spring.aspectJ.xml.ValidationAspect\"></bean>\n    <!--AOP:切面  通知 切入点表达式-->\n    <aop:config>\n        <!--切面-->\n        <aop:aspect ref=\"loggingAspect\">\n            <!--切入点表达式-->\n            <aop:pointcut id=\"myPointCut\" expression=\"execution(* com.hph.spring.aspectJ.xml.*.*(..))\"></aop:pointcut>\n            <!--通知-->\n            <aop:before method=\"beforeMethod\" pointcut-ref=\"myPointCut\"></aop:before>\n            <aop:after method=\"afterMethod\" pointcut-ref=\"myPointCut\"></aop:after>\n            <aop:after-returning method=\"afterReturningMethod\" pointcut-ref=\"myPointCut\" returning=\"result\"></aop:after-returning>\n            <aop:after-throwing method=\"afterThrowingMethod\" pointcut-ref=\"myPointCut\" throwing=\"ex\"></aop:after-throwing>\n            <aop:around method=\"arroundMethod\" pointcut-ref=\"myPointCut\"></aop:around>\n        </aop:aspect>\n\n    </aop:config>\n```\n\n在aop名称空间中，每种通知类型都对应一个特定的XML元素。\n\n通知元素需要使用&lt;pointcut-ref&gt;来引用切入点，或用&lt;pointcut&gt;直接嵌入切入点表达式。\n\nmethod属性指定切面类中通知方法的名称\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Spring"],"categories":["JavaWeb"]},{"title":"Spring IOC容器和Bean的配置","url":"/2019/03/02/SpringIOC容器和Bean的配置/","content":"\n {{ \"Spring  IOC容器 和Bean的配置 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n##  IOC和DI\n\n###  IOC(Inversion of Control)：反转控制\n\n在应用程序中的组件需要获取资源时，传统的方式是组件主动的从容器中获取所需要的资源，在这样的模式下开发人员往往需要知道在具体容器中特定资源的获取方式，增加了学习成本，同时降低了开发效率。\n\n反转控制的思想完全颠覆了应用程序组件获取资源的传统方式：反转了资源的获取方向——改由容器主动的将资源推送给需要的组件，开发人员不需要知道容器是如何创建资源对象的，只需要提供接收资源的方式即可，极大的降低了学习成本，提高了开发的效率。这种行为也称为查找的**被动形式**。\n\n###  DI(Dependency Injection)：依赖注入\n\nIOC的另一种表述方式：即组件以一些预先定义好的方式(例如：setter 方法)接受来自于容器的资源注入。相对于IOC而言，这种表述更直接。\n\n\n\n###  IOC容器在Spring中的实现\n\n在通过IOC容器读取Bean的实例之前，需要先将IOC容器本身实例化。Spring提供了IOC容器的两种实现方式:\n\nBeanFactory：IOC容器的基本实现，是Spring内部的基础设施，是面向Spring本身的，不是提供给开发人员使用的。\n\nApplicationContext：BeanFactory的子接口，提供了更多高级特性。面向Spring的使用者，几乎所有场合都使用ApplicationContext而不是底层的BeanFactory\n\n### ApplicationContext的主要实现类\n\n ClassPathXmlApplicationContext：对应类路径下的XML格式的配置文件\n\nFileSystemXmlApplicationContext：对应文件系统中的XML格式的配置文件\n\n在初始化时就创建单例的bean，也可以通过配置的方式指定创建的Bean是多实例的。\n\n###  WebApplicationContext\n\n专门为WEB应用而准备的，它允许从相对于WEB根目录的路径中完成初始化工作\n\n## 通过类型获取bean\n\n从IOC容器中获取bean时，除了通过id值获取，还可以通过bean的类型获取。但如果同一个类型的bean在XML文件中配置了多个，则获取时会抛出异常，所以同一个类型的bean在容器中必须是唯一的。\n\n```java\nStudent student = iocContainer.getBean(Student.class);\n```\n\n可以使用另外一个重载的方法，同时指定bean的id值和类型\n\n```java\nStudent student = iocContainer.getBean(\"student\",Student.class);\n```\n\n\n\n## 给bean的属性赋值\n\n###  依赖注入的方式\n\n使用的是set后面的名称忽略set后面首字母的大写\n\n![kqtI0A.png](https://s2.ax1x.com/2019/03/02/kqtI0A.png) \n\n#### Spring自动匹配合适的构造器\n\n```xml\n<bean id=\"book\" class=\"com.hph.helloworld.bean.Book\">\n    <constructor-arg value=\"10001\"></constructor-arg>\n    <constructor-arg value=\"Spring\"></constructor-arg>\n    <constructor-arg value=\"Rod Johnson\"></constructor-arg>\n    <constructor-arg value=\"2019.3\"></constructor-arg>\n</bean>\n```\n\n#### 通过索引值指定参数位置\n\n```xml\n    <bean id=\"book1\" class=\"com.hph.helloworld.bean.Book\">\n        <constructor-arg value=\"10002\" index=\"0\"></constructor-arg>\n        <constructor-arg value=\"Rod Johnson\" index=\"2\"></constructor-arg>\n        <constructor-arg value=\"Spring\" index=\"1\"></constructor-arg>\n        <constructor-arg value=\"2019.32\" index=\"3\"></constructor-arg>\n    </bean>\n```\n\n​     \n\n#### 通过类型区分重载的构造器\n\n```xml\n    <bean id=\"book2\" class=\"com.hph.helloworld.bean.Book\">\n        <constructor-arg value=\"10003\" index=\"0\" type=\"java.lang.Integer\"></constructor-arg>\n        <constructor-arg value=\"2019.3223\" index=\"3\" type=\"java.lang.Double\"></constructor-arg>\n        <constructor-arg value=\"Spring\" index=\"1\"></constructor-arg>\n        <constructor-arg value=\"Rod Johnson\" index=\"2\"></constructor-arg>\n    </bean>\n```\n\n创建Main方法\n\n```java\n public static void main(String[] args) {\n        ApplicationContext iocContainer =\n                new ClassPathXmlApplicationContext(\"helloworld.xml\");\n\n        Book book = iocContainer.getBean(\"book\", Book.class);\n        System.out.println(book);\n        Book book1 = iocContainer.getBean(\"book1\", Book.class);\n        System.out.println(book1);\n        Book book2 = iocContainer.getBean(\"book2\", Book.class);\n        System.out.println(book2);\n    }\n```\n\n结果\n\n![kqNcHs.png](https://s2.ax1x.com/2019/03/02/kqNcHs.png)\n\n### p名称空间\n\n为了简化XML文件的配置，越来越多的XML文件采用属性而非子元素配置信息。Spring从2.5版本开始引入了一个新的p命名空间，可以通过&lt;&lt;bean&gt;&gt;元素属性的方式配置Bean的属性。使用p命名空间后，基于XML的配置方式将进一步简化。\n\n```xml\n    <bean\n        id=\"bookp\"\n        class=\"com.hph.helloworld.bean.Book\"\n        p:id=\"10004\" p:bookName=\"spring_p\" p:author=\"Rod Johnson\" p:price=\"2019.33\"\n    ></bean>\n```\n\n### 可以使用的值\n\n- 可以使用字符串表示的值，可以通过value属性或value子节点的方式指定\n\n- 基本数据类型及其封装类、String等类型都可以采取字面值注入的方式\n\n- 若字面值中包含特殊字符，可以使用<![CDATA[]]>把字面值包裹起来\n\n#### null值\n\n```xml\n\n    <bean class=\"com.hph.helloworld.bean.Book\" id=\"bookNull\">\n        <property name=\"id\" value=\"100004\"></property>\n        <property name=\"bookName\">\n            <null></null>\n        </property>\n        <property name=\"author\" value=\"nullAuthhor\"></property>\n        <property name=\"price\" value=\"50\"></property>\n    </bean>\n\n```\n\n#### 级联属性赋值\n\n![kqWrFK.png](https://s2.ax1x.com/2019/03/03/kqWrFK.png)\n\n创建两个类结构如上\n\n配置xml文件\n\n```xml\n    <bean id=\"car\" class=\"com.hph.helloworld.bean.Car\">\n        <property name=\"brand\" value=\"奥迪\"></property>\n        <property name=\"price\" value=\"300000\"></property>\n        <property name=\"speed\" value=\"180\"></property>\n    </bean>\n    <bean id=\"person\" class=\"com.hph.helloworld.bean.Person\">\n        <property name=\"name\" value=\"张三\"></property>\n        <property name=\"age\" value=\"35\"></property>\n        <!--设置级联属性-->\n        <property name=\"car\" ref=\"car\"></property>\n    </bean>\n```\n\n#### 外部已声明的bean\n\n```xml\n  \t<!-- 外部已声明的bean-->\n    <bean id=\"person1\" class=\"com.hph.helloworld.bean.Person\">\n        <property name=\"car\" ref=\"car\"></property>\n    </bean>\n```\n\nBean就是一个对象\n\n```java\n    public static void main(String[] args) {\n        ApplicationContext iocContainer =\n                new ClassPathXmlApplicationContext(\"helloworld.xml\");\n\n        Person person1 = iocContainer.getBean(\"person1\", Person.class);\n        System.out.println(person1);\n    }\n```\n\n![kqfnfO.png](https://s2.ax1x.com/2019/03/03/kqfnfO.png)\n\n#### 内部bean\n\nbean实例仅仅给一个特定的属性使用时，可以将其声明为内部bean。内部bean声明直接包含在&lt;property&gt;或&lt;constructor-arg&gt;元素里，不需要设置任何id或name属性内部bean不能使用在任何其他地方\n\n## 集合属性\n\n### List\n\nSpring中可以通过一组内置的XML标签来配置集合属性，例如：&lt;list&gt;，&lt;set&gt;或&lt;map&gt;。\n\n配置java.util.List类型的属性，需要指定&lt;list&gt;标签，在标签里包含一些元素。这些标签   可以通过&lt;value&gt;指定简单的常量值，通过&lt;ref&gt;指定对其他Bean的引用。通过&lt;bean&gt;   指定内置bean定义。通过&lt;null/&gt;指定空元素。甚至可以内嵌其他集合。数组的定义和List一样，都使用&lt;list&gt;元素。\n\n 配置java.util.Set需要使用&lt;set&gt;标签，定义的方法与List一样。\n\n```xml\n <!--数组和List-->\n    <bean id=\"booktype\" class=\"com.hph.helloworld.bean.BookList\">\n        <property name=\"bookType\">\n            <!--字面量为值的List集合-->\n            <list>\n                <value>计算机</value>\n                <value>历史</value>\n            </list>\n        </property>\n        <property name=\"book\" ref=\"book\"></property>\n    </bean>\n```\n\n\n\n### Map\n\n​         Java.util.Map通过&lt;map&gt;标签定义，&lt;map&gt标签里可以使用多个&lt;entry&gt;作为子标签。每个条目包含一个键和一个值。  必须在&lt;key&gt;标签里定义键。因为键和值的类型没有限制，所以可以自由地为它们指定&lt;value&gt、&lt;ref&gt;、&lt;bean&gt或&lt;null/&gt;元素。​可以将Map的键和值作为&lt;entry&gt;的属性定义：简单常量使用key和value来定义；bean引用通过key-ref和value-ref属性定义。\n\n```xml\n    <bean id=\"bookMap\" class=\"com.hph.helloworld.bean.BookList\">\n        <property name=\"bookInfo\">\n            <map>\n                <entry value=\"38.8\">\n                    <key>\n                        <value>机械工鞋出版社</value>\n                    </key>\n                </entry>\n            </map>\n        </property>\n    </bean>\n```\n\n### 集合类型的bean\n\n​         如果只能将集合对象配置在某个bean内部，则这个集合的配置将不能重用。我们需要将集合bean的配置拿到外面，供其他bean引用。配置集合类型的bean需要引入util名称空间\n\n```xml\n    <util:list id=\"booklist\">\n        <ref bean=\"book\"></ref>\n        <ref bean=\"book1\"></ref>\n        <ref bean=\"book2\"></ref>\n    </util:list>\n\n    <util:list id=\"typeList\">\n        <value>编程</value>\n        <value>极客时间</value>\n        <value>历史</value>\n    </util:list>\n```\n\n## FactoryBean\n\nSpring中有两种类型的bean，一种是普通bean，另一种是工厂bean，即FactoryBean。\n\n​         工厂bean跟普通bean不同，其返回的对象不是指定类的一个实例，其返回的是该工厂bean的getObject方法所返回的对象。工厂bean必须实现org.springframework.beans.factory.FactoryBean接口。\n\n![kqLsYj.png](https://s2.ax1x.com/2019/03/03/kqLsYj.png)\n\n```java\nimport org.springframework.beans.factory.FactoryBean;\n\npublic class BookFactory implements FactoryBean<BookFactory> {\n\n    private String name;\n\n    public String getName() {\n        return name;\n    }\n\n    public void setName(String name) {\n        this.name = name;\n    }\n\n\n    @Override\n    public BookFactory getObject() throws Exception {\n        BookFactory bean = new BookFactory();\n        bean.setName(\"工厂模式\");\n        return bean;\n    }\n//返回类型\n    @Override\n    public Class<?> getObjectType() {\n        return BookFactory.class;\n    }\n    //返回由FactoryBean创建的bean实例，如果isSingleton()返回true，则该实例会放到Spring容器中单实例缓存池中。\n    @Override\n    public boolean isSingleton() {\n        return false;\n    }\n}\n```\n\nXML文件配置\n\n```xml\n<bean id=\"bookproduct\" class=\"com.hph.helloworld.bean.BookFactory\">\n        <property name=\"name\" value=\"工厂模式\"></property>\n</bean>\n```\n\n##  配置信息的继承\n\nSpring允许继承bean的配置，被继承的bean称为父bean。继承这个父bean的bean称为子bean子bean从父bean中继承配置，包括bean的属性配置子bean也可以覆盖从父bean继承过来的配置\n\n```xml\n <bean id=\"dept\" class=\"com.hph.helloworld.bean.Department\">\n        <property name=\"deptId\" value=\"10010\"></property>\n        <property name=\"deptName\" value=\"Web\"></property>\n    </bean>\n    <bean id=\"emp01\" class=\"com.hph.helloworld.bean.Employee\">\n        <property name=\"empId\" value=\"1001\"></property>\n        <property name=\"empName\" value=\"Jerry\"></property>\n        <property name=\"age\" value=\"20\"></property>\n    </bean>\n    <!-- 以emp01作为父bean，继承后可以省略公共属性值的配置 -->\n    <bean id=\"emp02\" parent=\"emp01\">\n        <property name=\"empId\" value=\"1002\"/>\n        <property name=\"empName\" value=\"Jerry\"/>\n        <property name=\"age\" value=\"25\"/>\n    </bean>\n```\n\n**注意:**  父bean可以作为配置模板，也可以作为bean实例。若只想把父bean作为模板，可以设置&lt;bean&gt;的abstract 属性为true，这样Spring将不会实例化这个bean如果一个bean的class属性没有指定，则必须是抽象bean 并不是&lt;bean&gt;元素里的所有属性都会被继承。比如：autowire，abstract等。也可以忽略父bean的class属性，让子bean指定自己的类，而共享相同的属性配置。    但此时abstract必须设为true。\n\n## bean的作用域(重要)\n\n​         在Spring中，可以在&lt;bean&gt;元素的scope属性里设置bean的作用域，以决定这个bean是单实例的还是多实例的。\n\n​         默认情况下，Spring只为每个在IOC容器里声明的bean创建唯一一个实例，整个IOC容器范围内都能共享该实例：所有后续的getBean()调用和bean引用都将返回这个唯一的bean实例。该作用域被称为singleton，它是所有bean的默认作用域。\n\n![kLpARe.png](https://s2.ax1x.com/2019/03/03/kLpARe.png)\n\n​         当bean的作用域为单例时，Spring会在IOC容器对象创建时就创建bean的对象实例。而当bean的作用域为prototype时，IOC容器在获取bean的实例时创建bean的实例对象。\n\n## bean的生命周期\n\nSpring IOC容器可以管理bean的生命周期，Spring允许在bean生命周期内特定的时间点执行指定的任务。\n\nSpring IOC容器对bean的生命周期进行管理的过程：\n\n​         1.通过构造器或工厂方法创建bean实例\n\n​         2. 为bean的属性设置值和对其他bean的引用\n\n​         3.调用bean的初始化方法\n\n​         4.bean可以使用了\n\n​         6.当容器关闭时，调用bean的销毁方法\n\n 在配置bean时，通过init-method和destroy-method 属性为bean指定初始化和销毁方法\n\n bean的后置处理器:\n\n​     bean后置处理器允许在调用**初始化方法前后**对bean进行额外的处理\n\n​     bean后置处理器对IOC容器里的所有bean实例逐一处理，而非单一实例。其典型应用是：检查bean属性的正确性或根据特定的标准更改bean的属性。\n\n​    bean后置处理器时需要实现接口：\n\norg.springframework.beans.factory.config.BeanPostProcessor。在初始化方法被调用前后，Spring将把每个bean实例分别传递给上述接口的以下两个方法：\n\npostProcessBeforeInitialization(Object, String)\n\npostProcessAfterInitialization(Object, String)\n\n 添加bean后置处理器后bean的生命周期\n\n​         1.通过构造器或工厂方法**创建**bean**实例**\n\n​         2.为bean的**属性设置值**和对其他bean的引用\n\n​         3.将bean实例传递给bean后置处理器的postProcessBeforeInitialization()方法\n\n​         4.调用bean的**初始化**方法\n\n​         5.将bean实例传递给bean后置处理器的postProcessAfterInitialization()方法\n\n​         6.bean可以使用了\n\n​         7.当容器关闭时调用bean的**销毁方法**\n\n## 引用外部属性文件\n\n​         当bean的配置信息逐渐增多时，查找和修改一些bean的配置信息就变得愈加困难。这时可以将一部分信息提取到bean配置文件的外部，以properties格式的属性文件保存起来，同时在bean的配置文件中引用properties属性文件中的内容，从而实现一部分属性值在发生变化时仅修改properties属性文件即可。这种技术多用于连接数据库的基本信息的配置。\n\njdbc.properties\n\n```xml\njdbc.username=root\njdbc.password=123456\njdbc.url=jdbc:mysql://localhost:3306/bigdata\njdbc.driver=com.mysql.jdbc.Driver\n```\n\nXML文件配置\n\n```xml\n    <context:property-placeholder location=\"classpath:jdbc.properties\"/>\n    <bean id=\"dataSource\" class=\"com.mchange.v2.c3p0.ComboPooledDataSource\">\n        <property name=\"user\" value=\"${jdbc.username}\"/>\n        <property name=\"password\" value=\"${jdbc.password}\"/>\n        <property name=\"jdbcUrl\" value=\"${jdbc.url}\"/>\n        <property name=\"driverClass\" value=\"${jdbc.driver}\"/>\n    </bean>\n```\n\n```java\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\nimport javax.sql.DataSource;\nimport java.sql.SQLException;\n\npublic class TestDataSource {\n        public static void main(String[] args) {\n            ClassPathXmlApplicationContext iocContainer = new ClassPathXmlApplicationContext(\"helloworld.xml\");\n            DataSource ds = iocContainer.getBean(\"dataSource\",DataSource.class);\n            System.out.println(\"ds:\"+ds);\n            try {\n                System.out.println(ds.getConnection());\n            } catch (SQLException e) {\n                e.printStackTrace();\n            }\n\n        }\n}\n```\n\n**注意: ** 要导入相关的jar包和Mysql的驱动.\n\n##  自动装配\n\n手动装配：以value或ref的方式**明确指定属性值**都是手动装配。\n\n自动装配：根据指定的装配规则，**不需要明确指定**，Spring**自动**将匹配的属性值**注入**bean中。\n\n### 装配模式\n\n根据**类型**自动装配：将类型匹配的bean作为属性注入到另一个bean中。若IOC容器中有多个与目标bean类型一致的bean，Spring将无法判定哪个bean最合适该属性，所以不能执行自动装配\n\n根据**名称**自动装配：必须将目标bean的名称和属性名设置的完全相同\n\n通过构造器自动装配：当bean中存在多个构造器时，此种自动装配方式将会很复杂。不推荐使用。\n\n### 注解配置bean\n\n相对于XML方式而言，通过注解的方式配置bean更加简洁和优雅，而且和MVC组件化开发的理念十分契合，是开发中常用的使用方式(推荐使用)。\n\n### 注解标识组件\n\n| 组件             | 注解        | 功能                                           |\n| ---------------- | ----------- | ---------------------------------------------- |\n| 普通组件         | @Component  | 标识一个受Spring IOC容器管理的组件             |\n| 持久化层组件     | @Repository | 标识一个受Spring IOC容器管理的持久化层组件     |\n| 业务逻辑层组件   | @Service    | 标识一个受Spring IOC容器管理的业务逻辑层组件   |\n| 表述层控制器组件 | @Controller | 标识一个受Spring IOC容器管理的表述层控制器组件 |\n\n\n#### 组件命名规则\n\n- 默认情况：使用组件的简单类名首字母小写后得到的字符串作为bean的id\n- 用组件注解的value属性指定bean的id\n\n注意:事实上Spring并没有能力识别一个组件到底是不是它所标记的类型，即使将 @Respository注解用在一个表述层控制器组件上面也不会产生任何错误，所以@Respository、@Service、@Controller这几个注解仅仅是为了让开发人员自己明确当前的组件扮演的角色。\n\n## 扫描组件\n\n 组件被上述注解标识后还需要通过Spring进行扫描才能够侦测到。\n\n```xml\n<context:component-scan base-package=\"com.hph.component\"></context:component-scan>\n```\n\n**base-package**属性指定一个需要扫描的基类包，Spring容器将会扫描这个基类包及其子包中的所有类。\n\n当需要扫描多个包时可以使用逗号分隔。\n\n如果仅希望扫描特定的类而非基包下的所有类，可使用resource-pattern属性过滤特定的类，示例：\n\n```xml\n<context:component-scan \n\tbase-package=\"com.hph.component\" \n\tresource-pattern=\"autowire/*.class\"/>\n```\n\n包含与排除:<context:include-filter>子节点表示要包含的目标类\n\n注意：通常需要与use-default-filters属性配合使用才能够达到“仅包含某些组件”这样的效果。即：通过将use-default-filters属性设置为false，禁用默认过滤器，然后扫描的就只是include-filter中的规则指定的组件了。&lt;ontext:exclude-filter&gt;子节点表示要排除在外的目标类.component-scan下可以拥有若干个include-filter和exclude-filter子节点。\n\n过滤表达式\n\n| 类别       | 示例                  | 说明                                                         |\n| :--------- | --------------------- | ------------------------------------------------------------ |\n| annotation | com.hph.XxxAnnotation | 过滤所有标注了XxxAnnotation的类。这个规则根据目标组件是否标注了指定类型的注解进行过滤。 |\n| assignable | com.hph.BaseXxx       | 过滤所有BaseXxx类的子类。这个规则根据目标组件是否是指定类型的子类的方式进行过滤。 |\n| aspectj    | com.hph.*Service+     | 所有类名是以Service结束的，或这样的类的子类。这个规则根据AspectJ表达式进行过滤。 |\n| regex      | com\\.hph\\.anno\\.*     | 所有com.hph.anno包下的类。这个规则根据正则表达式匹配到的类名进行过滤。 |\n| custom     | com.hph.XxxTypeFilter | 使用XxxTypeFilter类通过编码的方式自定义过滤规则。该类必须实现org.springframework.core.type.filter.TypeFilter接口 |\n\n##  组件装配\n\n 需求：Controller组件中往往需要用到Service组件的实例，Service组件中往往需要用到       Repository组件的实例。Spring可以通过注解的方式帮我们实现属性的装配。\n\n实现依据：在指定要扫描的包时，&lt;context:component-scan&gt; 元素会自动注册一个bean的后置处     理器：AutowiredAnnotationBeanPostProcessor的实例。该后置处理器可以自动装配标记了**@Autowired**、@Resource或@Inject注解的属性。\n\n###  @Autowired注解\n\n- 根据类型实现自动装配。\n- 构造器、普通字段(即使是非public)、一切具有参数的方法都可以应用\n- @Autowired注解默认情况下，所有使用@Autowired注解的属性都需要被设置。当Spring找不到匹配的bean装配属性时，会抛出异常。\n- 若某一属性允许不被设置，可以设置@Autowired注解的required属性为 false\n- 默认情况下，当IOC容器里存在多个类型兼容的bean时，Spring会尝试匹配bean 的id值是否与变量名相同，如果相同则进行装配。如果bean的id值不相同，通过类型的自动装配将无法工作。此时可以在@Qualifier注解里提供bean的名称。Spring    甚至允许在方法的形参上标注@Qualifiter注解以指定注入bean的名称。\n- @Autowired注解也可以应用在数组类型的属性上，此时Spring将会把所有匹配的bean进行自动装配。\n- @Autowired注解也可以应用在集合属性上，此时Spring读取该集合的类型信息，然后自动装配所有与之兼容的bean。\n- @Autowired注解用在java.util.Map上时，若该Map的键值为String，那么 Spring将自动装配与值类型兼容的bean作为值，并以bean的id值作为键。\n\n### @Resource\n\n @Resource注解要求提供一个bean名称的属性，若该属性为空，则自动采用标注处的变量或方法名作为bean的名称。\n\n### @Inject\n\n@Inject和@Autowired注解一样也是按类型注入匹配的bean，但没有reqired属性。\n\n\n\n\n\n","tags":["Spring"],"categories":["JavaWeb"]},{"title":"Spring概述","url":"/2019/03/02/Spring概述/","content":"\n {{ \"Spring的基本概述和开发\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n​       Spring是一个开源框架, Spring为简化企业级开发而生，使用Spring，JavaBean就可以实现很多以前要靠EJB才能实现的功能。同样的功能，在EJB中要通过繁琐的配置和复杂的代码才能够实现，而在Spring中却非常的优雅和简洁。   Spring是一个**IOC**(DI)和**AOP**容器框架。有着优良的特性:\n\n\n\n| 特性         | 介绍                                                         |\n| ------------ | ------------------------------------------------------------ |\n| 非侵入式     | 基于Spring开发的应用中的对象可以不依赖于Spring的API          |\n| 依赖注入     | DI——Dependency<br/>Injection，反转控制(IOC)最经典的实现。    |\n| 面向切面编程 | Aspect<br/>Oriented Programming——AOP                         |\n| 容器         | Spring是一个容器，因为它包含并且管理应用对象的生命周期       |\n| 组件化       | Spring实现了使用简单的组件配置组合成一个复杂的应用。在 Spring 中可以使用XML和Java注解组合这些对象。 |\n| 一站式       | 在IOC和AOP的基础上可以整合各种企业应用的开源框架和优秀的第三方类库（实际上Spring 自身也提供了表述层的SpringMVC和持久层的Spring JDBC）。 |\n\n\n\n##    Spring模块\n\n![kqlqrq.png](https://s2.ax1x.com/2019/03/02/kqlqrq.png)\n\n**核心容器（Spring Core）**\n\n　　核心容器提供Spring框架的基本功能。Spring以bean的方式组织和管理Java应用中的各个组件及其关系。Spring使用BeanFactory来产生和管理Bean，它是工厂模式的实现。BeanFactory使用控制反转(IoC)模式将应用的配置和依赖性规范与实际的应用程序代码分开。\n\n**应用上下文（Spring Context）**\n\n　　Spring上下文是一个配置文件，向Spring框架提供上下文信息。Spring上下文包括企业服务，如JNDI、EJB、电子邮件、国际化、校验和调度功能。\n\n**Spring面向切面编程（Spring AOP）**\n\n　　通过配置管理特性，Spring AOP 模块直接将面向方面的编程功能集成到了 Spring框架中。所以，可以很容易地使 Spring框架管理的任何对象支持 AOP。Spring AOP 模块为基于 Spring 的应用程序中的对象提供了事务管理服务。通过使用 Spring AOP，不用依赖 EJB 组件，就可以将声明性事务管理集成到应用程序中。\n\n**JDBC和DAO模块（Spring DAO）**\n\n　　JDBC、DAO的抽象层提供了有意义的异常层次结构，可用该结构来管理异常处理，和不同数据库供应商所抛出的错误信息。异常层次结构简化了错误处理，并且极大的降低了需要编写的代码数量，比如打开和关闭链接。\n\n**对象实体映射（Spring ORM）**\n\n　　Spring框架插入了若干个ORM框架，从而提供了ORM对象的关系工具，其中包括了Hibernate、JDO和 IBatis SQL Map等，所有这些都遵从Spring的通用事物和DAO异常层次结构。\n\n**Web模块（Spring Web）**\n\n　　Web上下文模块建立在应用程序上下文模块之上，为基于web的应用程序提供了上下文。所以Spring框架支持与Struts集成，web模块还简化了处理多部分请求以及将请求参数绑定到域对象的工作。\n\n**MVC模块（Spring Web MVC）**\n\n　　MVC框架是一个全功能的构建Web应用程序的MVC实现。通过策略接口，MVC框架变成为高度可配置的。MVC容纳了大量视图技术，其中包括JSP、POI等，模型来有JavaBean来构成，存放于m当中，而视图是一个街口，负责实现模型，控制器表示逻辑代码，由c的事情。Spring框架的功能可以用在任何J2EE服务器当中，大多数功能也适用于不受管理的环境。Spring的核心要点就是支持不绑定到特定J2EE服务的可重用业务和数据的访问的对象，毫无疑问这样的对象可以在不同的J2EE环境，独立应用程序和测试环境之间重用。\n\n## 开发工具\n\n可以使用STS  IDEA ECLIPSE 等 个人比较喜欢IDEA \n\n## 案例\n\n![kq1soT.md.png](https://s2.ax1x.com/2019/03/02/kq1soT.md.png)\n\n![kq1Wl9.png](https://s2.ax1x.com/2019/03/02/kq1Wl9.png)\n\n等待下载\n\n![kq1Iw6.png](https://s2.ax1x.com/2019/03/02/kq1Iw6.png)\n\n下载完后的目录结构IDE可以帮助我们把需要的jar包下载下来\n\n![kq1jOI.png](https://s2.ax1x.com/2019/03/02/kq1jOI.png)\n\n创建一个类结构如下\n\n![kq36Bt.png](https://s2.ax1x.com/2019/03/02/kq36Bt.png)\n\n\n\n配置\n\n```xml\n    <!-- 使用bean元素定义一个由IOC容器创建的对象 -->\n    <!-- class属性指定用于创建bean的全类名 -->\n    <!-- id属性指定用于引用bean实例的标识 -->\n    <bean id=\"student\" class=\"com.hph.helloworld.bean.Student\">\n       <!--使用property子元素为bean属性赋值-->\n    <property name=\"id\" value=\"1001\"></property>\n    <property name=\"name\" value=\"Spring\"></property>\n    <property name=\"age\" value=\"18\"></property>\n    </bean>\n```\n\n创建Main方法\n\n```java\nimport org.springframework.context.ApplicationContext;\nimport org.springframework.context.support.ClassPathXmlApplicationContext;\n\n\npublic class Main {\n    public static void main(String[] args) {\n        //1.创建IOC容器对象\n        ApplicationContext iocContainer =\n                new ClassPathXmlApplicationContext(\"helloworld.xml\");\n\t\t//2.根据id值获取bean实例对象\n        Student student = (Student) iocContainer.getBean(\"student\");\n\t\t//3.打印bean\n        System.out.println(student);\n\n    }\n}\n```\n\n运行结果\n\n![kq8k4O.png](https://s2.ax1x.com/2019/03/02/kq8k4O.png)\n\n\n\n","tags":["Spring"],"categories":["JavaWeb"]},{"title":"Hive调优","url":"/2019/01/18/Hive调优/","content":"\n {{ \"Hive存储格式选择 和Hive 相关优化\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n[压缩参考](http://hphblog.cn/2018/12/20/Hadoop%E7%9A%84IO%E6%93%8D%E4%BD%9C/#%E5%8E%8B%E7%BC%A9)\n\nHive支持的存储数的格式主要有：TEXTFILE 、SEQUENCEFILE、ORC、PARQUET。\n\n## 文件存储格式\n\n列式存储和行式存储\n\n![k9kwSf.png](https://s2.ax1x.com/2019/01/18/k9kwSf.png)\n\n行存储的特点:查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。\n\n列存储的特点:因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。\n\n**TEXTFILE和SEQUENCEFILE的存储格式都是基于行存储的；ORC和PARQUET是基于列式存储的。**\n\n### TextFile格式\n\n默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用，但使用Gzip这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。\n\n### Orc格式\n\nOrc (Optimized Row Columnar)是Hive 0.11版里引入的新的存储格式。\n\n每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB->250MB，这样应该能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data，Row Data，Stripe Footer：\n\n![k9ASne.png](https://s2.ax1x.com/2019/01/18/k9ASne.png)\n\n Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引应该只是记录某行的各字段在Row Data中的offset。\n\nRow Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。\n\n Stripe Footer：存的是各个Stream的类型，长度等信息。\n\n每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。\n\n### Parquet格式\n\nParquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。\n\nParquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。\n\n通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。\n\n![k9A97d.png](https://s2.ax1x.com/2019/01/18/k9A97d.png)\n\nParquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。\n\n**在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy，lzo。 **\n\n## 调优\n\n### Fetch抓取\n\nFetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。\n\n在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。\n\nhive.fetch.task.conversion设置成more，然后执行查询语句，如下查询方式都不会执行mapreduce程序。\n\n### 本地模式\n\n大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务消耗的时间可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。\n\n用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。\n\n```shell\n##开启本地mr\nset hive.exec.mode.local.auto=true; \n##设置local mr的最大输入数据量，当输入数据量小于这个值时采用local  mr的方式，默认为134217728，即128M\nset hive.exec.mode.local.auto.inputbytes.max=50000000;\n##设置local mr的最大输入文件个数，当输入文件个数小于这个值时采用local mr的方式，默认为4\nset hive.exec.mode.local.auto.input.files.max=10;\n```\n\n```sql\n--开启花费时间\n set hive.exec.mode.local.auto=true; \n select * from emp cluster by deptno;\nTime taken: 1.768 seconds, Fetched: 14 row(s)\n\n--不开启花费时间\n set hive.exec.mode.local.auto=false; \n select * from emp cluster by deptno;\nTime taken: 41.728 seconds, Fetched: 14 row(s)\n```\n\n### 表的调优\n\n####  小表、大表Join\n\n将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用map join让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。\n\n实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。\n\n```sql\n--创建大表\ncreate table bigtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--创建小表\ncreate table smalltable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--创建join后表的语句\ncreate table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--导入数据\nload data local inpath '/opt/module/datas/bigtable' into table bigtable;\nload data local inpath '/opt/module/datas/smalltable' into table smalltable;\n\n--关闭mapjoin功能（默认是打开的）\nset hive.auto.convert.join = false;\n\n--执行小表JOIN大表语句\ninsert overwrite table jointable\nselect b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\nleft join bigtable  b\non b.id = s.id;\nMapReduce Total cumulative CPU time: 37 seconds 840 msec\nTime taken: 79.018 seconds\n\n--执行大表JOIN小表语句\ninsert overwrite table jointable\nselect b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable  b\nleft join smalltable  s\non s.id = b.id;\n\nMapReduce Total cumulative CPU time: 38 seconds 700 msec\nTime taken: 77.68 seconds\n```\n\n#### 大表Join大表\n\n##### 空KEY过滤\n\n有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空，操作如下：\n\n```shell\n##启动jobhistory\n[hadoop@datanode1 ~]$ sbin/mr-jobhistory-daemon.sh start historyserver\n```\n\n```sql\n--创建原始表\ncreate table ori(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--创建空id表\ncreate table nullidtable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--创建join后表的语句\ncreate table jointable(id bigint, time bigint, uid string, keyword string, url_rank int, click_num int, click_url string) row format delimited fields terminated by '\\t';\n\n--加载数据\nload data local inpath '/opt/module/datas/ori' into table ori;\nload data local inpath '/opt/module/datas/nullid' into table nullidtable;\n\n--测试不过滤空id\ninsert overwrite table jointable select n.* from nullidtable n left join ori o on n.id = o.id;\n\nStage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 40.44 sec   HDFS Read: 248162775 HDFS Write: 122733608 SUCCESS\nTime taken: 91.165 seconds\n\n--测试过滤空id\n\n\nStage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 32.52 sec   HDFS Read: 248165536 HDFS Write: 25445558 SUCCESS\nTime taken: 76.158 seconds\n```\n\n##### 空key转换\n\n有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。\n\n##### 不设置空值\n\n```sql\ninsert overwrite table jointable\nselect n.* from nullidtable n left join ori b on n.id = b.id;\n```\n\n这里发生了数据偏斜：某些reducer的资源消耗远大于其他reduce。\n\n![k9Yh38.png](https://s2.ax1x.com/2019/01/18/k9Yh38.png)\n\n随机分布空null值\n\n```sql\n--设置5个reduce个数\nset mapreduce.job.reduces = 5;\n\n--JOIN两张表\ninsert overwrite table jointable\nselect n.* from nullidtable n full join ori o on \ncase when n.id is null then concat('hive', rand()) else n.id end = o.id;\n```\n\n![k9NEon.png](https://s2.ax1x.com/2019/01/18/k9NEon.png)\n\n我们可以看出总体平均消耗的资源差不多，因此，消除了数据倾斜，负载均衡reducer的资源消耗。\n\n#### MapJoin\n\n如果不指定MapJoin或者不符合MapJoin的条件，那么Hive解析器会将Join操作转换成Common Join，即：在Reduce阶段完成join。容易发生数据倾斜。可以用MapJoin把小表全部加载到内存在map端进行join，避免reducer处理。\n\n```sql\n--开启MapJoin参数设置\n--设置自动选择Mapjoin默认为true\nset hive.auto.convert.join = true; \n\n大表小表的阈值设置（默认25M一下认为是小表）\nset hive.mapjoin.smalltable.filesize=25000000;\n```\n\n![k9NJF1.png](https://s2.ax1x.com/2019/01/18/k9NJF1.png)\n\n```sql\n--执行小表JOIN大表语句\ninsert overwrite table jointable\nselect b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom smalltable s\njoin bigtable  b\non s.id = b.id;\n\nTime taken: 49.829 seconds\n\n--执行大表JOIN小表语句\ninsert overwrite table jointable\nselect b.id, b.time, b.uid, b.keyword, b.url_rank, b.click_num, b.click_url\nfrom bigtable  b\njoin smalltable  s\non s.id = b.id;\nTime taken: 48.565 seconds\n```\n\n####  Group By\n\n默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。\n\n并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。\n\n```sql\n--是否在Map端进行聚合，默认为True\nhive.map.aggr = true\n\n--在Map端进行聚合操作的条目数目\nhive.groupby.mapaggr.checkinterval = 100000\n\n--有数据倾斜的时候进行负载均衡（默认是false）\nhive.groupby.skewindata = true\n```\n\n当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。\n\n#### Count(Distinct) 去重统计\n\n数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换：\n\n```sql\n--创建一张大表\n create table bigtable(id bigint, time bigint, uid string, keyword\nstring, url_rank int, click_num int, click_url string) row format delimited\nfields terminated by '\\t';\n\n--加载数据\nload data local inpath '/opt/module/datas/bigtable' into table  bigtable;\n\n--设置5个reduce个数\nset mapreduce.job.reduces = 5;\n\n--执行去重id查询\n select count(distinct id) from bigtable;\n \nMapReduce Jobs Launched:\nStage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 13.53 sec   HDFS Read: 129164386 HDFS Write: 6 SUCCESS\nTotal MapReduce CPU Time Spent: 13 seconds 530 msec\nOK\n99947\nTime taken: 50.453 seconds, Fetched: 1 row(s)\n\n--采用GROUP by去重id\nStage-Stage-1: Map: 1  Reduce: 5   Cumulative CPU: 27.89 sec   HDFS Read: 129175319 HDFS Write: 580 SUCCESS\nStage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 7.83 sec   HDFS Read: 7729 HDFS Write: 6 SUCCESS\nTotal MapReduce CPU Time Spent: 35 seconds 720 msec\nOK\n99947\nTime taken: 110.944 seconds, Fetched: 1 row(s)\n```\n\n虽然会多用一个Job来完成，但在数据量大的情况下，这个绝对是值得。\n\n#### 笛卡尔积\n\n尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积。\n\n#### 行列过滤\n\n列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。\n\n行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤，比如：\n\n```sql\n--测试先关联两张表，再用where条件过滤\n select o.id from bigtable b\njoin ori o on o.id = b.id\nwhere o.id <= 10;\nTime taken: 68.819 seconds, Fetched: 100 row(s)\n\n--通过子查询后，再关联表\nselect b.id from bigtable b\njoin (select id from ori where id <= 10 ) o on b.id = o.id;\n\nTime taken: 66.287 seconds, Fetched: 100 row(s)\n```\n\n#### 动态分区调整\n\n关系型数据库中，对分区表Insert数据时候，数据库自动会根据分区字段的值，将数据插入到相应的分区中，Hive中也提供了类似的机制，即动态分区(Dynamic Partition)，只不过，使用Hive的动态分区，需要进行相应的配置。\n\n```sql\n--开启动态分区功能（默认true，开启）\nhive.exec.dynamic.partition=true\n\n--设置为非严格模式（动态分区的模式，默认strict，表示必须指定至少一个分区为静态分区，nonstrict模式表示允许所有的分区字段都可以使用动态分区。）\nhive.exec.dynamic.partition.mode=nonstrict\n\n--在所有执行MR的节点上，最大一共可以创建多少个动态分区。\nhive.exec.max.dynamic.partitions=1000\n\n--在每个执行MR的节点上，最大可以创建多少个动态分区。该参数需要根据实际的数据来设定。比如：源数据中包含了一年的数据，即day字段有365个值，那么该参数就需要设置成大于365，如果使用默认值100，则会报错。\nhive.exec.max.dynamic.partitions.pernode=100\n\n--整个MR Job中，最大可以创建多少个HDFS文件。\nhive.exec.max.created.files=100000\n\n--当有空分区生成时，是否抛出异常。一般不需要设置。\nhive.error.on.empty.partition=false\n\n--创建分区表\ncreate table ori_partitioned(id bigint, time bigint, uid string, keyword string,\n url_rank int, click_num int, click_url string) \npartitioned by (p_time bigint) \nrow format delimited fields terminated by '\\t';\n\n--加载数据到分区表中\nload data local inpath '/opt/module/datas/ds1' into table\n ori_partitioned partition(p_time='201901180000010');\n load data local inpath '/opt/module/datas/ds2' into table ori_partitioned partition(p_time='201901180000011');\n\n--创建目标分区表\ncreate table ori_partitioned_target(id bigint, time bigint, uid string,\nkeyword string, url_rank int, click_num int, click_url string) PARTITIONED BY (p_time STRING) row format delimited fields terminated by '\\t';\n\n--设置动态分区\nset hive.exec.dynamic.partition = true;\nset hive.exec.dynamic.partition.mode = nonstrict;\nset hive.exec.max.dynamic.partitions = 1000;\nset hive.exec.max.dynamic.partitions.pernode = 100;\nset hive.exec.max.created.files = 100000;\nset hive.error.on.empty.partition = false;\n\ninsert overwrite table ori_partitioned_target partition (p_time) \nselect id, time, uid, keyword, url_rank, click_num, click_url, p_time from ori_partitioned;\n\n--查看目标分区表的分区情况\nshow partitions ori_partitioned_target;\nOK\np_time=201901180000010\np_time=201901180000011\n```\n\n#### 分桶\n\n[桶表相关](http://hphblog.cn/2019/01/17/Hive查询/#桶表2)\n\n#### 分区\n\n[分区表相关](http://hphblog.cn/2019/01/16/Hive%E6%95%B0%E6%8D%AE%E6%8D%AE%E7%B1%BB%E5%9E%8B/#%E5%88%86%E5%8C%BA%E8%A1%A8)\n\n#### 数据倾斜\n\n##### 合理设置Map数\n\n1.通常情况下，作业会通过input的目录产生一个或者多个map任务。主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。\n\n2.是不是map数越多越好？\n\n答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。\n\n3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？\n\n答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。\n\n针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数；\n\n##### 小文件进行合并\n\n在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。\n\n```shell\nset hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;\n```\n\n##### 复杂文件增加Map数\n\n当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。\n\n增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。\t\n\n```sql\n--执行查询\nselect count(*) from emp;\nStage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.98 sec   HDFS Read: 7861 HDFS Write: 3 SUCCESS\n\n--设置最大切片值为100个字节\n set mapreduce.input.fileinputformat.split.maxsize=100;\n Hadoop job information for Stage-1: number of mappers: 6; number of reducers: 1\n```\n\n##### 合理设置Reduce数\n\n调整reduce个数方法一\n\n```sql\n--每个Reduce处理的数据量默认是256MB\nhive.exec.reducers.bytes.per.reducer=256000000\n\n--每个任务最大的reduce数，默认为1009\nhive.exec.reducers.max=1009\n\n--计算reducer数的公式\nN=min(参数2，总输入数据量/参数1)\n```\n\n调整reduce个数方法二\n\n```sql\n--在hadoop的mapred-default.xml文件中修改\n--设置每个job的Reduce个数\nset mapreduce.job.reduces = 15;\n```\n\nreduce个数并不是越多越好?\n1.过多的启动和初始化reduce也会消耗时间和资源；\n2.另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；\n在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适；\n\n#### 并行执行\n\nHive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。\n\n 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。\n\n```sql\nset hive.exec.parallel=true;              --打开任务并行执行\nset hive.exec.parallel.thread.number=16; --同一个sql允许最大并行度，默认为8。\n```\n\n当然，得是在系统资源比较空闲的时候才有优势，否则，没资源，并行也起不来。\n\n####  严格模式\n\nHive提供了一个严格模式，可以防止用户执行那些可能意向不到的不好的影响的查询。通过设置属性hive.mapred.mode值为默认是非严格模式nonstrict 。开启严格模式需要修改hive.mapred.mode值为strict，开启严格模式可以禁止3种类型的查询。\n\n```xml\n<property>\n    <name>hive.mapred.mode</name>\n    <value>strict</value>\n<description>\n      The mode in which the Hive operations are being performed. \n      In strict mode, some risky queries are not allowed to run. They include:\n        Cartesian Product.\n        No partition being picked up for a query.\n        Comparing bigints and strings.\n        Comparing bigints and doubles.\n        Orderby without limit.\n</description>\n</property>\n```\n\n对于分区表，除非where语句中含有分区字段过滤条件来限制范围，否则不允许执行。换句话说，就是用户不允许扫描所有分区。进行这个限制的原因是，通常分区表都拥有非常大的数据集，而且数据增加迅速。没有进行分区限制的查询可能会消耗令人不可接受的巨大资源来处理这个表。\n\n对于使用了order by语句的查询，要求必须使用limit语句。因为order by为了执行排序过程会将所有的结果数据分发到同一个Reducer中进行处理，强制要求用户增加这个LIMIT语句可以防止Reducer额外执行很长一段时间。\n\n限制笛卡尔积的查询。对关系型数据库非常了解的用户可能期望在执行JOIN查询的时候不使用ON语句而是使用where语句，这样关系数据库的执行优化器就可以高效地将WHERE语句转化成那个ON语句。不幸的是，Hive并不会执行这种优化，因此，如果表足够大，那么这个查询就会出现不可控的情况。\n\n#### JVM重用\n\nJVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。\n\nHadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的mapred-site.xml文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。\n\n```xml\n<property>\n  <name>mapreduce.job.jvm.numtasks</name>\n  <value>10</value>\n  <description>How many tasks to run per jvm. If set to -1, there is  dsno limit. \n  </description>\n</property>\n```\n\n缺点:开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。\n\n#### 推测执行\n\n在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。\n\n```xml\n<property>\n  <name>mapreduce.map.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some map tasks \n               may be executed in parallel.</description>\n</property>\n\n<property>\n  <name>mapreduce.reduce.speculative</name>\n  <value>true</value>\n  <description>If true, then multiple instances of some reduce tasks \n               may be executed in parallel.</description>\n</property>\n\nhive本身也提供了配置项来控制reduce-side的推测执行：\n\n  <property>\n    <name>hive.mapred.reduce.tasks.speculative.execution</name>\n    <value>true</value>\n    <description>Whether speculative execution for reducers should be turned on. </description>\n  </property>\n```\n\n关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。\n\n#### 压缩\n\n[压缩相关资料](http://hphblog.cn/2018/12/20/Hadoop%E7%9A%84IO%E6%93%8D%E4%BD%9C/#%E5%8E%8B%E7%BC%A9)\n\n#### 执行计划（Explain）\n\n```sql\n--语法\nEXPLAIN [EXTENDED | DEPENDENCY | AUTHORIZATION] query\n\n--查看下面这条语句的执行计划\n explain select * from emp;\n explain select deptno, avg(sal) avg_sal from emp group by deptno;\n \n --查看详细执行计划\n explain extended select * from emp;\n explain extended select deptno, avg(sal) avg_sal from emp group by deptno;\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hive"],"categories":["大数据"]},{"title":"Hive查询","url":"/2019/01/17/Hive查询/","content":"\n {{ \"Hive查询 分桶表 常用函数 自定义函数\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 查询语法\n\n```sql\n[WITH CommonTableExpression (, CommonTableExpression)*]    (Note: Only available\n starting with Hive 0.13.0)\nSELECT [ALL | DISTINCT] select_expr, select_expr, ...\n  FROM table_reference\n  [WHERE where_condition]\n  [GROUP BY col_list]\n  [ORDER BY col_list]\n  [CLUSTER BY col_list\n    | [DISTRIBUTE BY col_list] [SORT BY col_list]\n  ]\n [LIMIT number]\n```\n\n[参考文档](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select)\n\n## 基本查询\n\n```sql\nselect  …  from\n```\n\n### 全表查询\n\n```\nselect * from emp;\n```\n\n### 条件查询\n\n```\nselect empno, ename from emp;\n```\n\n注意:\n\n1. SQL 语言大小写不敏感。 \n2. SQL 可以写在一行或者多行\n3. 关键字不能被缩写也不能分行\n4. 各子句一般要分行写。\n5. 使用缩进提高语句的可读性。\n\n###  列别名\n\n1. 重命名一个列\n2. 便于计算\n3. 紧跟列名，也可以在列名和别名之间加入关键字‘AS’ \n\n```sql\n--询名称和部门\nhive > select ename AS name, deptno dn from emp;\n```\n\n### 算术运算符\n\n| 运算符 | 描述           |\n| ------ | -------------- |\n| A+B    | A和B   相加    |\n| A-B    | A减去B         |\n| A*B    | A和B   相乘    |\n| A/B    | A除以B         |\n| A%B    | A对B取余       |\n| A&B    | A和B按位取与   |\n| A\\|B   | A和B按位取或   |\n| A^B    | A和B按位取异或 |\n| ~A     | A按位取反      |\n\n```sql\n--薪水加100后的信息\nselect ename,sal +100 from emp;\n```\n\n### 常用函数\n\n```sql\n--求总行数（count）\nselect count(*) cnt from emp;\n\n--求工资的最大值（max）\nselect max(sal) max_sal from emp;\n\n--求工资的最小值（min）\nselect min(sal) min_sal from emp;\n\n--求工资的总和（sum）\nselect sum(sal) sum_sal from emp; \n\n--求工资的平均值（avg）\nselect sum(sal) sum_sal from emp; \n\n--Limit语句\nselect * from emp limit 5;\n```\n\n### Where语句\n\n1．使用WHERE子句，将不满足条件的行过滤掉\n\n2．WHERE子句紧随FROM子句\n\n```sql\n--查询出薪水大于1000的所有员工\nselect * from emp where sal >1000;\n```\n\n### 比较运算符\n\n| 操作符                  | 支持的数据类型 | 描述                                                         |\n| ----------------------- | -------------- | ------------------------------------------------------------ |\n| A=B                     | 基本数据类型   | 如果A等于B则返回TRUE，反之返回FALSE                          |\n| A<=>B                   | 基本数据类型   | 如果A和B都为NULL，则返回TRUE，其他的和等号（=）操作符的结果一致，如果任一为NULL则结果为NULL |\n| A<>B, A!=B              | 基本数据类型   | A或者B为NULL则返回NULL；如果A不等于B，则返回TRUE，反之返回FALSE |\n| A<B                     | 基本数据类型   | A或者B为NULL，则返回NULL；如果A小于B，则返回TRUE，反之返回FALSE |\n| A<=B                    | 基本数据类型   | A或者B为NULL，则返回NULL；如果A小于等于B，则返回TRUE，反之返回FALSE |\n| A>B                     | 基本数据类型   | A或者B为NULL，则返回NULL；如果A大于B，则返回TRUE，反之返回FALSE |\n| A>=B                    | 基本数据类型   | A或者B为NULL，则返回NULL；如果A大于等于B，则返回TRUE，反之返回FALSE |\n| A [NOT] BETWEEN B AND C | 基本数据类型   | 如果A，B或者C任一为NULL，则结果为NULL。如果A的值大于等于B而且小于或等于C，则结果为TRUE，反之为FALSE。如果使用NOT关键字则可达到相反的效果。 |\n| A IS NULL               | 所有数据类型   | 如果A等于NULL，则返回TRUE，反之返回FALSE                     |\n| A IS NOT NULL           | 所有数据类型   | 如果A不等于NULL，则返回TRUE，反之返回FALSE                   |\n| IN(数值1, 数值2)        | 所有数据类型   | 使用 IN运算显示列表中的值                                    |\n| A [NOT] LIKE B          | STRING 类型    | B是一个SQL下的简单正则表达式，如果A与其匹配的话，则返回TRUE；反之返回FALSE。B的表达式说明如下：‘x%’表示A必须以字母‘x’开头，‘%x’表示A必须以字母’x’结尾，而‘%x%’表示A包含有字母’x’,可以位于开头，结尾或者字符串中间。如果使用NOT关键字则可达到相反的效果。 |\n| A RLIKE B, A REGEXP B   | STRING 类型    | B是一个正则表达式，如果A与其匹配，则返回TRUE；反之返回FALSE。匹配使用的是JDK中的正则表达式接口实现的，因为正则也依据其中的规则。例如，正则表达式必须和整个字符串A相匹配，而不是只需与其字符串匹配。 |\n\n```sql\n--查询出薪水等于5000的所有员工\nselect * from emp where sal =5000;\n\n--查询工资在5000到10000的员工信息\n select * from emp where sal  between 500 and 10000;\n \n --查询comm为空的所有员工信\n select * from emp where comm is null;\n \n --查询工资是1500或5000的员工信息\nselect * from emp where sal in(1500,5000);\nselect * from emp where sal=1500 or sal= 5000;\n```\n\n### Like和RLike\n\n1. 使用LIKE运算选择类似的值\n2. 选择条件可以包含字符或数字:% 代表零个或多个字符(任意个字符)。_ 代表一个字符。\n3. RLIKE子句是Hive中这个功能的一个扩展，其可以通过Java的正则表达式这个更强大的语言来指定匹配条件。\n\n```sql\n-- 查找以2开头薪水的员工信息\n select * from emp where sal LIKE '2%';\n-- 查找第二个数值为2的薪水的员工信息\nselect * from emp where sal LIKE '_2%';\n-- 查找薪水中含有2的员工信息\nselect * from emp where sal RLIKE '[2]';\n```\n\n### 逻辑运算符\n\n| 操作符 | 含义   |\n| ------ | ------ |\n| AND    | 逻辑并 |\n| OR     | 逻辑或 |\n| NOT    | 逻辑否 |\n\n```sql\n--求每个部门的平均工资\nselect deptno, avg(sal) from emp group by deptno;\n--求每个部门的平均薪水大于2000的部门\nselect deptno, avg(sal) avg_sal from emp group by deptno having  avg_sal > 2000;\n```\n\n### Join语句\n\n#### 等值Join\n\n注意:Hive支持通常的SQL JOIN语句，但 **只支持等值连接，不支持非等值连接**。\n\n```sql\n--根据员工表和部门表中的部门编号相等，查询员工编号、员工名称和部门名称；\n select e.empno, e.ename, d.deptno, d.dname from emp e join dept d on e.deptno = d.deptno;\n--合并员工表和部门表\n select e.empno, e.ename, d.deptno from emp e join dept d on e.deptno = d.deptno;\n```\n\n 表的别名:（1）使用别名可以简化查询。（2）使用表名前缀可以提高执行效率。\n\n#### 左外连接\n\n左外连接：JOIN操作符左边表中符合WHERE子句的所有记录将会被返回。\n\n```sql\nselect e.empno, e.ename, d.deptno from emp e left join dept d on e.deptno = d.deptno;\n```\n\n#### 右外连接\n\n右外连接：JOIN操作符右边表中符合WHERE子句的所有记录将会被返回。\n\n```sql\nselect e.empno, e.ename, d.deptno from emp e right join dept d on e.deptno = d.deptno;\n```\n\n#### 满外连接\n\n满外连接：将会返回所有表中符合WHERE语句条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用NULL值替代。\n\n```sql\nselect e.empno, e.ename, d.deptno from emp e full join dept d on e.deptno = d.deptno;\n```\n\n#### 多表连接\n\n数据准备\n\n```shell\n[hadoop@datanode1 datas]$ vim location.txt\n1700    Beijing\n1800    London\n1900    Tokyo\n```\n\n创建位置表\n\n```sql\ncreate table if not exists default.location(\nloc int,\nloc_name string\n)\nrow format delimited fields terminated by '\\t';\n```\n\n导入数据\n\n```sql\n load data local inpath '/opt/module/datas/location.txt' into table default.location;\n```\n\n多表连接查询\n\n```sql\nSELECT e.ename, d.deptno, l. loc_name\nFROM   emp e \nJOIN   dept d\nON     d.deptno = e.deptno \nJOIN   location l\nON     d.loc = l.loc;\n```\n\n注意:大多数情况下，Hive会对每对JOIN连接对象启动一个MapReduce任务。本例中会首先启动一个MapReduce job对表e和表d进行连接操作，然后会再启动一个MapReduce job将第一个MapReduce job的输出和表l;进行连接操作。Hive总是按照从左到右的顺序执行的。因此不是Hive总是按照从左到右的顺序执行的。\n\n####  笛卡尔积\n\n笛卡尔集会在下面条件下产生:\n\n1. 省略连接条件\n2. 连接条件无效\n3. 所有表中的所有行互相连接\n\n```sql\n--案例实操\n select empno, dname from emp, dept;\n \n --连接谓词中不支持or\nselect e.empno, e.ename, d.deptno from emp e join dept d on e.deptno= d.deptno or e.ename=d.ename; ## 错误的\n```\n\n### 排序\n\n#### 全局排序\n\nOrder By：全局排序，一个Reducer\n\n1．使用 ORDER BY 子句排序\nASC（ascend）: 升序（默认）\nDESC（descend）: 降序\n\nORDER BY 子句在SELECT语句的结尾\n\n```sql\n--查询员工信息按工资升序排列\nhive (default)> select * from emp order by sal;\n--查询员工信息按工资降序排列\nselect * from emp order by sal desc;\n```\n\n#### 按照别名排序\n\n```sql\n--按照员工薪水的2倍排序\nselect ename, sal*2 twosal from emp order by twosal;\n--按照部门和工资升序排序\nselect ename, deptno, sal from emp order by deptno, sal;\n```\n\n#### MR内部排序（Sort By）\n\nSort By：每个Reducer内部进行排序，对全局结果集来说不是排序。\n\n```sql\n--设置reduce个数\nset mapreduce.job.reduces=3;\n\n--查看设置reduce个数\nset mapreduce.job.reduces;\n\n--根据部门编号降序查看员工信息\nselect empno,ename,sal,deptno from emp sort by empno desc;\n\n--按照部门编号降序排序\nselect empno,ename,sal,deptno from emp sort by deptno desc;\n```\n\n#### 分区排序 （Distribute By）\n\nDistribute By：类似MR中partition，进行分区，结合sort by使用。\n\n注意：Hive要求DISTRIBUTE BY语句要写在SORT BY语句之前。对于distribute by进行测试，一定要分配多reduce进行处理，否则无法看到distribute by的效果。\n\n案例实操：\n\n```sql\n--需求：先按照部门编号分区，再按照员工编号降序排序。\n set mapreduce.job.reduces=3;\n select * from emp distribute by deptno sort by empno desc;\n```\n\n#### Cluster By\n\n当distribute by和sorts by字段相同时，可以使用cluster by方式。\n\ncluster by除了具有distribute by的功能外还兼具sort by的功能。但是排序只能是倒序排序，不能指定排序规则为ASC或者DESC。\n\n```sql\nselect * from emp cluster by deptno;\nselect * from emp distribute by deptno sort by deptno;\n```\n\n### 桶表\n\n**分区针对的是数据的存储路径；分桶针对的是数据文件。**\n\n分区提供一个隔离数据和优化查询的便利方式。不过，并非所有的数据集都可形成合理的分区，特别是之前所提到过的要确定合适的划分大小这个疑虑。 分桶是将数据集分解成更容易管理的若干部分的另一个技术。\n\n```sql\n--创建分桶表\ncreate table stu_buck(id int, name string)\nclustered by(id) \ninto 4 buckets\nrow format delimited fields terminated by '\\t';\n\nNum Buckets:            4\n--加载数据\n load data local inpath '/opt/module/datas/student.txt' into table  stu_buck;\n```\n\n查看创建的分桶表\n\n![kpDZqO.png](https://s2.ax1x.com/2019/01/17/kpDZqO.png)\n\n```sql\n--创建分桶表时，数据通过子查询的方式导入\ncreate table stu(id int, name string)\nrow format delimited fields terminated by '\\t';\n\n--向普通的stu表中导入数据\nload data local inpath '/opt/module/datas/student.txt' into table stu;\n\n--清空stu_buck表中数据\ntruncate table stu_buck;\n\n--导入数据到分桶表，通过子查询的方式\ninsert into table stu_buck select id, name from stu;\n```\n\n![kpD5S1.png](https://s2.ax1x.com/2019/01/17/kpD5S1.png)\n\n```sql\n--为什么没有分桶呢 这里需要我们开启一个属性\n insert into table stu_buck select id, name from stu;\n```\n\n![kpDxSI.png](https://s2.ax1x.com/2019/01/17/kpDxSI.png)\n\n```sql\n--查询分桶的数据\nhive> select * from stu_buck;\nOK\n1016    ss16\n1012    ss12\n1008    ss8\n1004    ss4\n1009    ss9\n1005    ss5\n1001    ss1\n1013    ss13\n1010    ss10\n1002    ss2\n1006    ss6\n1014    ss14\n1003    ss3\n1011    ss11\n1007    ss7\n1015    ss15\nTime taken: 0.091 seconds, Fetched: 16 row(s)\n```\n\n![kpDxSI.png](https://s2.ax1x.com/2019/01/17/kpDxSI.png)\n\n![kps4r6.png](https://s2.ax1x.com/2019/01/17/kps4r6.png)\n\n#### 分桶抽样查询\n\n对于非常大的数据集，有时用户需要使用的是一个具有代表性的查询结果而不是全部结果。Hive可以通过对表进行抽样来满足这个需求。\n\n查询表stu_buck中的数据     select * from 表名 tablesample(bucket x out of y on id);   \n\ny必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了4份，当y=2时，抽取(4/2=)2个bucket的数据，当y=8时，抽取(4/8=)1/2个bucket的数据。\n\nx表示从哪个bucket开始抽取，如果需要取多个分区，以后的分区号为当前分区号加上y。例如，table总bucket数为4，tablesample(bucket 1 out of 2)，表示总共抽取（4/2=）2个bucket的数据，抽取第1(x)个和第4(x+y)个bucket的数据。\n\n```sql\n--抽取第一个分区的数据\nhive> select * from stu_buck  tablesample(bucket 1 out of 4 on id);\nOK\n1016    ss16\n1012    ss12\n1008    ss8\n1004    ss4\n--查询第一个分区一半的数据\nhive> select * from stu_buck  tablesample(bucket 1 out of 8 on id);\nOK\n1016    ss16\n1008    ss8\n\n--x的值必须小于等于y的值，否则\nhive> select * from stu_buck  tablesample(bucket 6 out of 3 on id);\nFAILED: SemanticException [Error 10061]: Numerator should not be bigger than denominator in sample clause for table stu_buck\n```\n\n###  其他常用查询函数\n\n####  空字段赋值\n\nNVL：给值为NULL的数据赋值，它的格式是NVL( string1,replace_with)。它的功能是如果string1为NULL，则NVL函数返回replace_with的值，否则返回string1的值，如果两个参数都为NULL ，则返回NULL。\n\n```sql\n--NULL用'无'代替\nhive> select ename, deptno, sal, nvl(comm,'无') from emp;\nOK\nSMITH   20      800.0   无\nALLEN   30      1600.0  300.0\nWARD    30      1250.0  500.0\nJONES   20      2975.0  无\nMARTIN  30      1250.0  1400.0\nBLAKE   30      2850.0  无\nCLARK   10      2450.0  无\nSCOTT   20      3000.0  无\nKING    10      5000.0  无\nTURNER  30      1500.0  0.0\nADAMS   20      1100.0  无\nJAMES   30      950.0   无\nFORD    20      3000.0  无\nMILLER  10      1300.0  无\n```\n\n#### CASE WHEN\n\n```sql\n--数据准备\n[hadoop@datanode1 datas]$ vim emp_sex.txt\n悟空    A       男\n八戒    A       男\n沙和尚  B       男\n唐僧    A       女\n白龙马  B       女\n白骨精  B       女\n\n--建表加载数据\ncreate table person_info(\nname string, \nconstellation string, \nblood_type string) \nrow format delimited fields terminated by \"\\t\";\n\n--查询\nselect \n t1.c_b,\n CONCAT_WS(\"|\",COLLECT_SET(t1.name))\nfrom (\n  select \n   CONCAT_WS(\",\",constellation,blood_type) c_b,\n   name \n  from person_info) t1\ngroup by \nt1.c_b;\n--结果\n\ndept_id male    female\nA       2       1\nB       1       2\n```\n\n#### 行转列\n\nCONCAT(string A/col, string B/col…)：返回输入字符串连接后的结果，支持任意个输入字符串;\n\nCONCAT_WS(separator, str1, str2,...)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;\n\nCOLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。\n\n```sql\n##需求:把星座和血型一样的人归类到一起\n[hadoop@datanode1 datas]$ vim person_info.txt\n孙悟空  白羊座  A\n唐僧    射手座  A\n沙和尚  白羊座  B\n猪八戒  白羊座  A\n白龙马  射手座  A\n\n--建表导入数据\ncreate table person_info(\nname string, \nconstellation string, \nblood_type string) \nrow format delimited fields terminated by \"\\t\";\nload data local inpath \"/opt/module/datas/ constellation.txt\" into table person_info;\n\n--查询\nselect t1.base,\n    concat_ws('|', collect_set(t1.name)) name\nfrom\n    (select\n        name,\n        concat(constellation, \",\", blood_type) base\n    from\n        person_info) t1\ngroup by\n    t1.base;\n--结果\n射手座,A        唐僧|白龙马\n白羊座,A        孙悟空|猪八戒\n白羊座,B        沙和尚\n```\n\n#### 列转行\n\nEXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。\n\nLATERAL VIEW用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias\n\n解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。\n\n```sql\n--准备数据\n[hadoop@datanode1 datas]$ vim movie.txt\n《疑犯追踪》    悬疑,动作,科幻,剧情\n《Lie to me》   悬疑,警匪,动作,心理,剧情\n《战狼2》       战争,动作,灾难\n\n--创建表加载数据\ncreate table movie_info(\n    movie string, \n    category array<string>) \nrow format delimited fields terminated by \"\\t\"\ncollection items terminated by \",\";\n\n--查询\nselect movie,category_name\nfrom movie_info\nLATERAL VIEW EXPLODE(category) tmpTable as category_name;\n\n--结果\nmovie   category_name\n《疑犯追踪》    悬疑\n《疑犯追踪》    动作\n《疑犯追踪》    科幻\n《疑犯追踪》    剧情\n《Lie to me》   悬疑\n《Lie to me》   警匪\n《Lie to me》   动作\n《Lie to me》   心理\n《Lie to me》   剧情\n《战狼2》       战争\n《战狼2》       动作\n《战狼2》       灾难\n```\n\n#### 窗口函数\n\n| 函数        | 功能                                                         |\n| ----------- | ------------------------------------------------------------ |\n| OVER()      | 指定分析函数工作的数据窗口大小，这个数据窗口大小可能会随着行的变而变化 |\n| CURRENT ROW | 当前行                                                       |\n| n PRECEDING | 往前n行数据                                                  |\n| n FOLLOWING | 往后n行数据                                                  |\n| UNBOUNDED   | 起点，UNBOUNDED PRECEDING 表示从前面的起点， UNBOUNDED FOLLOWING表示到后面的终点 |\n| LAG(col,n)  | 往前第n行数据                                                |\n| LEAD(col,n) | 往后第n行数据                                                |\n| NTILE(n)    | 把有序分区中的行分发到指定数据的组中，各个组有编号，编号从1开始：n必须为int类型。 |\n\n```sql\n--准备数据\n[hadoop@datanode1 datas]$ vim business.txt\njack,2018-01-01,10\ntony,2018-01-02,15\njack,2018-02-03,23\ntony,2018-01-04,29\njack,2018-01-05,46\njack,2018-04-06,42\ntony,2018-01-07,50\njack,2018-01-08,55\nmart,2018-04-08,62\nmart,2018-04-09,68\nneil,2018-05-10,12\nmart,2018-04-11,75\nneil,2018-06-12,80\nmart,2018-04-13,94\n\n--创建表导入数据\ncreate table business(\nname string, \norderdate string,\ncost int\n) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n\nload data local inpath \"/opt/module/datas/business.txt\" into table business;\n\n--查询在2018年4月份购买过的顾客及总人数\nselect name,count(*) over () \nfrom business \nwhere substring(orderdate,1,7) = '2018-04' \ngroup by name;\n\nname    count_window_0\nmart    2\njack    2\n\n--查询顾客的购买明细及月购买总额\nselect name,orderdate,cost,sum(cost) over(partition by month(orderdate)) from business;\n\n--查询顾客的购买明细及月购买总额\nselect *,sum(cost) over(partition by month(orderdate)) from business;\n\n--查询顾客上次的购买时间\nselect *,\nlag(orderdate,1) over(distribute by name sort by orderdate),\nlead(orderdate,1) over(distribute by name sort by orderdate) from business;\n\nselect *,\nlag(orderdate,1,\"2016-12-31\") over(distribute by name sort by orderdate)\nfrom business;\n\n--查询前20%时间的订单信息\nselect *,ntile(5) over(sort by orderdate) gid from business where gid=1;X\n\nselect *,ntile(5) over(sort by orderdate) gid from business having gid=1;X\n\n\tselect *\n\tfrom(\n\tselect *,ntile(5) over(sort by orderdate) gid from business\n\t) t\n\twhere\n\tgid=1;\n\nselect * from (\n    select name,orderdate,cost, ntile(5) over(order by orderdate) sorted\n    from business\n) t\nwhere sorted = 1;\n```\n\n#### Rank\n\n| 函数         | 功能                         |\n| ------------ | ---------------------------- |\n| RANK()       | 排序相同时会重复，总数不会变 |\n| DENSE_RANK() | 排序相同时会重复，总数会减少 |\n| ROW_NUMBER() | 会根据顺序计算               |\n\n\n\n```sql\n--数据准备\n[hadoop@datanode1 datas]$  vi score.txt\n孙悟空,语文,87,\n孙悟空,数学,95,\n孙悟空,英语,73,\n白龙马,语文,94,\n白龙马,数学,56,\n白龙马,英语,84,\n鲁智深,语文,89,\n鲁智深,数学,86,\n鲁智深,英语,84,\n白骨精,语文,79,\n白骨精,数学,85,\n白骨精,英语,78,\n\n--建表导入数据\ncreate table score(\nname string,\nsubject string, \nscore int) \nrow format delimited fields terminated by \"\\t\";\nload data local inpath '/opt/module/datas/score.txt' into table score;\n\n--结果集\nname    subject score   rp      drp     rmp\n孙悟空  数学    95      1       1       1\n鲁智深  数学    86      2       2       2\n白骨精  数学    85      3       3       3\n白龙马  数学    56      4       4       4\n鲁智深  英语    84      1       1       1\n白龙马  英语    84      1       1       2\n白骨精  英语    78      3       2       3\n孙悟空  英语    73      4       3       4\n白龙马  语文    94      1       1       1\n鲁智深  语文    89      2       2       2\n孙悟空  语文    87      3       3       3\n白骨精  语文    79      4       4       4\n\n\n```\n\n#### 自定义函数\n\nHive 自带了一些函数，比如：max/min等，但是数量有限，自己可以通过自定义UDF来方便的扩展。\n\n当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。\n\n根据用户自定义函数类别分为以下三种：\n\n（1）UDF（User-Defined-Function） 一进一出\n\n（2）UDAF（User-Defined Aggregation Function） 聚集函数，多进一出  类似于：count/max/min\n\n（3）UDTF（User-Defined Table-Generating Functions）   一进多出  如lateral view explore()\n\n[官方文档地址](https://cwiki.apache.org/confluence/display/Hive/HivePlugins)\n\n编程步骤:\n\n（1）继承org.apache.hadoop.hive.ql.UDF\n\n（2）需要实现evaluate函数；evaluate函数支持重载；\n\n（3）在hive的命令行窗口创建函数\n\n```shell\n## 添加jar\nadd jar jar_path\n\n## 创建function\ncreate [temporary] function [dbname.]function_name AS class_name;\n\n## 在hive的命令行窗口删除函数\nDrop [temporary] function [if exists] [dbname.]function_name;\n```\n\n​    自定义UDF函数\n\n```xml\n<dependencies>\n\t\t<!-- https://mvnrepository.com/artifact/org.apache.hive/hive-exec -->\n\t\t<dependency>\n\t\t\t<groupId>org.apache.hive</groupId>\n\t\t\t<artifactId>hive-exec</artifactId>\n\t\t\t<version>1.2.1</version>\n\t\t</dependency>\n</dependencies>\n```\n\njava类\n```\nimport org.apache.hadoop.hive.ql.exec.UDF;\n\npublic class Lower extends UDF {\n\n\tpublic String evaluate (final String s) {\n\t\t\n\t\tif (s == null) {\n\t\t\treturn null;\n\t\t}\n\t\t\n\t\treturn s.toLowerCase();\n\t}\n}\n```\n\n上传上服务器\n\n![kpopw9.png](https://s2.ax1x.com/2019/01/17/kpopw9.png)\n\n添加jar\n\n```sql\nhive> add jar /home/hadoop/udf.jar;\nAdded [/home/hadoop/udf.jar] to class path\nAdded resources: [/home/hadoop/udf.jar]\n--添加关联\nhive> add jar /home/hadoop/udf.jar;\nAdded [/home/hadoop/udf.jar] to class path\nAdded resources: [/home/hadoop/udf.jar]\nhive>  create temporary function mylower as \"com.hph.Lower\";\nOK\nTime taken: 0.033 seconds\n\n--调用\nhive>  select ename, mylower(ename) lowername from emp limit 2;\nOK\nename   lowername\nSMITH   smith\nALLEN   allen\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Hive"],"categories":["大数据"]},{"title":"Hive数据据类型 DDL DML","url":"/2019/01/16/Hive数据据类型/","content":"\n {{ \"Hive的基本数据类型  DDL  DML\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 基本数据类型\n\n| Hive数据类型 | Java数据类型 | 长度                                                 | 例子                                 |\n| ------------ | ------------ | ---------------------------------------------------- | ------------------------------------ |\n| TINYINT      | byte         | 1byte有符号整数                                      | 20                                   |\n| SMALINT      | short        | 2byte有符号整数                                      | 20                                   |\n| INT          | int          | 4byte有符号整数                                      | 20                                   |\n| BIGINT       | long         | 8byte有符号整数                                      | 20                                   |\n| BOOLEAN      | boolean      | 布尔类型，true或者false                              | TRUE  FALSE                          |\n| FLOAT        | float        | 单精度浮点数                                         | 3.14159                              |\n| DOUBLE       | double       | 双精度浮点数                                         | 3.14159                              |\n| STRING       | string       | 字符系列。可以指定字符集。可以使用单引号或者双引号。 | ‘now is the time’ “for all good men” |\n| TIMESTAMP    |              | 时间类型                                             |                                      |\n| BINARY       |              | 字节数组                                             |                                      |\n\n对于Hive而言String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。\n\n###  集合数据类型\n\n| 数据类型 | 描述                                                         | 语法示例 |\n| -------- | ------------------------------------------------------------ | -------- |\n| STRUCT   | 和c语言中的struct类似，都可以通过“点”符号访问元素内容。例如，如果某个列的数据类型是STRUCT{first STRING, last STRING},那么第1个元素可以通过字段.first来引用。 | struct() |\n| MAP      | MAP是一组键-值对元组集合，使用数组表示法可以访问数据。例如，如果某个列的数据类型是MAP，其中键->值对是’first’->’John’和’last’->’Doe’，那么可以通过字段名[‘last’]获取最后一个元素 | map()    |\n| ARRAY    | 数组是一组具有相同类型和名称的变量的集合。这些变量称为数组的元素，每个数组元素都有一个编号，编号从零开始。例如，数组值为[‘John’,   ‘Doe’]，那么第2个元素可以通过数组名[1]进行引用。 | Array()  |\n\nHive有三种复杂数据类型ARRAY、MAP 和 STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。\n\n### 类型转化\n\nHive的原子数据类型是可以进行隐式转换的，类似于Java的类型转换，例如某表达式使用INT类型，TINYINT会自动转换为INT类型，但是Hive不会进行反向转化，例如，某表达式使用TINYINT类型，INT不会自动转换为TINYINT类型，它会返回错误，除非使用CAST操作。\n\n1．隐式类型转换规则如下\n\n（1）任何整数类型都可以隐式地转换为一个范围更广的类型，如TINYINT可以转换成INT，INT可以转换成BIGINT。\n\n（2）所有整数类型、FLOAT和STRING类型都可以隐式地转换成DOUBLE。\n\n（3）TINYINT、SMALLINT、INT都可以转换为FLOAT。\n\n（4）BOOLEAN类型不可以转换为任何其它的类型。\n\n2．可以使用CAST操作显示进行数据类型转换\n\n例如CAST('1' AS INT)将把字符串'1' 转换成整数1；如果强制类型转换失败，如执行CAST('X' AS INT)，表达式返回空值 NULL。\n\n## DDL数据定义\n\n###  创建数据库\n\n```sql\nhive> create database if not exists db_hive;      --最标准写法\n\nhive> create database db_hive_HDFS location '/db_hive_hdfs.db';  --指定数据库在HDFS上存放的位置\n```\n\n![kS6mx1.png](https://s2.ax1x.com/2019/01/16/kS6mx1.png)\n\n### 查询数据库\n\n```sql\nhive> show databases;\nOK\ndb_hive\ndb_hive_hdfs\ndefault\n```\n\n查看数据库详情\n\n```sql\nhive> desc database db_hive;\nOK\ndb_hive         hdfs://datanode1:9000/user/hive/warehouse/db_hive.db    hadoop  USER\nTime taken: 0.043 seconds, Fetched: 1 row(s)\nhive> desc db_hive_hdfs;\n\nhive> desc database db_hive_hdfs;\nOK\ndb_hive_hdfs            hdfs://datanode1:9000/db_hive_hdfs.db   hadoop  USER\nTime taken: 0.041 seconds, Fetched: 1 row(s)\n```\n\n切换当前数据库\n\n```shell\nhive> use db_hive;\n```\n\n### 修改数据库\n\n用户可以使用ALTER DATABASE命令为某个数据库的DBPROPERTIES设置键-值对属性值，来描述这个数据库的属性信息。数据库的其他元数据信息都是不可更改的，包括数据库名和数据库所在的目录位置。**修改当前正在使用的数据库，要先退出使用**\n\n```sql\nhive> use default;\nOK\nTime taken: 0.038 seconds\n\nhive>  alter database db_hive set dbproperties('createtime'='20190116') ;\nOK\nTime taken: 0.105 seconds\n\nhive> desc database extended db_hive;\nOK\ndb_hive         hdfs://datanode1:9000/user/hive/warehouse/db_hive.db    hadoop  USER    {createtime=20190116}\nTime taken: 0.064 seconds, Fetched: 1 row(s)\n```\n\n### 删除数据库\n\n```sql\nhive> drop database db_hive_hdfs;\t\t\t\t\t   --删除空数据库\nhive> drop database if exists db_hive_hdfs;\t\t\t\t--这么些删除不存在的不会报错\nhive> drop database db_hive cascade;\t\t\t\t    --如果数据库非空可以强制删除\n```\n\n### 创建表\n\n```sql\nCREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name \n[(col_name data_type [COMMENT col_comment], ...)] \n[COMMENT table_comment] \n[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] \n[CLUSTERED BY (col_name, col_name, ...) \n[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] \n[ROW FORMAT row_format] \n[STORED AS file_format] \n[LOCATION hdfs_path]\n```\n\n| 参数                                                         | 功能                                                         |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| <font color=#0099ff size=2 face=\"黑体\">CREATE  TABLE</font>  | <font color=#0099ff size=2 face=\"黑体\">创建一个指定名字的表。如果表存在，抛出异常，用 IF NOT EXISTS忽略异常。</font> |\n| <font color=#0099ff size=2 face=\"黑体\">EXTERNA</font>        | <font color=#0099ff size=2 face=\"黑体\">用户创建一个外部表，在建表的同时指定一个指向实际数据的路径（LOCATION），Hive创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</font> |\n| <font color=#0099ff size=2 face=\"黑体\">PARTITIONED BY</font> | <font color=#0099ff size=2 face=\"黑体\">创建分区表</font>     |\n| <font color=#0099ff size=2 face=\"黑体\">COMMENT</font>        | <font color=#0099ff size=2 face=\"黑体\">为表和列添加注释。</font> |\n| <font color=#0099ff size=2 face=\"黑体\">CLUSTERED BY</font>   | <font color=#0099ff size=2 face=\"黑体\">创建分桶表</font>     |\n| <font color=#0099ff size=2 face=\"黑体\">SORTED BY</font>      | <font color=#0099ff size=2 face=\"黑体\">不常用</font>         |\n| <font color=#0099ff size=2 face=\"黑体\">ROW FORMAT</font>     | <font color=#0099ff size=2 face=\"黑体\"> 用户在建表的时候可以自定义SerDe或者使用自带的SerDe。如果没有指定ROW FORMAT 或者ROW FORMAT<br/>DELIMITED，将会使用自带的SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的SerDe，Hive通过SerDe确定表的具体的列的数据。 </font> |\n| <font color=#0099ff size=2 face=\"黑体\">ROW FORMAT</font>     | <font color=#0099ff size=2 face=\"黑体\">ROW FORMAT</font>     |\n| <font color=#0099ff size=2 face=\"黑体\">STORED AS</font>      | <font color=#0099ff size=2 face=\"黑体\">指定存储文件类型常用的存储文件类型：SEQUENCEFILE（二进制序列文件）、TEXTFILE（文本）、RCFILE（列式存储格式文件）</font> |\n| <font color=#0099ff size=2 face=\"黑体\">LOCATION</font>       | <font color=#0099ff size=2 face=\"黑体\">指定表在HDFS上的存储位置。</font> |\n| <font color=#0099ff size=2 face=\"黑体\">LIKE </font>          | <font color=#0099ff size=2 face=\"黑体\">LIKE允许用户复制现有的表结构，但是不复制数据。</font> |\n\n### 管理表\n\n默认创建的表都是管理表（内部表）。这种表，Hive会（或多或少地）控制着数据的生命周期。Hive默认情况下会将这些表的数据存储在由配置项hive.metastore.warehouse.dir(例如，/user/hive/warehouse)所定义的目录的子目录下。当我们删除一个管理表时，Hive也会删除这个表中数据。管理表不适合和其他工具共享数据。\n\n创建普通表\n\n```sql\ncreate table if not exists student2(\nid int, name string\n)\nrow format delimited fields terminated by '\\t'\nstored as textfile\nlocation '/user/hive/warehouse/student2';\n```\n\n根据查询结果创建表（查询的结果会添加到新创建的表中）会进行mapreduce作业\n\n```sql\ncreate table if not exists student3 as select id, name from student;\n```\n\n根据已经存在的表结构创建表\n\n```sql\ncreate table if not exists student4 like student;\n```\n\n查询表的详细类型\n\n```\ndesc formatted student2;\n```\n\n### 外部表\n\n外部表Hive并非认为其完全拥有这份数据。删除该表并不会删除掉这份数据，表的元数据信息会被删除掉。\n\n### 应用场景\n\n每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。\n\n### 案例实操\n\n数据准备\n\n```shell\n[hadoop@datanode1 datas]$ vim dept.txt\n\n10      ACCOUNTING      1700\n20      RESEARCH        1800\n30      SALES   1900\n40      OPERATIONS      1700\n--------------------------------------------------------------------------------------------------\n[hadoop@datanode1 datas]$ vim emp.txt\n7369    SMITH   CLERK   7902    1980-12-17      800.00          20\n7499    ALLEN   SALESMAN        7698    1981-2-20       1600.00 300.00  30\n7521    WARD    SALESMAN        7698    1981-2-22       1250.00 500.00  30\n7566    JONES   MANAGER 7839    1981-4-2        2975.00         20\n7654    MARTIN  SALESMAN        7698    1981-9-28       1250.00 1400.00 30\n7698    BLAKE   MANAGER 7839    1981-5-1        2850.00         30\n7782    CLARK   MANAGER 7839    1981-6-9        2450.00         10\n7788    SCOTT   ANALYST 7566    1987-4-19       3000.00         20\n7839    KING    PRESIDENT               1981-11-17      5000.00         10\n7844    TURNER  SALESMAN        7698    1981-9-8        1500.00 0.00    30\n7876    ADAMS   CLERK   7788    1987-5-23       1100.00         20\n7900    JAMES   CLERK   7698    1981-12-3       950.00          30\n7902    FORD    ANALYST 7566    1981-12-3       3000.00         20\n7934    MILLER  CLERK   7782    1982-1-23       1300.00         10\n```\n\n创建表 \n\n````sql\n--部门表\ncreate external table if not exists default.dept(\ndeptno int,\ndname string,\nloc int\n)\nrow format delimited fields terminated by '\\t';\n\n--员工表\ncreate external table if not exists default.emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string, \nsal double, \ncomm double,\ndeptno int)\nrow format delimited fields terminated by '\\t';\n\n--查询表\nhive> show tables;\nOK\ndept\nstudent\nstudent2\nstudent3\nstudent4\nTime taken: 0.04 seconds, Fetched: 5 row(s)\n\n--加载数据\nhive> load data local inpath '/opt/module/datas/dept.txt' into table default.dept;\nLoading data to table default.dept\nTable default.dept stats: [numFiles=1, totalSize=69]\nOK\nTime taken: 0.569 seconds\n\nhive> load data local inpath '/opt/module/datas/emp.txt' into table default.emp;\nLoading data to table default.emp\nTable default.emp stats: [numFiles=1, totalSize=657]\nOK\nTime taken: 0.604 seconds\n\n--查询结果\nhive> select * from emp;\nhive> select * from dept;\n\n--查看表格信息\ndesc formatted dept;\n````\n\n### 互相转换\n\n注意：**只能用单引号，严格区分大小写，如果不是完全符合，那么只会添加kv 而不生效**\n\n```sql\n --查询表类型\n desc formatted student2;\n Table Type:             MANAGED_TABLE\n--修改内部表student2为外部表\nalter table student2 set tblproperties('EXTERNAL'='TRUE');\n--查询表的类型\ndesc formatted student2;\nTable Type:             EXTERNAL_TABLE\n--修改外部表student2为内部表\nalter table student2 set tblproperties('EXTERNAL'='FALSE');\n--查询表的类型\ndesc formatted student2;\nTable Type:             MANAGED_TABLE\n```\n\n注意：**注意：('EXTERNAL'='TRUE')和('EXTERNAL'='FALSE')为固定写法，区分大小写！**\n\n## 分区表\n\n分区表实际上就是对应一个HDFS文件系统上的独立的文件夹，该文件夹下是该分区所有的数据文件。**Hive中的分区就是分目录**，把一个大的数据集根据业务需要分割成小的数据集。在查询时通过WHERE子句中的表达式选择查询所需要的指定的分区，这样的查询效率会提高很多。\n\n### 创建分区表\n\n```sql\ncreate table dept_partition(\ndeptno int, dname string, loc string\n)\npartitioned by (month string)\nrow format delimited fields terminated by '\\t';\n```\n\n```sql\nload data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201901');\nload data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201902');\nload data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition partition(month='201903');\n```\n\n![kShcAP.png](https://s2.ax1x.com/2019/01/16/kShcAP.png)\n\n### 查询分区表\n\n```sql\n--单分区查询\nselect * from dept_partition where month='201901';\n-- 多分区联合查询  union（排序）    or   in 三种方式\nselect * from dept_partition where month='201901'\n              union\n              select * from dept_partition where month='201902'\n              union\n              select * from dept_partition where month='201903';\n \n -------------------------------------------------------------------------------------------------\nHadoop job information for Stage-2: number of mappers: 2; number of reducers: 1\n2019-01-16 21:02:28,578 Stage-2 map = 0%,  reduce = 0%\n2019-01-16 21:02:48,169 Stage-2 map = 50%,  reduce = 0%, Cumulative CPU 2.29 sec\n2019-01-16 21:02:49,219 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 5.17 sec\n2019-01-16 21:02:58,782 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 7.58 sec\nMapReduce Total cumulative CPU time: 7 seconds 580 msec\nEnded Job = job_1547632574371_0003\nMapReduce Jobs Launched:\nStage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 7.82 sec   HDFS Read: 15318 HDFS Write: 410 SUCCESS\nStage-Stage-2: Map: 2  Reduce: 1   Cumulative CPU: 7.58 sec   HDFS Read: 15080 HDFS Write: 291 SUCCESS\nTotal MapReduce CPU Time Spent: 15 seconds 400 msec\nOK\n10      ACCOUNTING      1700    201901\n10      ACCOUNTING      1700    201902\n10      ACCOUNTING      1700    201903\n20      RESEARCH        1800    201901\n20      RESEARCH        1800    201902\n20      RESEARCH        1800    201903\n30      SALES   1900    201901\n30      SALES   1900    201902\n30      SALES   1900    201903\n40      OPERATIONS      1700    201901\n40      OPERATIONS      1700    201902\n40      OPERATIONS      1700    201903\nTime taken: 100.192 seconds, Fetched: 12 row(s)\n```\n\n### 增加分区\n\n```sql\n--添加单个分区\nalter table dept_partition add partition(month='201904') ;\n--增加分区  用空格分开\nalter table dept_partition drop partition (month='201802')  partition (month='201803');\n```\n\n### 删除分区\n\n```sql\n--删除单个分区\n alter table dept_partition drop partition (month='201801');\n \n--同时删除多个分区  用逗号分开\nalter table dept_partition drop partition (month='201802'), partition (month='201803');\n```\n\n### 查看分区\n\n```sql\n-- show partitions dept_partition;\nhive> show partitions dept_partition;\nOK\nmonth=201801\nmonth=201901\nmonth=201902\nmonth=201903\nmonth=201904\nTime taken: 0.123 seconds, Fetched: 5 row(s)\n\n--查看分区表结构\nhive> desc formatted dept_partition;\nOK\n# col_name              data_type               comment\n\ndeptno                  int\ndname                   string\nloc                     string\n\n# Partition Information\n# col_name              data_type               comment\n\nmonth                   string\n```\n\n### 分区表注意事项\n\n#### 二级分区表\n\n```sql\n--创建二级分区表\ncreate table dept_partition2(deptno int, dname string, loc string )\npartitioned by (month string, day string)\nrow format delimited fields terminated by '\\t';\n\n--加载数据到二级分区表\nload data local inpath '/opt/module/datas/dept.txt' into table default.dept_partition2 partition(month='201812', day='31');\n\n--查询分区数据\nselect * from dept_partition2 where month='201812' and day='31';\n```\n\n#### 数据关联\n\n1.上传数据后修复\n\n```sql\ndfs -mkdir -p  /user/hive/warehouse/dept_partition2/month=201812/day=26;\ndfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201812/day=26;\n\n--修复命令\nhive> msck repair table dept_partition2;\nOK\nPartitions not in metastore:    dept_partition2:month=201812/day=26\nRepair: Added partition to metastore dept_partition2:month=201812/day=26\nTime taken: 0.294 seconds, Fetched: 2 row(s)\n\n--查询  如果没有执行修复命令  刚开始是查询不到的\nhive> select * from dept_partition2 where month='201812' and day='26';\n```\n\n2.上传数据后修复\n\n```sql\n--上传数据\ndfs -mkdir -p  /user/hive/warehouse/dept_partition2/month=201809/day=11;\ndfs -put /opt/module/datas/dept.txt  /user/hive/warehouse/dept_partition2/month=201809/day=11;\n\n--执行添加分区\nalter table dept_partition2 add partition(month='201809',day='11');\n\n--查询数据\n select * from dept_partition2 where month='201809' and day='11';\n```\n\n3.上传数据后load数据到分区\n\n```sql\n--创建目录\ndfs -mkdir -p  /user/hive/warehouse/dept_partition2/month=201810/day=25;\n\n--上传数据\nload data local inpath '/opt/module/datas/dept.txt' into table  dept_partition2 partition(month='201810',day='25');\n\n--查询数据\nselect * from dept_partition2 where month='201810' and day='25';\n```\n\n#### 修改表\n\n```sql\n--语法\nALTER TABLE table_name RENAME TO new_table_name\n\n--实例\nhive> alter table dept_partition2 rename to dept_partition_rename;\n```\n\n#### 增加/修改/替换列信息\n\n```sql\n--更新列\nALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name]\n\n--增加和替换列\nALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) \n```\n\n**注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。**\n\n实际案例\n\n```sql\n--查询表结构\ndesc dept_partition;\n\n--添加列\nalter table dept_partition add columns(deptdesc string);\n\n--查询表结构\ndesc dept_partition;\n\n--更新列\nalter table dept_partition change column deptdesc desc int;\n\n--查询表结构\ndesc dept_partition;\n\n--替换列\nalter table dept_partition replace columns(deptno string, dname,string, loc string);\n\n--替换列\n desc dept_partition;\n```\n\n#### 删除表\n\n```sql\n--语法\ndrop table 表名;\n\n--实例\ndrop table dept_partition;\n```\n\n### DML数据操作\n\n#### 向表中装载数据（Load）\n\n```sql\nload data [local] inpath '/opt/module/datas/student.txt' [overwrite] into table student [partition (partcol1=val1,…)];\n```\n\n| 参数           | 意义                                                         |\n| -------------- | ------------------------------------------------------------ |\n| load data      | 表示加载数据                                                 |\n| local          | 从本地加载数据到hive表（复制）；否则从HDFS加载数据到hive表（移动） |\n| inpath         | 表示加载数据的路径                                           |\n| overwrite into | 表示覆盖表中已有数据，否则表示追加                           |\n| into table     | 表示加载到哪张表                                             |\n| student        | 表示具体的表                                                 |\n| partition      | 表示上传到指定分区                                           |\n\n```sql\n--创建表\ncreate table student(id string, name string) row format delimited fields terminated by '\\t';\n\n--加载本地文件到hive\n load data local inpath '/opt/module/datas/student.txt' into table default.student;\n\n********************************* 加载HDFS文件到hive *********************************\n--上传文件到HDFS\nhive> dfs -put /opt/module/datas/student.txt /user/hadoop/hive;\n\n--加载HDFS上数据\nload data inpath '/user/hadoop/hive/student.txt' into table default.student;\n\n********************************* 加载数据覆盖表中已有的数据 *********************************\n\n--加载数据覆盖表中已有的\nload data inpath '/user/hadoop/hive/student.txt' overwrite into table default.student;\n```\n\n####  通过查询语句向表中插入数据（Insert）\n\n```sql\n --创建一张分区表\n create table student(id int, name string) partitioned by (month string) row format delimited fields terminated by '\\t';\n \n --基本插入数据\n insert into table  student partition(month='201901') values(1,'Hive');\n \n --基本模式插入（根据单张表查询结果）\ninsert overwrite table student partition(month='201812') select id, name from student where month='201901';\n\n--多插入模式（根据多张表查询结果）\nfrom student\n              insert overwrite table student partition(month='201902')\n              select id, name where month='201901'\n              insert overwrite table student partition(month='201903')\n              select id, name where month='201901';\n```\n\n#### 查询语句中创建表并加载数据（As Select）\n\n```sql\n--创建表，并指定在hdfs上的位置\ncreate table if not exists student5(\n              id int, name string\n              )\n              row format delimited fields terminated by '\\t'\n              location '/user/hive/warehouse/student5';\n\n--上传数据到hdfs上\ndfs -put /opt/module/datas/student.txt /user/hive/warehouse/student5;\n\n--查询\n select * from student5;\n hive> select * from student5;\nOK\n1       hadoop\n2       spark\n3       flink\n4       oozie\n5       java\n6       python\n7       MachineLearning\n8       Scala\n9       Hive\nTime taken: 0.082 seconds, Fetched: 9 row(s)\n```\n\n\n\n```sql\n --导出数据\n[hadoop@datanode1 hive]$  bin/hive -e \"EXPORT TABLE student    TO '/export/hive/student';\"\n\n--查看表\nhive> show tables;\nOK\ndept\ndept_partition\ndept_partition_rename\nemp\nstudent\nstudent3\nstudent4\nstudent5\nTime taken: 0.019 seconds, Fetched: 8 row(s)\n\n--导入数据\nimport table student_import partition(month='201901') from '/export/hive/student';\n\n--查看表\nhive> show tables;\nOK\ndept\ndept_partition\ndept_partition_rename\nemp\nstudent\nstudent3\nstudent4\nstudent5\nstudent_import\nTime taken: 0.019 seconds, Fetched: 9 row(s)\n\n--查询数据\nhive> select * from student2;\n```\n\n\n\n####  Insert导出\n\n```sql\n--将查询的结果导出到本地\ninsert overwrite local directory '/opt/module/datas/export/student'  select * from student;\n\n--将查询的结果格式化导出到本地\ninsert overwrite local directory '/opt/module/datas/export/student1'\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'    select * from student;\n\n--将查询的结果导出到HDFS上(没有local)\ninsert overwrite directory '/export/hive/student2' ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t' \nselect * from student;\n```\n\n#### 命令导出\n\n```sql\n-- Hadoop命令导出到本地\n\ndfs -get /user/hive/warehouse/student/month=201901/000000_0  /opt/module/datas/export/student3.txt;\n\n-- Hive Shell 命令导出\n--基本语法：（hive -f/-e 执行语句或者脚本 > file）\n[hadoop@datanode1 hive]$ bin/hive -e 'select * from default.student;' > /opt/module/datas/export/student4.txt;\n\n--Export导出到HDFS上\nhive> export table default.student to '/export/hive/student';\n```\n\n#### Sqoop导出\n\n[Sqoop参考](http://hphblog.cn/2019/01/08/Sqoop/)\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n","tags":["Hive"],"categories":["大数据"]},{"title":"KafkaAPI实战","url":"/2019/01/15/Kafka-API实战/","content":" {{ \"新旧API使用 Flume和Kafka集成\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\nKafka有两套API: 过时的API  和新API\n\n## 准备工作\n\n```xml\n <dependencies>\n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka-clients</artifactId>\n            <version>0.11.0.0</version>\n        </dependency>\n        \n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka_2.12</artifactId>\n            <version>0.11.0.0</version>\n        </dependency>\n        \n        <dependency>\n            <groupId>org.apache.kafka</groupId>\n            <artifactId>kafka-streams</artifactId>\n            <version>0.11.0.0</version>\n        </dependency>\n\n    </dependencies>\n```\n\n##  Kafka生产者API\n\n### 旧\n\n```java\nimport java.util.Properties;\nimport kafka.javaapi.producer.Producer;\nimport kafka.producer.KeyedMessage;\nimport kafka.producer.ProducerConfig;\n\npublic class OldProducer {\n\n\t@SuppressWarnings(\"deprecation\")\n\tpublic static void main(String[] args) {\n\n\t\t//配置信息\n\t\tProperties properties = new Properties();\n\t\tproperties.put(\"metadata.broker.list\", \"datanode1:9092\");\n\t\tproperties.put(\"request.required.acks\", \"1\");\n\t\tproperties.put(\"serializer.class\", \"kafka.serializer.StringEncoder\");\n\t\t\n\t\tProducer<Integer, String> producer = new Producer<Integer,String>(new ProducerConfig(properties));\n\t\t\n\t\tKeyedMessage<Integer, String> message = new KeyedMessage<Integer, String>(\"first\", \"hello kafka\");\n\t\tproducer.send(message );\n\t}\n}\n```\n\n### 高级\n\n```java\nimport java.util.Properties;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\npublic class NewProducer {\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tProperties props = new Properties();\n\t\t// Kafka服务端的主机名和端口号\n\t\tprops.put(\"bootstrap.servers\", \"datanode1:9092\");\n\t\t// 等待所有副本节点的应答\n\t\tprops.put(\"acks\", \"all\");\n\t\t// 消息发送最大尝试次数\n\t\tprops.put(\"retries\", 0);\n\t\t// 一批消息处理大小\n\t\tprops.put(\"batch.size\", 16384);\n\t\t// 请求延时\n\t\tprops.put(\"linger.ms\", 1);\n\t\t// 发送缓存区内存大小\n\t\tprops.put(\"buffer.memory\", 33554432);\n\t\t// key序列化\n\t\tprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\t\t// value序列化\n\t\tprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n\t\tProducer<String, String> producer = new KafkaProducer<>(props);\n\t\tfor (int i = 0; i < 50; i++) {\n\t\t\tproducer.send(new ProducerRecord<String, String>(\"first\", Integer.toString(i), \"hello kafka-\" + i));\n\t\t}\n\n\t\tproducer.close();\n\t}\n}\n```\n\n### 生产者带回调函数\n\n```java\nimport java.util.Properties;\nimport org.apache.kafka.clients.producer.Callback;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\npublic class CallBackProducer {\n\n\tpublic static void main(String[] args) {\n\nProperties props = new Properties();\n\t\t// Kafka服务端的主机名和端口号\n\t\tprops.put(\"bootstrap.servers\", \"datanode2:9092\");\n\t\t// 等待所有副本节点的应答\n\t\tprops.put(\"acks\", \"all\");\n\t\t// 消息发送最大尝试次数\n\t\tprops.put(\"retries\", 0);\n\t\t// 一批消息处理大小\n\t\tprops.put(\"batch.size\", 16384);\n\t\t// 增加服务端请求延时\n\t\tprops.put(\"linger.ms\", 1);\n// 发送缓存区内存大小\n\t\tprops.put(\"buffer.memory\", 33554432);\n\t\t// key序列化\n\t\tprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\t\t// value序列化\n\t\tprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n\t\tKafkaProducer<String, String> kafkaProducer = new KafkaProducer<>(props);\n\n\t\tfor (int i = 0; i < 50; i++) {\n\n\t\t\tkafkaProducer.send(new ProducerRecord<String, String>(\"first\", \"hello  --\" + i), new Callback() {\n\n\t\t\t\t@Override\n\t\t\t\tpublic void onCompletion(RecordMetadata metadata, Exception exception) {\n\n\t\t\t\t\tif (metadata != null) {\n\n\t\t\t\t\t\tSystem.err.println(metadata.partition() + \"---\" + metadata.offset());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t});\n\t\t}\n\n\t\tkafkaProducer.close();\n\t}\n}\n```\n\n### 自定义分区\n\n需求:将所有数据存储到topic的第0号分区上\n\n```java\nimport java.util.Map;\nimport org.apache.kafka.clients.producer.Partitioner;\nimport org.apache.kafka.common.Cluster;\n\npublic class CustomPartitioner implements Partitioner {\n\n   @Override\n   public void configure(Map<String, ?> configs) {\n      \n   }\n\n   @Override\n   public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n        // 控制分区\n      return 0;\n   }\n\n   @Override\n   public void close() {\n      \n   }\n}\n```\n\n```java\nimport java.util.Properties;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.Producer;\nimport org.apache.kafka.clients.producer.ProducerRecord;\n\npublic class PartitionerProducer {\n\n\tpublic static void main(String[] args) {\n\t\t\n\t\tProperties props = new Properties();\n\t\t// Kafka服务端的主机名和端口号\n\t\tprops.put(\"bootstrap.servers\", \"datanode1:9092\");\n\t\t// 等待所有副本节点的应答\n\t\tprops.put(\"acks\", \"all\");\n\t\t// 消息发送最大尝试次数\n\t\tprops.put(\"retries\", 0);\n\t\t// 一批消息处理大小\n\t\tprops.put(\"batch.size\", 16384);\n\t\t// 增加服务端请求延时\n\t\tprops.put(\"linger.ms\", 1);\n\t\t// 发送缓存区内存大小\n\t\tprops.put(\"buffer.memory\", 33554432);\n\t\t// key序列化\n\t\tprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\t\t// value序列化\n\t\tprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\t\t// 自定义分区\n\t\tprops.put(\"partitioner.class\", \"producer.CustomPartitioner\");\n\n\t\tProducer<String, String> producer = new KafkaProducer<>(props);\n\t\tproducer.send(new ProducerRecord<String, String>(\"first\", \"1\", \"hello kafka\"));\n\n\t\tproducer.close();\n\t}\n}\n```\n\n## Kafka消费者\n\n### 旧\n\n使用低级API读取指定topic，指定partition,指定offset的数据。\n\n1消费者使用低级API 的主要步骤：\n\n| 步骤 | 主要工作                               |\n| ---- | -------------------------------------- |\n| 1    | 根据指定的分区从主题元数据中找到主副本 |\n| 2    | 获取分区最新的消费进度                 |\n| 3    | 从主副本拉取分区的消息                 |\n| 4    | 识别主副本的变化，重试                 |\n\n2方法描述：\n\n| findLeader()    | 客户端向种子节点发送主题元数据，将副本集加入备用节点 |\n| --------------- | ---------------------------------------------------- |\n| getLastOffset() | 消费者客户端发送偏移量请求，获取分区最近的偏移量     |\n| run()           | 消费者低级AP I拉取消息的主要方法                     |\n| findNewLeader() | 当分区的主副本节点发生故障，客户将要找出新的主副本   |\n\n```java\nimport java.nio.ByteBuffer;\nimport java.util.ArrayList;\nimport java.util.Collections;\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\n\nimport kafka.api.FetchRequest;\nimport kafka.api.FetchRequestBuilder;\nimport kafka.api.PartitionOffsetRequestInfo;\nimport kafka.cluster.BrokerEndPoint;\nimport kafka.common.ErrorMapping;\nimport kafka.common.TopicAndPartition;\nimport kafka.javaapi.FetchResponse;\nimport kafka.javaapi.OffsetResponse;\nimport kafka.javaapi.PartitionMetadata;\nimport kafka.javaapi.TopicMetadata;\nimport kafka.javaapi.TopicMetadataRequest;\nimport kafka.javaapi.consumer.SimpleConsumer;\nimport kafka.message.MessageAndOffset;\n\npublic class OldConsumer {\n    private List<String> m_replicaBrokers = new ArrayList<>();\n\n    public OldConsumer() {\n        m_replicaBrokers = new ArrayList<>();\n    }\n\n    public static void main(String args[]) {\n        OldConsumer example = new OldConsumer();\n        // 最大读取消息数量\n        long maxReads = Long.parseLong(\"3\");\n        // 要订阅的topic\n        String topic = \"first\";\n        // 要查找的分区\n        int partition = Integer.parseInt(\"0\");\n        // broker节点的ip\n        List<String> seeds = new ArrayList<>();\n        seeds.add(\"192.168.1.101\");\n        seeds.add(\"192.168.1.102\");\n        seeds.add(\"192.168.1.103\");\n        // 端口\n        int port = Integer.parseInt(\"9092\");\n        try {\n            example.run(maxReads, topic, partition, seeds, port);\n        } catch (Exception e) {\n            System.out.println(\"Oops:\" + e);\n            e.printStackTrace();\n        }\n    }\n\n    public void run(long a_maxReads, String a_topic, int a_partition, List<String> a_seedBrokers, int a_port) throws Exception {\n        // 获取指定Topic partition的元数据\n        PartitionMetadata metadata = findLeader(a_seedBrokers, a_port, a_topic, a_partition);\n        if (metadata == null) {\n            System.out.println(\"Can't find metadata for Topic and Partition. Exiting\");\n            return;\n        }\n        if (metadata.leader() == null) {\n            System.out.println(\"Can't find Leader for Topic and Partition. Exiting\");\n            return;\n        }\n        String leadBroker = metadata.leader().host();\n        String clientName = \"Client_\" + a_topic + \"_\" + a_partition;\n\n        SimpleConsumer consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);\n        long readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.EarliestTime(), clientName);\n        int numErrors = 0;\n        while (a_maxReads > 0) {\n            if (consumer == null) {\n                consumer = new SimpleConsumer(leadBroker, a_port, 100000, 64 * 1024, clientName);\n            }\n            FetchRequest req = new FetchRequestBuilder().clientId(clientName).addFetch(a_topic, a_partition, readOffset, 100000).build();\n            FetchResponse fetchResponse = consumer.fetch(req);\n\n            if (fetchResponse.hasError()) {\n                numErrors++;\n                // Something went wrong!\n                short code = fetchResponse.errorCode(a_topic, a_partition);\n                System.out.println(\"Error fetching data from the Broker:\" + leadBroker + \" Reason: \" + code);\n                if (numErrors > 5)\n                    break;\n                if (code == ErrorMapping.OffsetOutOfRangeCode()) {\n                    // We asked for an invalid offset. For simple case ask for\n                    // the last element to reset\n                    readOffset = getLastOffset(consumer, a_topic, a_partition, kafka.api.OffsetRequest.LatestTime(), clientName);\n                    continue;\n                }\n                consumer.close();\n                consumer = null;\n                leadBroker = findNewLeader(leadBroker, a_topic, a_partition, a_port);\n                continue;\n            }\n            numErrors = 0;\n\n            long numRead = 0;\n            for (MessageAndOffset messageAndOffset : fetchResponse.messageSet(a_topic, a_partition)) {\n                long currentOffset = messageAndOffset.offset();\n                if (currentOffset < readOffset) {\n                    System.out.println(\"Found an old offset: \" + currentOffset + \" Expecting: \" + readOffset);\n                    continue;\n                }\n                readOffset = messageAndOffset.nextOffset();\n                ByteBuffer payload = messageAndOffset.message().payload();\n\n                byte[] bytes = new byte[payload.limit()];\n                payload.get(bytes);\n                System.out.println(String.valueOf(messageAndOffset.offset()) + \": \" + new String(bytes, \"UTF-8\"));\n                numRead++;\n                a_maxReads--;\n            }\n\n            if (numRead == 0) {\n                try {\n                    Thread.sleep(1000);\n                } catch (InterruptedException ie) {\n                }\n            }\n        }\n        if (consumer != null)\n            consumer.close();\n    }\n\n    public static long getLastOffset(SimpleConsumer consumer, String topic, int partition, long whichTime, String clientName) {\n        TopicAndPartition topicAndPartition = new TopicAndPartition(topic, partition);\n        Map<TopicAndPartition, PartitionOffsetRequestInfo> requestInfo = new HashMap<TopicAndPartition, PartitionOffsetRequestInfo>();\n        requestInfo.put(topicAndPartition, new PartitionOffsetRequestInfo(whichTime, 1));\n        kafka.javaapi.OffsetRequest request = new kafka.javaapi.OffsetRequest(requestInfo, kafka.api.OffsetRequest.CurrentVersion(), clientName);\n        OffsetResponse response = consumer.getOffsetsBefore(request);\n\n        if (response.hasError()) {\n            System.out.println(\"Error fetching data Offset Data the Broker. Reason: \" + response.errorCode(topic, partition));\n            return 0;\n        }\n        long[] offsets = response.offsets(topic, partition);\n        return offsets[0];\n    }\n\n\n    private String findNewLeader(String a_oldLeader, String a_topic, int a_partition, int a_port) throws Exception {\n        for (int i = 0; i < 3; i++) {\n            boolean goToSleep = false;\n            PartitionMetadata metadata = findLeader(m_replicaBrokers, a_port, a_topic, a_partition);\n            if (metadata == null) {\n                goToSleep = true;\n            } else if (metadata.leader() == null) {\n                goToSleep = true;\n            } else if (a_oldLeader.equalsIgnoreCase(metadata.leader().host()) && i == 0) {\n                // first time through if the leader hasn't changed give\n                // ZooKeeper a second to recover\n                // second time, assume the broker did recover before failover,\n                // or it was a non-Broker issue\n                //\n                goToSleep = true;\n            } else {\n                return metadata.leader().host();\n            }\n            if (goToSleep) {\n                Thread.sleep(1000);\n            }\n        }\n        System.out.println(\"Unable to find new leader after Broker failure. Exiting\");\n        throw new Exception(\"Unable to find new leader after Broker failure. Exiting\");\n    }\n\n    private PartitionMetadata findLeader(List<String> a_seedBrokers, int a_port, String a_topic, int a_partition) {\n        PartitionMetadata returnMetaData = null;\n        loop:\n        for (String seed : a_seedBrokers) {\n            SimpleConsumer consumer = null;\n            try {\n                consumer = new SimpleConsumer(seed, a_port, 100000, 64 * 1024, \"leaderLookup\");\n                List<String> topics = Collections.singletonList(a_topic);\n                TopicMetadataRequest req = new TopicMetadataRequest(topics);\n                kafka.javaapi.TopicMetadataResponse resp = consumer.send(req);\n\n                List<TopicMetadata> metaData = resp.topicsMetadata();\n                for (TopicMetadata item : metaData) {\n                    for (PartitionMetadata part : item.partitionsMetadata()) {\n                        if (part.partitionId() == a_partition) {\n                            returnMetaData = part;\n                            break loop;\n                        }\n                    }\n                }\n            } catch (Exception e) {\n                System.out.println(\"Error communicating with Broker [\" + seed + \"] to find Leader for [\" + a_topic + \", \" + a_partition + \"] Reason: \" + e);\n            } finally {\n                if (consumer != null)\n                    consumer.close();\n            }\n        }\n        if (returnMetaData != null) {\n            m_replicaBrokers.clear();\n            for (BrokerEndPoint replica : returnMetaData.replicas()) {\n                m_replicaBrokers.add(replica.host());\n            }\n        }\n        return returnMetaData;\n    }\n}\n```\n\n![FzomLj.png](https://s2.ax1x.com/2019/01/15/FzomLj.png)\n\n### 新\n\n```java\nimport java.util.HashMap;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.Properties;\nimport kafka.consumer.Consumer;\nimport kafka.consumer.ConsumerConfig;\nimport kafka.consumer.ConsumerIterator;\nimport kafka.consumer.KafkaStream;\nimport kafka.javaapi.consumer.ConsumerConnector;\n\npublic class CustomConsumer {\n\n\t@SuppressWarnings(\"deprecation\")\n\tpublic static void main(String[] args) {\n\t\tProperties properties = new Properties();\n\t\t\n\t\tproperties.put(\"zookeeper.connect\", \"datanode1:2181\");\n\t\tproperties.put(\"group.id\", \"g1\");\n\t\tproperties.put(\"zookeeper.session.timeout.ms\", \"500\");\n\t\tproperties.put(\"zookeeper.sync.time.ms\", \"250\");\n\t\tproperties.put(\"auto.commit.interval.ms\", \"1000\");\n\t\t\n\t\t// 创建消费者连接器\n\t\tConsumerConnector consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(properties));\n\t\t\n\t\tHashMap<String, Integer> topicCount = new HashMap<>();\n\t\ttopicCount.put(\"first\", 1);\n\t\t\n\t\tMap<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(topicCount);\n\t\t\n\t\tKafkaStream<byte[], byte[]> stream = consumerMap.get(\"first\").get(0);\n\t\t\n\t\tConsumerIterator<byte[], byte[]> it = stream.iterator();\n\t\t\n\t\twhile (it.hasNext()) {\n\t\t\tSystem.out.println(new String(it.next().message()));\n\t\t}\n\t}\n}\n\n```\n\n![FzIAKJ.png](https://s2.ax1x.com/2019/01/15/FzIAKJ.png)\n\n##  Kafka拦截器\n\n### 拦截器原理\n\nProducer拦截器(interceptor)在Kafka 0.10版本被引入，主要用于实现clients端的定制化控制逻辑。\n\n对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：\n\n（1）configure(configs)\n\n获取配置信息和初始化数据时调用。\n\n（2）onSend(ProducerRecord)：\n\n该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算\n\n（3）onAcknowledgement(RecordMetadata, Exception)：\n\n该方法会在消息被应答或消息发送失败时调用，并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率\n\n（4）close：\n\n关闭interceptor，主要用于执行一些资源清理工作\n\n如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。\n\n### 案例\n\n#### TimeInterceptor\n\n```java\nimport org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport java.util.Map;\n\npublic class TimeInterceptor implements ProducerInterceptor<String, String> {\n    @Override\n    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {\n        //给value添加时间戳\n        return new ProducerRecord<String, String>(record.topic(), record.partition(), record.timestamp(),\n                record.key(), \"时间戳\" +\"-----\"+ record.value());\n    }\n\n    @Override\n    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n\n    }\n\n    @Override\n    public void close() {\n\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n\n    }\n}\n```\n\n#### CounterInterceptor\n\n```Java\nimport org.apache.kafka.clients.producer.ProducerInterceptor;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\n\nimport java.util.Map;\n\npublic class CounterInterceptor implements ProducerInterceptor<String, String> {\n    private int sucessCount = 0;\n    private int errorCount = 0;\n\n    @Override\n    public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {\n        return new ProducerRecord<String, String>(record.topic(), record.partition(), record.timestamp(),\n                record.key(), \"计数的\" +\"-----\"+ record.value());\n    }\n\n    @Override\n    public void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n        if (exception == null) {\n            sucessCount++;\n        } else {\n            errorCount++;\n        }\n    }\n\n    @Override\n    public void close() {\n        System.out.println(\"成功条数\" + sucessCount);\n        System.out.println(\"失败条数\" + errorCount);\n    }\n\n    @Override\n    public void configure(Map<String, ?> configs) {\n\n    }\n}\n\n```\n\n#### InterceptorProducer\n\n```java\nimport org.apache.kafka.clients.producer.*;\n\nimport java.util.ArrayList;\nimport java.util.Properties;\n\npublic class InterceptorProducer {\n    public static void main(String[] args) {\n\n        //配置信息\n        Properties props = new Properties();\n        //Kafka集群\n        props.put(\"bootstrap.servers\", \"datanode1:9092\");\n\n        ArrayList<String> interceptors = new ArrayList<>();\n        //这个调用是有逻辑顺序的 按照顺序调用的 \n        interceptors.add(\"intercepter.TimeInterceptor\");\n        interceptors.add(\"intercepter.CounterInterceptor\");\n\n        props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors);\n        //KV 序列化类\n        props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n\n        Producer<String, String> producer = new KafkaProducer<>(props);\n        for (int i = 0; i < 10; i++)\n            producer.send(new ProducerRecord<>(\"second\", String.valueOf(i), String.valueOf(i)),\n                    (metadata, exception) -> System.out.println(metadata.partition() + \"-----------\" + metadata.offset()));\n        producer.close();\n    }\n}\n```\n\n![FzTk7R.png](https://s2.ax1x.com/2019/01/15/FzTk7R.png)\n\n### Kafka Streams \n\nApache Kafka开源项目的一个组成部分。是一个功能强大，易于使用的库。用于在Kafka上构建高可分布式、拓展性，容错的应用程序。但是用的不多.目前用Spark和Flink做流式实时计算的比较多。\n\n#### 特点\n\n功能强大 ：高扩展性，弹性，容错 \n\n轻量级 ：无需专门的集群  一个库，而不是框架\n\n完全集成：100%的Kafka 0.10.0版本兼容 易于集成到现有的应用程序 \n\n实时性： 毫秒级延迟 并非微批处理  窗口允许乱序数据  允许迟到数据\n\n#### 由来\n\n当前已经有非常多的流式处理系统，最知名且应用最多的开源流式处理系统有Spark Streaming和Apache Storm。Apache Storm发展多年，应用广泛，提供记录级别的处理能力，当前也支持SQL on Stream。而Spark Streaming基于Apache Spark，可以非常方便与图计算，SQL处理等集成，功能强大，对于熟悉其它Spark应用开发的用户而言使用门槛低。另外，目前主流的Hadoop发行版，如Cloudera和Hortonworks，都集成了Apache Storm和Apache Spark，使得部署更容易。\n\n既然Apache Spark与Apache Storm拥用如此多的优势，那为何还需要Kafka Stream呢？主要有如下原因。\n\n第一，Spark和Storm都是流式处理框架，而Kafka Stream提供的是一个基于Kafka的流式处理类库。框架要求开发者按照特定的方式去开发逻辑部分，供框架调用。开发者很难了解框架的具体运行方式，从而使得调试成本高，并且使用受限。而Kafka Stream作为流式处理类库，直接提供具体的类给开发者调用，整个应用的运行方式主要由开发者控制，方便使用和调试。\n\n![Fz7SUI.png](https://s2.ax1x.com/2019/01/15/Fz7SUI.png)\n\n第二，虽然Cloudera与Hortonworks方便了Storm和Spark的部署，但是这些框架的部署仍然相对复杂。而Kafka Stream作为类库，可以非常方便的嵌入应用程序中，它对应用的打包和部署基本没有任何要求。\n\n第三，就流式处理系统而言，基本都支持Kafka作为数据源。例如Storm具有专门的kafka-spout，而Spark也提供专门的spark-streaming-kafka模块。事实上，Kafka基本上是主流的流式处理系统的标准数据源。换言之，大部分流式系统中都已部署了Kafka，此时使用Kafka Stream的成本非常低。\n\n第四，使用Storm或Spark Streaming时，需要为框架本身的进程预留资源，如Storm的supervisor和Spark on YARN的node manager。即使对于应用实例而言，框架本身也会占用部分资源，如Spark Streaming需要为shuffle和storage预留内存。但是Kafka作为类库不占用系统资源。\n\n第五，由于Kafka本身提供数据持久化，因此Kafka Stream提供滚动部署和滚动升级以及重新计算的能力。\n\n第六，由于Kafka Consumer Rebalance机制，Kafka Stream可以在线动态调整并行度。\n\n#### 数据清洗\n\n![Fz7GqJ.png](https://s2.ax1x.com/2019/01/15/Fz7GqJ.png)\n\n##### LogProcessor\n\n```java\nimport org.apache.kafka.streams.processor.Processor;\nimport org.apache.kafka.streams.processor.ProcessorContext;\n\npublic class LogProcessor implements Processor<byte[], byte[]> {\n    private ProcessorContext context = null;\n\n    @Override\n    public void init(ProcessorContext context) {\n        this.context = context;\n    }\n\n    @Override\n    public void process(byte[] key, byte[] value) {\n\n        String line = new String(value);\n\n        if (line.contains(\">>>\")) {\n            String[] split = line.split(\">>>\");\n            line = split[1];\n        }\n        context.forward(key, line.getBytes());\n    }\n\n\n    @Override\n    public void punctuate(long l) {\n\n    }\n\n    @Override\n    public void close() {\n\n    }\n}\n\n```\n\n##### MyStream\n\n```java\nimport org.apache.kafka.streams.KafkaStreams;\nimport org.apache.kafka.streams.StreamsConfig;\nimport org.apache.kafka.streams.processor.ProcessorSupplier;\nimport org.apache.kafka.streams.processor.TopologyBuilder;\n\nimport java.util.Properties;\n\npublic class MyStream {\n    public static void main(String[] args) {\n\n        //配置信息\n        Properties props = new Properties();\n        //Kafka集群\n        props.put(\"bootstrap.servers\", \"datanode1:9092\");\n        props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"LogStream\");\n        //创建拓扑对象\n        TopologyBuilder builder = new TopologyBuilder();\n\n         builder.addSource(\"SOURCE\", \"first\").addProcessor(\"PROCESSOR\", (ProcessorSupplier<byte[], byte[]>)\n                LogProcessor::new, \"SOURCE\")\n                .addSink(\"SINK\", \"second\", \"PROCESSOR\");\n        //创建kafkastream\n        KafkaStreams streams = new KafkaStreams(builder, props);\n        streams.start();\n    }\n}\n```\n\n![FzHkJx.png](https://s2.ax1x.com/2019/01/15/FzHkJx.png)\n\n### Kafka与Flume比较\n\n| flume                          | Kafka                                           |\n| ------------------------------ | ----------------------------------------------- |\n| cloudera公司研发:              | linkedin公司研发:                               |\n| 适合多个生产者；               | 适合数据下游消费众多的情况；                    |\n| 适合下游数据消费者不多的情况； | 适合数据安全性要求较高的操作，支持replication。 |\n| 适合数据安全性要求不高的操作； |                                                 |\n| 适合与Hadoop生态圈对接的操作。 |                                                 |\n\n常用模型\n\n线上数据 --> flume--> kafka --> flume(根据情景增删该流程) --> HDFS\n\n#### Flume和Kafka集成\n\n```properties\n# define\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F -c +0 /opt/module/datas/flume.log\na1.sources.r1.shell = /bin/bash -c\n\n# sink\na1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink\na1.sinks.k1.kafka.bootstrap.servers = datanode1:9092,datanode2:9092,datanode3:9092\na1.sinks.k1.kafka.topic = three\na1.sinks.k1.kafka.flumeBatchSize = 20\na1.sinks.k1.kafka.producer.acks = 1\na1.sinks.k1.kafka.producer.linger.ms = 1\n\n# channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# bind\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n```\n\n启动消费者\n\n```shell\n[hadoop@datanode2 kafka]$ bin/kafka-console-consumer.sh --zookeeper datanode1:2181 --from-beginning --topic three\n```\n启动flume\n```shell\n[hadoop@datanode1 flume]$ bin/flume-ng agent -c conf/ -n a1 -f job/flume-kafka.conf\n```\n\n测试脚本\n\n```shell\n#!bin/bash\ni=1\nwhile [ true ]\nlet i+=1\nd=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\ndo\n echo \"$d $i\">>/opt/module/datas/flume.log\n sleep 1\ndone\n```\n\n![kSFRxg.png](https://s2.ax1x.com/2019/01/16/kSFRxg.png)\n\n### Kafka配置信息\n\n#### Broker配置信息\n\n| **属性**                                | **默认值**         | **描述**                                                     |\n| --------------------------------------- | ------------------ | ------------------------------------------------------------ |\n| broker.id                               |                    | 必填参数，broker的唯一标识                                   |\n| log.dirs                                | /tmp/kafka-logs    | Kafka数据存放的目录。可以指定多个目录，中间用逗号分隔，当新partition被创建的时会被存放到当前存放partition最少的目录。 |\n| port                                    | 9092               | BrokerServer接受客户端连接的端口号                           |\n| zookeeper.connect                       | null               | Zookeeper的连接串，格式为：hostname1:port1,hostname2:port2,hostname3:port3。可以填一个或多个，为了提高可靠性，建议都填上。注意，此配置允许我们指定一个zookeeper路径来存放此kafka集群的所有数据，为了与其他应用集群区分开，建议在此配置中指定本集群存放目录，格式为：hostname1:port1,hostname2:port2,hostname3:port3/chroot/path   。需要注意的是，消费者的参数要和此参数一致。 |\n| message.max.bytes                       | 1000000            | 服务器可以接收到的最大的消息大小。注意此参数要和consumer的maximum.message.size大小一致，否则会因为生产者生产的消息太大导致消费者无法消费。 |\n| num.io.threads                          | 8                  | 服务器用来执行读写请求的IO线程数，此参数的数量至少要等于服务器上磁盘的数量。 |\n| queued.max.requests                     | 500                | I/O线程可以处理请求的队列大小，若实际请求数超过此大小，网络线程将停止接收新的请求。 |\n| socket.send.buffer.bytes                | 100 * 1024         | The SO_SNDBUFF buffer the server prefers for socket   connections. |\n| socket.receive.buffer.bytes             | 100 * 1024         | The SO_RCVBUFF buffer the server prefers for socket   connections. |\n| socket.request.max.bytes                | 100 * 1024 * 1024  | 服务器允许请求的最大值， 用来防止内存溢出，其值应该小于 Java heap size. |\n| num.partitions                          | 1                  | 默认partition数量，如果topic在创建时没有指定partition数量，默认使用此值，建议改为5 |\n| log.segment.bytes                       | 1024 * 1024 * 1024 | Segment文件的大小，超过此值将会自动新建一个segment，此值可以被topic级别的参数覆盖。 |\n| log.roll.{ms,hours}                     | 24 * 7 hours       | 新建segment文件的时间，此值可以被topic级别的参数覆盖。       |\n| log.retention.{ms,minutes,hours}        | 7 days             | Kafka segment log的保存周期，保存周期超过此时间日志就会被删除。此参数可以被topic级别参数覆盖。数据量大时，建议减小此值。 |\n| log.retention.bytes                     | -1                 | 每个partition的最大容量，若数据量超过此值，partition数据将会被删除。注意这个参数控制的是每个partition而不是topic。此参数可以被log级别参数覆盖。 |\n| log.retention.check.interval.ms         | 5 minutes          | 删除策略的检查周期                                           |\n| auto.create.topics.enable               | true               | 自动创建topic参数，建议此值设置为false，严格控制topic管理，防止生产者错写topic。 |\n| default.replication.factor              | 1                  | 默认副本数量，建议改为2。                                    |\n| replica.lag.time.max.ms                 | 10000              | 在此窗口时间内没有收到follower的fetch请求，leader会将其从ISR(in-sync replicas)中移除。 |\n| replica.lag.max.messages                | 4000               | 如果replica节点落后leader节点此值大小的消息数量，leader节点就会将其从ISR中移除。 |\n| replica.socket.timeout.ms               | 30 * 1000          | replica向leader发送请求的超时时间。                          |\n| replica.socket.receive.buffer.bytes     | 64 * 1024          | The socket receive buffer for network requests to the   leader for replicating data. |\n| replica.fetch.max.bytes                 | 1024 * 1024        | The number of byes of messages to attempt to fetch for each   partition in the fetch requests the replicas send to the leader. |\n| replica.fetch.wait.max.ms               | 500                | The maximum amount of time to wait time for data to arrive   on the leader in the fetch requests sent by the replicas to the leader. |\n| num.replica.fetchers                    | 1                  | Number of threads used to replicate messages from leaders.   Increasing this value can increase the degree of I/O parallelism in the   follower broker. |\n| fetch.purgatory.purge.interval.requests | 1000               | The purge interval (in number of requests) of the fetch   request purgatory. |\n| zookeeper.session.timeout.ms            | 6000               | ZooKeeper session 超时时间。如果在此时间内server没有向zookeeper发送心跳，zookeeper就会认为此节点已挂掉。 此值太低导致节点容易被标记死亡；若太高，.会导致太迟发现节点死亡。 |\n| zookeeper.connection.timeout.ms         | 6000               | 客户端连接zookeeper的超时时间。                              |\n| zookeeper.sync.time.ms                  | 2000               | H ZK follower落后 ZK   leader的时间。                        |\n| controlled.shutdown.enable              | true               | 允许broker   shutdown。如果启用，broker在关闭自己之前会把它上面的所有leaders转移到其它brokers上，建议启用，增加集群稳定性。 |\n| auto.leader.rebalance.enable            | true               | If this is enabled the controller will automatically try to   balance leadership for partitions among the brokers by periodically returning   leadership to the “preferred” replica for each partition if it is available. |\n| leader.imbalance.per.broker.percentage  | 10                 | The percentage of leader imbalance allowed per broker. The   controller will rebalance leadership if this ratio goes above the configured   value per broker. |\n| leader.imbalance.check.interval.seconds | 300                | The frequency with which to check for leader imbalance.      |\n| offset.metadata.max.bytes               | 4096               | The maximum amount of metadata to allow clients to save   with their offsets. |\n| connections.max.idle.ms                 | 600000             | Idle connections timeout: the server socket processor   threads close the connections that idle more than this. |\n| num.recovery.threads.per.data.dir       | 1                  | The number of threads per data directory to be used for log   recovery at startup and flushing at shutdown. |\n| unclean.leader.election.enable          | true               | Indicates whether to enable replicas not in the ISR set to   be elected as leader as a last resort, even though doing so may result in   data loss. |\n| delete.topic.enable                     | false              | 启用deletetopic参数，建议设置为true。                        |\n| offsets.topic.num.partitions            | 50                 | The number of partitions for the offset commit topic. Since   changing this after deployment is currently unsupported, we recommend using a   higher setting for production (e.g., 100-200). |\n| offsets.topic.retention.minutes         | 1440               | Offsets that are older than this age will be marked for   deletion. The actual purge will occur when the log cleaner compacts the   offsets topic. |\n| offsets.retention.check.interval.ms     | 600000             | The frequency at which the offset manager checks for stale   offsets. |\n| offsets.topic.replication.factor        | 3                  | The replication factor for the offset commit topic. A   higher setting (e.g., three or four) is recommended in order to ensure higher   availability. If the offsets topic is created when fewer brokers than the   replication factor then the offsets topic will be created with fewer   replicas. |\n| offsets.topic.segment.bytes             | 104857600          | Segment size for the offsets topic. Since it uses a   compacted topic, this should be kept relatively low in order to facilitate   faster log compaction and loads. |\n| offsets.load.buffer.size                | 5242880            | An offset load occurs when a broker becomes the offset   manager for a set of consumer groups (i.e., when it becomes a leader for an   offsets topic partition). This setting corresponds to the batch size (in   bytes) to use when reading from the offsets segments when loading offsets   into the offset manager’s cache. |\n| offsets.commit.required.acks            | -1                 | The number of acknowledgements that are required before the   offset commit can be accepted. This is similar to the producer’s   acknowledgement setting. In general, the default should not be overridden. |\n| offsets.commit.timeout.ms               | 5000               | The offset commit will be delayed until this timeout or the   required number of replicas have received the offset commit. This is similar   to the producer request timeout. |\n\n####  Producer配置信息\n\n| **属性**                           | **默认值**                        | **描述**                                                     |\n| ---------------------------------- | --------------------------------- | ------------------------------------------------------------ |\n| metadata.broker.list               |                                   | 启动时producer查询brokers的列表，可以是集群中所有brokers的一个子集。注意，这个参数只是用来获取topic的元信息用，producer会从元信息中挑选合适的broker并与之建立socket连接。格式是：host1:port1,host2:port2。 |\n| request.required.acks              | 0                                 | 参见3.2节介绍                                                |\n| request.timeout.ms                 | 10000                             | Broker等待ack的超时时间，若等待时间超过此值，会返回客户端错误信息。 |\n| producer.type                      | sync                              | 同步异步模式。async表示异步，sync表示同步。如果设置成异步模式，可以允许生产者以batch的形式push数据，这样会极大的提高broker性能，推荐设置为异步。 |\n| serializer.class                   | kafka.serializer.DefaultEncoder   | 序列号类，.默认序列化成 byte[] 。                            |\n| key.serializer.class               |                                   | Key的序列化类，默认同上。                                    |\n| partitioner.class                  | kafka.producer.DefaultPartitioner | Partition类，默认对key进行hash。                             |\n| compression.codec                  | none                              | 指定producer消息的压缩格式，可选参数为： “none”, “gzip” and “snappy”。关于压缩参见4.1节 |\n| compressed.topics                  | null                              | 启用压缩的topic名称。若上面参数选择了一个压缩格式，那么压缩仅对本参数指定的topic有效，若本参数为空，则对所有topic有效。 |\n| message.send.max.retries           | 3                                 | Producer发送失败时重试次数。若网络出现问题，可能会导致不断重试。 |\n| retry.backoff.ms                   | 100                               | Before each retry, the producer refreshes the metadata of   relevant topics to see if a new leader has been elected. Since leader   election takes a bit of time, this property specifies the amount of time that   the producer waits before refreshing the metadata. |\n| topic.metadata.refresh.interval.ms | 600 * 1000                        | The producer generally refreshes the topic metadata from   brokers when there is a failure (partition missing, leader not available…).   It will also poll regularly (default: every 10min so 600000ms). If you set   this to a negative value, metadata will only get refreshed on failure. If you   set this to zero, the metadata will get refreshed after each message sent   (not recommended). Important note: the refresh happen only AFTER the message   is sent, so if the producer never sends a message the metadata is never   refreshed |\n| queue.buffering.max.ms             | 5000                              | 启用异步模式时，producer缓存消息的时间。比如我们设置成1000时，它会缓存1秒的数据再一次发送出去，这样可以极大的增加broker吞吐量，但也会造成时效性的降低。 |\n| queue.buffering.max.messages       | 10000                             | 采用异步模式时producer   buffer 队列里最大缓存的消息数量，如果超过这个数值，producer就会阻塞或者丢掉消息。 |\n| queue.enqueue.timeout.ms           | -1                                | 当达到上面参数值时producer阻塞等待的时间。如果值设置为0，buffer队列满时producer不会阻塞，消息直接被丢掉。若值设置为-1，producer会被阻塞，不会丢消息。 |\n| batch.num.messages                 | 200                               | 采用异步模式时，一个batch缓存的消息数量。达到这个数量值时producer才会发送消息。 |\n| send.buffer.bytes                  | 100 * 1024                        | Socket write buffer size                                     |\n| client.id                          | “”                                | The client id is a user-specified string sent in each   request to help trace calls. It should logically identify the application   making the request. |\n\n#### Consumer配置信息\n\n| **属性**                        | **默认值**  | **描述**                                                     |\n| ------------------------------- | ----------- | ------------------------------------------------------------ |\n| group.id                        |             | Consumer的组ID，相同goup.id的consumer属于同一个组。          |\n| zookeeper.connect               |             | Consumer的zookeeper连接串，要和broker的配置一致。            |\n| consumer.id                     | null        | 如果不设置会自动生成。                                       |\n| socket.timeout.ms               | 30 * 1000   | 网络请求的socket超时时间。实际超时时间由max.fetch.wait + socket.timeout.ms 确定。 |\n| socket.receive.buffer.bytes     | 64 * 1024   | The socket receive buffer for network requests.              |\n| fetch.message.max.bytes         | 1024 * 1024 | 查询topic-partition时允许的最大消息大小。consumer会为每个partition缓存此大小的消息到内存，因此，这个参数可以控制consumer的内存使用量。这个值应该至少比server允许的最大消息大小大，以免producer发送的消息大于consumer允许的消息。 |\n| num.consumer.fetchers           | 1           | The number fetcher threads used to fetch data.               |\n| auto.commit.enable              | true        | 如果此值设置为true，consumer会周期性的把当前消费的offset值保存到zookeeper。当consumer失败重启之后将会使用此值作为新开始消费的值。 |\n| auto.commit.interval.ms         | 60 * 1000   | Consumer提交offset值到zookeeper的周期。                      |\n| queued.max.message.chunks       | 2           | 用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。 |\n| auto.commit.interval.ms         | 60 * 1000   | Consumer提交offset值到zookeeper的周期。                      |\n| queued.max.message.chunks       | 2           | 用来被consumer消费的message chunks 数量， 每个chunk可以缓存fetch.message.max.bytes大小的数据量。 |\n| fetch.min.bytes                 | 1           | The minimum amount of data the server should return for a   fetch request. If insufficient data is available the request will wait for   that much data to accumulate before answering the request. |\n| fetch.wait.max.ms               | 100         | The maximum amount of time the server will block before   answering the fetch request if there isn’t sufficient data to immediately   satisfy fetch.min.bytes. |\n| rebalance.backoff.ms            | 2000        | Backoff time between retries during rebalance.               |\n| refresh.leader.backoff.ms       | 200         | Backoff time to wait before trying to determine the leader   of a partition that has just lost its leader. |\n| auto.offset.reset               | largest     | What to do when there is no initial offset in ZooKeeper or   if an offset is out of range ;smallest : automatically reset the offset to   the smallest offset; largest : automatically reset the offset to the largest   offset;anything else: throw exception to the consumer |\n| consumer.timeout.ms             | -1          | 若在指定时间内没有消息消费，consumer将会抛出异常。           |\n| exclude.internal.topics         | true        | Whether messages from internal topics (such as offsets)   should be exposed to the consumer. |\n| zookeeper.session.timeout.ms    | 6000        | ZooKeeper session timeout. If the consumer fails to   heartbeat to ZooKeeper for this period of time it is considered dead and a   rebalance will occur. |\n| zookeeper.connection.timeout.ms | 6000        | The max time that the client waits while establishing a   connection to zookeeper. |\n| zookeeper.sync.time.ms          | 2000        | How far a ZK follower can be behind a ZK leader              |\n\n参考资料 :尚硅谷大数据\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Kafka"],"categories":["大数据"]},{"title":"Git使用","url":"/2019/01/11/Git使用/","content":" {{ \"Git的简介和使用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\n​\tGit是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。  Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。Torvalds 开始着手开发 Git 是为了作为一种过渡方案来替代 BitKeeper。\n\n## 功能\n\n![FX2NtA.png](https://s2.ax1x.com/2019/01/11/FX2NtA.png)\n\n## 对比\n\n### 集中管理型版本管理\n\n![FX22hn.png](https://s2.ax1x.com/2019/01/11/FX22hn.png)\n\n代表：CVS、VSS、SVN\n\n优点：实现了大部分开发中对版本管理的需求  结构简单，上手容易。\n\n问题：\n\n1、版本管理的服务器一旦崩溃，硬盘损坏，代码如何恢复？\n\n2、程序员上传到服务器的代码要求是完整版本，但是程序员开发过程中想做小版本的管理，以便追溯查询，怎么办？\n\n3、系统正在上线运行，时不时还要修改bug，要增加好几个功能要几个月，如何管理几个版本？\n\n4、如何管理一个分布在世界各地、互不相识的大型开发团队？\n\n## 解决方案\n\n![FX2vjK.png](https://s2.ax1x.com/2019/01/11/FX2vjK.png)\n\n## 安装\n\n![FXRbqS.png](https://s2.ax1x.com/2019/01/11/FXRbqS.png)\n\n![FXRX5j.png](https://s2.ax1x.com/2019/01/11/FXRX5j.png)\n\n![FXRz2q.png](https://s2.ax1x.com/2019/01/11/FXRz2q.png)\n\n![FXWCrT.png](https://s2.ax1x.com/2019/01/11/FXWCrT.png)\n\n![FXWka4.png](https://s2.ax1x.com/2019/01/11/FXWka4.png)\n\n​\t选择Git命令的执行环境，这里推荐选择第一个，就是单独用户Git自己的命令行窗口。不推荐和windows的命令行窗口混用。\n\n![FXWmxx.png](https://s2.ax1x.com/2019/01/11/FXWmxx.png)\n\n在“Configuring the line ending conversions”选项中，\n\n第一个选项：如果是跨平台项目，在windows系统安装，选择；\n\n第二个选项：如果是跨平台项目，在Unix系统安装，选择；\n\n第三个选项：非跨平台项目，选择。\n\n![FXWlZD.png](https://s2.ax1x.com/2019/01/11/FXWlZD.png)\n\n![FXWtzt.png](https://s2.ax1x.com/2019/01/11/FXWtzt.png)\n\n![FXWdL8.png](https://s2.ax1x.com/2019/01/11/FXWdL8.png)\n\n![FXfpYd.png](https://s2.ax1x.com/2019/01/11/FXfpYd.png)\n\nGit是分布式版本控制系统，所以需要填写用户名和邮箱作为一个标识。\n\n![FXfW9A.png](https://s2.ax1x.com/2019/01/11/FXfW9A.png)\n\n## 使用\n\n- 工作区(Working Directory):就是你电脑本地硬盘目录\n- 本地库(Repository):工作区有个隐藏目录.git，它就是Git的本地版本库\n- 暂存区(stage):一般存放在\"git目录\"下的index文件（.git/index）中，所以我们把暂存区有时也叫作索引（index）。\n\n![FXhi4J.png](https://s2.ax1x.com/2019/01/11/FXhi4J.png)\n\n1. 创建项目文件夹设置文件夹属性\n\n任意位置创建空文件夹，作为项目文件夹设置文件夹属性，可查看隐藏文件\n\n2. 创建本地版本仓库\n\n在项目文件夹内右键打开git bash窗口  输入命令:  git  init\n\n![FvJ6HK.png](https://s2.ax1x.com/2019/01/13/FvJ6HK.png)\n\n![FvJf9H.png](https://s2.ax1x.com/2019/01/13/FvJf9H.png)\n\n### 提交文件\n\n\n| 命令                        | 作用                               |\n| --------------------------- | ---------------------------------- |\n| git  init                   | 初始化本地仓库                     |\n| git add 文件名              | 将添加到暂存区                     |\n| git rm  - - cached <文件名> | 删除暂存区的文件                   |\n| git rm <文件名>             | 和git没什么关系，就相当于linux命令 |\n| git commit                  | 编写注释，完成提交文件到本地库     |\n| git commit  –m “注释内容”   | 直接带注释提交(推荐使用)           |\n\n下面进行更新操作\n\n```shell\necho 11111 >> hello.txt\ngit add hello.txt\ngit commit -m \"update 1\"\n\necho 22222 >> hello.txt\ngit add hello.txt\ngit commit -m \"update 2\"\n\necho 33333 >> hello.txt\ngit add hello.txt\ngit commit -m \"update 3\"\n\necho 44444 >> hello.txt\ngit add hello.txt\ngit commit -m \"update 4\"\n\necho 55555 >> hello.txt\ngit add hello.txt\ngit commit -m \"update 5\"\n```\n\n### 查看提交记录\n\n| 命令                               | 功能               |\n| ---------------------------------- | ------------------ |\n| git log 文件名                     | 查看仓库的历史记录 |\n| git log  --pretty=oneline   文件名 | 查看简易信息       |\n\n![FvYTz9.png](https://s2.ax1x.com/2019/01/13/FvYTz9.png)\n\n### 回退历史\n\n| 命令                     | 功能                 |\n| ------------------------ | -------------------- |\n| git reset  --hard HEAD^  | 回退到上一次提交     |\n| git reset  --hard HEAD~n | 回退n次操作          |\n| git reset 文件名         | 撤销文件缓存区的状态 |\n\n![Fvtnzj.png](https://s2.ax1x.com/2019/01/13/Fvtnzj.png)\n\n![FvtKQs.png](https://s2.ax1x.com/2019/01/13/FvtKQs.png)\n\n### 版本穿越\n\n| 命令                       | 功能                 |\n| -------------------------- | -------------------- |\n| git reflog 文件名          | 查看历史记录的版本号 |\n| git  reset  --hard  版本号 | 穿越到特定版本       |\n\n![FvtXXn.png](https://s2.ax1x.com/2019/01/13/FvtXXn.png)\n\n### 还原文件\n\n| 命令                 | 功能                                               |\n| -------------------- | -------------------------------------------------- |\n| git  checkout 文件名 | 仓库中的文件依然存在，所以可以从本地仓库中还原文件 |\n\n### 删除文件\n\n 删除项目文件夹中的文件\n\ngit  add 文件名 （而是把上面的操作添加进git）\n\ngit  commit, 真正地删除仓库中的文件\n\n•注意：删除只是这一次操作的版本号没有了，其他的都可以恢复。\n\n![FvUcMd.png](https://s2.ax1x.com/2019/01/13/FvUcMd.png)\n\n###  分支管理\n\n#### 创建分支\n\n| 命令                       | 功能     |\n| -------------------------- | -------- |\n| git    branch  <分支名>    | 创建分支 |\n| git    branch  -v 查看分支 | 查看分支 |\n\n![FvaeJO.png](https://s2.ax1x.com/2019/01/13/FvaeJO.png)\n\n#### 切换分支\n\n| 命令                            | 功能                       |\n| ------------------------------- | -------------------------- |\n| git     checkout  <分支名>      | 创建分支                   |\n| git      checkout  –b  <分支名> | 创建分支，切换分支一起完成 |\n\n#### 合并分支\n\n| 命令                   | 功能         |\n| ---------------------- | ------------ |\n| git  checkout   master | 切换到master |\n| git  merge  <分支名>   | 合并分支     |\n\n#### 冲突解决\n\n冲突：冲突一般指同一个文件同一位置的代码，在两种版本合并时版本管理软件无法判断到底应该保留哪个版本，因此会提示该文件发生冲突，需要程序员来手工判断解决冲突。\n\n合并时冲突：程序合并时发生冲突系统会提示**CONFLICT**关键字，命令行后缀会进入**MERGING**状态，表示此时是解决冲突的状态。\n\n![Fv4MeP.png](https://s2.ax1x.com/2019/01/13/Fv4MeP.png)\n\n解决冲突：此时通过git diff 可以找到发生冲突的文件及冲突的内容。\n\n![Fv43FS.png](https://s2.ax1x.com/2019/01/13/Fv43FS.png)\n\n手动解决：\n\n![Fv47SH.png](https://s2.ax1x.com/2019/01/13/Fv47SH.png)\n\n## GitHub使用\n\nGitHub是什么\n\nHUB是一个多端口的转发器，在以HUB为中心设备时，即使网络中某条线路产生了故障，并不影响其它线路的工作。\n\nGitHub是一个Git项目托管网站,主要提供基于Git的版本托管服务\n\n![Fv529g.png](https://s2.ax1x.com/2019/01/13/Fv529g.png)\n\n创建项目,现在私仓也免费了。\n\n![Fv56N8.png](https://s2.ax1x.com/2019/01/13/Fv56N8.png)\n\n### 添加远程地址\n\n• git remote add  远端代号   远端地址 。\n\n• 远端代号 是指远程链接的代号，一般直接用origin作代号，也可以自定义。\n\n•远端地址  默认远程链接的url\n\n•例： git  remote  add  origin  https://github.com/xxxxxx.git\n\n![FvIiCD.png](https://s2.ax1x.com/2019/01/13/FvIiCD.png)\n\n![FvIAvd.png](https://s2.ax1x.com/2019/01/13/FvIAvd.png)\n\n### 克隆项目\n\n![FvIYbq.png](https://s2.ax1x.com/2019/01/13/FvIYbq.png)\n\n从GitHub上克隆（复制）一个项目\n\ngit  clone   远端地址   新项目目录名。\n\n• 远端地址 是指远程链接的地址。\n\n•项目目录名  是指为克隆的项目在本地新建的目录名称，可以不填，默认是GitHub的项目名。\n\n•命令执行完后，会自动为这个远端地址建一个名为origin的代号。\n\n•例 git  clone  https://github.com/xxxxxxx.git   文件夹名\n\n### 添加合作伙伴\n\n![FvIsM9.png](https://s2.ax1x.com/2019/01/13/FvIsM9.png)\n\n### 拷贝连接\n\n![Fvony9.png](https://s2.ax1x.com/2019/01/13/Fvony9.png)\n\n![FvoZz4.png](https://s2.ax1x.com/2019/01/13/FvoZz4.png)\n\n![FvouLR.png](https://s2.ax1x.com/2019/01/13/FvouLR.png)\n\n![FvoDFf.png](https://s2.ax1x.com/2019/01/13/FvoDFf.png)\n\n### 更新项目\n\n\n\n从GitHub更新项目\n\n•git  pull   远端代号   远端分支名。\n\n• 远端代号 是指远程链接的代号。\n\n•远端分支名是指远端的分支名称，如master。 \n\n例 git pull origin  master\n\n![FvohT0.png](https://s2.ax1x.com/2019/01/13/FvohT0.png)\n\n### 协作冲突\n\n![。FvTU9U.png](https://s2.ax1x.com/2019/01/13/FvTU9U.png)\n\n仍需要手动去解决。\n\n\n\n","tags":["Git"],"categories":["工具使用"]},{"title":"Oozie","url":"/2019/01/10/Oozie/","content":"\n {{ \"Oozie的安装和任务调度\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\nOozie英文翻译为：驯象人。一个基于工作流引擎的开源框架，由Cloudera公司贡献给Apache，提供对Hadoop\nMapreduce、Pig Jobs的任务调度与协调。Oozie需要部署到Java Servlet容器中运行。主要用于定时调度任务，多任务可以按照执行的逻辑顺序调度。\n\n## 功能\n\nOozie是一个管理Hdoop作业（job）的工作流程调度管理系统\nOozie的工作流是一系列动作的直接周期图（DAG）\nOozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程\nOozie是Yahoo针对Apache Hadoop开发的一个开源工作流引擎。用于管理和协调运行在Hadoop平台上（包括：HDFS、Pig和MapReduce）的Jobs。Oozie是专为雅虎的全球大规模复杂工作流程和数据管道而设计\nOozie围绕两个核心：工作流和协调器，前者定义任务的拓扑和执行逻辑，后者负责工作流的依赖和触发\n\n## 模块\n\n1. Workflow：顺序执行流程节点，支持fork（分支多个节点），join（合并多个节点为一个）\n\n2. Coordinator：定时触发workflow\n\n3. Bundle Job：绑定多个Coordinator\n\n### 常用节点\n\n1. 控制流节点（Control Flow Nodes）：控制流节点一般都是定义在工作流开始或者结束的位置，比如start,end,kill等。以及提供工作流的执行路径机制，如decision，fork，join等。\n\n2. 动作节点（Action  Nodes）：负责执行具体动作的节点，比如：拷贝文件，执行某个Shell脚本等等。\n\n## 部署\n\n所需软件链接  链接：链接：https://pan.baidu.com/s/18_iOFGL06g7_Ye-mZZRwag   提取码：qlbu \n\n\n###  部署 Hadoop\n\n这里不详细介绍，请查阅Hadoop安装，这里用的是Clouder公司的CDH版本的Hadop。\n\n### 修改配置\n\n#### core-site.xml\n\n```xml\n[hadoop@datanode1 hadoop]$ vim core-site.xml\n<configuration>\n        <!-- 指定HDFS中NameNode的地址 -->\n        <property>\n                <name>fs.defaultFS</name>\n                <value>hdfs://datanode1:9000</value>\n        </property>\n        <!-- 指定hadoop运行时产生文件的存储目录 -->\n        <property>\n                <name>hadoop.tmp.dir</name>\n                <value>/opt/module/cdh/hadoop-2.5.0-cdh5.3.6/data</value>\n        </property>\n         <property>\n                <name>fs.trash.interval </name>\n                <value>60</value>\n        </property>\n        <!-- Oozie Server的Hostname -->\n        <property>\n                <name>hadoop.proxyuser.hadoop.hosts</name>\n                <value>*</value>\n        </property>\n\n        <!-- 允许被Oozie代理的用户组 -->\n        <property>\n                <name>hadoop.proxyuser.hadoop.groups</name>\n                <value>*</value>\n        </property>\n</configuration>\n```\n\nhadoop.proxyuser.admin.hosts类似属性中的hadoop用户替换成你的hadoop用户。因为我的用户名就是hadoop\n\n####  yarn-site.xml\n\n```xml\n[hadoop@datanode1 hadoop]$ vim yarn-site.xml\n<configuration>\n        <property>\n                <name>yarn.nodemanager.aux-services</name>\n                <value>mapreduce_shuffle</value>\n        </property>\n\n        <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>datanode2</value>\n        </property>\n\n        <property>\n                <name>yarn.log-aggregation-enable</name>\n                <value>true</value>\n        </property>\n\n        <property>\n                <name>yarn.log-aggregation.retain-seconds</name>\n                <value>86400</value>\n        </property>\n\n        <!-- 任务历史服务 -->\n        <property>\n                <name>yarn.log.server.url</name>\n                <value>http://datanode1:19888/jobhistory/logs/</value>\n        </property>\n</configuration>\n\n```\n\n#### mapred-site.xml\n\n```xml\n<configuration>\n        <property>\n        <name>mapreduce.framework.name</name>\n        <value>yarn</value>\n    </property>\n    <!-- 配置 MapReduce JobHistory Server 地址 ，默认端口10020 -->\n    <property>\n        <name>mapreduce.jobhistory.address</name>\n        <value>datanode1:10020</value>\n    </property>\n    <!-- 配置 MapReduce JobHistory Server web ui 地址， 默认端口19888 -->\n    <property>\n        <name>mapreduce.jobhistory.webapp.address</name>\n        <value>datanode1:19888</value>\n    </property>\n</configuration>\n```\n\n不要忘记同步到其他集群 然后namenode -for mate 执行初始化\n\n### 部署 Oozie\n\n#### oozie根目录下解压hadooplibs\n\n```shell\n tar -zxf oozie-hadooplibs-4.0.0-cdh5.3.6.tar.gz -C ../\n```\n\n#### 在Oozie根目录下创建libext目录\n\n```shell\nmkdir libext/\n```\n\n#### 拷贝依赖Jar包\n\n```shell\ncp -ra hadooplibs/hadooplib-2.5.0-cdh5.3.6.oozie-4.0.0-cdh5.3.6/* libext/\n```\n\n####  上传Mysql驱动包到libext目录下\n\n#### 上传ext-2.2.zip拷贝到libext目录下\n\n#### 修改oozie-site.xml\n\n```\n属性：oozie.service.JPAService.jdbc.driver\n属性值：com.mysql.jdbc.Driver\n解释：JDBC的驱动\n\n属性：oozie.service.JPAService.jdbc.url\n属性值：jdbc:mysql://datanode1:3306/oozie\n解释：oozie所需的数据库地址\n\n属性：oozie.service.JPAService.jdbc.username\n属性值：root\n解释：数据库用户名\n\n属性：oozie.service.JPAService.jdbc.password\n属性值：123456\n解释：数据库密码\n\n属性：oozie.service.HadoopAccessorService.hadoop.configurations\n属性值：*=/opt/module/cdh/hadoop-2.5.0-cdh5.3.6/etc/hadoop\n解释：让Oozie引用Hadoop的配置文件\n```\n\n#### 在Mysql中创建Oozie的数据库\n\n``` shell\nmysql -uroot -p123456\nmysql> create database oozie;\n```\n\n#### 初始化Oozie\n\n```shell\n bin/oozie-setup.sh sharelib create -fs hdfs://datanode1:9000 -locallib oozie-sharelib-4.0.0-cdh5.3.6-yarn.tar.gz\n```\n\n###### 创建oozie.sql文件\n\n```\nbin/oozie-setup.sh db create -run -sqlfile oozie.sql\n```\n\n###### 打包项目，生成war包\n\n```\nbin/oozie-setup.sh prepare-war\n```\n\n需要zip命令 最小化安装可能需要\n\n#### Oozie服务\n\n```shell\n bin/oozied.sh start\n//如需正常关闭Oozie服务，请使用：\n bin/oozied.sh stop\n```\n\n##  Web页面\n\n![FOQD2T.png](https://s2.ax1x.com/2019/01/10/FOQD2T.png)\n\n## Oozie任务\n\n### 调度shell\n\n1.解压官方模板\n\n```shell\ntar -zxf oozie-examples.tar.gz\n```\n\n2.创建工作目录\n\n```shell\nmkdir oozie-apps/\n```\n\n3.拷贝任务模板\n\n```shell\ncp -r examples/apps/shell/ oozie-apps/\n```\n\n4.shell脚本\n\n```shell\n#!/bin/bash\ni=1\nmkdir /home/hadoop/oozie-test1\ncd /home/hadoop/oozie-test1\nfor(( i=1;i<=100;i++ ))\ndo\n d=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\n echo \"data:$d $i\">>/home/hadoop/oozie-test1/logs.log\ndone\n```\n\n5.job.properties\n\n```\nnameNode=hdfs://datanode1:9000\njobTracker=datanode2:8032\nqueueName=shell\nexamplesRoot=oozie-apps\n\noozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/shell\nEXEC=p1.sh\n\n```\n\n6.workflow.xml\n\n```xml\n<workflow-app xmlns=\"uri:oozie:workflow:0.4\" name=\"shell-wf\">\n    <start to=\"shell-node\"/>\n    <action name=\"shell-node\">\n        <shell xmlns=\"uri:oozie:shell-action:0.2\">\n            <job-tracker>${jobTracker}</job-tracker>\n            <name-node>${nameNode}</name-node>\n            <configuration>\n                <property>\n                    <name>mapred.job.queue.name</name>\n                    <value>${queueName}</value>\n                </property>\n            </configuration>\n            <exec>${EXEC}</exec>\n            <file>/user/hadoop/oozie-apps/shell/${EXEC}#${EXEC}</file>\n            <capture-output/>\n        </shell>\n        <ok to=\"end\"/>\n        <error to=\"fail\"/>\n    </action>\n    <decision name=\"check-output\">\n        <switch>\n            <case to=\"end\">\n                ${wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'}\n            </case>\n            <default to=\"fail-output\"/>\n        </switch>\n    </decision>\n    <kill name=\"fail\">\n        <message>Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>\n    </kill>\n    <kill name=\"fail-output\">\n        <message>Incorrect output, expected [Hello Oozie] but was [${wf:actionData('shell-node')['my_output']}]</message>\n    </kill>\n    <end name=\"end\"/>\n</workflow-app>\n```\n\n7.上传任务配置\n\n```\n/opt/module/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put  -f  oozie-apps/ /user/hadoop\n```\n\n8.执行任务\n\n```shell\n bin/oozie job -oozie http://datanode1:11000/oozie -config oozie-apps/shell/job.properties -run\n```\n\n9.杀死任务\n\n```\nbin/oozie job -oozie http://datanode1:11000/oozie -kill 0000004-170425105153692-oozie-z-W\n```\n\n\n\n![FOlpQS.png](https://s2.ax1x.com/2019/01/10/FOlpQS.png)\n\n### 调度逻辑shell\n\n在原有的基础上进行适当修改\n\n1.job.properties\n\n```properties\nnameNode=hdfs://datanode1:9000\njobTracker=datanode2:8032\nqueueName=shell\nexamplesRoot=oozie-apps\n\noozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/shell\nEXEC1=p1.sh\nEXEC2=p2.sh\n```\n\n2.脚本  p1.sh  \n\n```properties\n#!/bin/bash               \nmkdir /home/hadoop/Oozie2_test_p1                 \ncd /home/hadoop/Oozie2_test_p1\ni=1\nfor(( i=1;i<=100;i++ ))\ndo\n d=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\n echo \"data:$d $i\">>/home/hadoop/Oozie2_test_p1/Oozie2_p1.log\ndone\n```\n\n2.脚本  p2.sh\n\n```shell\n#!/bin/bash\nmkdir /home/hadoop/Oozie2_test_p1\ncd /home/hadoop/Oozie2_test_p1\ni=1\nfor(( i=1;i<=100;i++ ))\ndo\n d=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\n echo \"data:$d $i\">>/home/hadoop/Oozie2_test_p1/Oozie2_p1.log\ndone\n```\n\n3.workflow.xml\n\n```xml\n<workflow-app xmlns=\"uri:oozie:workflow:0.4\" name=\"shell-wf\">\n    <start to=\"shell-node\"/>\n    <action name=\"shell-node\">\n        <shell xmlns=\"uri:oozie:shell-action:0.2\">\n            <job-tracker>${jobTracker}</job-tracker>\n            <name-node>${nameNode}</name-node>\n            <configuration>\n                <property>\n                    <name>mapred.job.queue.name</name>\n                    <value>${queueName}</value>\n                </property>\n            </configuration>\n            <exec>${EXEC1}</exec>\n            <file>/user/hadoop/oozie-apps/shell/${EXEC1}#${EXEC1}</file>\n            <capture-output/>\n        </shell>\n        <ok to=\"p2-shell-node\"/>\n        <error to=\"fail\"/>\n    </action>\n\n    <action name=\"p2-shell-node\">\n        <shell xmlns=\"uri:oozie:shell-action:0.2\">\n            <job-tracker>${jobTracker}</job-tracker>\n            <name-node>${nameNode}</name-node>\n            <configuration>\n                <property>\n                    <name>mapred.job.queue.name</name>\n                    <value>${queueName}</value>\n                </property>\n            </configuration>\n            <exec>${EXEC2}</exec>\n            <file>/user/hadoop/oozie-apps/shell/${EXEC2}#${EXEC2}</file>\n            <!-- <argument>my_output=Hello Oozie</argument>-->\n            <capture-output/>\n        </shell>\n        <ok to=\"end\"/>\n        <error to=\"fail\"/>\n    </action>\n    \n    <decision name=\"check-output\">\n        <switch>\n            <case to=\"end\">\n                ${wf:actionData('shell-node')['my_output'] eq 'Hello Oozie'}\n            </case>\n            <default to=\"fail-output\"/>\n        </switch>\n    </decision>\n    <kill name=\"fail\">\n        <message>Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>\n    </kill>\n    <kill name=\"fail-output\">\n        <message>Incorrect output, expected [Hello Oozie] but was [${wf:actionData('shell-node')['my_output']}]</message>\n    </kill>\n    <end name=\"end\"/>\n</workflow-app>\n```\n\n### 调度MapReduce\n\n前提：确定YARN可用\n\n1.拷贝官方模板到oozie-apps\n\n```\n[hadoop@datanode1 lib]$ cp /opt/module/cdh/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar ./\n```\n\n2.配置job.properties\n\n```properties\nnameNode=hdfs://datanode1:9000\njobTracker=datanode2:8032\nqueueName=map-reduce\nexamplesRoot=oozie-apps\n\noozie.wf.application.path=${nameNode}/user/${user.name}/${examplesRoot}/map-reduce/workflow.xml\noutputDir=/output\n```\n\n3.workflow.xml\n\n```xml\n<workflow-app xmlns=\"uri:oozie:workflow:0.2\" name=\"map-reduce-wf\">\n    <start to=\"mr-node\"/>\n    <action name=\"mr-node\">\n        <map-reduce>\n            <job-tracker>${jobTracker}</job-tracker>\n            <name-node>${nameNode}</name-node>\n            <prepare>\n                <delete path=\"/output\"/>\n            </prepare>\n            <configuration>\n                <property>\n                    <name>mapred.job.queue.name</name>\n                    <value>${queueName}</value>\n                </property>\n            <!-- 配置调度MR任务时，使用新的API -->\n                <property>\n                    <name>mapred.mapper.new-api</name>\n                    <value>true</value>\n                </property>\n\n                <property>\n                    <name>mapred.reducer.new-api</name>\n                    <value>true</value>\n                </property>\n            <!-- 指定Job Key输出类型 -->\n                <property>\n                    <name>mapreduce.job.output.key.class</name>\n                    <value>org.apache.hadoop.io.Text</value>\n                </property>\n            <!-- 指定Job Value输出类型 -->\n                <property>\n                    <name>mapreduce.job.output.value.class</name>\n                    <value>org.apache.hadoop.io.IntWritable</value>\n                </property>\n            <!-- 指定Map类 -->\n                <property>\n                    <name>mapreduce.job.map.class</name>\n                    <value>org.apache.hadoop.examples.WordCount$TokenizerMapper</value>\n                </property>\n             <!-- 指定Reduce类 -->\n                <property>\n                    <name>mapreduce.job.reduce.class</name>\n                    <value>org.apache.hadoop.examples.WordCount$IntSumReducer</value>\n                </property>\n                <property>\n                    <name>mapred.map.tasks</name>\n                    <value>1</value>\n                </property>\n                <property>\n                    <name>mapred.input.dir</name>\n                    <value>/input</value>\n                </property>\n                <property>\n                    <name>mapred.output.dir</name>\n                    <value>/_output</value>\n                </property>\n            </configuration>\n        </map-reduce>\n        <ok to=\"end\"/>\n        <error to=\"fail\"/>\n    </action>\n    <kill name=\"fail\">\n        <message>Map/Reduce failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>\n    </kill>\n    <end name=\"end\"/>\n</workflow-app>\n```\n\n4.拷贝jar包\n\n```shell\n[hadoop@datanode1 lib]$ cp /opt/module/cdh/hadoop-2.5.0-cdh5.3.6/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.5.0-cdh5.3.6.jar ./\n```\n\n5.上传任务配置\n\n```shell\n/opt/module/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put -f oozie-apps /user/hadoop/oozie-apps\n```\n\n6.执行任务\n\n```shell\n[hadoop@datanode1 oozie-4.0.0-cdh5.3.6]$  bin/oozie job -oozie http://datanode1:11000/oozie -config oozie-apps/map-reduce/job.properties -run\n```\n\n7.查看结果\n\n``` \n[hadoop@datanode1 module]$ /opt/module/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -cat /input/*.txt\n19/01/10 19:13:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nI\nLove\nHadoop\nand\nSopark\nI\nLove\nBigData\nand\nAI\n[hadoop@datanode1 module]$ /opt/module/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -cat /_output/p*\n19/01/10 19:13:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nAI      1\nBigData 1\nHadoop  1\nI       2\nLove    2\nSopark  1\nand     2\n```\n\n### 调度定时任务/循环任务\n\n前提：\n\n```shell\n##检查系统当前时区： \n date -R\n##注意这里，如果显示的时区不是+0800，你可以删除localtime文件夹后，再关联一个正确时区的链接过去，命令如下：\n rm -rf /etc/localtime\n ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \n```\n\nntp配置\n\n```\nvim /etc/ntp.conf\n```\n\n主机配置\n[![F0EkqI.png](https://s1.ax1x.com/2018/12/17/F0EkqI.png)](https://s1.ax1x.com/2018/12/17/F0EkqI.png)\n\n从机配置\n[![F0mrcD.md.png](https://s1.ax1x.com/2018/12/17/F0mrcD.md.png)](https://s1.ax1x.com/2018/12/17/F0mrcD.md.png)\n\n从节点同步时间\n\n```shell\nservice ntpd restart\nchkconfig ntpd on  # 开机启动\nntpdate -u datanode1\ncrontab -e\n* */1 * * * /usr/sbin/ntpdate datanode1     #每一小时同步一次  注意 要用root创建\n```\n\n1.配置oozie-site.xml文件\n\n```\n属性：oozie.processing.timezone\n属性值：GMT+0800\n解释：修改时区为东八区区时\n```\n\n2.修改js框架代码\n\n```js\n vi /opt/module/cdh/oozie-4.0.0-cdh5.3.6/oozie-server/webapps/oozie/oozie-console.js\n修改如下：\nfunction getTimeZone() {\n    Ext.state.Manager.setProvider(new Ext.state.CookieProvider());\n    return Ext.state.Manager.get(\"TimezoneId\",\"GMT+0800\");\n}\n```\n\n3.重启oozie服务，并重启浏览器（一定要注意清除缓存）\n\n```\nbin/oozied.sh stop\nbin/oozied.sh start\n```\n\n4.拷贝官方模板配置定时任务\n\n```\ncp -r examples/apps/cron/ oozie-apps/\n```\n\n5.修改job.properties\n\n```properties\nnameNode=hdfs://datanode1:9000\njobTracker=datanode2:8032\nqueueName=cronTask\nexamplesRoot=oozie-apps\n\noozie.coord.application.path=${nameNode}/user/${user.name}/${examplesRoot}/cron\nstart=2019-01-10T21:40+0800\nend=2019-01-10T22:00+0800\nworkflowAppUri=${nameNode}/user/${user.name}/${examplesRoot}/cron\n\nEXEC3=p3.sh\n\n```\n\n6.修改coordinator.xml  注意${coord:minutes(5)}的5是最小值不能比5再小了\n\n```xml\n<coordinator-app name=\"cron-coord\" frequency=\"${coord:minutes(5)}\" start=\"${start}\" end=\"${end}\" timezone=\"GMT+0800\"\n                 xmlns=\"uri:oozie:coordinator:0.2\">\n        <action>\n        <workflow>\n            <app-path>${workflowAppUri}</app-path>\n            <configuration>\n                <property>\n                    <name>jobTracker</name>\n                    <value>${jobTracker}</value>\n                </property>\n                <property>\n                    <name>nameNode</name>\n                    <value>${nameNode}</value>\n                </property>\n                <property>\n                    <name>queueName</name>\n                    <value>${queueName}</value>\n                </property>\n            </configuration>\n        </workflow>\n    </action>\n</coordinator-app>\n```\n\n7.创建脚本\n\n```shell\n#!/bin/bash\nd=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\necho \"data:$d $i\">>/home/hadoop/Oozie3_p3.log\n```\n\n8.修改\n\n```xml\n<workflow-app xmlns=\"uri:oozie:workflow:0.5\" name=\"one-op-wf\">\n<start to=\"p3-shell-node\"/>\n  <action name=\"p3-shell-node\">\n      <shell xmlns=\"uri:oozie:shell-action:0.2\">\n          <job-tracker>${jobTracker}</job-tracker>\n          <name-node>${nameNode}</name-node>\n          <configuration>\n              <property>\n                  <name>mapred.job.queue.name</name>\n                  <value>${queueName}</value>\n              </property>\n          </configuration>\n          <exec>${EXEC3}</exec>\n          <file>/user/hadoop/oozie-apps/cron/${EXEC3}#${EXEC3}</file>\n          <!-- <argument>my_output=Hello Oozie</argument>-->\n          <capture-output/>\n      </shell>\n      <ok to=\"end\"/>\n      <error to=\"fail\"/>\n  </action>\n<kill name=\"fail\">\n    <message>Shell action failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>\n</kill>\n<kill name=\"fail-output\">\n    <message>Incorrect output, expected [Hello Oozie] but was [${wf:actionData('shell-node')['my_output']}]</message>\n</kill>\n<end name=\"end\"/>\n</workflow-app>\n```\n\n9.提交配置\n\n```shell\n/opt/module/cdh/hadoop-2.5.0-cdh5.3.6/bin/hdfs dfs -put oozie-apps/cron/ /user/hadoop/oozie-apps\n```\n\n10.提交任务\n\n```shell\nbin/oozie job -oozie http://datanode1:11000/oozie -config oozie-apps/cron/job.properties -run\n```\n\n![FOxtXD.png](https://s2.ax1x.com/2019/01/10/FOxtXD.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Oozie"],"categories":["大数据"]},{"title":"Sqoop","url":"/2019/01/08/Sqoop/","content":"\n {{ \" Sqoop的基本原理和相关参数配置\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n## 简介\n\nSqoop(发音：skup)是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql、postgresql...)间进行数据的传递，可以将一个关系型数据库*（例如 ： MySQL ,Oracle ,Postgres等）*中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。\n\nSqoop项目开始于2009年，最早是作为Hadoop的一个第三方模块存在，后来为了让使用者能够快速部署，也为了让开发人员能够更快速的迭代开发，Sqoop独立成为一个Apache项目。\n\n\n\n## 原理\n\n### 导入\n\n在导入开始之前，Sqoop使用JDBC来检查将要导入的表。他检索出表中所有的列以及列的SQL数据类型。这些SQL类型（VARCHAR、INTEGER）被映射到Java数据类型（String、Integer等）,在MapReduce应用中将使用这些对应的java类型来保存字段的值。Sqoop的代码生成器使用这些信息来创建对应表的类，用于保存从表中抽取的记录。例如前面提到过的example类。\n\n对于导入来说，更关键的是DBWritable接口的序列化方法，这些方法能使Widget类和JDBC进行交互：\n\n​         Public void readFields(resultSet _dbResults)throws SQLException;\n\n​         Public void write(PreparedStatement _dbstmt)throws SQLException;\n\nJDBC的ResultSet接口提供了一个用户从检查结果中检索记录的游标；这里的readFields()方法将用ResultSet中一行数据的列来填充Example对象的字段。\n\nSqoop启动的MapReduce作业用到一个InputFormat,他可以通过JDBC从一个数据库表中读取部分内容。Hadoop提供的DataDriverDBInputFormat能够为几个Map任务对查询结果进行划分。为了获取更好的导入性能，查询会根据一个“划分列”来进行划分的。Sqoop会选择一个合适的列作为划分列（通常是表的主键）。\n\n在生成反序列化代码和配置InputFormat之后，Sqoop将作业发送到MapReduce集群。Map任务将执行查询并将ResultSet中的数据反序列化到生成类的实例，这些数据要么直接保存在SequenceFile文件中，要么在写到HDFS之前被转换成分割的文本。\n\nSqoop不需要每次都导入整张表，用户也可以在查询中加入到where子句，以此来限定需要导入的记录：Sqoop –query (SQL)。\n\n​         导入和一致性：在向HDFS导入数据时，重要的是要确保访问的是数据源的一致性快照。从一个数据库中并行读取数据的MAP任务分别运行在不同的进程中。因此，他们不能共享一个数据库任务。保证一致性的最好方法就是在导入时不允许运行任何进行对表中现有数据进行更新。\n\n![FqsIJO.png](https://s2.ax1x.com/2019/01/08/FqsIJO.png)\n\n### 导出\n\nSqoop导出功能的架构与其导入功能非常相似，在执行导出操作之前，sqoop会根据数据库连接字符串来选择一个导出方法。一般为jdbc。然后，sqoop会根据目标表的定义生成一个java类。这个生成的类能够从文本文件中解析记录，并能够向表中插入类型合适的值。接着会启动一个MapReduce作业，从HDFS中读取源数据文件，使用生成的类解析记录，并且执行选定的导出方法。\n\n基于jdbc的导出方法会产生一批insert语句，每条语句都会向目标表中插入多条记录。多个单独的线程被用于从HDFS读取数据并与数据库进行通信，以确保涉及不同系统的I/O操作能够尽可能重叠执行。\n\n虽然HDFS读取数据的MapReduce作业大多根据所处理文件的数量和大小来选择并行度（map任务的数量），但sqoop的导出工具允许用户明确设定任务的数量。由于导出性能会受并行的数据库写入线程数量的影响，所以sqoop使用combinefileinput类将输入文件分组分配给少数几个map任务去执行。\n\n系统使用固定大小的缓冲区来存储事务数据，这时一个任务中的所有操作不可能在一个事务中完成。因此，在导出操作进行过程中，提交过的中间结果都是可见的。在导出过程完成前，不要启动那些使用导出结果的应用程序，否则这些应用会看到不完整的导出结果。\n\n更有问题的是，如果任务失败，他会从头开始重新导入自己负责的那部分数据，因此可能会插入重复的记录。当前sqoop还不能避免这种可能性。在启动导出作业前，应当在数据库中设置表的约束（例如，定义一个主键列）以保证数据行的唯一性。\n\n​     Sqoop还可以将存储在SequenceFile中的记录导出到输出表，不过有一些限制。SequenceFile中可以保存任意类型的记录。Sqoop的导出工具从SequenceFile中读取对象，然后直接发送到OutputCollector，由他将这些对象传递给数据库导出OutputFormat。为了能让Sqoop使用，记录必须被保存在SequenceFile键值对格式的值部分，并且必须继承抽象类com.cloudera.sqoop.lib.SqoopRecord。\n\n![FqyClQ.png](https://s2.ax1x.com/2019/01/08/FqyClQ.png)\n\n## 安装\n\n1. 下载软件 : http://mirrors.hust.edu.cn/apache/sqoop/1.4.6/\n\n2. 上传安装包sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz到虚拟机中\n\n3. 软件到指定目录\n ```shell\n    [hadoop@datanode1 software]$ tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C /opt/module/\n    [hadoop@datanode1 module]$ mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop\n ```\n\n4. 修改配置文件\n\n```shell\n[hadoop@datanode1 conf]$ mv sqoop-env-template.sh sqoop-env.sh\n[hadoop@datanode1 conf]$ mv sqoop-site-template.xml sqoop-site.xml\n```\n\n5. 修改配置文件sqoop-env.sh\n\n```properties\n#Set path to where bin/hadoop is available\nexport HADOOP_COMMON_HOME=/opt/module/hadoop\n\n#Set path to where hadoop-*-core.jar is available\nexport HADOOP_MAPRED_HOME=/opt/module/hadoop\n\n#set the path to where bin/hbase is available\nexport HBASE_HOME=/opt/module/hbase\n\n#Set the path to where bin/hive is available\nexport HIVE_HOME=/opt/module/hive\n\n#Set the path for where zookeper config dir is\nexport ZOOKEEPER_HOME=/opt/module/zookeeper-3.4.10/\nexport ZOOCFGDIR=$ZOOKEEPER_HOME/conf\n```\n\n6. 拷贝JDBC驱动sqoop的lib目录下\n\n```\ncp -a mysql-connector-java-5.1.27-bin.jar /opt/module/sqoop/lib\n```\n\n7. 验证Sqoop\n\n```shell\n[hadoop@datanode1 sqoop]$ bin/sqoop help\nWarning: /opt/module/sqoop/bin/../../hcatalog does not exist! HCatalog jobs will fail.\nPlease set $HCAT_HOME to the root of your HCatalog installation.\nWarning: /opt/module/sqoop/bin/../../accumulo does not exist! Accumulo imports will fail.\nPlease set $ACCUMULO_HOME to the root of your Accumulo installation.\n-----------------------------------------------------------------------------------------\n##上面的警告可以忽略\n19/01/08 13:35:58 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6\nusage: sqoop COMMAND [ARGS]\n\nAvailable commands:\n  codegen            Generate code to interact with database records\n  create-hive-table  Import a table definition into Hive\n  eval               Evaluate a SQL statement and display the results\n  export             Export an HDFS directory to a database table\n  help               List available commands\n  import             Import a table from a database to HDFS\n  import-all-tables  Import tables from a database to HDFS\n  import-mainframe   Import datasets from a mainframe server to HDFS\n  job                Work with saved jobs\n  list-databases     List available databases on a server\n  list-tables        List available tables in a database\n  merge              Merge results of incremental imports\n  metastore          Run a standalone Sqoop metastore\n  version            Display version information\n\nSee 'sqoop help COMMAND' for information on a specific command.\n```\n\n8. 测试是否成功连接数据库\n\n```\n[hadoop@datanode1 sqoop]$ bin/sqoop list-databases --connect jdbc:mysql://datanode1:3306/ --username root --password 123456\n19/01/08 13:38:11 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6\n19/01/08 13:38:11 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.\n19/01/08 13:38:12 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.\ninformation_schema\nmetastore\nmysql\nmysqlsource\nperformance_schema\n[hadoop@datanode1 sqoop]$\n```\n\n## 案例\n\n### 导入数据\n\n#### RDBMS到HDFS\n\n```\nmysql) create database student;\nmysql) create table student.class(id int(4) primary key not null auto_increment, name varchar(255), age int);\n```\n\n数据库脚本\n\n```shell\n[hadoop@datanode1 sqoop]$ vim mysql.sh\nSTNAME=\"192.168.1.101\"    #数据库信息\nPORT=\"3306\"\nUSERNAME=\"root\"\nPASSWORD=\"123456\"\n\nDBNAME=\"student\"        #数据库名称\nTABLENAME=\"class\"         #数据库中表的名称\n\nfor ((i=1;i(=100;i++))              ##添加100条数据\ndo\ninsert_sql=\"insert into ${TABLENAME}(name,age) values('student_$i',($RANDOM%4)+18)\"\nmysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e  \"${insert_sql}\"\n```\n\n#####  全部导入\n\n```shell\nbin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--table class \\\n--target-dir /student/class \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\"\n```\n\n##### **查询导入**\n\n```shell\n bin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--target-dir /student/class_query \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--query 'select name,age from class where id (=20 and $CONDITIONS;'\n```\n\n注意:must contain '$CONDITIONS' in WHERE clause.\n\n如果query后使用的是双引号，则$CONDITIONS前必须加转移符，防止shell识别为自己的变量。\n\n--query选项，不能同时与--table选项使用\n\n##### 导入指定列\n\n```shell\nbin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--target-dir /student/ \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--columns id,name \\\n--table class\n```\n\n注意:columns中如果涉及到多列，用逗号分隔，分隔时不要添加空格\n\n##### 使用sqoop关键字筛选查询导入数据\n\n```shell\nbin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--target-dir /student/limit \\\n--delete-target-dir \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--table class \\\n--where \"id(10\"\n```\n\n\n\n#### RDBMS到Hive\n\n```shell\n  bin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\ \n--table class \\\n--num-mappers 1 \\\n--hive-import \\\n--fields-terminated-by \"\\t\" \\\n--columns name,age \\\n--hive-overwrite \\\n--hive-table class_hive\n```\n\n该过程分为两步:\n\n第一步将数据导入到HDFS  默认临时目录是/user/admin/表名\n\n第二步将导入到HDFS的数据迁移到Hive仓库\n\n#### RDBMS到HBase\n\n```\nbin/sqoop import  \\\n--connect jdbc:mysql://datanode1:3306/student  \\\n--username 'root'  \\\n--password '123456'   \\\n--table 'class'  \\\n--hbase-table 'test'   \\\n--hbase-row-key 'id'  \\\n --column-family 'info'\n```\n\n需要先创建hbase表\n\n### 导出数据\n\n#### HIVE到RDBMS\n\n```\nbin/sqoop export \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--table class_from_hive \\\n--num-mappers 1 \\\n--export-dir /user/hive/warehouse/class_hive \\\n--input-fields-terminated-by \"\\t\"\n```\n\n表要先创先好\n\n#### HDFS到RDBMS\n\n ```shell\nmkdir opt\n vi opt/job_HDFS2RDBMS.opt\nexport --connect\njdbc:mysql://linux01:3306/company\n--username\nroot\n--password\n123456\n--table\nstaff\n--num-mappers\n1\n--export-dir\n/user/hive/warehouse/staff_hive\n--input-fields-terminated-by\n\"\\t\"\n\n bin/sqoop --options-file opt/job_HDFS2RDBMS.opt\n ```\n\n#### Sqoop 常用参数\n\n| **序号** | **命令**          | **类**              | **说明**                                                     |\n| -------- | ----------------- | ------------------- | ------------------------------------------------------------ |\n| 1        | import            | ImportTool          | 将数据导入到集群                                             |\n| 2        | export            | ExportTool          | 将集群数据导出                                               |\n| 3        | codegen           | CodeGenTool         | 获取数据库中某张表数据生成Java并打包Jar                      |\n| 4        | create-hive-table | CreateHiveTableTool | 创建Hive表                                                   |\n| 5        | eval              | EvalSqlTool         | 查看SQL执行结果                                              |\n| 6        | import-all-tables | ImportAllTablesTool | 导入某个数据库下所有表到HDFS中                               |\n| 7        | job               | JobTool             | 用来生成一个sqoop的任务，生成后，该任务并不执行，除非使用命令执行该任务。 |\n| 8        | list-databases    | ListDatabasesTool   | 列出所有数据库名                                             |\n| 9        | list-tables       | ListTablesTool      | 列出某个数据库下所有表                                       |\n| 10       | merge             | MergeTool           | 将HDFS中不同目录下面的数据合在一起，并存放在指定的目录中     |\n| 11       | metastore         | MetastoreTool       | 记录sqoop job的元数据信息，如果不启动metastore实例，则默认的元数据存储目录为：~/.sqoop，如果要更改存储目录，可以在配置文件sqoop-site.xml中进行更改。 |\n| 12       | help              | HelpTool            | 打印sqoop帮助信息                                            |\n| 13       | version           | VersionTool         | 打印sqoop版本信息                                            |\n\n##### import\n\n| **序号** | **参数**                        | **说明**                                                     |\n| -------- | ------------------------------- | ------------------------------------------------------------ |\n| 1        | --enclosed-by (char)            | 给字段值前后加上指定的字符                                   |\n| 2        | --escaped-by (char)             | 对字段中的双引号加转义符                                     |\n| 3        | --fields-terminated-by (char)   | 设定每个字段是以什么符号作为结束，默认为逗号                 |\n| 4        | --lines-terminated-by (char)    | 设定每行记录之间的分隔符，默认是\\n                           |\n| 5        | --mysql-delimiters              | Mysql默认的分隔符设置，字段之间以逗号分隔，行之间以\\n分隔，默认转义符是\\，字段值以单引号包裹。 |\n| 6        | --optionally-enclosed-by (char) | 给带有双引号或单引号的字段值前后加上指定字符。               |\n\n##### export\n\n| **序号** | **参数**                              | **说明**                                   |\n| -------- | ------------------------------------- | ------------------------------------------ |\n| 1        | --input-enclosed-by (char)            | 对字段值前后加上指定字符                   |\n| 2        | --input-escaped-by (char)             | 对含有转移符的字段做转义处理               |\n| 3        | --input-fields-terminated-by (char)   | 字段之间的分隔符                           |\n| 4        | --input-lines-terminated-by (char)    | 行之间的分隔符                             |\n| 5        | --input-optionally-enclosed-by (char) | 给带有双引号或单引号的字段前后加上指定字符 |\n\n##### hive\n\n| **序号** | **参数**                        | **说明**                                                  |\n| -------- | ------------------------------- | --------------------------------------------------------- |\n| 1        | --hive-delims-replacement (arg) | 用自定义的字符串替换掉数据中的\\r\\n和\\013 \\010等字符       |\n| 2        | --hive-drop-import-delims       | 在导入数据到hive时，去掉数据中的\\r\\n\\013\\010这样的字符    |\n| 3        | --map-column-hive (map)         | 生成hive表时，可以更改生成字段的数据类型                  |\n| 4        | --hive-partition-key            | 创建分区，后面直接跟分区名，分区字段的默认类型为string    |\n| 5        | --hive-partition-value (v)      | 导入数据时，指定某个分区的值                              |\n| 6        | --hive-home (dir)               | hive的安装目录，可以通过该参数覆盖之前默认配置的目录      |\n| 7        | --hive-import                   | 将数据从关系数据库中导入到hive表中                        |\n| 8        | --hive-overwrite                | 覆盖掉在hive表中已经存在的数据                            |\n| 9        | --create-hive-table             | 默认是false，即，如果目标表已经存在了，那么创建任务失败。 |\n| 10       | --hive-table                    | 后面接要创建的hive表,默认使用MySQL的表名                  |\n| 11       | --table                         | 指定关系数据库的表名                                      |\n\n#### Sqoop常用命令\n\n##### import\n\n1. 导入数据到HIVE中\n\n````shell\nbin/sqoop import   \\\n--connect jdbc:mysql://datanode1:3306/student  \\\n--username root  \\\n--password 123456  \\\n--table class    \\\n--hive-import\n````\n\n2. 增量导入数据到hive中，mode=append\n\n给MySQL添加几条数据\n\n```shell\n bin/sqoop import \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--table class \\\n--num-mappers 1 \\\n--fields-terminated-by \"\\t\" \\\n--target-dir /user/hive/warehouse/class \\\n--check-column id \\\n--incremental append \\\n--last-value 3\n```\n\n注意:append不能与--hive-等参数同时使用（Append mode for hive imports is not yet supported. Please remove the\nparameter --append-mode）\n\n 使用lastmodified方式导入数据要指定增量数据是要--append（追加）还是要--merge-key（合并）：last-value指定的值是会包含于增量导入的数据中\n\n| **序号** | **参数**                          | **说明**                                                     |\n| -------- | --------------------------------- | ------------------------------------------------------------ |\n| 1        | --append                          | 将数据追加到HDFS中已经存在的DataSet中，如果使用该参数，sqoop会把数据先导入到临时文件目录，再合并。 |\n| 2        | --as-avrodatafile                 | 将数据导入到一个Avro数据文件中                               |\n| 3        | --as-sequencefile                 | 将数据导入到一个sequence文件中                               |\n| 4        | --as-textfile                     | 将数据导入到一个普通文本文件中                               |\n| 5        | --boundary-query (statement)      | 边界查询，导入的数据为该参数的值（一条sql语句）所执行的结果区间内的数据。 |\n| 6        | --columns   <col1, col2, col3>    | 指定要导入的字段                                             |\n| 7        | --direct                          | 直接导入模式，使用的是关系数据库自带的导入导出工具，以便加快导入导出过程。 |\n| 8        | --direct-split-size               | 在使用上面direct直接导入的基础上，对导入的流按字节分块，即达到该阈值就产生一个新的文件 |\n| 9        | --inline-lob-limit                | 设定大对象数据类型的最大值                                   |\n| 10       | --m或–num-mappers                 | 启动N个map来并行导入数据，默认4个。                          |\n| 11       | --query或--e (statement)          | 将查询结果的数据导入，使用时必须伴随参--target-dir，--hive-table，如果查询中有where条件，则条件后必须加上$CONDITIONS关键字 |\n| 12       | --split-by   <column-name)        | 按照某一列来切分表的工作单元，不能与--autoreset-to-one-mapper连用（请参考官方文档） |\n| 13       | --table   (table-name)            | 关系数据库的表名                                             |\n| 14       | --target-dir   (dir)              | 指定HDFS路径                                                 |\n| 15       | --warehouse-dir   (dir)           | 与14参数不能同时使用，导入数据到HDFS时指定的目录             |\n| 16       | --where                           | 从关系数据库导入数据时的查询条件                             |\n| 17       | --z或--compress                   | 允许压缩                                                     |\n| 18       | --compression-codec               | 指定hadoop压缩编码类，默认为gzip(Use Hadoop codec default gzip) |\n| 19       | --null-string   (null-string)     | string类型的列如果null，替换为指定字符串                     |\n| 20       | --null-non-string   (null-string) | 非string类型的列如果null，替换为指定字符串                   |\n| 21       | --check-column   (col)            | 作为增量导入判断的列名                                       |\n| 22       | --incremental   (mode)            | mode：append或lastmodified                                   |\n| 23       | --last-value   (value)            | 指定某一个值，用于标记增量导入的位置                         |\n\n##### export\n\n从HDFS（包括Hive和HBase）中把数据导出到关系型数据库中。\n\n```shell\nbin/sqoop export \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--table class2mysql \\\n--export-dir /student \\\n--input-fields-terminated-by \"\\t\" \\\n--num-mappers 1\n```\n\n| **序号** | **参数**                                | **说明**                                                     |\n| -------- | --------------------------------------- | ------------------------------------------------------------ |\n| 1        | --direct                                | 利用数据库自带的导入导出工具，以便于提高效率                 |\n| 2        | --export-dir <dir>                      | 存放数据的HDFS的源目录                                       |\n| 3        | -m或--num-mappers <n>                   | 启动N个map来并行导入数据，默认4个                            |\n| 4        | --table <table-name>                    | 指定导出到哪个RDBMS中的表                                    |\n| 5        | --update-key <col-name>                 | 对某一列的字段进行更新操作                                   |\n| 6        | --update-mode <mode>                    | updateonly   allowinsert(默认)                               |\n| 7        | --input-null-string <null-string>       | 请参考import该类似参数说明                                   |\n| 8        | --input-null-non-string   <null-string> | 请参考import该类似参数说明                                   |\n| 9        | --staging-table   <staging-table-name>  | 创建一张临时表，用于存放所有事务的结果，然后将所有事务结果一次性导入到目标表中，防止错误。 |\n| 10       | --clear-staging-table                   | 如果第9个参数非空，则可以在导出操作执行前，清空临时事务结果表 |\n\n##### codegen\n\n将关系型数据库中的表映射为一个Java类，在该类中有各列对应的各个字段。\n\n```shell\n bin/sqoop codegen \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--table class \\\n--bindir /opt/moudle \\\n--class-name class_mysql \\\n--fields-terminated-by \"\\t\"\n```\n\n| **序号** | **参数**                           | **说明**                                                     |\n| -------- | ---------------------------------- | ------------------------------------------------------------ |\n| 1        | --bindir <dir>                     | 指定生成的Java文件、编译成的class文件及将生成文件打包为jar的文件输出路径 |\n| 2        | --class-name <name>                | 设定生成的Java文件指定的名称                                 |\n| 3        | --outdir <dir>                     | 生成Java文件存放的路径                                       |\n| 4        | --package-name <name>              | 包名，如com.z，就会生成com和z两级目录                        |\n| 5        | --input-null-non-string <null-str> | 在生成的Java文件中，可以将null字符串或者不存在的字符串设置为想要设定的值（例如空字符串） |\n| 6        | --input-null-string <null-str>     | 将null字符串替换成想要替换的值（一般与5同时使用）            |\n| 7        | --map-column-java   <arg>          | 数据库字段在生成的Java文件中会映射成各种属性，且默认的数据类型与数据库类型保持对应关系。该参数可以改变默认类型，例如：--map-column-java id=long, name=String |\n| 8        | --null-non-string   <null-str>     | 在生成Java文件时，可以将不存在或者null的字符串设置为其他值   |\n| 9        | --null-string   <null-str>         | 在生成Java文件时，将null字符串设置为其他值（一般与8同时使用） |\n| 10       | --table   <table-name>             | 对应关系数据库中的表名，生成的Java文件中的各个属性与该表的各个字段一一对应 |\n\n##### eval\n\n可以快速的使用SQL语句对关系型数据库进行操作，经常用于在import数据之前，了解一下SQL语句是否正确，数据是否正常，并可以将结果显示在控制台。\n\n```shell\nbin/sqoop eval \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--query \"SELECT * FROM class\"\n\n```\n\n| **序号** | **参数**     | **说明**          |\n| -------- | ------------ | ----------------- |\n| 1        | --query或--e | 后跟查询的SQL语句 |\n\n##### import-all-tables\n\n可以将RDBMS中的所有表导入到HDFS中，每一个表都对应一个HDFS目录\n\n```\nbin/sqoop import-all-tables \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456 \\\n--warehouse-dir /all_tables\n```\n\n| **序号** | **参数**                |\n| -------- | ----------------------- |\n| 1        | --as-avrodatafile       |\n| 2        | --as-sequencefile       |\n| 3        | --as-textfile           |\n| 4        | --direct                |\n| 5        | --direct-split-size <n> |\n| 6        | --inline-lob-limit <n>  |\n| 7        | --m或—num-mappers <n>   |\n| 8        | --warehouse-dir <dir>   |\n| 9        | -z或--compress          |\n| 10       | --compression-codec     |\n\n##### job\n\n用来生成一个sqoop任务，生成后不会立即执行，需要手动执行。\n\n```\n$ bin/sqoop job \\\n --create myjob -- import-all-tables \\\n --connect jdbc:mysql://datanode1:3306/student \\\n --username root \\\n --password 123456\n$ bin/sqoop job --list\n$ bin/sqoop job --exec myjob\n```\n\nlist-databases\n\n```shell\nbin/sqoop list-databases \\\n--connect jdbc:mysql://datanode1:3306/ \\\n--username root \\\n--password 123456\n```\n\n##### list-tables\n\n```shell\nbin/sqoop list-tables \\\n--connect jdbc:mysql://datanode1:3306/student \\\n--username root \\\n--password 123456\n```\n\n##### merge\n\n```\nbin/sqoop codegen \\\n--connect jdbc:mysql://datanode1/student \\\n--username root  \\\n--password 123456 \\\n--table class  \\\n--bindir /home/hadoop  \\\n--class-name student    \\\n--fields-terminated-by \"\\t\"\n```\n\n```\n bin/sqoop merge  \\\n--new-data /test/new  \\\n--onto /test/old  \\\n--target-dir /test/merged  \\\n--jar-file /home/hadoop/student.jar \\\n--class-name student  \\\n--merge-key id\n\n```\n\n| **序号** | **参数**             | **说明**                                               |\n| -------- | -------------------- | ------------------------------------------------------ |\n| 1        | --new-data (path)    | HDFS 待合并的数据目录，合并后在新的数据集中保留        |\n| 2        | --onto(path)         | HDFS合并后，重复的部分在新的数据集中被覆盖             |\n| 3        | --merge-key (col)    | 合并键，一般是主键ID                                   |\n| 4        | --jar-file (file)    | 合并时引入的jar包，该jar包是通过Codegen工具生成的jar包 |\n| 5        | --class-name (class) | 对应的表名或对象名，该class类是包含在jar包中的         |\n| 6        | --target-dir (path)  | 合并后的数据在HDFS里存放的目录                         |\n\n##### metastore\n\n记录了Sqoop job的元数据信息，如果不启动该服务，那么默认job元数据的存储目录为~/.sqoop，可在sqoop-site.xml中修改。\n\n```\nbin/sqoop metastore\n```\n\n| **序号** | **参数**   | **说明**      |\n| -------- | ---------- | ------------- |\n| 1        | --shutdown | 关闭metastore |\n\n\n\n#### 参考资料\n\nhttps://student-lp.iteye.com/blog/2157983\n\n尚硅谷Sqoop\n\n\n\n\n\n","tags":["Sqoop"],"categories":["大数据"]},{"title":"Flume案例Ganglia监控","url":"/2019/01/07/Flume案例Ganglia监控/","content":"\n {{ \"Flume案例和Flume监控系统的使用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n安装过程\n\n1. 将apache-flume-1.7.0-bin.tar.gz上传到linux的/opt/software目录下\n2. 解压apache-flume-1.7.0-bin.tar.gz到/opt/module/目录下\n```shell\n[hadoop@datanode1 software]$ tar -zxf apache-flume-1.7.0-bin.tar.gz -C /opt/module/\n```\n3. 修改apache-flume-1.7.0-bin的名称为flume\n\n```shell\n[hadoop@datanode1 module]$ mv apache-flume-1.7.0-bin flume\n```\n\n4. 将flume/conf下的flume-env.sh.template文件修改为flume-env.sh，并配置flume-env.sh文件\n\n```shell\n[hadoop@datanode1 module]$ mv flume-env.sh.template flume-env.sh\n[hadoop@datanode1 module]$ vi flume-env.sh\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n```\n\n## 案例实操\n\n### 监控端口数据\n\n案例需求：首先，Flume监控本机44444端口，然后通过telnet工具向本机44444端口发送消息，最后Flume将监听的数据实时显示在控制台。\n\n判断端口是否被占用\n\n```shell\nsudo netstat -tunlp | grep 44444\n```\n\n功能描述：netstat命令是一个监控TCP/IP网络的非常有用的工具，它可以显示路由表、实际的网络连接以及每一个网络接口设备的状态信息。\n\n基本语法：netstat [选项]\n\n选项参数：\n\n​       -t或--tcp：显示TCP传输协议的连线状况； \n\n​\t-u或--udp：显示UDP传输协议的连线状况；\n\n​       -n或--numeric：直接使用ip地址，而不通过域名服务器； \n\n​       -l或--listening：显示监控中的服务器的Socket； \n\n​       -p或--programs：显示正在使用Socket的程序识别码和程序名称；\n\n#### 配置\n\n```properties\nhadoop@datanode1 job]$ vim flume-telnet-logger.conf\n# Name the components on this agent\na1.sources = r1\t\t#r1:表示a1的输入源\na1.sinks = k1\t\t#k1表示a1的输出目的地\t\na1.channels = c1     #C1表示a1的缓冲区\n\n# Describe/configure the source\na1.sources.r1.type = netcat           #表示a1的输入源类型为netcat类型\na1.sources.r1.bind = localhost\t\t #标识a1的监听的主机\na1.sources.r1.port = 44444\t\t\t #标识a1监听的端口号\n\n# Describe the sink\na1.sinks.k1.type = logger\t\t\t#标识a1的输出目的地是logger类型\n\n# Use a channel which buffers events in memory\na1.channels.c1.type = memory\t\t\t\t#表示a1的channel类型是memory内存型\na1.channels.c1.capacity = 1000\t\t\t\t#表示a1的channel总容量1000\na1.channels.c1.transactionCapacity = 100     #表示a1的channel传输总容量100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1              #表示将r1和c1连接起来\na1.sinks.k1.channel = c1\t\t\t    #表示将k1和c1连接起来\n```\n\n#### 启动\n\n```shell\n[hadoop@datanode1 flume]$  bin/flume-ng agent --conf conf/ --name a1 --conf-file job/flume-telnet-logger.conf -Dflume.root.logger=INFO,console\n\n参数说明：\n\t--conf conf/  ：表示配置文件存储在conf/目录\n\t--name a1\t：表示给agent起名为a1\n\t--conf-file job/flume-telnet.conf ：flume本次启动读取的配置文件是在job文件夹下的flume-telnet.conf文件。\n\t-Dflume.root.logger==INFO,console ：-D表示flume运行时动态修改flume.root.logger参数属性值，并将控制台日志打印级别设置为INFO级别。日志级别包括:log、info、warn、error。\n\n```\n\n```\n telnet localhost 44444\n```\n\n![Fb1qot.png](https://s2.ax1x.com/2019/01/07/Fb1qot.png)\n\n### 实时读取本地文件到HDFS案例\n\n#### 测试脚本\n\n```shell\n[hadoop@datanode1 data]$ vim test.sh\n#!bin/bash\ni=1\nwhile [ true ]\nlet i+=1\nd=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\ndo\n echo \"data:$d $i\"\ndone\n```\n\n#### flume-file-hdfs.conf\n\n```properties\n[hadoop@datanode1 job]$ vim flume-file-hdfs.conf\n# Name the components on this agent\na2.sources = r2\na2.sinks = k2\na2.channels = c2\n\n# Describe/configure the source\na2.sources.r2.type = exec\na2.sources.r2.command = tail -F  /opt/module/flume/job/data/data1.log\na2.sources.r2.shell = /bin/bash -c\n\n# Describe the sink\na2.sinks.k2.type = hdfs\na2.sinks.k2.hdfs.path = hdfs://datanode1:9000/flume/%Y%m%d/%H\n#上传文件的前缀\na2.sinks.k2.hdfs.filePrefix = logs-\n#是否按照时间滚动文件夹\na2.sinks.k2.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na2.sinks.k2.hdfs.roundValue = 1\n#重新定义时间单位\na2.sinks.k2.hdfs.roundUnit = hour\n#是否使用本地时间戳\na2.sinks.k2.hdfs.useLocalTimeStamp = true\n#积攒多少个Event才flush到HDFS一次\na2.sinks.k2.hdfs.batchSize = 1000\n#设置文件类型，可支持压缩\na2.sinks.k2.hdfs.fileType = DataStream\n#多久生成一个新的文件\na2.sinks.k2.hdfs.rollInterval = 600\n#设置每个文件的滚动大小\na2.sinks.k2.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关\na2.sinks.k2.hdfs.rollCount = 0\n#最小冗余数\na2.sinks.k2.hdfs.minBlockReplicas = 1\n\n# Use a channel which buffers events in memory\na2.channels.c2.type = memory\na2.channels.c2.capacity = 1000\na2.channels.c2.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.r2.channels = c2\na2.sinks.k2.channel = c2\n\n```\n\n![Fb0ndU.png](https://s2.ax1x.com/2019/01/07/Fb0ndU.png)\n\n### 实时读取目录文件到HDFS案例\n\n#### 需求分析\n\n![Fb05Ss.png](https://s2.ax1x.com/2019/01/07/Fb05Ss.png)\n\n#### 配置\n\n```properties\n[hadoop@datanode1 job]$ vim flume-dir-hdfs.conf\n[hadoop@datanode1 job]$ vim flume-dir-hdfs.conf\na3.sources = r3\na3.sinks = k3\na3.channels = c3\n\n# Describe/configure the source\na3.sources.r3.type = spooldir\na3.sources.r3.spoolDir = /opt/module/flume/upload\na3.sources.r3.fileSuffix = .COMPLETED\na3.sources.r3.fileHeader = true\n#忽略所有以.tmp结尾的文件，不上传\na3.sources.r3.ignorePattern = ([^ ]*\\.tmp)\n\n# Describe the sink\na3.sinks.k3.type = hdfs\na3.sinks.k3.hdfs.path = hdfs://datanode1:9000/flume/upload/%Y%m%d/%H\n#上传文件的前缀\na3.sinks.k3.hdfs.filePrefix = upload-\n#是否按照时间滚动文件夹\na3.sinks.k3.hdfs.round = true\n#多少时间单位创建一个新的文件夹\na3.sinks.k3.hdfs.roundValue = 1\n#重新定义时间单位\na3.sinks.k3.hdfs.roundUnit = hour\n#是否使用本地时间戳\na3.sinks.k3.hdfs.useLocalTimeStamp = true\n#积攒多少个Event才flush到HDFS一次\na3.sinks.k3.hdfs.batchSize = 100\n#设置文件类型，可支持压缩\na3.sinks.k3.hdfs.fileType = DataStream\n#多久生成一个新的文件\na3.sinks.k3.hdfs.rollInterval = 600\n#设置每个文件的滚动大小大概是128M\na3.sinks.k3.hdfs.rollSize = 134217700\n#文件的滚动与Event数量无关\na3.sinks.k3.hdfs.rollCount = 0\n#最小冗余数\na3.sinks.k3.hdfs.minBlockReplicas = 1\n\n# Use a channel which buffers events in memory\na3.channels.c3.type = memory\na3.channels.c3.capacity = 1000\na3.channels.c3.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r3.channels = c3\na3.sinks.k3.channel = c3\n```\n\n#### 测试脚本\n\n```shell\n#!bin/bash\ni=1\ncd /opt/module/flume/upload\nwhile [ true ]\nlet i+=1\nd=$( date +%Y-%m-%d\\ %H\\:%M\\:%S )\ndo\n touch \"文档$i.txt\"\n touch \"$d-$i.log\"\n touch \"$i.tmp\"\n sleep 1\ndone\n\n```\n\n#### 启动\n\n```shell\n[hadoop@datanode1 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/flume-dir-hdfs.conf\n```\n\n#### 注意\n\n1. 在使用Spooling Directory Source时\n2.  不要在监控目录中创建并持续修改文件\n3.  上传完成的文件会以.COMPLETED结尾\n4.  被监控文件夹每600毫秒扫描一次文件变动\n\n#### 查看\n\n![Fb2PFP.png](https://s2.ax1x.com/2019/01/07/Fb2PFP.png)\n\n### 查看本地文件\n\n![Fb2uoq.png](https://s2.ax1x.com/2019/01/07/Fb2uoq.png)\n\n###  单数据源多出口案例(一)\n\n![FSNdhR.png](https://s1.ax1x.com/2018/11/18/FSNdhR.png)\n\n#### 分析\n\n案例需求：使用flume-1监控文件变动，flume-1将变动内容传递给flume-2，flume-2负责存储到HDFS。同时flume-1将变动内容传递给flume-3，flume-3负责输出到local filesystem。\n\n需求分析：\n\n![Fb2Gy4.png](https://s2.ax1x.com/2019/01/07/Fb2Gy4.png)\n\n#### 步骤\n\n1. 在/opt/module/flume/job目录下创建group1文件夹\n\n    ```\n    [hadoop@datanode1 job]$ cd group1/\n    ```\n\n 2. 在datanode3节点上/opt/module/datas/目录下创建flume3文件夹\n\n    ```\n    [hadoop@datanode3 datas]$ mkdir flume3/\n    ```\n\n3. 配置1个接收日志文件的source和两个channel、两个sink，分别输送给flume-flume-hdfs和flume-flume-dir。\n\n    datanode1配置文件\n\n    ```properties\n    [hadoop@datanode1 group1]$ vim flume-file-flume.conf\n    # Name the components on this agent\n    a1.sources = r1\n    a1.sinks = k1 k2\n    a1.channels = c1 c2\n    # 将数据流复制给多个channel\n    a1.sources.r1.selector.type = replicating\n    \n    # Describe/configure the source\n    a1.sources.r1.type = exec\n    a1.sources.r1.command = tail -F /opt/module/datas/logs.log\n    a1.sources.r1.shell = /bin/bash -c\n    \n    # Describe the sink\n    a1.sinks.k1.type = avro\n    a1.sinks.k1.hostname = datanode2\n    a1.sinks.k1.port = 4141\n    \n    a1.sinks.k2.type = avro\n    a1.sinks.k2.hostname = datanode3\n    a1.sinks.k2.port = 4142\n    \n    # Describe the channel\n    a1.channels.c1.type = memory\n    a1.channels.c1.capacity = 1000\n    a1.channels.c1.transactionCapacity = 100\n    \n    a1.channels.c2.type = memory\n    a1.channels.c2.capacity = 1000\n    a1.channels.c2.transactionCapacity = 100\n    \n    # Bind the source and sink to the channel\n    a1.sources.r1.channels = c1 c2\n    a1.sinks.k1.channel = c1\n    a1.sinks.k2.channel = c2\n    ```\n\n    datanode2配置文件\n\n    ```properties\n    [hadoop@datanode2 group1]$ vim flume-flume-hdfs.conf\n    # Name the components on this agent\n    a2.sources = r1\n    a2.sinks = k1\n    a2.channels = c1\n    \n    # Describe/configure the source\n    a2.sources.r1.type = avro\n    a2.sources.r1.bind = datanode2\n    a2.sources.r1.port = 4141\n    \n    # Describe the sink\n    a2.sinks.k1.type = hdfs\n    a2.sinks.k1.hdfs.path = hdfs://datanode1:9000/flume2/%Y%m%d/%H\n    #上传文件的前缀\n    a2.sinks.k1.hdfs.filePrefix = flume2-\n    #是否按照时间滚动文件夹\n    a2.sinks.k1.hdfs.round = true\n    #多少时间单位创建一个新的文件夹\n    a2.sinks.k1.hdfs.roundValue = 1\n    #重新定义时间单位\n    a2.sinks.k1.hdfs.roundUnit = hour\n    #是否使用本地时间戳\n    a2.sinks.k1.hdfs.useLocalTimeStamp = true\n    #积攒多少个Event才flush到HDFS一次\n    a2.sinks.k1.hdfs.batchSize = 100\n    #设置文件类型，可支持压缩\n    a2.sinks.k1.hdfs.fileType = DataStream\n    #多久生成一个新的文件\n    a2.sinks.k1.hdfs.rollInterval = 600\n    #设置每个文件的滚动大小大概是128M\n    a2.sinks.k1.hdfs.rollSize = 134217700\n    #文件的滚动与Event数量无关\n    a2.sinks.k1.hdfs.rollCount = 0\n    #最小冗余数\n    a2.sinks.k1.hdfs.minBlockReplicas = 1\n    \n    # Describe the channel\n    a2.channels.c1.type = memory\n    a2.channels.c1.capacity = 1000\n    a2.channels.c1.transactionCapacity = 100\n    ```\n\n    datanode3配置文件\n\n    ```properties\n    [hadoop@datanode3 group1]$ vim flume-flume-dir.conf\n    me the components on this agent\n    a3.sources = r1\n    a3.sinks = k1\n    a3.channels = c2\n    \n    # Describe/configure the source\n    a3.sources.r1.type = avro\n    a3.sources.r1.bind = datanode3\n    a3.sources.r1.port = 4142\n    \n    # Describe the sink\n    a3.sinks.k1.type = file_roll\n    a3.sinks.k1.sink.directory = /opt/module/datas/flume3\n    \n    # Describe the channel\n    a3.channels.c2.type = memory\n    a3.channels.c2.capacity = 1000\n    a3.channels.c2.transactionCapacity = 100\n    \n    # Bind the source and sink to the channel\n    a3.sources.r1.channels = c2\n    a3.sinks.k1.channel = c2启动\n    ```\n\n#### 启动\n\ndatanode1\n\n```shell\n[hadoop@datanode1 flume]$  bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-file-flume.conf\n```\n\ndatanode2\n\n```shell\n[hadoop@datanode2 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume-hdfs.conf\n```\n\ndatanode3\n\n```shell\n[hadoop@datanode3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume-dir.conf\n```\n\n### 单数据源多出口案例(二)\n\n![FSN9kd.png](https://s1.ax1x.com/2018/11/18/FSN9kd.png)\n\n#### 需求\n\n案例需求：使用flume-1监控文件变动，flume-1将变动内容传递给flume-2，flume-2负责存储到HDFS。同时flume-1将变动内容传递给flume-3，flume-3也负责存储到HDFS \n\n![FbzfSO.png](https://s2.ax1x.com/2019/01/07/FbzfSO.png)\n\n#### 实现\n\ndatanode1\n\n```properties\n[hadoop@datanode1 flume]$ vim job/group1/flume-netcat-flume.conf\n# Name the components on this agent\na1.sources = r1\na1.channels = c1\na1.sinkgroups = g1\na1.sinks = k1 k2\n\n# Describe/configure the source\na1.sources.r1.type = netcat\na1.sources.r1.bind = datanode1\na1.sources.r1.port = 44444\n\na1.sinkgroups.g1.processor.type = load_balance\na1.sinkgroups.g1.processor.backoff = true\na1.sinkgroups.g1.processor.selector = round_robin\na1.sinkgroups.g1.processor.selector.maxTimeOut=10000\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = datanode2\na1.sinks.k1.port = 4141\n\na1.sinks.k2.type = avro\na1.sinks.k2.hostname = datanode3\na1.sinks.k2.port = 4142\n\n# Describe the channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinkgroups.g1.sinks = k1 k2\na1.sinks.k1.channel = c1\na1.sinks.k2.channel = c1\n```\ndatanode2\n```properties\n# Name the components on this agent\na2.sources = r1\na2.sinks = k1\na2.channels = c1\n\n# Describe/configure the source\na2.sources.r1.type = avro\na2.sources.r1.bind = datanode2\na2.sources.r1.port = 4141\n\n# Describe the sink\na2.sinks.k1.type = logger\n\n# Describe the channel\na2.channels.c1.type = memory\na2.channels.c1.capacity = 1000\na2.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na2.sources.r1.channels = c1\na2.sinks.k1.channel = c1\n```\n\ndatanode3\n\n```shell\n[hadoop@datanode3 flume]$ vim job/group1/flume-flume2.conf\n# Name the components on this agent\na3.sources = r1\na3.sinks = k1\na3.channels = c2\n\n# Describe/configure the source\na3.sources.r1.type = avro\na3.sources.r1.bind = datanode3\na3.sources.r1.port = 4142\n\n# Describe the sink\na3.sinks.k1.type = logger\n\n# Describe the channel\na3.channels.c2.type = memory\na3.channels.c2.capacity = 1000\na3.channels.c2.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na3.sources.r1.channels = c2\na3.sinks.k1.channel = c2\n```\n\n#### 启动\n\ndatanode1\n\n```shell\n[hadoop@datanode1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group1/flume-netcat-flume.conf\n```\n\ndatanode2\n\n```shell\n[hadoop@datanode2 flume]$ bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group1/flume-flume1.conf -Dflume.root.logger=INFO,console\n```\n\ndatanod3\n\n```shell\n[hadoop@datanode3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group1/flume-flume2.conf -Dflume.root.logger=INFO,console\n```\n\n![Fq9gkn.png](https://s2.ax1x.com/2019/01/07/Fq9gkn.png)\n\n###  多数据源汇总案例\n\n#### 需求\n\n![FSN110.png](https://s1.ax1x.com/2018/11/18/FSN110.png)\n\ndatanode1上的flume-1监控一个软件的log日志，\n\ndatanode2上的flume-2监控某一个端口的数据流，\n\nflume-1与flume-2将数据发送给datanode3上的flume-3，flume-3将最终数据打印到控制台\n\n\n\n#### 步骤\n\n1. 分发flume\n\n```\n[hadoop@datanode2 job]$ mkdir group2\n[hadoop@datanode2 job]$ xsync /opt/module/flume/\n```\n\n2. datanode1配置source用于监控hive.log文件，配置sink输出数据到下一级flume。\n\n```properties\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /opt/module/datas/logs.log\na1.sources.r1.shell = /bin/bash -c\n\n# Describe the sink\na1.sinks.k1.type = avro\na1.sinks.k1.hostname = datanode1\na1.sinks.k1.port = 4141\n\n# Describe the channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n\n```\n\n#### 启动\n\n```shell\n[hadoop@datanode3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume3.conf -Dflume.root.logger=INFO,console\n[hadoop@datanode2 flume]$  bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume2.conf\n[hadoop@datanode1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume1.conf\n```\n\n\n\n![FqFh59.png](https://s2.ax1x.com/2019/01/07/FqFh59.png)\n\n### 自定义MYSQLSource\n\n#### SQLSourceHelper\n\n```java\nimport org.apache.flume.Context;\nimport org.apache.flume.conf.ConfigurationException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\nimport java.sql.*;\nimport java.text.ParseException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Properties;\n\npublic class SQLSourceHelper {\n\n    private static final Logger LOG = LoggerFactory.getLogger(SQLSourceHelper.class);\n\n    private int runQueryDelay, //两次查询的时间间隔\n            startFrom,            //开始id\n            currentIndex,\t     //当前id\n            recordSixe = 0,      //每次查询返回结果的条数\n            maxRow;                //每次查询的最大条数\n\n\n    private String table,       //要操作的表\n            columnsToSelect,     //用户传入的查询的列\n            customQuery,          //用户传入的查询语句\n            query,                 //构建的查询语句\n            defaultCharsetResultSet;//编码集\n\n    //上下文，用来获取配置文件\n    private Context context;\n\n    //为定义的变量赋值（默认值），可在flume任务的配置文件中修改\n    private static final int DEFAULT_QUERY_DELAY = 10000;\n    private static final int DEFAULT_START_VALUE = 0;\n    private static final int DEFAULT_MAX_ROWS = 2000;\n    private static final String DEFAULT_COLUMNS_SELECT = \"*\";\n    private static final String DEFAULT_CHARSET_RESULTSET = \"UTF-8\";\n\n    private static Connection conn = null;\n    private static PreparedStatement ps = null;\n    private static String connectionURL, connectionUserName, connectionPassword;\n\n    //加载静态资源\n    static {\n        Properties p = new Properties();\n        try {\n            p.load(SQLSourceHelper.class.getClassLoader().getResourceAsStream(\"jdbc.properties\"));\n            connectionURL = p.getProperty(\"dbUrl\");\n            connectionUserName = p.getProperty(\"dbUser\");\n            connectionPassword = p.getProperty(\"dbPassword\");\n            Class.forName(p.getProperty(\"dbDriver\"));\n        } catch (IOException | ClassNotFoundException e) {\n            LOG.error(e.toString());\n        }\n    }\n\n    //获取JDBC连接\n    private static Connection InitConnection(String url, String user, String pw) {\n        try {\n            Connection conn = DriverManager.getConnection(url, user, pw);\n            if (conn == null)\n                throw new SQLException();\n            return conn;\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n    //构造方法\n    SQLSourceHelper(Context context) throws ParseException {\n        //初始化上下文\n        this.context = context;\n\n        //有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值\n        this.columnsToSelect = context.getString(\"columns.to.select\", DEFAULT_COLUMNS_SELECT);\n        this.runQueryDelay = context.getInteger(\"run.query.delay\", DEFAULT_QUERY_DELAY);\n        this.startFrom = context.getInteger(\"start.from\", DEFAULT_START_VALUE);\n        this.defaultCharsetResultSet = context.getString(\"default.charset.resultset\", DEFAULT_CHARSET_RESULTSET);\n\n        //无默认值参数：获取flume任务配置文件中的参数\n        this.table = context.getString(\"table\");\n        this.customQuery = context.getString(\"custom.query\");\n        connectionURL = context.getString(\"connection.url\");\n        connectionUserName = context.getString(\"connection.user\");\n        connectionPassword = context.getString(\"connection.password\");\n        conn = InitConnection(connectionURL, connectionUserName, connectionPassword);\n\n        //校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常\n        checkMandatoryProperties();\n        //获取当前的id\n        currentIndex = getStatusDBIndex(startFrom);\n        //构建查询语句\n        query = buildQuery();\n    }\n\n    //校验相应的配置信息（表，查询语句以及数据库连接的参数）\n    private void checkMandatoryProperties() {\n        if (table == null) {\n            throw new ConfigurationException(\"property table not set\");\n        }\n        if (connectionURL == null) {\n            throw new ConfigurationException(\"connection.url property not set\");\n        }\n        if (connectionUserName == null) {\n            throw new ConfigurationException(\"connection.user property not set\");\n        }\n        if (connectionPassword == null) {\n            throw new ConfigurationException(\"connection.password property not set\");\n        }\n    }\n\n    //构建sql语句\n    private String buildQuery() {\n        String sql = \"\";\n        //获取当前id\n        currentIndex = getStatusDBIndex(startFrom);\n        LOG.info(currentIndex + \"\");\n        if (customQuery == null) {\n            sql = \"SELECT \" + columnsToSelect + \" FROM \" + table;\n        } else {\n            sql = customQuery;\n        }\n        StringBuilder execSql = new StringBuilder(sql);\n        //以id作为offset\n        if (!sql.contains(\"where\")) {\n            execSql.append(\" where \");\n            execSql.append(\"id\").append(\">\").append(currentIndex);\n            return execSql.toString();\n        } else {\n            int length = execSql.toString().length();\n            return execSql.toString().substring(0, length - String.valueOf(currentIndex).length()) + currentIndex;\n        }\n    }\n\n    //执行查询\n    List<List<Object>> executeQuery() {\n        try {\n            //每次执行查询时都要重新生成sql，因为id不同\n            customQuery = buildQuery();\n            //存放结果的集合\n            List<List<Object>> results = new ArrayList<>();\n            if (ps == null) {\n                //\n                ps = conn.prepareStatement(customQuery);\n            }\n            ResultSet result = ps.executeQuery(customQuery);\n            while (result.next()) {\n                //存放一条数据的集合（多个列）\n                List<Object> row = new ArrayList<>();\n                //将返回结果放入集合\n                for (int i = 1; i <= result.getMetaData().getColumnCount(); i++) {\n                    row.add(result.getObject(i));\n                }\n                results.add(row);\n            }\n            LOG.info(\"execSql:\" + customQuery + \"\\nresultSize:\" + results.size());\n            return results;\n        } catch (SQLException e) {\n            LOG.error(e.toString());\n            // 重新连接\n            conn = InitConnection(connectionURL, connectionUserName, connectionPassword);\n        }\n        return null;\n    }\n\n    //将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串\n    List<String> getAllRows(List<List<Object>> queryResult) {\n        List<String> allRows = new ArrayList<>();\n        if (queryResult == null || queryResult.isEmpty())\n            return allRows;\n        StringBuilder row = new StringBuilder();\n        for (List<Object> rawRow : queryResult) {\n            Object value = null;\n            for (Object aRawRow : rawRow) {\n                value = aRawRow;\n                if (value == null) {\n                    row.append(\",\");\n                } else {\n                    row.append(aRawRow.toString()).append(\",\");\n                }\n            }\n            allRows.add(row.toString());\n            row = new StringBuilder();\n        }\n        return allRows;\n    }\n\n    //更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset\n    void updateOffset2DB(int size) {\n        //以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录）\n        String sql = \"insert into flume_meta(source_tab,currentIndex) VALUES('\"\n                + this.table\n                + \"','\" + (recordSixe += size)\n                + \"') on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)\";\n        LOG.info(\"updateStatus Sql:\" + sql);\n        execSql(sql);\n    }\n\n    //执行sql语句\n    private void execSql(String sql) {\n        try {\n            ps = conn.prepareStatement(sql);\n            LOG.info(\"exec::\" + sql);\n            ps.execute();\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n    }\n\n    //获取当前id的offset\n    private Integer getStatusDBIndex(int startFrom) {\n        //从flume_meta表中查询出当前的id是多少\n        String dbIndex = queryOne(\"select currentIndex from flume_meta where source_tab='\" + table + \"'\");\n        if (dbIndex != null) {\n            return Integer.parseInt(dbIndex);\n        }\n        //如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值\n        return startFrom;\n    }\n\n    //查询一条数据的执行语句(当前id)\n    private String queryOne(String sql) {\n        ResultSet result = null;\n        try {\n            ps = conn.prepareStatement(sql);\n            result = ps.executeQuery();\n            while (result.next()) {\n                return result.getString(1);\n            }\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n        return null;\n    }\n\n    //关闭相关资源\n    void close() {\n        try {\n            ps.close();\n            conn.close();\n        } catch (SQLException e) {\n            e.printStackTrace();\n        }\n    }\n\n    int getCurrentIndex() {\n        return currentIndex;\n    }\n\n    void setCurrentIndex(int newValue) {\n        currentIndex = newValue;\n    }\n\n    int getRunQueryDelay() {\n        return runQueryDelay;\n    }\n\n    String getQuery() {\n        return query;\n    }\n\n    String getConnectionURL() {\n        return connectionURL;\n    }\n\n    private boolean isCustomQuerySet() {\n        return (customQuery != null);\n    }\n\n    Context getContext() {\n        return context;\n    }\n\n    public String getConnectionUserName() {\n        return connectionUserName;\n    }\n\n    public String getConnectionPassword() {\n        return connectionPassword;\n    }\n}\n\n```\n\n| 属性                    | 说明（括号中为默认值）                     |\n| ----------------------- | ------------------------------------------ |\n| runQueryDelay           | 查询时间间隔（10000）                      |\n| batchSize               | 缓存大小（100）                            |\n| startFrom               | 查询语句开始id（0）                        |\n| currentIndex            | 查询语句当前id，每次查询之前需要查元数据表 |\n| recordSixe              | 查询返回条数                               |\n| table                   | 监控的表名                                 |\n| columnsToSelect         | 查询字段（*）                              |\n| customQuery             | 用户传入的查询语句                         |\n| query                   | 查询语句                                   |\n| defaultCharsetResultSet | 编码格式（UTF-8）                          |\n\n#### SQLSource\n\n```java\nimport org.apache.flume.Context;\nimport org.apache.flume.Event;\nimport org.apache.flume.EventDeliveryException;\nimport org.apache.flume.PollableSource;\nimport org.apache.flume.conf.Configurable;\nimport org.apache.flume.event.SimpleEvent;\nimport org.apache.flume.source.AbstractSource;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.text.ParseException;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.List;\n\npublic class SQLSource extends AbstractSource implements Configurable, PollableSource {\n\n    //打印日志\n    private static final Logger LOG = LoggerFactory.getLogger(SQLSource.class);\n    //定义sqlHelper\n    private SQLSourceHelper sqlSourceHelper;\n\n\n    @Override\n    public long getBackOffSleepIncrement() {\n        return 0;\n    }\n\n    @Override\n    public long getMaxBackOffSleepInterval() {\n        return 0;\n    }\n\n    @Override\n    public void configure(Context context) {\n        try {\n            //初始化\n            sqlSourceHelper = new SQLSourceHelper(context);\n        } catch (ParseException e) {\n            e.printStackTrace();\n        }\n    }\n\n    @Override\n    public Status process() throws EventDeliveryException {\n        try {\n            //查询数据表\n            List<List<Object>> result = sqlSourceHelper.executeQuery();\n            //存放event的集合\n            List<Event> events = new ArrayList<>();\n            //存放event头集合\n            HashMap<String, String> header = new HashMap<>();\n            //如果有返回数据，则将数据封装为event\n            if (!result.isEmpty()) {\n                List<String> allRows = sqlSourceHelper.getAllRows(result);\n                Event event = null;\n                for (String row : allRows) {\n                    event = new SimpleEvent();\n                    event.setBody(row.getBytes());\n                    event.setHeaders(header);\n                    events.add(event);\n                }\n                //将event写入channel\n                this.getChannelProcessor().processEventBatch(events);\n                //更新数据表中的offset信息\n                sqlSourceHelper.updateOffset2DB(result.size());\n            }\n            //等待时长\n            Thread.sleep(sqlSourceHelper.getRunQueryDelay());\n            return Status.READY;\n        } catch (InterruptedException e) {\n            LOG.error(\"Error procesing row\", e);\n            return Status.BACKOFF;\n        }\n    }\n\n    @Override\n    public synchronized void stop() {\n        LOG.info(\"Stopping sql source {} ...\", getName());\n        try {\n            //关闭资源\n            sqlSourceHelper.close();\n        } finally {\n            super.stop();\n        }\n    }\n}\n```\n\n| SQLSourceHelper(Context context)                     | 构造方法，初始化属性及获取JDBC连接              |\n| ---------------------------------------------------- | ----------------------------------------------- |\n| InitConnection(String url, String user,   String pw) | 获取JDBC连接                                    |\n| checkMandatoryProperties()                           | 校验相关属性是否设置（实际开发中可增加内容）    |\n| buildQuery()                                         | 根据实际情况构建sql语句，返回值String           |\n| executeQuery()                                       | 执行sql语句的查询操作，返回值List<List<Object>> |\n| getAllRows(List<List<Object>>   queryResult)         | 将查询结果转换为String，方便后续操作            |\n| updateOffset2DB(int size)                            | 根据每次查询结果将offset写入元数据表            |\n| execSql(String   sql)                                | 具体执行sql语句方法                             |\n| getStatusDBIndex(int   startFrom)                    | 获取元数据表中的offset                          |\n| queryOne(String   sql)                               | 获取元数据表中的offset实际sql语句执行方法       |\n| close()                                              | 关闭资源                                        |\n\n#### 测试准备\n\n#### 驱动包\n\n````\n[hadoop@datanode1 flume]$ cp \\\n/opt/sorfware/mysql-libs/mysql-connector-java-5.1.27/mysql-connector-java-5.1.27-bin.jar \\\n/opt/module/flume/lib/\n````\n\n**打包项目并将jar放入flume的lib目录下**\n\n#### 配置文件\n\n```properties\n[hadoop@datanode1 flume]$ vim job/mysql.conf\n# Name the components on this agent\na1.sources = r1\na1.sinks = k1\na1.channels = c1\n\n# Describe/configure the source\na1.sources.r1.type = com.hph.SQLSource\na1.sources.r1.connection.url = jdbc:mysql://192.168.1.101:3306/mysqlsource\na1.sources.r1.connection.user = root\na1.sources.r1.connection.password = 123456\na1.sources.r1.table = student\na1.sources.r1.columns.to.select = *\na1.sources.r1.incremental.column.name = id\na1.sources.r1.incremental.value = 0\na1.sources.r1.run.query.delay=5000\n\n# Describe the sink\na1.sinks.k1.type = logger\n\n# Describe the channel\na1.channels.c1.type = memory\na1.channels.c1.capacity = 1000\na1.channels.c1.transactionCapacity = 100\n\n# Bind the source and sink to the channel\na1.sources.r1.channels = c1\na1.sinks.k1.channel = c1\n```\n\n#### Mysql表准备\n\n```sql\nCREATE TABLE `student` (\n`id` int(11) NOT NULL AUTO_INCREMENT,\n`name` varchar(255) NOT NULL,\nPRIMARY KEY (`id`)\n);\nCREATE TABLE `flume_meta` (\n`source_tab` varchar(255) NOT NULL,\n`currentIndex` varchar(255) NOT NULL,\nPRIMARY KEY (`source_tab`)\n);\n```\n\n#### 测试脚本\n\n```shell\n#!/bin/bash　　\n\nHOSTNAME=\"192.168.1.101\"    #数据库信息\nPORT=\"3306\"\nUSERNAME=\"root\"\nPASSWORD=\"123456\"\n\nDBNAME=\"mysqlsource\"        #数据库名称\nTABLENAME=\"student\"         #数据库中表的名称\n\ni=0\nwhile [true]\nlet i+=1;\ndo\ninsert_sql=\"insert into ${TABLENAME}(id,name) values($i,'student$i')\"\nmysql -h${HOSTNAME}  -P${PORT}  -u${USERNAME} -p${PASSWORD} ${DBNAME} -e  \"${insert_sql}\"\nsleep 5\ndone\n```\n\n#### 测试并查看\n\n```shell\n[hadoop@datanode1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/mysql.conf -Dflume.root.logger=INFO,console\t\t#启动agent\n[hadoop@datanode1 job]$ sh mysql.sh  #启动测试脚本\n```\n\n![FqecK1.png](https://s2.ax1x.com/2019/01/07/FqecK1.png)\n\n##  Flume监控Ganglia\n\n### 步骤\n\n1.**安装httpd服务与php**\n\n```shell\n[hadoop@datanode1 flume]$ sudo yum -y install httpd php\n```\n\n2. 安装其他依赖\n\n```shell\n[hadoop@datanode1 flume]$ sudo yum -y install rrdtool perl-rrdtool rrdtool-devel\n[hadoop@datanode1 flume]$ sudo yum -y install apr-devel\n```\n\n3. 安装ganglia\n```shell\n[hadoop@datanode1 flume]$ sudo rpm -Uvh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm\n[hadoop@datanode1 flume]$ sudo yum -y install ganglia-gmetad \n[hadoop@datanode1 flume]$ sudo yum -y install ganglia-web\n[hadoop@datanode1 flume]$ sudo yum install -y ganglia-gmond\n```\n\n4. 修改ganglia\n\n```properties\n[hadoop@datanode1 flume]$ sudo vim /etc/httpd/conf.d/ganglia.conf\n# Ganglia monitoring system php web frontend\nAlias /ganglia /usr/share/ganglia\n<Location /ganglia>\n  Order deny,allow\n  Deny from all\n  Allow from all\n  # Allow from 127.0.0.1\n  # Allow from ::1\n  # Allow from .example.com\n</Location>\n```\n\n5. 修改配置文件gmetad.conf\n\n```properties\ndata_source \"datanode1\" 192.168.1.101\n```\n\n6.  修改配置文件gmond.conf\n\n```properties\ncluster {\n  name = \"datanode1\"   #自己的主机名\n  owner = \"unspecified\"\n  latlong = \"unspecified\"\n  url = \"unspecified\"\n}\nudp_send_channel {\n  #bind_hostname = yes # Highly recommended, soon to be default.\n                       # This option tells gmond to use a source address\n                       # that resolves to the machine's hostname.  Without\n                       # this, the metrics may appear to come from any\n                       # interface and the DNS names associated with\n                       # those IPs will be used to create the RRDs.\n  #mcast_join = 239.2.11.71                        #注释掉\n  host=192.168.1.101                 \t\t\t   #自己的主机IP\n  port = 8649\t\t\t\t\t\t\t\t     #端口号\n  ttl = 1\n}\n```\n\n7. 修改配置文件config\n\n```shell\n[hadoop@datanode1 flume]$ sudo vim /etc/selinux/config\n\n# This file controls the state of SELinux on the system.\n# SELINUX= can take one of these three values:\n#     enforcing - SELinux security policy is enforced.\n#     permissive - SELinux prints warnings instead of enforcing.\n#     disabled - No SELinux policy is loaded.\nSELINUX=disabled\n# SELINUXTYPE= can take one of these two values:\n#     targeted - Targeted processes are protected,\n#     mls - Multi Level Security protection.\nSELINUXTYPE=targeted\n```\n\n注意selinux本次生效关闭必须重启，如果此时不想重启，可以临时生效之：\n\n```shell\n[hadoop@datanode1 flume]$  sudo setenforce 0\n```\n\n### 启动ganglia\n\n```shell\n[hadoop@datanode1 flume]$ sudo service httpd start\nStarting httpd:\n[hadoop@datanode1 flume]$ sudo service gmetad start\n[hadoop@datanode1 flume]$ sudo service gmond start\n[hadoop@datanode1 flume]$\n```\n\n如果完成以上操作依然出现权限不足错误，请修改/var/lib/ganglia目录的权限\n\n```shell\n[hadoop@datanode1 flume]$ sudo chmod -R 777 /var/lib/ganglia\n```\n\n### 操作Flume测试监控\n\n1.修改/opt/module/flume/conf目录下的flume-env.sh配置：\n\n```shell\n[hadoop@datanode1 conf]$ vim flume-env.sh\nJAVA_OPTS=\"-Dflume.monitoring.type=ganglia\n-Dflume.monitoring.hosts=192.168.1.101:8649\n-Xms100m\n-Xmx200m\"\n[hadoop@datanode1 conf]$ xsync flume-env.sh\n```\n\n2. 启动flume任务\n\n```shell\n[hadoop@datanode3 flume]$ bin/flume-ng agent --conf conf/ --name a3 --conf-file job/group2/flume3.conf -Dflume.root.logger=INFO,console\n[hadoop@datanode2 flume]$  bin/flume-ng agent --conf conf/ --name a2 --conf-file job/group2/flume2.conf\n[hadoop@datanode1 flume]$ bin/flume-ng agent --conf conf/ --name a1 --conf-file job/group2/flume1.conf\n```\n\n![FqnX9g.png](https://s2.ax1x.com/2019/01/08/FqnX9g.png)\n\n![FquPEV.png](https://s2.ax1x.com/2019/01/08/FquPEV.png)\n\n| 字段（图表名称）      | 字段含义                                                     |\n| --------------------- | ------------------------------------------------------------ |\n| EventPutAttemptCount  | source尝试写入channel的事件总数量                            |\n| EventPutSuccessCount  | 成功写入channel且提交的事件总数量                            |\n| EventTakeAttemptCount | sink尝试从channel拉取事件的总数量。这不意味着每次事件都被返回，因为sink拉取的时候channel可能没有任何数据。 |\n| EventTakeSuccessCount | sink成功读取的事件的总数量                                   |\n| StartTime             | channel启动的时间（毫秒）                                    |\n| StopTime              | channel停止的时间（毫秒）                                    |\n| ChannelSize           | 目前channel中事件的总数量                                    |\n| ChannelFillPercentage | channel占用百分比                                            |\n| ChannelCapacity       | channel的容量                                                |\n\n","tags":["Flume"],"categories":["大数据"]},{"title":"ZooKeeper的安装和API","url":"/2019/01/06/ZooKeeper的安装和API/","content":"\n {{ \"Zookeeper的分布式安装和API介绍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 安装教程\n\n在datanode1、datanode2和datanode3三个节点上部署Zookeeper。\n\n### 步骤\n\n1. 解压zookeeper安装包到/opt/module/目录下\n\n```shell\ntar -zxvf zookeeper-3.4.10.tar.gz -C /opt/module/\n```\n\n2. /opt/module/zookeeper-3.4.10/这个目录下创建zkData\n\n```shell\nmkdir -p zkData\n```\n\n3. 重命名/opt/module/zookeeper-3.4.10/conf这个目录下的zoo_sample.cfg为zoo.cfg\n\n```shell\nmv zoo_sample.cfg zoo.cfg\n```\n\n4. 配置zoo.cfg文件\n\n```shell\n#######################cluster##########################\nserver.2=datanode1:2888:3888\nserver.3=datanode2:2888:3888\nserver.4=datanode3:2888:3888\n```\n\nserver.A=B:C:D。\n\nA是一个数字，表示这个是第几号服务器；\n\nB是这个服务器的ip地址或者主机名；\n\nC是这个服务器与集群中的Leader服务器交换信息的端口；\n\nD是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。\n\n集群模式下配置一个文件myid，这个文件在dataDir目录下，这个文件里面有一个数据就是A的值，Zookeeper启动时读取此文件，拿到里面的数据与zoo.cfg里面的配置信息比较从而判断到底是哪个server。\n\n5. 在/opt/module/zookeeper-3.4.10/zkData目录下创建一个myid的文件\n\n```shell\ntouch myid\n```\n\n6. 编辑myid文件,各个节点的值根据配置文件zoo.cfg来\n\n7. 拷贝配置好的zookeeper到其他机器上并分别修改myid文件中内容为3、4\n\n```shell\ntouch myid\n```\n\n8. 启动脚本\n\n```shell\n#!/bin/sh\necho \"starting zookeeper server...\"\nhosts=\"datanode1 datanode2 datanode3\" \nfor host in $hosts\ndo\n  ssh $host  \"source /etc/profile; /opt/module/zookeeper-3.4.10/bin/zkServer.sh start\"\ndone\n```\n\n停止脚本换成stop即可\n\n9. 查看zhua状态\n\n```shell\nzkServer.sh status\n```\n\n### 客户端命令行操作\n\n| 命令基本语法      | 功能描述                                               |\n| ----------------- | ------------------------------------------------------ |\n| help              | 显示所有操作命令                                       |\n| ls   path [watch] | 使用 ls 命令来查看当前znode中所包含的内容              |\n| ls2 path [watch]  | 查看当前节点数据并能看到更新次数等数据                 |\n| create            | 普通创建   -s  含有序列   -e  临时（重启或者超时消失） |\n| get path [watch]  | 获得节点的值                                           |\n| set               | 设置节点的具体值                                       |\n| stat              | 查看节点状态                                           |\n| delete            | 删除节点                                               |\n| rmr               | 递归删除节点                                           |\n\n## Shell命令\n\n### 启动客户端\n\n```shell\nbin/zkCli.sh\n```\n\n### 显示所有操作命令\n\n```shell\n[zk: localhost:2181(CONNECTED) 0] help\nZooKeeper -server host:port cmd args\n        connect host:port\n        get path [watch]\n        ls path [watch]\n        set path data [version]\n        rmr path\n        delquota [-n|-b] path\n        quit\n        printwatches on|off\n        create [-s] [-e] path data acl\n        stat path [watch]\n        close\n        ls2 path [watch]\n        history\n        listquota path\n        setAcl path acl\n        getAcl path\n        sync path\n        redo cmdno\n        addauth scheme auth\n        delete path [version]\n        setquota -n|-b val path\n```\n\n### 查看当前znode中所包含的内容\n\n```shell\n[zk: localhost:2181(CONNECTED) 1] ls /\n[isr_change_notification, test, hbase, zookeeper, admin, consumers, cluster, config, latest_producer_id_block, kafka-manager, brokers, controller_epoch]\n```\n\n### 查看当前节点数据并能看到更新次数等数据\n\n```shell\n[zk: localhost:2181(CONNECTED) 2]  ls2 /\n[isr_change_notification, test, hbase, zookeeper, admin, consumers, cluster, config, latest_producer_id_block, kafka-manager, brokers, controller_epoch]\ncZxid = 0x0\nctime = Thu Jan 01 08:00:00 CST 1970\nmZxid = 0x0\nmtime = Thu Jan 01 08:00:00 CST 1970\npZxid = 0x1200000104\ncversion = 100\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 0\nnumChildren = 12\n```\n\n### 创建普通节点\n\n```shell\n[zk: localhost:2181(CONNECTED) 3]  create /app1 \"hello app1\"\nCreated /app1\n[zk: localhost:2181(CONNECTED) 4] create /app1/server101 \"192.168.1.101\"\nCreated /app1/server101\n```\n\n### 获得节点的值\n\n```shell\n[zk: localhost:2181(CONNECTED) 5] get /app1\nhello app1\ncZxid = 0x120000010a\nctime = Sat Jan 05 13:34:15 CST 2019\nmZxid = 0x120000010a\nmtime = Sat Jan 05 13:34:15 CST 2019\npZxid = 0x120000010b\ncversion = 1\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 10\nnumChildren = 1\n[zk: localhost:2181(CONNECTED) 6] get /app1/server101\n192.168.1.101\ncZxid = 0x120000010b\nctime = Sat Jan 05 13:34:46 CST 2019\nmZxid = 0x120000010b\nmtime = Sat Jan 05 13:34:46 CST 2019\npZxid = 0x120000010b\ncversion = 0\ndataVersion = 0\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 13\nnumChildren = 0\n```\n\n### 创建短暂节点\n\n```shell\n[zk: localhost:2181(CONNECTED) 7] create -e /app-emphemeral 8888\nCreated /app-emphemeral\n## 在当前客户端是能查看到的\n[zk: localhost:2181(CONNECTED) 8]  ls /\n[app1, isr_change_notification, test, hbase, zookeeper, admin, consumers, cluster, config, latest_producer_id_block, kafka-manager, app-emphemeral, brokers, controller_epoch]\n## 退出当前客户端然后再重启客户端\n[zk: localhost:2181(CONNECTED) 9] quit\nQuitting...\n2019-01-06 17:43:54,558 [myid:] - INFO  [main:ZooKeeper@684] - Session: 0x1681924b94d0014 closed\n2019-01-06 17:43:54,564 [myid:] - INFO  [main-EventThread:ClientCnxn$EventThread@519] - EventThread shut down for session: 0x1681924b94d0014\n## 重启客户端\n[hadoop@datanode1 bin]$ ./zkCli.sh\n....\n2019-01-06 17:46:04,711 [myid:] - INFO  [main-SendThread(localhost:2181):ClientCnxn$SendThread@1299] - Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x1681924b94d0015, negotiated timeout = 30000\n\nWATCHER::\n\nWatchedEvent state:SyncConnected type:None path:null\n## 再次查看根目录下短暂节点已经删除\n\n[zk: localhost:2181(CONNECTED) 0] ls /\n[app1, isr_change_notification, test, hbase, zookeeper, admin, consumers, cluster, config, latest_producer_id_block, kafka-manager, brokers, controller_epoch]\n```\n\n### 创建带序号的节点\n\n```shell\n## 先创建一个普通的根节点app2\n[zk: localhost:2181(CONNECTED) 1] create /app2 \"app2\"\nCreated /app2\n## 创建带序号的节点\n[zk: localhost:2181(CONNECTED) 2] create /app2 \"app2\"\n[zk: localhost:2181(CONNECTED) 3] create -s /app2/aa 888\nCreated /app2/aa0000000000\n[zk: localhost:2181(CONNECTED) 4] create -s /app2/bb 888\nCreated /app2/bb0000000001\n[zk: localhost:2181(CONNECTED) 5] create -s /app2/cc 888\nCreated /app2/cc0000000002\n##如果原节点下有1个节点，则再排序时从1开始，以此类推。\n[zk: localhost:2181(CONNECTED) 6] create -s /app1/aa 888\nCreated /app1/aa0000000001\n```\n\n### 修改节点数据值\n\n```shell\n[zk: localhost:2181(CONNECTED) 8] set /app1 999\ncZxid = 0x120000010a\nctime = Sat Jan 05 13:34:15 CST 2019\nmZxid = 0x1200000116\nmtime = Sat Jan 05 14:12:56 CST 2019\npZxid = 0x1200000114\ncversion = 3\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 3\n```\n\n### 节点的值变化监听\n\n在datanode1主机上注册监听/app1节点数据变化\n\n```shell\n[zk: localhost:2181(CONNECTED) 9] get /app1 watch\n999\ncZxid = 0x120000010a\nctime = Sat Jan 05 13:34:15 CST 2019\nmZxid = 0x1200000116\nmtime = Sat Jan 05 14:12:56 CST 2019\npZxid = 0x1200000114\ncversion = 3\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 3\n```\n\n在datanode2主机上修改/app1节点的数据\n\n```shell\n[zk: localhost:2181(CONNECTED) 0] set /app1  777\ncZxid = 0x120000010a\nctime = Sat Jan 05 13:34:15 CST 2019\nmZxid = 0x1200000118\nmtime = Sat Jan 05 14:14:43 CST 2019\npZxid = 0x1200000114\ncversion = 3\ndataVersion = 2\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 3\n```\n\ndatanode1主机上的变化\n\n```shell\n[zk: localhost:2181(CONNECTED) 10]\nWATCHER::\n\nWatchedEvent state:SyncConnected type:NodeDataChanged path:/app1\n```\n\n### 节点的子节点变化监听（路径变化）\n\n在datanode1主机上注册监听/app1节点的子节点变化\n\n```shell\n[zk: localhost:2181(CONNECTED) 0] ls /app1 watch\n[aa0000000001, server101, cc0000000002]\n```\n\n在datanode2主机/app1节点上创建子节点\n\n```shell\n[zk: localhost:2181(CONNECTED) 1] create /app1/bb 666\nCreated /app1/bb\n```\n\n观察datanode1主机收到子节点变化的监听\n\n```shell\nWATCHER::\nWatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1\n```\n\n### 删除节点\n\n```shell\n[zk: localhost:2181(CONNECTED) 2] delete /app1/bb\n```\n\n### 递归删除节点\n\n```\n[zk: localhost:2181(CONNECTED) 3] rmr /app2\n```\n\n### 查看节点状态\n\n```\n[zk: localhost:2181(CONNECTED) 1]  stat /app1\ncZxid = 0x120000010a\nctime = Sat Jan 05 13:34:15 CST 2019\nmZxid = 0x1200000118\nmtime = Sat Jan 05 14:14:43 CST 2019\npZxid = 0x120000011d\ncversion = 5\ndataVersion = 2\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 3\nnumChildren = 3\n```\n\n## API应用\n\n### IDEA环境搭建\n\n1. 创建一个Maven工程\n\n2. 添加pom文件\n\n    ```xml\n    \t<dependencies>\n    \t\t<dependency>\n    \t\t\t<groupId>junit</groupId>\n    \t\t\t<artifactId>junit</artifactId>\n    \t\t\t<version>RELEASE</version>\n    \t\t</dependency>\n    \t\t<dependency>\n    \t\t\t<groupId>org.apache.logging.log4j</groupId>\n    \t\t\t<artifactId>log4j-core</artifactId>\n    \t\t\t<version>2.8.2</version>\n    \t\t</dependency>\n    \t\t<!-- https://mvnrepository.com/artifact/org.apache.zookeeper/zookeeper -->\n    \t\t<dependency>\n    \t\t\t<groupId>org.apache.zookeeper</groupId>\n    \t\t\t<artifactId>zookeeper</artifactId>\n    \t\t\t<version>3.4.10</version>\n    \t\t</dependency>\n    \t</dependencies>\n    ```\n\n### log4j.propertie\n\n```properties\nlog4j.rootLogger=INFO, stdout  \nlog4j.appender.stdout=org.apache.log4j.ConsoleAppender  \nlog4j.appender.stdout.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n  \nlog4j.appender.logfile=org.apache.log4j.FileAppender  \nlog4j.appender.logfile.File=target/spring.log  \nlog4j.appender.logfile.layout=org.apache.log4j.PatternLayout  \nlog4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n  \n```\n\n### 创建ZooKeeper客户端\n\n```java\npublic class ZKDemo {\n    private String connect = \"datanode1:2181,datanode2:2181,datanode3:2181\";\n    private int timeout = 2000;\n    private ZooKeeper zooKeeper = null;\n\n    //获取Zookeeper的客户端\n    @Before\n    public void getClient() throws Exception {\n        zooKeeper = new ZooKeeper(connect, timeout, new Watcher() {\n            //接收到Zookeeper发来的通知以后做出的处理措施(自己处理的业务逻辑)\n            @Override\n            public void process(WatchedEvent watchedEvent) {\n                System.out.println(watchedEvent.getType() + \"-----\" + watchedEvent.getPath());\n            }\n        });\n    }\n```\n\n### 创建子节点\n\n```java\n    //创建节点\n    @Test\n    public void testCreate() throws KeeperException, InterruptedException {\n     // 参数1：要创建的节点的路径； 参数2：节点数据 ； 参数3：节点权限 ；参数4：节点的类型\n        String path = zooKeeper.create(\n                \"/cainiaoqingfeng\",\n                \"bigtadaLearing\".getBytes(),\n                ZooDefs.Ids.OPEN_ACL_UNSAFE,\n                CreateMode.PERSISTENT);\n        System.out.println(path);\n    }\n```\n\n```shell\n[zk: localhost:2181(CONNECTED) 2] ls /\n[test, cainiaoqingfeng, consumers, latest_producer_id_block, controller_epoch, app2, app1, isr_change_notification, hbase, admin, zookeeper, config, cluster, kafka-manager, brokers]\n```\n\n### 判断节点是否存在\n\n```java\n   public void testExist() throws KeeperException, InterruptedException {\n        Stat stat = zooKeeper.exists(\"/cainiaoqingfeng\", false);\n        System.out.println(stat==null?\"not exist\":\"exist\");\n    }\n```\n\n### 循环监听\n\n```java\n      try {\n                    List<String> children = zooKeeper.getChildren(\"/cainiaoqingfeng\", true);\n                    for (String child : children) {\n                        System.out.println(child);\n                    }\n\n                } catch (Exception e) {\n                    e.printStackTrace();\n                }\n//在创建Zookeeper客户端的代码中加此以上代码\n```\n\n### 改变节点的内容\n\n```java\n    public void testSet() throws KeeperException, InterruptedException {\n        Stat stat = zooKeeper.setData(\"/cainiaoqingfeng/bigdata\", \"I love bigdata\".getBytes(), -1);\n\n    }\n```\n\n```shell\n[zk: localhost:2181(CONNECTED) 20] get /cainiaoqingfeng/bigdata\nI love bigdata\ncZxid = 0x1200000126\nctime = Sat Jan 05 16:11:19 CST 2019\nmZxid = 0x1200000136\nmtime = Sat Jan 05 16:27:51 CST 2019\npZxid = 0x1200000126\ncversion = 0\ndataVersion = 1\naclVersion = 0\nephemeralOwner = 0x0\ndataLength = 14\n```\n\n\n\n## 监听器原理\n\n![FHz0gg.png](https://s2.ax1x.com/2019/01/06/FHz0gg.png)\n\n### 过程\n\n1. 先要有一个main()线程\n2. 在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。\n3. 通过connect线程将注册的监听事件发送给Zookeeper。\n4. 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。\n5. Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。\n6. listener线程内部调用了process（）方法。\n\n### 常见监听\n\n2．常见的监听\n\n（1）监听节点数据的变化：\n\n```shell\nget path [watch]\n```\n\n\n\n（2）监听子节点增减的变化\n\n```shell\nls path [watch]\n```\n\n### 写数据\n\n![FHzor9.png](https://s2.ax1x.com/2019/01/06/FHzor9.png)\n\nZooKeeper 的写数据流程主要分为以下几步，如图所示：\n\n#### 流程\n\n1. 比如 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。\n2. 如果Server1不是Leader，那么Server1 会把接受到的请求进一步转发给Leader，因为每个ZooKeeper的Server里面有一个是Leader。这个Leader 会将写请求广播给各个Server，比如Server1和Server2， 各个Server写成功后就会通知Leader。\n3. 当Leader收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader会告诉Server1数据写成功了。\n4. Server1会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。ZooKeeper 整个写数据流程就是这样的。\n\n## 服务器节点动态上下线\n\n![FbSLes.png](https://s2.ax1x.com/2019/01/06/FbSLes.png)\n\n### ZkServer\n\n```java\nimport org.apache.zookeeper.*;\n\nimport java.io.IOException;\n\npublic class zkServer {\n    private static String connect = \"datanode1:2181,datanode2:2181,datanode3:2181\";  //写自己的集群的ip和端口\n    private static int timeout = 2000;\n    private static ZooKeeper zooKeeper = null;\n    private static String parentPahth = \"/servers/\";\n\n    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {\n        //获取Zookeeper的客户端\n        getClient();\n        //启动注册\n        registsServer(args[0]);\n        //业务逻辑\n        business(args[0]);\n    }\n\n    private static void business(String hostname) throws InterruptedException {\n        System.out.println(hostname+\"  is working...\");\n        Thread.sleep(Long.MAX_VALUE);\n\n\n    }\n\n    private static void registsServer(String hostname) throws KeeperException, InterruptedException {\n        //创建临时节点\n        String path = zooKeeper.create(parentPahth + \"server\",\n                hostname.getBytes(),\n                ZooDefs.Ids.OPEN_ACL_UNSAFE,\n                CreateMode.EPHEMERAL_SEQUENTIAL);\n        System.out.println(hostname+\"  is online  \"+path);\n        Thread.sleep(Long.MAX_VALUE);\n    }\n\n\n    private static void getClient() throws IOException {\n        zooKeeper = new ZooKeeper(connect, timeout, new Watcher() {\n            @Override\n            public void process(WatchedEvent watchedEvent) {\n                System.out.println(watchedEvent.getType() + \"**********\" + watchedEvent.getPath());\n            }\n        });\n    }\n}\n\n```\n\n### ZkClient\n\n```java\nimport org.apache.zookeeper.*;\n\nimport java.io.IOException;\n\npublic class zkServer {\n    private static String connect = \"datanode1:2181,datanode2:2181,datanode3:2181\";  //写自己的集群的ip和端口\n    private static int timeout = 2000;\n    private static ZooKeeper zooKeeper = null;\n    private static String parentPahth = \"/servers/\";\n\n    public static void main(String[] args) throws IOException, KeeperException, InterruptedException {\n        //获取Zookeeper的客户端\n        getClient();\n        //启动注册\n        registsServer(args[0]);\n        //业务逻辑\n        business(args[0]);\n    }\n\n    private static void business(String hostname) throws InterruptedException {\n        System.out.println(hostname+\"  is working...\");\n        Thread.sleep(Long.MAX_VALUE);\n\n\n    }\n\n    private static void registsServer(String hostname) throws KeeperException, InterruptedException {\n        //创建临时节点\n        String path = zooKeeper.create(parentPahth + \"server\",\n                hostname.getBytes(),\n                ZooDefs.Ids.OPEN_ACL_UNSAFE,\n                CreateMode.EPHEMERAL_SEQUENTIAL);\n        System.out.println(hostname+\"  is online  \"+path);\n        Thread.sleep(Long.MAX_VALUE);\n    }\n\n\n    private static void getClient() throws IOException {\n        zooKeeper = new ZooKeeper(connect, timeout, new Watcher() {\n            @Override\n            public void process(WatchedEvent watchedEvent) {\n                System.out.println(watchedEvent.getType() + \"**********\" + watchedEvent.getPath());\n            }\n        });\n    }\n}\n\n```\n\n![FbP57j.png](https://s2.ax1x.com/2019/01/06/FbP57j.png)\n\n\n\n![FbPTNn.png](https://s2.ax1x.com/2019/01/06/FbPTNn.png)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t参考资料:尚硅谷Zookeeper\n\n\n\n\n\n\n\n\n\n\n\n","tags":["Zookeeper"],"categories":["大数据"]},{"title":"Zookeeper入门","url":"/2019/01/05/Zookeeper入门/","content":"\n {{ \"Zookeeper相关介绍和选举算法、应用场景等\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Zookeeper 的简介\n\nZookeeper是一个开源的分布式的，一个针对大型分布式系统的可靠协调系统的Apache项目。\n\n目标是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户；\n\nZooKeeper已经成为Hadoop生态系统的基础组件;功能包括：配置服务、名字服务、分布式同步、组服务等\n\n从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似Master/Slave管理模式\n\nZookeeper=文件系统+通知机制\n\n## 特点\n\n1. Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。\n2. Leader负责进行投票的发起和决议，更新系统状态\n3. Follower用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票\n4. 集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。\n5. 全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。\n6. 更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。\n7. 数据更新原子性，一次数据更新要么成功，要么失败。\n8. 实时性，在一定时间范围内，client能读到最新数据。\n\n## 特性\n\n1. 简单:Zookeeper的核心是一个简单的文件系统,文件系统提供一些简单的操作和抽象操作比如排序和通知.\n2. 富有表现力的:Zookeeper的基本操作是一组丰富的构建(building block),可以用于实现多种协调数据结构和协议如：分布式队列、分布式锁、领导选举(leader election)。\n3. 高可用性：Zookeeper是运行在一组机器上的，负责机器资源和进程管理。设计上有高可用性，因此应用程序完全可以依赖它，Zookeeper可以解决系统中出现单点故障的情况，实现系统的高可靠性，Zookeeper可以构建可靠的应用程序。比如Hadoop的HA。\n4. 松耦合交互方式：在Zookeeper支持的交互过程中,参与者不需要彼此了解.如Zookeeper可以被用于实现“数据汇聚(rendezvous)”机制，让进程在不了解其他进程(或者网络状况)的情况下能够彼此发现并进行信息交互，参与的各方甚至可以不必要同时存在，因为一个进程可以在Zookeeper中留下一条记录i信息，在该进程结束后，另一个进程可以在Zookeeper中读取到这条记录信息。\n5. 资源库：Zookeeper提供了一个通用协调模式实现方法的开源啊共享库，使得程序员免于编写协调协议程序(这类程序很难编写)，所有人都能够对这个资源库进行修改和改进，这样每个人都能够从中受益。\n6. 高性能：对于以写操作为主的工作负载来说：Zookeeper的基准吞吐量已经超过每秒10000个操作；对于常规的以读操作为主的工作负载来说，吞吐量要高出好几倍。\n\n## 数据模型\n\nZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。Zookeeper拥有一个层次的命名空间，这和分布式的文件系统十分相似。唯一不同的地方是命名空间中的每个节点可以有和他自身或它的子节点相关联的数据。这就好像是一个文件系统只不过文件系统中的文件和还具有目录的功能。另外，指向节点的路径必须使用规范的绝对路径来标识，并且以斜线“/”来分隔。ZooKeeper中不允许使用相对路径\n\n很显然zookeeper集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个znode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。\n\n[![F7iYDJ.png](https://s2.ax1x.com/2019/01/05/F7iYDJ.png)](https://s2.ax1x.com/2019/01/05/F7iYDJ.png)\n\n### Znode\n\nZookeeper目录树中的每一个节点对应着每一个Znode。每个Znode维护者一个属性结构，它包括数据的版本号(dataVersion)、时间戳(ctime、mtime)的状态信息。ZooKeeper使用节点的这些特性来实现它的特定功能。当Znode数据发生改变时，它的版本号将会增加，当客户端检索数据时，会同时检索数据的版本号，如果一个客户端执行了某个节点的更新或者删除操作，它必须提供要被操作的数据的版本号，如果所提供的版本号与实际不匹配，那么这个操作将会失败。\n\n#### Watches\n\n客户端可以在节点上设置watches(监视器)。当节点的状态发生改变时(数据的增、删、改)将会触发watch对应的操作。当watch被触发时,Zookeeper将会向客户端发送且发送一个通知，因为watch只能触发一次。\n\n#### 数据访问\n\nZookeeper中的每一个节点上存储的数据需要被原子性的操作：读操作将获取与节点相关的数据，写操作也将替换掉节点相关的所有数据。另外，每一个节点都拥有自己的ACL（访问控制列表），这个列表规定了用户的权限，即限定了用户对目标节点可以执行的操作。\n\n#### 临时节点\n\nZookeeper的节点分为两种：临时节点和永久节点。节点的类型在创建时即被确定，并且不能该改变。Zookeeper临时节点的生命周期依赖于他们的会话，一旦会话结束，临时节点会被自动删除，当然也可以手动删除。Zookeeper的临时节点不允许拥有子节点，相反，永久节点的生命周期不依赖于会话，并且只有客户端显示执行删除操作的时候，他们才被删除。\n\n#### 顺序节点\n\n唯一性保证：当创建Znode的时候，用户可以请求在Zookeeper的路径末尾添加一个递增的计数。这个计数对于此节点的父节点时唯一的格式为“%010d”(10位数组，没有数值的数据位用0填充，例如0000000001)。当计数值大于2147483647，计数器会溢出。\n\n### 时间\n\nZookeeper中有多重记录时间的形式，主要包括以下几个属性：\n\n#### Zxid\n\n导致Zookeeper节点状态改变的每一个操作都将使节点接收到一个zxid格式的时间戳，并且这个时间戳使全局有序的，也就是说，每一个对节点的改变都将产生一个唯一的zxid。如果zxid1的值小于zxid2的值，那么zxid1所对应的时间发生在zxid2所对应的事件之前，实际中Zookeeper的每个节点都维护着三个zxid值，分别为：cZxid、mZxid和pZid。cZxid使节点的创建时间所对应的Zxid时间戳，mZxid是节点的修改时间对应的Zxid格式时间戳。\n\n#### 版本号\n\n对节点的每一个操作都将导致这个节点的版本号增加。每个节点维护者三个版本号，他们维护者三个版本号，他们分别是：version(节点数据版本号)、cversion(子节点版本号)、aversion(节点所拥有的ACL版本号)。\n\n### 节点属性结构\n\n| 属性        | 描述                                                         |\n| ----------- | ------------------------------------------------------------ |\n| cZxid       | 创建节点时的事务ID                                           |\n| ctime       | 创建节点的时间                                               |\n| mZxid       | 最后修改节点时的事务ID                                       |\n| mtime       | 最后修改节点时的时间                                         |\n| pZxid       | 表示该子节点列表最后一次修改的事务ID，添加子节点或删除子节点就会影响子节点列表，但是修改子节点的数据内容则不影响该ID |\n| cversion    | 子节点版本号，子节点每次修改版本号加1                        |\n| dataversion | 数据版本号，数据每次修改该版本号加1                          |\n| aclversion  | 权限版本号，权限每次修改该版本号加1                          |\n| dataLength  | 该节点的数据长度                                             |\n| numChildren | 该节点拥有子节点的数量                                       |\n\n## 观察触发器\n\n读操作exists、getChildren和getData都被设置了watch，并且这些watch都由写操作来触发：create、delete和setData。ACL操作并不参与到watch中。当watch被触发时，watch事件被生成，它的类型由watch和触发它的操作共同决定，Zookeeper所管理的watch可以分为两类：\n\n1. data watchs：getData和exists负责设置data watch；\n2. child watchs：getChildren负责设置child watch\n\n我们可以通过操作返回的数据来设置不同的watch：\n\n1. getData和exists：返回关于节点的数据信息\n2. getChildren：返回孩子列表\n\n因此，一个成功的setData操作将触发Znode的数据watch，一个成功的create操作将触发Znode的数据watch，一个成功的delete操作将触发Znode的data wachts以及child watches。\n\nwatch由客户端所连接的Zookeeper服务器在本地维护，因此可以非常容易地设置，管理和分派，当客户端连接到另一个新的服务器上时，任何的会话事件都可能触发watch。另外当从服务器断开连接的时候，watch将不会被接收。但是，当一个客户端重新建立连接的时候，任何先前注册过的watch都会被重新注册。\n\nexists操作上的watch，在被监视的Znode创建、删除或数据更新时被触发。\n\ngetData操作上的watch，在被监视的Znode删除或数据更新时被触发，在创建时不能被触发，因为只有Znode存在，getData操作才会成功。\n\ngetChildren操作上的watch，在被监视的Znode的子节点创建或删除，或是这个Znode自身被删除时被触发。可以通过查看watch事件类型来区分是Znode还是其他的子节点被删除；NodeDelete表示Znode被删除，NodeDeletedChanged表示子节点被删除。\n\n[![F7QBTK.png](https://s2.ax1x.com/2019/01/05/F7QBTK.png)](https://s2.ax1x.com/2019/01/05/F7QBTK.png)\n\nwatch事件包括了设计的Znode的路径，因此对于NodeCreated和NodeDeleted事件来说，根据路径就可以简单区分出那个Znode被创建或者时被删除了。为了查询在NodeChildrenChanged事件后产生的新数据，需要调用getData。在两种情况下，Znode可能在获取watch事件或执行读写操作这两种状态下切换，在写应用程序时，必须记住这一点。\n\nZookeeper的watch实际上需要处理两类事件：\n\n- 连接状态事件(type=None,path=null)\n\n这类事件不需要注册，也不需要我们连续触发，我们只需要处理就行了\n\n- 节点事件\n\n节点的创建，删除数据的修改。他是 one time trigger,我们需要不停的注册触触发，还可能发生事件丢失的情况。\n\n上面2类事件都在Watch中处理，也就是重载的**process(Event event)**\n\n**节点事件的触发，通过函数exists，getData或getChildren来处理这类函数，有双重作用：**\n\n**① 注册触发事件**\n\n**② 函数本身的功能**\n\n函数的本身的功能又可以用异步的回调函数来实现，重载processResult)()过程中处理函数本身的功能。函数还可以指定自己的watch所以每个函数有4个版本。根据自己的需求来选择不同的函数，不同的版本。\n\n## ACL列表\n\nZooKeeper使用ACL来对Znode进行访问控制。ACL的实现和Unix文件访问许可非常相似：它使用许可位来对一个节点的不同操作进行允许或禁止的权限控制。但是，和标准的Unix许可不同的是，Zookeeper对于用户类别的区分，不知局限于所有者(owner)、组(group)、所有人(world)、三个级别，Zookeeper中，数据节点没有“所有者”的概念。访问者利用id表示自己的身份，并且获得与之相应的不同的访问权限。\n\n注意：\n\n传统的文件系统中，ACL分为两个维度，一个是属组，一个是权限，子目录/文件默认继承父目录的ACL。而在Zookeeper中一个ACL和一个ZooKeeper节点相对应。并且，父节点的ACL与子节点的ACL是相互独立的。也就是说，ACL不能被子节点所继承，父节点所拥有的权限与子节点所用的权限都没有任何关系。\n\nZookeeper支持可配置的认证机制。它利用一个三元组来定义客户端的访问权限：(scheme:expression, perms) 。其中：\n\n1.scheme：定义了expression的含义。\n\n如：host:host1.corp.com,READ）,标识了一个名为host1.corp.com的主机,有该数据节点的读权限。\n\n2.Perms：标识了操作权限。\n\n如：（ip:19.22.0.0/16, READ）,表示IP地址以19.22开头的主机,有该数据节点的读权限。\n\nZookeeper的ACL也可以从三个维度来理解：一是，scheme; 二是，user; 三是，permission，通常表示为scheme:id：permissions，如下图所示。\n\n[![F71cqI.png](https://s2.ax1x.com/2019/01/05/F71cqI.png)](https://s2.ax1x.com/2019/01/05/F71cqI.png)\n\n1.world : id格式：anyone。\n\n如：world:anyone代表任何人，zookeeper中对所有人有权限的结点就是属于world:anyone的。\n\n2.auth : 它不需要id。\n\n注：只要是通过authentication的user都有权限，zookeeper支持通过kerberos来进行认证, 也支持username/password形式的认证。\n\n3.digest: id格式：username:BASE64(SHA1(password))。\n\n它需要先通过username:password形式的authentication。\n\n4.ip: id格式：客户机的IP地址。\n\n设置的时候可以设置一个ip段。如：ip:192.168.1.0/16, 表示匹配前16个bit的IP段\n\n5.super: 超级用户模式。\n\n在这种scheme情况下，对应的id拥有超级权限，可以做任何事情\n\nZooKeeper权限定义如下图所示：\n\n[![F717ss.png](https://s2.ax1x.com/2019/01/05/F717ss.png)](https://s2.ax1x.com/2019/01/05/F717ss.png)\n\nZooKeeper内置的ACL模式如下图所示：\n\n[![F71OoV.png](https://s2.ax1x.com/2019/01/05/F71OoV.png)](https://s2.ax1x.com/2019/01/05/F71OoV.png)\n\n当会话建立的时候，客户端将会进行自我验证。另外，ZooKeeper Java API支持三种标准的用户权限，它们分别为：\n\n1.ZOO_PEN_ACL_UNSAFE：对于所有的ACL来说都是完全开放的，任何应用程序可以在节点上执行任何操作，比如创建、列出并删除子节点。\n\n2.ZOO_READ_ACL_UNSAFE：对于任意的应用程序来说，仅仅具有读权限。\n\n3.ZOO_CREATOR_ALL_ACL：授予节点创建者所有权限。需要注意的是，设置此权限之前，创建者必须已经通了服务器的认证。\n\n代码实现：\n\n```java\nimport org.apache.zookeeper.CreateMode;\nimport org.apache.zookeeper.KeeperException;\nimport org.apache.zookeeper.ZooDefs;\nimport org.apache.zookeeper.ZooKeeper;\nimport org.apache.zookeeper.data.ACL;\nimport org.apache.zookeeper.data.Id;\nimport org.apache.zookeeper.server.auth.DigestAuthenticationProvider;\n\nimport java.io.IOException;\nimport java.security.NoSuchAlgorithmException;\nimport java.util.ArrayList;\nimport java.util.List;\n\n\npublic class NewDigest {\n    public static void main(String[] args) throws NoSuchAlgorithmException, KeeperException, InterruptedException, IOException {\n\n        List acls = new ArrayList<>();\n        //添加第一个id,采用用户名密码的形式\n        Id id1 = new Id(\"digest\", DigestAuthenticationProvider.generateDigest(\"admin:admin\"));\n        ACL acl1 = new ACL(ZooDefs.Perms.ALL, id1);\n        acls.add(acl1);\n\n        //添加第二个id,所有用户可获得去权限\n        Id id2 = new Id(\"world\", \"anyone\");\n        ACL acl2 = new ACL(ZooDefs.Perms.READ, id2);\n        ZooKeeper zk = new ZooKeeper(\"datanode1:2181,datanode2:2181,datanode3:2181\", 200, null);\n\n        zk.addAuthInfo(\"digest\",\"admin:admin\".getBytes());\n        zk.create(\"/test\",\"data\".getBytes(),acls,CreateMode.PERSISTENT);\n        System.out.println(\"创建成功\");\n\n    }\n}\n```\n\n## 执行过程\n\nZookeeper服务可以以以两种模式运行，在单机模式下，只有一个Zookeeper服务器，便于用来测试，但是它没有高可用和性能恢复的保障。在工业界，Zookeeper以复合模式运行在一组键ensemble的集群上。Zookeeper通过复制来获得高可用，同时，只要ensemble中大部分机器运作，就可以提供服务，在2n+1个节点的ensemble中，可以承受n台机器出现故障。\n\nZooKeeper的思想非常简单：它所需要做的就是保证对Znode树的每一次修改都复制到ensemble中的大部分机器上去。如果机器中的小部分出故障了，那么至少有一台机器将会恢复到最新状态，其他的则保存这副本，直到最终达到最新状态。Zookeeper采用Zab协议，它分为两个阶段，并且可能被无限的重复。\n\n### 领导者选举\n\n在ensemble中的机器要参与一个选择特殊成员的进程，这个成员叫领导者，其他机器脚跟随者。在大部分的跟随者与他们的领导者同步了状态以后，这个阶段才算完成。\n\n当leader崩溃或者leader失去大多数的follower，这时候zk进入**恢复模式**，恢复模式需要重新选举出一个新的leader，让所有的Server都恢复到一个正确的状态。Zk的选举算法有两种：一种是基于**basic paxos**实现的，另外一种是基于**fast paxos**算法实现的。系统默认的选举算法为fast paxos。先介绍basic paxos流程：\n\n1.选举线程由**当前Server**发起选举的线程担任，其主要功能是对投票结果进行统计，并选出推荐的Server；\n\n2.选举线程首先向所有Server发起一次**询问**(包括自己)；\n\n3 .选举线程收到回复后，验证是否是自己发起的询问(验证zxid是否一致)，然后获取对方的id(myid)，并存储到当前询问对象列表中，最后获取对方提议的leader相关信息(id,zxid)，并将这些信息存储到当次选举的投票记录表中；\n\n4.收到所有Server回复以后，就计算出zxid最大的那个Server，并将这个Server相关信息设置成**下一次要投票的Server**\n\n5.线程将当前zxid最大的Server设置为当前Server要推荐的Leader，如果此时获胜的Server获得n/2 + 1的Server票数， 设置当前推荐的leader为获胜的Server，将根据获胜的Server相关信息设置自己的状态，否则，继续这个过程，直到leader被选举出来。\n\n通过流程分析我们可以得出：要使Leader获得多数Server的支持，则Server总数必须是奇数2n+1，且存活的Server的数目不得少于n+1。\n\nfast paxos流程是在选举过程中，某Server首先向所有Server提议自己要成为leader，当其它Server收到提议以后，解决epoch和zxid的冲突，并接受对方的提议，然后向对方发送接受提议完成的消息，重复这个流程，最后一定能选举出Leader。\n\n[![F7Yn4P.png](https://s2.ax1x.com/2019/01/05/F7Yn4P.png)](https://s2.ax1x.com/2019/01/05/F7Yn4P.png)\n\n### 原子广播\n\n所有的**写操作**请求被传送给领导者，并通过**广播**将更新信息告诉跟随者。当大部分跟随者执行了修改之后，领导者就提交更新操作，客户端将得到更新成功的回应。未获得一致性的协议被设计为原子的，因此无论修改失败与否，他都分两阶段提交。\n\n[![F7YtNq.png](https://s2.ax1x.com/2019/01/05/F7YtNq.png)](https://s2.ax1x.com/2019/01/05/F7YtNq.png)\n\n如果领导者出故障了，剩下的机器将会再次进行领导者选举，并在新领导被选出前继续执行任务。如果在不久后老的领导者恢复了，那么它将以跟随者的身份继续运行。领导者选举非常快，由发布的结果所知，大约是200毫秒，因此在选举时性能不会明显减慢。\n\n所有在ensemble中的机器在更新他们内存中的Znode树之前会将更新信息写入磁盘。读操作请求可由任何机器服务，他们只设计内存查找，因此非常快。\n\n1. leader等待server连接；\n2. Follower连接leader，将最大的zxid发送给leader；\n3. Leader根据follower的zxid确定同步点；\n4. 完成同步后通知follower 已经成为uptodate状态；\n5. Follower收到uptodate消息后，又可以重新接受client的请求进行服务了。\n\n[![F7Yb5t.png](https://s2.ax1x.com/2019/01/05/F7Yb5t.png)](https://s2.ax1x.com/2019/01/05/F7Yb5t.png)\n\n每一个Znode树的更新都会给定一个唯一的全局标识，叫zxid（表示ZooKeeper事务“ID”）。更新是被排序的，因此如果zxid的z1<z2，那么z1就比z2先执行。对于ZooKeeper来说，这是分布式系统中排序的唯一标准。\n\n所有在ensemble中的机器在更新它们内存中的Znode树之前会先**将更新信息写入磁盘**。读操作请求可由任何机器服务，同时，由于他们只涉及内存查找，因此非常快。\n\n### Leader工作流程\n\nLeader主要有三个功能：\n\n1 .**恢复数据**；\n\n2 .维持与Learner的心跳，接收Learner请求并判断Learner的请求消息类型；\n\n3 .Learner的消息类型主要有PING消息、REQUEST消息、ACK消息、REVALIDATE消息，根据不同的消息类型，进行不同的处理。\n\nPING消息是指Learner的心跳信息；\n\nREQUEST消息是Follower发送的提议信息，包括写请求及同步请求；\n\nACK消息是Follower的对提议的回复，超过半数的Follower通过，则commit该提议；\n\nREVALIDATE消息是用来延长SESSION有效时间。\n\n[![F7tkGV.png](https://s2.ax1x.com/2019/01/05/F7tkGV.png)](https://s2.ax1x.com/2019/01/05/F7tkGV.png)\n\n### Follower工作流程\n\nFollower主要有四个功能：\n\n1. 向Leader发送请求（PING消息、REQUEST消息、ACK消息、REVALIDATE消息）；\n2. 接收Leader消息并进行处理；\n3. 接收Client的请求，如果为写请求，发送给Leader进行投票；\n4. 返回Client结果。\n\nFollower的消息循环处理如下几种来自Leader的消息：\n\n1 .PING消息： 心跳消息；\n\n2 .PROPOSAL消息：Leader发起的提案，要求Follower投票；\n\n3 .COMMIT消息：服务器端最新一次提案的信息；\n\n4 .UPTODATE消息：表明同步完成；\n\n5 .REVALIDATE消息：根据Leader的REVALIDATE结果，关闭待revalidate的session还是允许其接受消息；\n\n6 .SYNC消息：返回SYNC结果到客户端，这个消息最初由客户端发起，用来强制得到最新的更新。\n\nFollower的工作流程简图如下所示，在实际实现中，Follower是通过5个线程来实现功能的。\n\n[![F7tuZ9.png](https://s2.ax1x.com/2019/01/05/F7tuZ9.png)](https://s2.ax1x.com/2019/01/05/F7tuZ9.png)\n\n## ZooKeeper一致性\n\n在ensemble中的领导者和跟随着非常灵活，跟随者通过更新号来滞后领导者11，结果导致了只要大部分而不是所有的ensemble中的元素确认更新，就能被提交了。对于ZooKeeper来说，一个较好的智能模式是将客户端连接到跟着领导者的ZooKeeper服务器上。客户端可能被连接到领导者上，但他不能控制它，而且在如下情况时，甚至可能不知道。参见下图：\n\n[![F708zt.png](https://s2.ax1x.com/2019/01/05/F708zt.png)](https://s2.ax1x.com/2019/01/05/F708zt.png)\n\n每一个Znode树的更新都会给定一个唯一的全局标识，叫zxid（表示ZooKeeper事务“ID”）。更新是被排序的，因此如果zxid的z1<z2，那么z1就比z2先执行。对于ZooKeeper来说，这是分布式系统中排序的唯一标准。\n\nZooKeeper是一种高性能、可扩展的服务。ZooKeeper的读写速度非常快，并且读的速度要比写快。另外，在进行读操作的时候，ZooKeeper依然能够为旧的数据提供服务。这些都是由ZooKeeper所提供的一致性保证的，它具有如下特点：\n\n（1）顺序一致性\n\n任何一个客户端的更新都按他们发送的顺序排序，也就意味着如果一个客户端将Znode z的值更新为值a，那么在之后的操作中，他会将z更新为b，在客户端发现z带有值b之后，就不会再看见带有值a的z。\n\n（2）原子性\n\n更新不成功就失败，这意味着如果更新失败了，没有客户端会知道。☆☆\n\n（3）单系统映像\n\n无论客户端连接的是哪台服务器，他与系统看见的视图一样。这就意味着，如果一个客户端在相同的会话时连接了一台新的服务器，他将不会再看见比在之前服务器上看见的更老的系统状态，当服务器系统出故障，同时客户端尝试连接ensemble中的其他机器时，故障服务器的后面那台机器将不会接受连接，直到它连接到故障服务器。\n\n（4）容错性\n\n一旦更新成功后，那么在客户端再次更新他之前，他就固定了，将不再被修改，这就会保证产生下面两种结果：\n\n如果客户端成功的获得了正确的返回代码，那么说明更新已经成功。如果不能够获得返回代码（由于通信错误、超时等原因），那么客户端将不知道更新是否生效。\n\n当故障恢复的时候，任何客户端能够看到的执行成功的更新操作将不会回滚。\n\n（5）实时性\n\n在任何客户端的系统视图上的的时间间隔是有限的，因此他在超过几十秒的时间内部会过期。这就意味着，服务器不会让客户端看一些过时的数据，而是关闭，强制客户端转到一个更新的服务器上。\n\n解释一下：\n\n由于性能原因，读操作由ZooKeeper服务器的内存提供，而且不参与写操作的全局排序。这一特性可能会导致来自使用ZooKeeper外部机制交流的客户端与ZooKeeper状态的不一致。举例来说，客户端A将Znode z的值a更新为a’，A让B来读z，B读到z的值是a而不是a’。这与ZooKeeper的保证机制是相容的（不允许的情况较作“同步一致的交叉客户端视 图”）。为了避免这种情况的发生，B在读取z的值之前，应该先调用z上的sync。Sync操作强制B连接上的ZooKeeper服务器与leader保 持一致这样，当B读到z的值时，他将成为A设置的值（或是之后的值）\n\n容易混淆的是：\n\nsync操作只能被异步调用12。这样操作的原因是你不需要等待他的返回，因为ZooKeeper保证了任何接下去的操作将会发生在sync在服务器上执行以后，即使操作是在sync完成前被调用的。\n\n这些已执行的保证后，ZooKeeper更高级功能的设计与实现将会变得非常容易，例如：leader选举、队列，以及可撤销锁等机制的实现。\n\n## 会话\n\nZooKeeper**客户端**与ensemble中的服务器列表配置一致，在启动时，他尝试与表中的一个服务器相连接。如果连接失败了，他就尝试表中的其他服务器，以此类推，知道他最终连接到其中一个，或者ZooKeeper的所有服务器都无法获得时，连接失败。\n一旦与ZooKeeper服务器连接成功，服务器会创建与客户端的一个新的对话。每个回话都有**超时时段**，这是应用程序在创建它时设定的。如果服务器没有在超时时段内得到请求，他可能会中断这个会话。一旦会话被中断了，他可能不再被打开，而且任何与会话相连接的临时节点都将丢失。\n无论什么时候会话持续空闲长达一定时间，都会由客户端发送ping请求保持活跃（犹如心跳）。时间段要足够小以监测服务器故障（由读操作超时反应），并且能再回话超市时间段内重新连接到另一个服务器。\n在ZooKeeper中有几个time参数。**tick time**是ZooKeeper中的基本时间长度，为ensemble里的服务器所使用，用来定义对于交互运行的调度。其他设置以tick time的名义定义，或者至少由它来约束。\n创建更复杂的临时性状态的应用程序应该支持更长的会话超时，因为重新构建的代价会更昂贵。在一些情况下，我们可以让应用程序在一定会话时间内能够重启，并且避免会话过期。（这可能更适合执行维护或是升级）每个会话都由服务器给定一个唯一的身份和密码，而且如果是在建立连接时被传递给ZooKeeper的话，只要没有过期它能够恢复会话。\n这些特性可以视为一种可以避免会话过期的优化，但它并不能代替用来处理会话过期。会话过期可能出现在机器突然故障时，或是由于任何原因导致的应用程序安全关闭了，但在会话中断前没有重启。\n\nZookeeper对象的转变是通过生命周期中的不同状态来实现的。可以使用getState()方法在任何时候查询他的状态：\n\npublic states getSate()\n\nZookeeper状态事务：\n\n[![F70yQ0.png](https://s2.ax1x.com/2019/01/05/F70yQ0.png)](https://s2.ax1x.com/2019/01/05/F70yQ0.png)\n\ngetState()方法的返回类型是states，states是枚举类型代表Zookeeper对象可能所处的不同状态，一个Zookeeper实例可能一次只处于一个状态。一个新建的Zookeeper实例正在于Zookeeper服务器建立连接时，是处于**CONNECTING**状态的。一旦连接建立好以后，他就变成了**Connected**状态。\n使用Zookeeper的客户端可以通过注册Watcher的方法来获取状态转变的消息。一旦进入了CONNNECTED状态，Watcher将获得一个KeepState值为SyncConnected的WatchedEvent。\n\n注意Zookeeper的状态改变。传递给Zookeeper对象构造函数的(默认)watcher，被用来检测状态的改变。\n\n1. 了解Zookeeper的状态改变。传递给Zookeeper对象构造函数的(默认)watcher，被用来检测状态的改变。\n2. 了解Znode的改变。检测Znode的改变既可以使用专门的实例设置到读操说，也可以使用读操作默认的watcher。\n\nZookeeper实例可能时区或重新连接Zookeeper服务，在CONNETCTED和CONNECTING状态中切换。如果连接段考，watcher得到一个Disconnected事件，需要注意的是这些状态的迁移是由Z哦哦keeper实例自己发起的，如果连接断开他将自动尝试自动连接。\n\n如果任何一个close()方法调用，或者会话由Expired了的KeepState提示过期时,Zookeeper可能会装变成第三种状态CLOSED状态。一旦处于CLOSE状态,Zookeeper的对象将不再是活动的了(可以使用states的isActive()方法进行测试,而且不能被重用。客户端必须建立一个新的Zookeeper实例才能连接到Zookeeper服务。\n\n## 应用场景\n\nZookeeper是一个高可用的分布式数据管理与系统协调框架。基于Paxos算法实现的，使该框架保证了分布式环境中数据的强一致性，也是基于这样的特性，使得Zookeeper解决了很多分布式问题。\n\n### 数据发布与订阅(配置中心)\n\n　数据发布/订阅系统，即配置中心。需要发布者将数据发布到Zookeeper的节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。发布/订阅一般有两种设计模式：推模式和拉模式，服务端主动将数据更新发送给所有订阅的客户端称为推模式；客户端主动请求获取最新数据称为拉模式，Zookeeper采用了推拉相结合的模式，客户端向服务端注册自己需要关注的节点，一旦该节点数据发生变更，那么服务端就会向相应的客户端推送Watcher事件通知，客户端接收到此通知后，主动到服务端获取最新的数据。\n\n　　若将配置信息存放到Zookeeper上进行集中管理，在通常情况下，应用在启动时会主动到Zookeeper服务端上进行一次配置信息的获取，同时，在指定节点上注册一个Watcher监听，这样在配置信息发生变更，服务端都会实时通知所有订阅的客户端，从而达到实时获取最新配置的目的。\n\n 分布式搜索服务中，索引的元数据信息和服务器集群机器的节点状态放在ZK的一些指定系欸但，共各个客户端订阅使用。\n\n 分布式日志收集系统。这个系统的核心工作使收集分布在不同机器的日志。收集器通常使按照应用来分配收集任务端元，因此需要在ZK上创建一个以应用名位path的节点P，并将这个应用的所有机器ip，以子节点的形式注册到节点P上，这样一来能够实现机器变动的时候，能够实时通知到收集器调整任务分配。\n\n系统中有些信息需要动态获取，并且还会存在人工手动去修改这个信息的发问。通常是暴露出接口，例如JMX接口，来获取一些运行时的信息。引入ZK之后，就不用自己实现一套方案了，只要将这些信息存放到指定的ZK节点上即可。\n\n[![F7rBJH.png](https://s2.ax1x.com/2019/01/05/F7rBJH.png)](https://s2.ax1x.com/2019/01/05/F7rBJH.png)\n\n### 负载均衡\n\n　　负载均衡是一种相当常见的计算机网络技术，用来对多个计算机、网络连接、CPU、磁盘驱动或其他资源进行分配负载，以达到优化资源使用、最大化吞吐率、最小化响应时间和避免过载的目的。\n\n　　使用Zookeeper实现动态DNS服务\n\n- 域名配置：首先在Zookeeper上创建一个节点来进行域名配置，如DDNS/app1/server.app1.company1.com。\n- 域名解析：**应用首先从域名节点中获取IP地址和端口的配置，进行自行解析。同时，应用程序还会在域名节点上注册一个数据变更Watcher监听，以便及时收到域名变更的通知。**\n- 域名变更：若发生IP或端口号变更，此时需要进行域名变更操作，此时，只需要对指定的域名节点进行更新操作，Zookeeper就会向订阅的客户端发送这个事件通知，客户端之后就再次进行域名配置的获取。\n\n### 命名服务\n\n　命名服务是分步实现系统中较为常见的一类场景，分布式系统中，被命名的实体通常可以是集群中的机器、提供的服务地址或远程对象等，通过命名服务，客户端可以根据指定名字来获取资源的实体、服务地址和提供者的信息。Zookeeper也可帮助应用系统通过资源引用的方式来实现对资源的定位和使用，广义上的命名服务的资源定位都不是真正意义上的实体资源，在分布式环境中，上层应用仅仅需要一个全局唯一的名字。Zookeeper可以实现一套分布式全局唯一ID的分配机制。\n\n阿里巴巴集团开源的分布式服务框架Dubbo中使用Zookeeper为命名服务，维护全局的地址服务列表在Dubbo实现中：\n\n服务提供自己的提供者在启动的时候，向ZK上指定节点/dubbo/${serviceName}/providers目录下写入自己的URL地址，这个操作就完成了服务的发布。\n\n服务消费者启动的时候订阅/dubbo/serviceName/providers目录下提供者URL地址，并向/dubbo/serviceName/providers目录下提供者URL地址，并向/dubbo/serviceName}/consumers写下自己的URL地址。\n\n**注意：所有向ZK上注册的地址都是临时节点，这样就能够保证服务提供者和消费者能过够自动感应资源的拜年话。另外，Dubbo还针对服务粒度的监控，方法是订阅/dubbo/${serviceName}目录下所有提供者和消费者信息**\n\n[![F7skTO.png](https://s2.ax1x.com/2019/01/05/F7skTO.png)](https://s2.ax1x.com/2019/01/05/F7skTO.png)\n\n### 分布式通知/协调\n\n　　Zookeeper中特有的Watcher注册于异步通知机制，能够很好地实现分布式环境下不同机器，甚至不同系统之间的协调与通知，从而实现对数据变更的实时处理。通常的做法是不同的客户端都对Zookeeper上的同一个数据节点进行Watcher注册，监听数据节点的变化（包括节点本身和子节点），若数据节点发生变化，那么所有订阅的客户端都能够接收到相应的Watcher通知，并作出相应处理。\n\n　在绝大多数分布式系统中，系统机器间的通信无外乎**心跳检测**、**工作进度汇报**和**系统调度**。\n\n　　① **心跳检测**，不同机器间需要检测到彼此是否在正常运行，可以使用Zookeeper实现机器间的心跳检测，基于其临时节点特性（临时节点的生存周期是客户端会话，客户端若当即后，其临时节点自然不再存在），可以让不同机器都在Zookeeper的一个指定节点下创建临时子节点，不同的机器之间可以根据这个临时子节点来判断对应的客户端机器是否存活。通过Zookeeper可以大大减少系统耦合。\n\n　　② **工作进度汇报**，通常任务被分发到不同机器后，需要实时地将自己的任务执行进度汇报给分发系统，可以在Zookeeper上选择一个节点，每个任务客户端都在这个节点下面创建临时子节点，这样不仅可以判断机器是否存活，同时各个机器可以将自己的任务执行进度写到该临时节点中去，以便中心系统能够实时获取任务的执行进度。\n\n　　③ **系统调度**，Zookeeper能够实现如下系统调度模式：分布式系统由控制台和一些客户端系统两部分构成，控制台的职责就是需要将一些指令信息发送给所有的客户端，以控制他们进行相应的业务逻辑，后台管理人员在控制台上做一些操作，实际上就是修改Zookeeper上某些节点的数据，Zookeeper可以把数据变更以时间通知的形式发送给订阅客户端。\n\n### 集群管理\n\n　　Zookeeper的两大特性：\n\n　　**· 客户端如果对Zookeeper的数据节点注册Watcher监听，那么当该数据及诶单内容或是其子节点列表发生变更时，Zookeeper服务器就会向订阅的客户端发送变更通知。**\n\n　　**· 对在Zookeeper上创建的临时节点，一旦客户端与服务器之间的会话失效，那么临时节点也会被自动删除。**\n\n　　利用其两大特性，可以实现集群机器存活监控系统，若监控系统在/clusterServers节点上注册一个Watcher监听，那么但凡进行动态添加机器的操作，就会在/clusterServers节点下创建一个临时节点：/clusterServers/[Hostname]，这样，监控系统就能够实时监测机器的变动情况。下面通过分布式日志收集系统的典型应用来学习Zookeeper如何实现集群管理。\n\n　　分布式日志收集系统的核心工作就是收集分布在不同机器上的系统日志，在典型的日志系统架构设计中，整个日志系统会把所有需要收集的日志机器分为多个组别，每个组别对应一个收集器，这个收集器其实就是一个后台机器，用于收集日志，对于大规模的分布式日志收集系统场景，通常需要解决两个问题：\n\n　　**· 变化的日志源机器**\n\n　　**· 变化的收集器机器**\n\n　　无论是日志源机器还是收集器机器的变更，最终都可以归结为如何快速、合理、动态地为每个收集器分配对应的日志源机器。使用Zookeeper的场景步骤如下\n\n　　① **注册收集器机器**，在Zookeeper上创建一个节点作为收集器的根节点，例如/logs/collector的收集器节点，每个收集器机器启动时都会在收集器节点下创建自己的节点，如/logs/collector/[Hostname]\n\n[![img](https://images2015.cnblogs.com/blog/616953/201611/616953-20161112085605967-1265109617.png)](https://images2015.cnblogs.com/blog/616953/201611/616953-20161112085605967-1265109617.png)\n\n　　② **任务分发**，所有收集器机器都创建完对应节点后，系统根据收集器节点下子节点的个数，将所有日志源机器分成对应的若干组，然后将分组后的机器列表分别写到这些收集器机器创建的子节点，如/logs/collector/host1上去。这样，收集器机器就能够根据自己对应的收集器节点上获取日志源机器列表，进而开始进行日志收集工作。\n\n　　③ **状态汇报**，完成任务分发后，机器随时会宕机，所以需要有一个收集器的状态汇报机制，每个收集器机器上创建完节点后，还需要再对应子节点上创建一个状态子节点，如/logs/collector/host/status，每个收集器机器都需要定期向该结点写入自己的状态信息，这可看做是心跳检测机制，通常收集器机器都会写入日志收集状态信息，日志系统通过判断状态子节点最后的更新时间来确定收集器机器是否存活。\n\n　　④ **动态分配**，若收集器机器宕机，则需要动态进行收集任务的分配，收集系统运行过程中关注/logs/collector节点下所有子节点的变更，一旦有机器停止汇报或有新机器加入，就开始进行任务的重新分配，此时通常由两种做法：\n\n　　**· 全局动态分配**，当收集器机器宕机或有新的机器加入，系统根据新的收集器机器列表，立即对所有的日志源机器重新进行一次分组，然后将其分配给剩下的收集器机器。\n\n　　**· 局部动态分配**，每个收集器机器在汇报自己日志收集状态的同时，也会把自己的负载汇报上去，如果一个机器宕机了，那么日志系统就会把之前分配给这个机器的任务重新分配到那些负载较低的机器，同样，如果有新机器加入，会从那些负载高的机器上转移一部分任务给新机器。\n\n　　上述步骤已经完整的说明了整个日志收集系统的工作流程，其中有两点注意事项。\n\n　　①**节点类型**，在/logs/collector节点下创建临时节点可以很好的判断机器是否存活，但是，若机器挂了，其节点会被删除，记录在节点上的日志源机器列表也被清除，所以需要选择持久节点来标识每一台机器，同时在节点下分别创建/logs/collector/[Hostname]/status节点来表征每一个收集器机器的状态，这样，既能实现对所有机器的监控，同时机器挂掉后，依然能够将分配任务还原。\n\n　　② **日志系统节点监听**，若采用Watcher机制，那么通知的消息量的网络开销非常大，需要采用日志系统主动轮询收集器节点的策略，这样可以节省网络流量，但是存在一定的延时。\n\n　　2.6 Master选举\n\n　　在分布式系统中，Master往往用来协调集群中其他系统单元，具有对分布式系统状态变更的决定权，如在读写分离的应用场景中，客户端的写请求往往是由Master来处理，或者其常常处理一些复杂的逻辑并将处理结果同步给其他系统单元。利用Zookeeper的强一致性，能够很好地保证在分布式高并发情况下节点的创建一定能够保证全局唯一性，即Zookeeper将会保证客户端无法重复创建一个已经存在的数据节点。\n\n　　首先创建/master_election/2016-11-12节点，客户端集群每天会定时往该节点下创建临时节点，如/master_election/2016-11-12/binding，这个过程中，只有一个客户端能够成功创建，此时其变成master，其他节点都会在节点/master_election/2016-11-12上注册一个子节点变更的Watcher，用于监控当前的Master机器是否存活，一旦发现当前Master挂了，其余客户端将会重新进行Master选举。\n\n[![img](https://images2015.cnblogs.com/blog/616953/201611/616953-20161112093212483-1690012026.png)](https://images2015.cnblogs.com/blog/616953/201611/616953-20161112093212483-1690012026.png)\n\n### 分布式锁\n\n在Zookeeper，分布式锁使全部同步的，在同一时刻不会由相同的客户客户端认为他们持有相同的锁：\n\n1. Zookeeper调用create()方法来床架哪一个路径格式位“_locknode_/lock”的节点，此节点类型为sequence(连续)和ephemeral(临时)。创建的节点位临时节点，并且所有的节点连续编号，即为“lock-i”的格式。\n2. 在创建锁节点上调用getChildren()方法，以获取锁目录下最小编号节点，并且不设置watch；\n3. 步骤2获取的节点桥好事步骤1客户端创建的节点，客户端会获取该种类型的锁，然后退出操作。\n4. 客户端在锁目录上调用exists()方法，并且设置watch来监视锁目录下序号对应自己次小的连续临时节点的状态。\n5. 如果监视节点状态发生变化，则跳转到步骤2，继续进行后续的操作，直至退出锁竞争。\n\nZookeeper的解锁非常简单，客户端只需要将加索步骤1中创建的临时节点和删除即可。\n\n[![F76kJe.png](https://s2.ax1x.com/2019/01/05/F76kJe.png)](https://s2.ax1x.com/2019/01/05/F76kJe.png)\n\n## 参考博客\n\n<https://www.cnblogs.com/leesf456/p/6036548.html>\n\n<https://blog.csdn.net/kingcat666/article/details/77749547>","tags":["Zookeeper"],"categories":["大数据"]},{"title":"HBase优化","url":"/2018/12/31/HBase优化/","content":"\n {{ \"Hbase 高可用、预先分区、布隆过滤器\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 高可用\n\nHBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。\n\n### 步骤\n\n关闭HBase集群（如果没有开启则跳过此步）\n\n```\nstop-hbase.sh\n```\n\n在conf目录下创建backup-masters文件\n\n```\ntouch conf/backup-masters\n```\n\n在backup-masters文件中配置高可用HMaster节点\n\n```\necho datanode2 > conf/backup-masters\n```\n\n将整个conf目录scp到其他节点\n\n```\nscp -r conf/ datanode1:/opt/module/hbase/    \nscp -r conf/ datanode3:/opt/module/hbase/\n```\n\n### Web界面\n\n[![FTMBfH.png](https://s2.ax1x.com/2019/01/04/FTMBfH.png)](https://s2.ax1x.com/2019/01/04/FTMBfH.png)\n\n## 预分区\n\n每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。\n\n### 手动设定预分区\n\n```\ncreate 'staff1','info','partition1',SPLITS => ['1000','2000','3000','4000']\n```\n\n### 生成16进制序列预分区\n\n```\ncreate 'staff2','info','partition2',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n```\n\n### 按照文件中设置的规则预分区\n\n```\n[hadoop@datanode1 hbase]$ vim splits.txt  #在hbase创建该文件\naaaa\nbbbb\ncccc\ndddd\ncreate 'staff2','info','partition2',{NUMREGIONS => 15, SPLITALGO => 'HexStringSplit'}\n```\n\n### 使用JavaAPI创建预分区\n\n```\n//自定义算法，产生一系列Hash散列值存储在二维数组中\nbyte[][] splitKeys = 某个散列值函数\n//创建HBaseAdmin实例\nHBaseAdmin hAdmin = new HBaseAdmin(HBaseConfiguration.create());\n//创建HTableDescriptor实例\nHTableDescriptor tableDesc = new HTableDescriptor(tableName);\n//通过HTableDescriptor实例和散列值二维数组创建带有预分区的HBase表\nhAdmin.createTable(tableDesc, splitKeys);\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.hbase.HBaseConfiguration;\nimport org.apache.hadoop.hbase.HColumnDescriptor;\nimport org.apache.hadoop.hbase.HTableDescriptor;\nimport org.apache.hadoop.hbase.TableName;\nimport org.apache.hadoop.hbase.client.Admin;\nimport org.apache.hadoop.hbase.client.Connection;\nimport org.apache.hadoop.hbase.client.ConnectionFactory;\nimport org.apache.hadoop.hbase.util.Bytes;\n \npublic class create_table_sample {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = HBaseConfiguration.create();\n        conf.set(\"hbase.zookeeper.quorum\", \"192.168.1.101,192.168.1.102,192.168.1.103\");\n        Connection connection = ConnectionFactory.createConnection(conf);\n        Admin admin = connection.getAdmin();\n \n        TableName table_name = TableName.valueOf(\"Java_Table\");\n        if (admin.tableExists(table_name)) {\n            admin.disableTable(table_name);\n            admin.deleteTable(table_name);\n        }\n \n        HTableDescriptor desc = new HTableDescriptor(table_name);\n        HColumnDescriptor family1 = \n           new HColumnDescriptor(Bytes.toBytes(\"info\"));\n        family1.setTimeToLive(3 * 60 * 60 * 24);     //过期时间\n        family1.setMaxVersions(3);                   //版本数\n        desc.addFamily(family1);\n \n        byte[][] splitKeys = {\n            Bytes.toBytes(\"row01\"),\n            Bytes.toBytes(\"row02\"),\n        };\n \n        admin.createTable(desc, splitKeys);\n        admin.close();\n        connection.close();\n        System.out.println(\"创建成功!!!\");\n    }\n}\n```\n\n## RowKey设计\n\n一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜。接下来我们就谈一谈rowkey常用的设计方案。\n\n### 生成随机数、hash、散列值\n\n原本rowKey为1001的，SHA1后变成：dd01903921ea24941c26a48f2cec24e0bb0e8cc7\n\n原本rowKey为3001的，SHA1后变成：49042c54de64a1e9bf0b33e00245660ef92dc7bd\n\n原本rowKey为5001的，SHA1后变成：7b61dec07e02c188790670af43e717f0f46e8913\n\n在做此操作之前，一般我们会选择从数据集中抽取样本，来决定什么样的rowKey来Hash后作为每个分区的临界值。\n\n### 字符串反转\n\n20170524000001转成10000042507102\n\n20170524000002转成20000042507102\n\n常见的是将URL反转比如 www.baidu.com 转成 com.baidu.wwww\n\n### 字符串拼接\n\n20170524000001_a12e\n\n20170524000001_93i7\n\n主要是为了防止数据倾斜比如电话号码作为Rowkey的时候是有规律可循的，在进行MapReduce作业的时候会导致数据倾斜。\n\n## 内存优化\n\nHBase操作过程中需要大量的内存开销，Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。\n\n## 基础优化\n\n- 允许在HDFS的文件中追加内容\n\nhdfs-site.xml、hbase-site.xml\n\n属性：dfs.support.append 解释：开启HDFS追加同步，可以优秀的配合HBase的数据同步和持久化。默认值为true。\n\n- 优化DataNode允许的最大文件打开数\n\nhdfs-site.xml\n\n属性：dfs.datanode.max.transfer.threads 解释：HBase一般都会同一时间操作大量的文件，根据集群的数量和规模以及数据动作，设置为4096或者更高。默认值：4096\n\n- 优化延迟高的数据操作的等待时间\n\nhdfs-site.xml\n\n属性：dfs.image.transfer.timeout 解释：如果对于某一次数据操作来讲，延迟非常高，socket需要等待更长的时间，建议把该值设置为更大的值（默认60000毫秒），以确保socket不会被timeout掉。\n\n- 优化数据的写入效率\n\nmapred-site.xml\n\n属性： mapreduce.map.output.compress mapreduce.map.output.compress.codec 解释：开启这两个数据可以大大提高文件的写入效率，减少写入时间。第一个属性值修改为true，第二个属性值修改为：org.apache.hadoop.io.compress.GzipCodec或者其他压缩方式。\n\n- 优化DataNode存储\n\n    属性：dfs.datanode.failed.volumes.tolerated 解释： 默认为0，意思是当DataNode中有一个磁盘出现故障，则会认为该DataNode shutdown了。如果修改为1，则一个磁盘出现故障时，数据会被复制到其他正常的DataNode上，当前的DataNode继续工作。\n\n- 设置RPC监听数量\n\nhbase-site.xml\n\n属性：hbase.regionserver.handler.count 解释：默认值为30，用于指定RPC监听的数量，可以根据客户端的请求数进行调整，读写请求较多时，增加此值。\n\n- 优化HStore文件大小\n\nhbase-site.xml\n\n属性：hbase.hregion.max.filesize 解释：默认值10737418240（10GB），如果需要运行HBase的MR任务，可以减小此值，因为一个region对应一个map任务，如果单个region过大，会导致map任务执行时间过长。该值的意思就是，如果HFile的大小达到这个数值，则这个region会被切分为两个Hfile。\n\n- 优化hbase客户端缓存\n\nhbase-site.xml\n\n属性：hbase.client.write.buffer 解释：用于指定HBase客户端缓存，增大该值可以减少RPC调用次数，但是会消耗更多内存，反之则反之。一般我们需要设定一定的缓存大小，以达到减少RPC次数的目的。\n\n- 指定scan.next扫描HBase所获取的行数\n\nhbase-site.xml\n\n属性：hbase.client.scanner.caching 解释：用于指定scan.next方法获取的默认行数，值越大，消耗内存越大。\n\n- flush、compact、split机制\n\n当MemStore达到阈值，将Memstore中的数据Flush进Storefile；compact机制则是把flush出来的小文件合并成大的Storefile文件。split则是当Region达到阈值，会把过大的Region一分为二。\n\n**涉及属性：**\n\n即：128M就是Memstore的默认阈值\n\nhbase.hregion.memstore.flush.size：134217728\n\n即：这个参数的作用是当单个HRegion内所有的Memstore大小总和超过指定值时，flush该HRegion的所有memstore。RegionServer的flush是通过将请求添加一个队列，模拟生产消费模型来异步处理的。那这里就有一个问题，当队列来不及消费，产生大量积压请求时，可能会导致内存陡增，最坏的情况是触发OOM。\n\nhbase.regionserver.global.memstore.upperLimit：0.4 hbase.regionserver.global.memstore.lowerLimit：0.38\n\n即：当MemStore使用内存总量达到hbase.regionserver.global.memstore.upperLimit指定值时，将会有多个MemStores flush到文件中，MemStore flush 顺序是按照大小降序执行的，直到刷新到MemStore使用内存略小于lowerLimit\n\n## 扩展\n\n### 商业项目中的能力\n\n1. 消息量：发送和接收的消息数超过60亿\n2. 将近1000亿条数据的读写\n3. 高峰期每秒150万左右操作\n4. 整体读取数据占有约55%，写入占有45%\n5. 超过2PB的数据，涉及冗余共6PB数据\n6. 数据每月大概增长300千兆字节。\n\n### 布隆过滤器\n\n在日常生活中，包括在设计计算机软件时，我们经常要判断一个元素是否在一个集合中。比如在字处理软件中，需要检查一个英语单词是否拼写正确（也就是要判断它是否在已知的字典中）；在 FBI，一个嫌疑人的名字是否已经在嫌疑名单上；在网络爬虫里，一个网址是否被访问过等等。最直接的方法就是将集合中全部的元素存在计算机中，遇到一个新元素时，将它和集合中的元素直接比较即可。**一般来讲，计算机中的集合是用哈希表（hash table）来存储的。它的好处是快速准确，缺点是费存储空间。** 当集合比较小时，这个问题不显著，但是当集合巨大时，哈希表存储效率低的问题就显现出来了。比如说，一个像 Yahoo,Hotmail 和 Gmai 那样的公众电子邮件（email）提供商，总是需要过滤来自发送垃圾邮件的人（spamer）的垃圾邮件。一个办法就是记录下那些发垃圾邮件的 email 地址。由于那些发送者不停地在注册新的地址，全世界少说也有几十亿个发垃圾邮件的地址，将他们都存起来则需要大量的网络服务器。如果用哈希表，每存储一亿个 email 地址， 就需要 1.6GB 的内存（用哈希表实现的具体办法是将每一个 email 地址对应成一个八字节的信息指纹googlechinablog.com/2006/08/blog-post.html，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email 地址需要占用十六个字节。一亿个地址大约要 1.6GB， 即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB 的内存。除非是超级计算机，一般服务器是无法存储的。\n\n **布隆过滤器只需要哈希表 1/8 到 1/4 的大小就能解决同样的问题。**\n\n**Bloom Filter是一种空间效率很高的随机数据结构，它利用位数组很简洁地表示一个集合，并能判断一个元素是否属于这个集合。**Bloom Filter的这种高效是有一定代价的：在判断一个元素是否属于某个集合时，有可能会把不属于这个集合的元素误认为属于这个集合（false positive）。因此，Bloom Filter不适合那些“零错误”的应用场合。而在能容忍低错误率的应用场合下，Bloom Filter通过极少的错误换取了存储空间的极大节省。\n\n下面我们具体来看Bloom Filter是如何用位数组表示集合的。初始状态时，Bloom Filter是一个包含m位的位数组，每一位都置为0，如图9-5\n\n[![FTDAyV.png](https://s2.ax1x.com/2019/01/04/FTDAyV.png)](https://s2.ax1x.com/2019/01/04/FTDAyV.png)\n\n为了表达S={x1, x2,…,xn}这样一个n个元素的集合，Bloom Filter使用k个相互独立的哈希函数（Hash Function），它们分别将集合中的每个元素映射到{1,…,m}的范围中。对任意一个元素x，第i个哈希函数映射的位置hi(x)就会被置为1（1≤i≤k）。注意，如果一个位置多次被置为1，那么只有第一次会起作用，后面几次将没有任何效果。如图9-6所示，k=3，且有两个哈希函数选中同一个位置（从左边数第五位）。\n\n[![FTDuFJ.png](https://s2.ax1x.com/2019/01/04/FTDuFJ.png)](https://s2.ax1x.com/2019/01/04/FTDuFJ.png)\n\n在判断y是否属于这个集合时，我们对y应用k次哈希函数，如果所有hi(y)的位置都是1（1≤i≤k），那么我们就认为y是集合中的元素，否则就认为y不是集合中的元素。如图9-7所示y1就不是集合中的元素。y2或者属于这个集合，或者刚好是一个false positive。\n\n[![FTDMWR.png](https://s2.ax1x.com/2019/01/04/FTDMWR.png)](https://s2.ax1x.com/2019/01/04/FTDMWR.png)\n\n为了add一个元素，用k个hash function将它hash得到bloom filter中k个bit位，将这k个bit位置1。\n\n为了query一个元素，即判断它是否在集合中，用k个hash function将它hash得到k个bit位。若这k bits全为1，则此元素在集合中；若其中任一位不为1，则此元素比不在集合中（因为如果在，则在add时已经把对应的k个bits位置为1）。\n\n· 不允许remove元素，因为那样的话会把相应的k个bits位置为0，而其中很有可能有其他元素对应的位。因此remove会引入false negative，这是绝对不被允许的。\n\n布隆过滤器决不会漏掉任何一个在黑名单中的可疑地址。但是，它有一条不足之处，也就是**它有极小的可能将一个不在黑名单中的电子邮件地址判定为在黑名单中，因为有可能某个好的邮件地址正巧对应一个八个都被设置成一的二进制位。好在这种可能性很小，我们把它称为误识概率。**\n\n布隆过滤器的好处在于快速，省空间，但是有一定的误识别率，常见的补救办法是在建立一个小的白名单，存储那些可能个别误判的邮件地址。\n\n布隆过滤器具体算法高级内容，如错误率估计，最优哈希函数个数计算，位数组大小计算，请参见<http://blog.csdn.net/jiaomeng/article/details/1495500>。","tags":["Hbase"],"categories":["大数据"]},{"title":"HBase的Shell命令和JavaAPI","url":"/2018/12/31/HBase的Shell命令和JavaAPI/","content":"\n {{ \"HBase的shell操作和JavaAPI的使用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n# Shell\n\n## 表操作\n\n### 创建表\n\n```shell\ncreate 'student','info'           #表名  列族\n```\n\n### 插入表\n\n```shell\nput 'student','1001','info:sex','male'\nput 'student','1001','info:age','18'\nput 'student','1002','info:name','Janna'\nput 'student','1002','info:sex','female'\nput 'student','1002','info:age','20'\n```\n\n### 查看表数据\n\n```shell\nscan 'student'\nscan 'student',{STARTROW => '1001', STOPROW  => '1002'}   #左闭右开\nscan 'student',{STARTROW => '1001'}\n```\n\n### 查看表结构\n\n```shell\ndescribe 'student'\ndesc 'student'             #效果相同\n```\n\n### 更新指定字段\n\n```shell\nput 'student','1001','info:name','Nick'\nput 'student','1001','info:age','100'  #  表明  rowkey  列族:列 值\n```\n\n### 查看指定行数据\n\n```shell\nget 'student','1001'\n```\n\n### 查看指定列族:列\n\n```shell\nget 'student','1001','info:name'\n```\n\n### 统计表行数\n\n```shell\ncount 'student'\n```\n\n### 删除数据\n\n```shell\ndeleteall 'student','1001'\n```\n\n### 删除rowkey的某一列\n\n```shell\ndelete 'student','1002','info:sex'\n```\n\n### 清空数据\n\n```shell\ntruncate 'student'\n#表的操作顺序为先disable，然后再truncate。\n```\n\n### 删除表\n\n```\ndisable 'student'\n```\n\n### 表更表信息\n\n```shell\nalter 'student',{NAME=>'info',VERSIONS=>3} ##将info列族中的数据存放3个版本：\n```\n\n# Java API\n\n### 环境准备\n\n```xml\n<dependency>\n    <groupId>org.apache.hbase</groupId>\n    <artifactId>hbase-server</artifactId>\n    <version>1.3.1</version>\n</dependency>\n\n<dependency>\n    <groupId>org.apache.hbase</groupId>\n    <artifactId>hbase-client</artifactId>\n    <version>1.3.1</version>\n</dependency>\n```\n\n## HBaseAPI\n\n### 获取Configuration对象\n\n```java\npublic static Configuration conf;\nstatic{\n\t//使用HBaseConfiguration的单例方法实例化\n\tconf = HBaseConfiguration.create();\nconf.set(\"hbase.zookeeper.quorum\", \"192.168.1.101\"); ## 换成自己的ZK节点IP\nconf.set(\"hbase.zookeeper.property.clientPort\", \"2181\");\n}\n```\n\n### 判断表是否存在\n\n```java\npublic static boolean isTableExist(String tableName) throws MasterNotRunningException,\n ZooKeeperConnectionException, IOException{\n//在HBase中管理、访问表需要先创建HBaseAdmin对象\n\tHBaseAdmin admin = new HBaseAdmin(conf);\n\treturn admin.tableExists(tableName);\n}\n```\n\n### 创建表\n\n```java\npublic static void createTable(String tableName, String... columnFamily) throws\n MasterNotRunningException, ZooKeeperConnectionException, IOException{\n\tHBaseAdmin admin = new HBaseAdmin(conf);\n\t//判断表是否存在\n\tif(isTableExist(tableName)){\n\t\tSystem.out.println(\"表\" + tableName + \"已存在\");\n\t\t//System.exit(0);\n\t}else{\n\t\t//创建表属性对象,表名需要转字节\n\t\tHTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName));\n\t\t//创建多个列族\n\t\tfor(String cf : columnFamily){\n\t\t\tdescriptor.addFamily(new HColumnDescriptor(cf));\n\t\t}\n\t\t//根据对表的配置，创建表\n\t\tadmin.createTable(descriptor);\n\t\tSystem.out.println(\"表\" + tableName + \"创建成功！\");\n\t}\n}\n```\n\n### 删除表\n\n```java\npublic static void dropTable(String tableName) throws MasterNotRunningException,\n ZooKeeperConnectionException, IOException{\n\tHBaseAdmin admin = new HBaseAdmin(conf);\n\tif(isTableExist(tableName)){\n\t\tadmin.disableTable(tableName);\n\t\tadmin.deleteTable(tableName);\n\t\tSystem.out.println(\"表\" + tableName + \"删除成功！\");\n\t}else{\n\t\tSystem.out.println(\"表\" + tableName + \"不存在！\");\n\t}\n}\n```\n\n### 插入数据\n\n```java\npublic static void addRowData(String tableName, String rowKey, String columnFamily, String\n column, String value) throws IOException{\n\t//创建HTable对象\n\tHTable hTable = new HTable(conf, tableName);\n\t//向表中插入数据\n\tPut put = new Put(Bytes.toBytes(rowKey));\n\t//向Put对象中组装数据\n\tput.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value));\n\thTable.put(put);\n\thTable.close();\n\tSystem.out.println(\"插入数据成功\");\n}\n```\n\n### 删除多行数据\n\n```java\npublic static void deleteMultiRow(String tableName, String... rows) throws IOException{\n\tHTable hTable = new HTable(conf, tableName);\n\tList<Delete> deleteList = new ArrayList<Delete>();\n\tfor(String row : rows){\n\t\tDelete delete = new Delete(Bytes.toBytes(row));\n\t\tdeleteList.add(delete);\n\t}\n\thTable.delete(deleteList);\n\thTable.close();\n}\n```\n\n### 获取所有数据\n\n```java\npublic static void getAllRows(String tableName) throws IOException{\n\tHTable hTable = new HTable(conf, tableName);\n\t//得到用于扫描region的对象\n\tScan scan = new Scan();\n\t//使用HTable得到resultcanner实现类的对象\n\tResultScanner resultScanner = hTable.getScanner(scan);\n\tfor(Result result : resultScanner){\n\t\tCell[] cells = result.rawCells();\n\t\tfor(Cell cell : cells){\n\t\t\t//得到rowkey\n\t\t\tSystem.out.println(\"行键:\" + Bytes.toString(CellUtil.cloneRow(cell)));\n\t\t\t//得到列族\n\t\t\tSystem.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell)));\n\t\t\tSystem.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell)));\n\t\t\tSystem.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell)));\n\t\t}\n\t}\n}\n```\n\n### 获取某一行数据\n\n```java\npublic static void getRow(String tableName, String rowKey) throws IOException{\n\tHTable table = new HTable(conf, tableName);\n\tGet get = new Get(Bytes.toBytes(rowKey));\n\t//get.setMaxVersions();显示所有版本\n    //get.setTimeStamp();显示指定时间戳的版本\n\tResult result = table.get(get);\n\tfor(Cell cell : result.rawCells()){\n\t\tSystem.out.println(\"行键:\" + Bytes.toString(result.getRow()));\n\t\tSystem.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell)));\n\t\tSystem.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell)));\n\t\tSystem.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell)));\n\t\tSystem.out.println(\"时间戳:\" + cell.getTimestamp());\n\t}\n}\n```\n\n### 获取某一行指定“列族:列”的数据\n\n```java\npublic static void getRowQualifier(String tableName, String rowKey, String family, String\n qualifier) throws IOException{\n\tHTable table = new HTable(conf, tableName);\n\tGet get = new Get(Bytes.toBytes(rowKey));\n\tget.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier));\n\tResult result = table.get(get);\n\tfor(Cell cell : result.rawCells()){\n\t\tSystem.out.println(\"行键:\" + Bytes.toString(result.getRow()));\n\t\tSystem.out.println(\"列族\" + Bytes.toString(CellUtil.cloneFamily(cell)));\n\t\tSystem.out.println(\"列:\" + Bytes.toString(CellUtil.cloneQualifier(cell)));\n\t\tSystem.out.println(\"值:\" + Bytes.toString(CellUtil.cloneValue(cell)));\n\t}\n}\n```\n\n## MapReduce\n\nHBase的相关JavaAPI，可以实现伴随HBase操作的MapReduce过程，使用MapReduce将数据从本地文件系统导入到HBase的表中或者从HBase中读取一些原始数据后使用MapReduce做数据分析。\n\n## Hive集成Hbase\n\n编译:hive-hbase-handler-1.2.2.jar\n\n```shell\nln -s $HBASE_HOME/lib/hbase-common-1.3.1.jar  $HIVE_HOME/lib/hbase-common-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-server-1.3.1.jar $HIVE_HOME/lib/hbase-server-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-client-1.3.1.jar $HIVE_HOME/lib/hbase-client-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-protocol-1.3.1.jar $HIVE_HOME/lib/hbase-protocol-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-it-1.3.1.jar $HIVE_HOME/lib/hbase-it-1.3.1.jar\nln -s $HBASE_HOME/lib/htrace-core-3.1.0-incubating.jar $HIVE_HOME/lib/htrace-core-3.1.0-incubating.jar\nln -s $HBASE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop2-compat-1.3.1.jar\nln -s $HBASE_HOME/lib/hbase-hadoop-compat-1.3.1.jar $HIVE_HOME/lib/hbase-hadoop-compat-1.3.1.jar\n```\n\nhive-site.xml中修改zookeeper的属性，如下：\n\n```xml\n<property>\n\t  <name>hive.zookeeper.quorum</name>\n \t <value>datanode1,datanode2,datanode3</value>\n \t <description>The list of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>\n</property>\n<property>\n \t <name>hive.zookeeper.client.port</name>\n \t <value>2181</value>\n  \t<description>The port of ZooKeeper servers to talk to. This is only needed for read/write locks.</description>\n</property>\n```\n\n### 案例\n\n创建hive关联表\n\n```sql\nCREATE TABLE hive_hbase_emp_table(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nSTORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\")\nTBLPROPERTIES (\"hbase.table.name\" = \"hbase_hive_emp_table_\");\n```\n\n在Hive中创建临时中间表，用于load文件中的数据\n\n```sql\nCREATE TABLE emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nrow format delimited fields terminated by ',';\n```\n\n向Hive中间表中load数据\n\n```shell\nload data local inpath '/home/hadoop/emp.csv' into table emp;\n```\n\n通过insert命令将中间表中的数据导入到Hive关联HBase的那张表中\n\n```sql\ninsert into table hive_hbase_emp_table select * from emp;\n```\n\n查看HIVE表\n\n```\nselect * from hive_hbase_emp_table;\n```\n\n查看HBase表\n\n```sql\nscan ‘hbase_emp_table’\n```\n\n在HBase中已经存储了某一张表hbase_emp_table，然后在Hive中创建一个外部表来关联HBase中的hbase_emp_table这张表，使之可以借助Hive来分析HBase这张表中的数据。\n\nHive创建表\n\n```sql\nCREATE EXTERNAL TABLE relevance_hbase_emp(\nempno int,\nename string,\njob string,\nmgr int,\nhiredate string,\nsal double,\ncomm double,\ndeptno int)\nSTORED BY \n'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\nWITH SERDEPROPERTIES (\"hbase.columns.mapping\" = \n\":key,info:ename,info:job,info:mgr,info:hiredate,info:sal,info:comm,info:deptno\") \nTBLPROPERTIES (\"hbase.table.name\" = \"hbase_emp_table\");\n```\n\n关联后就可以使用Hive函数进行一些分析操作了\n\n```sql\nselect * from relevance_hbase_emp;\n```","tags":["HBase"],"categories":["大数据"]},{"title":"HBase数据模型和读写原理","url":"/2018/12/30/HBase数据模型和读写原理/","content":"\n {{ \"Hbase的数据模型和读写原理\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n​\tHBase是一个开源可伸缩的分布式数据库,他根据Google Bigtable数据模型构建在hadoop的hdfs存储系统之上。\n\n​\tHBase是一个稀疏、多维度、排序的映射表。表的索引是行键、列族、列限定符和时间戳，一个列族中可以包含任意多个列，同一个列族里面数据存储在一起。同一张表的每行数据的列的值都可以为空，所以说HBase是稀疏的。\n\n​\tHbase在执行执行更新操作的时候，旧版本的数据不会被删除，新的版本的数据与旧版本的数据的区分以时间戳为准，数据在存储的时候，会按照时间戳排序，客户端可以根据时间戳排序。客户端可以根据时间戳获取不同时间的数据，如果在查询的时候不提供时间戳，那么会发挥距离现在最近的那一个版本的数据。HBase提供了两种数据版本回收方式：1.保留数据的最后n个版本，2.保存近7天的版本。\n\n## 数据模型\n\n​\tHBase是一个稀疏、多维度、排序的映射表，这张表的索引是行键、列族、列限定符和时间戳每个值是一个未经解释的字符串。\n\n表(Table)：一个HBase表由行和列组成，列划分为若干个 列族。\n\n行(Row)：在表里面，每一行代表着一个数据对象，每一行都是以一个行键(Row Key)来进行唯一标识，行键可以是任意字符串，在HBase内部，行键保存为字符数组，数据存储时，按照行键的字典序排列。\n\n列族(Column Family)：列族支持动态扩展，可以很轻松地添加一个列族或列，无需预先定义列的数量以及类型，所有列均以字符串形式存储，用户需要自行进行数据类型地转换。一个HBase表被分组成为许多“列族”的集合，它是基本的访问控制单元。\n\n列标识(Column Qualifier)：列族中的数据通过列标识符来进行定位，列标识也没有特定的数据类型，以二进制字节来存储。\n\n单元格(Cell)：每一个行键，列族和列标识符共同确定一个单元，存储在单元里的数据称为单元数据，单元和单元数据也没有的挺的数据类型，以二进制字节来存储。\n\n时间戳(Timestap)：默认每一个单元中的数据插入时都会用时间戳来进行版本标识。读取单元数据时，如果时间戳没有被指定，则默认返回最新你的数据，写入新的单元格数据时，如果没有设置时间戳，默认用当前时间，每一个列族的单元数据的版本数量都被HBase单独维护默认情况下HBase保留3个版本数据。\n\n## 逻辑模型\n\n​\t时候，你也可以把HBase看成一个多维度的Map模型去理解它的数据模型。正如下图，一个行键映射一个列族数组，列族数组中的每个列族又映射一个列标识数组，列标识数组中的每一个列标识(Column Qualifier)又映射到一个时间戳数组，里面是不同时间戳映射下不同版本的值，但是默认取最近时间的值，所以可以看成是列标识(Column Qualifier)和它所对应的值的映射。用户也可以通过HBase的API去同时获取到多个版本的单元数据的值。Row Key在HBase中也就相当于关系型数据库的主键，并且Row Key在创建表的时候就已经设置好，用户无法指定某个列作为Row Key。\n\n![Fbm0nx.png](https://s2.ax1x.com/2019/01/07/Fbm0nx.png)\n\n\n\n​\t又有的时候，你也可以把HBase看成是一个类似Redis那样的Key-Value数据库。如下图，当你要查询某一行的所有数据时，Row Key就相当于Key，而Value就是单元中的数据(列族，列族里的列和列中时间戳所对应的不同版本的值)；当深入到HBase底层的存储机制时，用户要查询指定行里某一条单元数据时，HBase会去读取一个数据块，里面除了有要查询的单元数据，可能同时也会获取到其它单元数据，因为这个数据块还包含着这个Row Key所对应的其它列族或其它的列信息，这些信息实际也代表着另一个单元数据，这也是HBase的API内部实际的工作原理。\n\n![FbmBB6.png](https://s2.ax1x.com/2019/01/07/FbmBB6.png)\n\n## 物理模型\n\n​\t逻辑数据模型中空白cell在物理上是不存储的，因此若一个请求为要获取t8时间的contents:html，他的结果就是空。相似的，若请求为获取t9时间的anchor:my.look.ca，结果也是空。但是，如果不指明时间，将会返回最新时间的行，每个最新的都会返回。\n\n​\t在一个HBase中，存储了很多HBase表，当表中包含的行数量非常庞大，无法在一台机器上存储时，需要分布存储到多台机器上，需要根据行键的值对表中进行分区，每个行分区被称为“Region”。\n\n​\tMaster主服务器把不同的Region分配到不同的Region服务器上，同一个Region不会拆分到多个Region服务器上，每个Region服务器负责管理一个Region集合，通常每个Region服务器上会放置10~1000个Region。\n\n![FbnTqx.png](https://s2.ax1x.com/2019/01/07/FbnTqx.png)\n\n## 复合键设计\n\n​\tHbase由两种基本的键结构：行键和列键，行键存储本身的内容，行键存储键的排序顺序，列键包含了列族和特定的列限定符，磁盘上一个列族下的所有单元格的内容都存储在文件中，不同同列族的单元格不会出现在同一个存储文件中。不同列族的单元格不会出现在同一个存储文件中。实际上，每个单元格在存储时，同时保存了列键、行键和时间戳，羁绊存了在表中的位置信息。\n\n​\t在设计HBase表时，行键是最重要的，行键决定了访问HBase表可以得到的性能，这个结论根植于两个实时：region基于行键为一个区间的行提供忘，并负责区间的每一行；HFile在硬盘上存储有序的行，当Region刷写到内存里的行生成HFile。这些行已经排过序，也会有序地刷写到硬盘上。\n\n​\t优秀地行键设计可以保证良好地HBase性能：\n\n​\t1.行键存储在HBase中地每个单元格中，如果行键越长，用于存储单元格地I/O开销会越大。\n\n​\t2.对于组合行每个组件地排序顺序取决于访问模式。\n\n​\t主机名 事件类型 时间戳：适合访问模式使用主机名和事件类型地查询日志方式。\n\n​\t事件类型 时间戳 主机名：适合访问模式用于时间类型和时间戳查询日志地方式。\n\n​\t事件类型 反转时间戳 主机名 ：反转时间戳的值是Long.MAX_VALUE减去时间戳；这样可以确保最近发生的事件排在前面，适用于按照事件发生顺序及逆行处理的场合。\n\n​\t倒叙时间戳作为键的一部分可以快速的找到数据库中数据的最近版本。由于HBase的键是排序的，该键排在任何比它老的行键的前面，所以最近的版本必然是第一个。\n\n**注意： 行键不能发改变，唯一可以“改变”的方式是删除然后再插入。这是一个网上常问的问题，所以需要开始就要让行键正确。** \n\n## 数据结构\n\n### RowKey\n\n与nosql数据库们一样,RowKey是用来检索记录的主键。访问HBASE table中的行，只有三种方式：\n\n1.通过单个RowKey访问\n\n2.通过RowKey的range（正则）\n\n3.全表扫描\n\nRowKey行键 (RowKey)可以是任意字符串(最大长度是64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，RowKey保存为字节数组。存储时，数据按照RowKey的字典序(byte order)排序存储。设计RowKey时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)\n\n### Column Family\n\n列族：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses这个列族。\n\n###  Cell\n\n由{rowkey, column Family:columu, version} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。\n\n关键字：无类型、字节码\n\n###  Time Stamp\n\nHBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒 的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版 本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。\n\n为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。\n\n###  命名空间\n\n![FhsWfx.png](https://s1.ax1x.com/2018/12/30/FhsWfx.png)\n\n**Table**：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。\n\n**RegionServer group：**一个命名空间包含了默认的RegionServer Group。\n\n**Permission：**权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。\n\n**Quota：**限额，可以强制一个命名空间可包含的region的数量。\n\n## Hbase原理\n\n### HBase读流程\n\n![Fh4vIU.png](https://s1.ax1x.com/2018/12/30/Fh4vIU.png)\n\n1. Client先访问zookeeper，从meta表读取region的位置，然后读取meta表中的数据。meta中又存储了用户表的region信息；\n\n2. 根据namespace、表名和rowkey在meta表中找到对应的region信息；\n3. 找到这个region对应的regionserver；\n4. 查找对应的region；\n5. 先从MemStore找数据，如果没有，再到BlockCache里面读；\n6. BlockCache还没有，再到StoreFile上读(为了读取的效率)；\n7. 如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。\n\n### HBase写流程\n\n![Fh5pRJ.png](https://s1.ax1x.com/2018/12/30/Fh5pRJ.png)\n\n1. Client向HregionServer发送写请求；\n2. HregionServer将数据写到HLog（write ahead log）。为了数据的持久化和恢复；\n3. HregionServer将数据写到内存（MemStore）；\n4. 反馈Client写成功。\n\n### 数据flush过程\n\n1. 当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；\n2. 并将数据存储到HDFS中；\n\n###  数据合并过程\n\n1. 当数据块达到4块，**Hmaster**将数据块加载到本地，进行合并；\n2. 当合并的数据超过256M，进行拆分，将拆分后的Region分配给不同的HregionServer管理；\n3. 当HregionServer宕机后，将HregionServer上的hlog拆分，然后分配给不同的HregionServer加载，修改.META.；\n4. **注意：HLog会同步到HDFS。**\n\n","tags":["HBase"],"categories":["大数据"]},{"title":"HBase原理和安装","url":"/2018/12/30/Hbase原理和安装/","content":"\n {{ \"HBase的基本概念和安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Hbase简介\n\nHBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。\n\n官方网站：http://hbase.apache.org\n\nHBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统，利用HBASE技术可在廉价PC Server上搭建起大规模结构化存储集群。\n\nHBase的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。\n\nHBase是Google Bigtable的开源实现，但是也有很多不同之处。比如：Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MAPREDUCE来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。\n\n## Hbase特点\n\n### 海量存储\n\nHbase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与Hbase的极易扩展性息息相关。正式因为Hbase良好的扩展性，才为海量数据的存储提供了便利。Hbase的单表可以有百亿行，百万列，对于关系型数据库单表记录在亿级别时，查询和写入的性能都会出现级数下降，对于传统数据库来说这种庞大的数据量时一种灾难，Hbase限定某个列的情况下对于单表存储百亿或者更多数据没有性能问题，它采用了LSM树为内部数据的存储结构，这种数据结构会周期性地合并成大文件以减少对磁盘地访问，但是牺牲了部分读的性能。\n\n### 列式存储\n\n这里的列式存储其实说的是列族存储，Hbase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。每个列都是单独存储的，且持支基于列的独立检索。列存储下表按照列分开保存，数据查询操作时，列存储只需要读取相关列，可以降低系统的I/O。\n\n###  极易扩展\n\nHbase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。\n 通过横向添加RegionSever的机器，进行水平扩展，提升Hbase上层的处理能力，提升Hbsae服务更多Region的能力。\n\nRegionServer的作用是管理region、承接业务的访问，通过横向添加Datanode的机器，进行存储层扩容，提升Hbase的数据存储能力和提升后端存储的读写能力。\n\n### 高并发\n\n由于目前大部分使用Hbase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，Hbase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。\n\n### 高可靠\n\nHbase运行在HDFS上，HDFS的多副本存储可以让它在出现故障的时候自动恢复，同时Hbase内部也提供WAL和Replication机制WAL（Write-Ahead-Log）预写日志在Hbase服务器处理数据插入和删除的过程中用来记录操作内容的日志，保证了数据写入时不会因为集群异常而导致写入数据的丢失，而Replication机制是基于数据操作来数据同步的。当集群中单个节点出现故障时，ZooKeeper通知集群的主节点，将故障节点的Hlog中的日志信息发送到从节点进行数据恢复。\n\n### 稀疏\n\n稀疏主要是针对Hbase列的灵活性，在列族中，你可以指定任意多的列，Hbase的数据都是以字符串进行存储的，在列数据为空的情况下，是不会占用存储空间的。最大程度上节省了存储开销，所以Hbase通常可以设计成系数的矩阵，这种方式比较接近实际的应用场景。\n\n## HBase架构\n\n![Ffvrb4.png](https://s1.ax1x.com/2018/12/30/Ffvrb4.png)\n\n### Client\n\nClient包含了访问Hbase的接口，是整个HBase系统的入口，使用者可以通过客户端操作Hbase客户端使用HBase的RPC机制和HMaster和RegionServer进行通信，一般情况下，客户端与HMaster进行管理类的操作通信，在获取RegionServer的信息后，直接与RegionServer进行数据的读写类操作，另外Client还维护了对应的cache来加速Hbase的访问，比如cache的.META.元数据的信息。客户端可以使用Java语言来实现、也可以是Thtift、Rest等客户端模式，甚至是MapReduce也可以算作是一种客户端。\n\n### ZooKeeper\n\nZooKeeper是一个高性能、集中化、分布式应用程序协调服务，主要用了解决分布式应用中常遇到的数据管理问题，比如数据发布/订阅、命名服务、分布式协调通知、集群管理、Master选举、分布式锁、分布式队列等。其中Master选举是Zookeeper中最经典的应用场景，在Hadoop中，Zookeeper主要实现HA高可用，包括NameNode和YARN的ResourceManager的HA。\n\n#### Master选举\n\n同HDFS的HA机制一样，Hbase集群中有多个HMaster并存，通过竞争选举保证同一时刻只有一个HMaster处于活跃状态，一旦这个HMaster无法使用，则从备用节点选举一个顶上，保证集群的高扩展性。\n\n#### 系统容错\n\n在HBase启动时，每个RegioServer在加入集群时都需要到ZooKeeper中进行注册，创建一个状态节点，ZooKeeper会实时监控每个RegionServer的状态同时，HMaster会对这些注册的RegionServer进行监听，当某个RegionServer挂掉之后,ZooKeeper会因为一段时间内接收不到它的心跳信息而删除该RegionServer对于的状态节点，把并且给HMaster发送节点删除的通知，这时,HMaster获知集群中某节点断开，会立即调度其他节点开启容错机制。\n\n#### Region元数据管理\n\n在Hbase集群中，region元数据被存储在.META.表中 每次客户端发起新的请求时，需要查询.META.表来获取region的位置。.META.表是存在ZooKeeper中的。当RegionServer发生变化时，比如region的手工移动、进行负载均衡移动或者region所在的RegionServer出现故障等，就通过ZooKeeper来感知这一变化，保证客户端能够获得正确的region元数据信息。\n\n#### Region状态管理\n\nHBase集群中region会经常发生变更，变更的原因可能是系统故障，或者是配置修改，还有region的分裂和合并。只要region发生变化，就需要集群的所有节点知晓，否则就会出现某些事务性的异常。对于Hbase集群，region发生变化，就需要集群的所有节点知晓，否则会出现某些事务性的异常，对于HBase集群，region的数量会达到10万级别，甚至更多，如果这样规模的region状态管理直接由HMaster来实现，可想而知HMaster负担会很重，因此只有通过ZooKeeper系统来完成。\n\n#### 元数据信息\n\nZooKeeper存储HBase中有哪些表，每个表包含那些列族信息，存储元数据的统一入口地址。\n\n### HMaster\n\nHMaster是HBase集群中的主服务器，负责监控集群中的所有RegionServer，并且是所有元数据更改的接口，分布式环境中HMaster服务通常运行在HDFS的NameNode上，HMaster通过ZooKeeper来避免单点故障，在集群当中可以启动多个HMaster，但ZooKeeper的选举机制能够保证同时只有一个HMaster处于Active状态，其他的HMaster处于热备状态。HMaster主要负责表和region的管理作业。\n\n1. 管理用户对于表的增删改查\n\n| 相关的        | 功能                                |\n| ------------- | ----------------------------------- |\n| HBase表       | 创建表、删除表、启用/失效表、修改表 |\n| HBase列族     | 添加列、修改列、删除列              |\n| HBase表region | 移动region、region的分配和合并      |\n\n\n2. 管理RegionServer的负载均衡，调整region的分布。\n\n3. Region的分配和移除\n\n4. 处理RegionServer的故障转移\n    当某台RegionServer挂掉时，总有一部分新写入的数据还没有持久化到磁盘上，因此在迁移RegionServer服务是，需要从修改记录中恢复这部分还在内存中的数据，HMaster需要遍历该RegionServer的修改记录，并按region切分成小块移动到新的地址下。\n\n  当HMaster节点发生故障时，由于客户端时进行与RegionServer交互的，且.META.表也时存在ZooKeeper当中，整个集群的工作会继续正常运行，所以HMaster发生故障时，集群仍然可以稳定运行。但是HMaster还会执行一些重要的工作，比如region的切片，RegionServer的故障冠以等，如果HMaster发生故障而没有及时处理，这些功能都会受到影响，所以HMaster要尽快恢复工作。ZooKeeper组件就提供了这种多HMaster的机制提高HBase的可用性和鲁棒性。\n\n### RegionServer\n\nRegionServer主要负责用户的请求，向HDFS中读写数据。一般在分布式集群中，RegionServer运行在DataNode服务器上，实现数据的本地性，每个RegionServer包含多个region，它负责的功能有：\n\n1. 处理给它的region\n2. 处理客户端读写请求\n3. 刷新缓存到HDFS中\n4. 处理region分配\n5. 执行压缩\n\nRegionServer时HBase中最核心的部分，其内部管理了一系列的region对象，每个region由多个HStore组成，每个HStore对应列表中一个列族的存储。这可以看到HBase时按照列进行存储的，将列族作为一个集中的存储单元，并且HBase将具备同I/O特性的列放到一个列族中，这样可以保证写的高效性。RegionServer最终将region数据存在HDFS上，采用HDFS作为底层存储。HBase自身并不具备数据和维护数据副本的功能，而依赖HDFS可以为HBase提供可靠和稳定的存储。当然HBase也可以不采用HDFS，比如它可以使用本地文件系统或云计算环境中的Amzon S3。\n\n### 其他组件\n\n#### WAL\n\nHBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。\n\n#### Region\n\nHbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。\n\n#### Store\n\nHFile存储在Store中，一个Store对应HBase表中的一个列族。\n\n#### MemStore\n\n顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。\n\n#### HFile\n\n这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的。\n\n## 安装\n\n### 前置条件：\n\n Zookeeper正常部署    Hadoop正常部署\n\n### 解压文件\n\n```\ntar -zxvf hbase-1.3.1-bin.tar.gz -C /opt/module\n```\n\n### 配置文件\n\n#### hbase-env.sh\n\n```shell\nexport JAVA_HOME=/opt/module/jdk1.8.0_144\nexport HBASE_MANAGES_ZK=false\n```\n\n#### hbase-site.xml\n\n```xml\n<configuration>\n        <property>\n                <name>hbase.rootdir</name>\n                <value>hdfs://datanode1:9000/hbase</value>\n        </property>\n\n        <property>\n                <name>hbase.cluster.distributed</name>\n                <value>true</value>\n        </property>\n   <!-- 0.98后的新变动，之前版本没有.port,默认端口为60000 -->\n        <property>\n                <name>hbase.master.port</name>\n                <value>16000</value>\n        </property>\n\n        <property>\n                <name>hbase.zookeeper.quorum</name>\n             <value>datanode1:2181,datanode2:2181,datanode3:2181</value>\n        </property>\n\n        <property>\n                <name>hbase.zookeeper.property.dataDir</name>\n             <value>/opt/module/zookeeper-3.4.10/data/zkData</value>\n        </property>\n</configuration>\n```\n\n#### regionservers\n\n```\ndatanode1\ndatanode2\ndatanode3\n```\n\n#### 软连接 \n\n```shell\nln -s /opt/module/hadoop/etc/hadoop/core-site.xml   /opt/module/hbase/conf/core-site.xml\nln -s /opt/module/hadoop/etc/hadoop/hdfs-site.xml   /opt/module/hbase/conf/hdfs-site.xml\n```\n\n#### 同步\n\n```\nxsync hbase/ \n```\n\n#### 启动\n\n```\nstart-hbase.sh\n```\n\n#### 查看 web界面\n\n![FhP8SA.png](https://s1.ax1x.com/2018/12/30/FhP8SA.png)\n\n##  使用场景\n\n- 数据模式时动态的或者可变的，且支持半结构化和非结构化的数据。\n- 数据库中很多列都包含了很多空字段，在HBase中空字段不会像关系型数据库占用空间。\n- 需要很高的吞吐量，瞬间写入量大。\n- 数据很多版本需要维护，HBase利用时间戳来区分不同版本的数据。\n- 具有高可拓展性，能动态拓展整个系统。\n\n![FhNedf.png](https://s1.ax1x.com/2018/12/30/FhNedf.png)\n\n#### 业务场景\n\n对象存储：我们知道不少的头条类、新闻类的的新闻、网页、图片存储在HBase之中，一些病毒公司的病毒库也是存储在HBase之中\n\n时序数据：HBase之上有OpenTSDB模块，可以满足时序类场景的需求\n\n推荐画像：特别是用户的画像，是一个比较大的稀疏矩阵，蚂蚁的风控就是构建在HBase之上\n\n时空数据：主要是轨迹、气象网格之类，滴滴打车的轨迹数据主要存在HBase之中，另外在技术所有大一点的数据量的车联网企业，数据都是存在HBase之中\n\nCubeDB OLAP：Kylin一个cube分析工具，底层的数据就是存储在HBase之中，不少客户自己基于离线计算构建cube存储在hbase之中，满足在线报表查询的需求\n\n消息/订单：在电信领域、银行领域，不少的订单查询底层的存储，另外不少通信、消息同步的应用构建在HBase之上\n\nFeeds流：典型的应用就是xx朋友圈类似的应用\n\nNewSQL：之上有Phoenix的插件，可以满足二级索引、SQL的需求，对接传统数据需要SQL非事务的需求\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t参考资料：[Hbase技术社区](https://mp.weixin.qq.com/s/A3_qPtuDOxUtsk67D7qFNQ)\n\n","tags":["Hbase"],"categories":["大数据"]},{"title":"MapReduce高级编程2","url":"/2018/12/28/MapReduce高级编程2/","content":"\n {{ \"MapReduce Top N 、二次排序，MapJoin\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\nTOP N\n\n对于一组输入List(key,value),我们要创建一个Top N 列表,这是一种过滤模式,查看输入数据特定子集,观察用户的行为。\n\n### 解决方案\n\nkey是唯一键，需要对输入进行额外的聚集处理，先把输入分区成小块，然后把每个小块发送到一个映射器中。每个映射器会创建一个本地Top N 列表发送到一个规约器中，即最终由一个规约其产生一个Top  N 列表。对于大多数的MapReduce算法，由一个规约器接收所有数据会使负载不均衡，从而产生瓶颈问题。但是本解决方案产生的Top N 是很少量的数据，如果有映射器，则会需要处理1000 X N的数据。\n\n### 数据\n\n![FWGp9I.png](https://s1.ax1x.com/2018/12/28/FWGp9I.png)\n\n### 思路\n\n计算过程以K V对传输\n\n可以创建一个最小堆，最常用的是SortMap&lt;K,V&gt; 和TreeMap&lt;K,V&gt;，如果top N.size()>N，则删除第一个元素（频数最小）如果求Bottom N构造最大堆。\n\n![FWGL2q.png](https://s1.ax1x.com/2018/12/28/FWGL2q.png)\n\n### 代码\n\n#### Mapper\n\n```java\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\nimport java.util.TreeMap;\n\npublic class TopTenMapper extends Mapper<Object, Text, NullWritable, Text> {\n    private TreeMap<Integer, Text> visitimesMap = new TreeMap<Integer, Text>();\n\n    @Override\n    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n        if (value == null) {\n            return;\n        }\n        String[] strs = value.toString().split(\" \");\n        String tId = strs[0];\n        String reputation = strs[1];\n\n        if (tId == null || reputation == null) {\n            return;\n        }\n        visitimesMap.put(Integer.parseInt(reputation), new Text(value));\n        if (visitimesMap.size() > 10) {\n            visitimesMap.remove(visitimesMap.firstKey());\n        }\n    }\n\n    @Override\n    protected void cleanup(Context context) throws IOException, InterruptedException {\n        for (Text t : visitimesMap.values()) {\n            context.write(NullWritable.get(), t);\n\n        }\n    }\n}\n```\n\n#### Reduce\n\n```java\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.util.TreeMap;\n\npublic class TopTenReduce extends Reducer<NullWritable, Text, NullWritable, Text> {\n    private TreeMap<Integer, Text> visittimesMap = new TreeMap<Integer, Text>();\n\n    @Override\n    protected void reduce(NullWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {\n        for (Text value : values) {\n            String[] strs = value.toString().split(\" \");\n            visittimesMap.put(Integer.parseInt(strs[1]), new Text(value));\n            if (visittimesMap.size() > 10) {\n                visittimesMap.remove(visittimesMap.firstKey());\n            }\n        }\n        for (Text t : visittimesMap.values()) {\n            context.write(NullWritable.get(), t);\n        }\n    }\n}\n```\n\n#### Job\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class TopTenJob {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        if (args.length != 2) {\n            System.out.println(\"Usage:TopTenDriver<in> <out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(TopTenJob.class);\n        job.setMapperClass(TopTenMapper.class);\n        job.setReducerClass(TopTenReduce.class);\n\n        job.setNumReduceTasks(1);\n\n        job.setOutputKeyClass(NullWritable.class);\n        job.setOutputValueClass(Text.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n```\n\n#### 结果\n\n![FWYanK.png](https://s1.ax1x.com/2018/12/28/FWYanK.png)\n\n## 非唯一键解决方案\n\n假设所有的输入键是不唯一的，即会出现key相同而value不同的情况，则需要为两个阶段解决。\n\n第一阶段：通过MapReduce先聚集具有相同键的元组，并将频数相加，将不唯一的键转换为唯一的键，其value值为频数和，输出为唯一键值对&lt;key,value&gt;\n\n第二阶段：将第一阶段的输出的唯一键值对作为输入，采用唯一键的解决方案求Top N 即可。\n\n## 全排序\n\n创建随机数据\n\n```shell\nfor i in {1..100000};\ndo\n        echo $RANDOM\ndone;\n```\n\n```shell\nsh createdata.sh >data1\nsh createdata.sh >data2\nsh createdata.sh >data3\nsh createdata.sh >data4\n```\n\n### MyPartitioner\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class MyPartitioner extends Partitioner<IntWritable, IntWritable> {\n\n    public int getPartition(IntWritable key, IntWritable value, int numPartitions) {\n        int keyInt = Integer.parseInt(key.toString());\n        if (keyInt > 20000) {   //数据区间在在[0,350000]随机生成概率比较均匀.\n            return 2;\n        } else if (keyInt > 10000) {\n            return 1;\n        } else {\n            return 0;\n        }\n    }\n}\n```\n\n### Mysort\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\npublic class Mysort extends WritableComparator {\n    public Mysort() {\n        super(IntWritable.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n        IntWritable v1 = (IntWritable) a;\n        IntWritable v2 = (IntWritable) b;\n        return v2.compareTo(v1);\n    }\n}\n```\n\n### SimpleMapper\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class SimpleMapper extends Mapper<LongWritable, Text, IntWritable,IntWritable> {\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //将Text类的value转换为IntWritable类型\n        IntWritable intvalue = new IntWritable(Integer.parseInt(value.toString()));\n        //值写入context\n        context.write(intvalue,intvalue);\n    }\n}\n```\n\n### SimpleReducer\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class SimpleReducer extends Reducer<IntWritable, IntWritable, IntWritable, NullWritable> {\n    @Override\n    protected void reduce(IntWritable key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        for (IntWritable value : values) {\n            context.write(value, NullWritable.get());\n        }\n    }\n}\n```\n\n### SimpleJob\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\n\npublic class SimpleJob {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n\n        job.setJarByClass(SimpleJob.class);\n\n        job.setPartitionerClass(MyPartitioner.class);\n        job.setSortComparatorClass(Mysort.class);\n        job.setMapperClass(SimpleMapper.class);\n        job.setMapOutputKeyClass(IntWritable.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setReducerClass(SimpleReducer.class);\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        job.setNumReduceTasks(3);\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        job.waitForCompletion(true);\n    }\n}\n```\n\n## 抽样实现全局排序\n\n自定分区下实现用户按业务分区与Reduce任务数据一 一对于,使用全排序最终的解结果在文件中一次按顺序输出,可能存在认为对数据了解不够多，造成分区的数据个数不一致，导致发生数据倾斜的问题。\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.partition.InputSampler;\nimport org.apache.hadoop.mapreduce.lib.partition.TotalOrderPartitioner;\nimport org.apache.hadoop.util.Tool;\nimport org.apache.hadoop.util.ToolRunner;\n\nimport java.io.IOException;\n\npublic class TotalOrderingPartition extends Configured implements Tool {\n    //Map类\n    static class SimpleMapper extends Mapper<Text, Text, Text, IntWritable> {\n        @Override\n        protected void map(Text key, Text value, Context context) throws IOException, InterruptedException {\n\n            IntWritable intWritable = new IntWritable(Integer.parseInt(key.toString()));\n            context.write(key, intWritable);\n        }\n    }\n\n    //Reduce类\n    static class SimpleReducer extends Reducer<Text, IntWritable, IntWritable, NullWritable> {\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            for (IntWritable value : values) {\n                context.write(value, NullWritable.get());\n            }\n        }\n    }\n\n    public int run(String[] args) throws Exception {\n        Configuration conf = getConf();\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(TotalOrderingPartition.class);\n\n        job.setInputFormatClass(KeyValueTextInputFormat.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.setNumReduceTasks(3);\n\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setOutputKeyClass(IntWritable.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        TotalOrderPartitioner.setPartitionFile(job.getConfiguration(), new Path(args[2]));\n        InputSampler.Sampler<Text, Text> sampler = new InputSampler.SplitSampler<Text, Text>(1000, 10);\n\n        InputSampler.writePartitionFile(job, sampler);\n        job.setPartitionerClass(TotalOrderPartitioner.class);\n        job.setMapperClass(SimpleMapper.class);\n        job.setReducerClass(SimpleReducer.class);\n\n        job.setJobName(\"Ite Blog\");\n\n        return job.waitForCompletion(true) ? 0 : 1;\n    }\n\n    //主方法\n    public static void main(String[] args) throws Exception {\n        args = new String[]{\"/input/simple\",\"/output/random_simple1\",\"/output/random_simple2\"};\n        int exitCode = ToolRunner.run(new TotalOrderingPartition(), args);\n        System.exit(exitCode);\n    }\n}\n```\n\n通过抽样分区，可以使个分区所含的记录大致相等，使作业的总体执行时间不会因为一个Reducer任务之后而拖慢整体进度。\n\n## 二次排序\n\n二次排序不同于全排序，它在规约阶段对与中间键相关联的中间值的某个属性进行排序。可以对传如各个Reducer的值进行升序或降序排序，即首先按照第一个字段排序，在对第二个字段相同的排序，且不能破坏第一个排序结果。\n\n### 解决方案\n\n1. Reducer内排序。让Reducer接收（k2,list(v2)）之后,利用Reducer内存进行排序，生成(k2,list(v2)),在进行处理，在数值过多的情况下，容易导致内存溢出。\n2. 利用MapReduce框架的排序框架对Reducer值进行排序。为自然键增加自然值得部分或整个值，构造组合键进行排序。由于排序工作是由MapReduce框架完成的，因此不会溢出。\n\n#### 流程\n\n1. 构造组合键(k,v1),其中v1是次键，K为自然键值，要在Reduce中注入一个值（v1）或者一个组合键。Reducer值按照什么来排序，就将其加入到自然键中，共同称为组合键。\n\n    对于Key=k2，若所有的映射器生成键值对为(k2,A1),(k2,A2),…(k2,An),对于一每个Ai，对于一个每一个Ai，设Ai为一个m元组值(ai2,ai3,…,aim)用bi来表示。因而映射器生成键值对可以做如下表示:\n\n    (k2,(a1,b1)),(k2(a2,b2)),…(k2,(an,bn))\n\n    自然键位k2,加入ai形成组合键((k2,ai))最终的映射器发出键值对如下：\n\n    ((k2,a1),(a1,b1)),((k2,a2),(a2,b2)),….((k2,an),(an,bn))\n\n2. 在分区器中加入两个插件类：定制分区器，确保相同自然键的数据到达相同Reducer，也就是说，同一Reducer中可能由key=k2,key=k3的所有数据，定制比较器就将这些数据按照自然键进行分组。\n\n3. 分组比较器，可以使v1按有序的顺序到达Reducer，使用MapReduce执行框架完成排序，可以保证到达Reducer的值按键有序并按值有序。\n\n#### 实现类\n\n##### MyWritable\n\n```java\nimport java.io.*;\n\nimport org.apache.hadoop.io.*;\n\npublic class MyWritable implements WritableComparable<MyWritable> {\n\n    private int first;\n    private int second;\n\n    public MyWritable() {\n    }\n\n    public MyWritable(int first, int second) {\n        set(first, second);\n    }\n\n    public void set(int first, int second) {\n        this.first = first;\n        this.second = second;\n    }\n\n    public int getFirst() {\n        return first;\n    }\n\n    public int getSecond() {\n        return second;\n    }\n\n    public void write(DataOutput out) throws IOException {\n        out.writeInt(first);\n        out.writeInt(second);\n    }\n\n    public void readFields(DataInput in) throws IOException {\n        first = in.readInt();\n        second = in.readInt();\n    }\n\n    @Override\n    public int hashCode() {\n        return first * 163 + second;\n    }\n    @Override\n    public boolean equals(Object o) {\n        if (o instanceof MyWritable) {\n            MyWritable ip = (MyWritable) o;\n            return first == ip.first && second == ip.second;\n        }\n        return false;\n    }\n\n    @Override\n    public String toString() {\n        return first + \"\\t\" + second;\n    }\n\n    public int compareTo(MyWritable ip) {\n        int cmp = compare(first, ip.first);\n        if (cmp != 0) {\n            return cmp;\n        }\n        return compare(second, ip.second);\n    }\n\n    public static int compare(int a, int b) {\n        return (a < b ? -1 : (a == b ? 0 : 1));\n    }\n}\n```\n\n##### MyComparator\n\n```java\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\npublic class MyComparator extends WritableComparator {\n    protected MyComparator() {\n        super(MyWritable.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable w1, WritableComparable w2) {\n        MyWritable ip1 = (MyWritable) w1;\n        MyWritable ip2 = (MyWritable) w2;\n        int cmp = MyWritable.compare(ip1.getFirst(), ip2.getFirst());\n        if (cmp != 0) {\n            return cmp;\n        }\n        return -MyWritable.compare(ip1.getSecond(), ip2.getSecond());\n    }\n}\n```\n\n#####  MyPartitioner\n\n```java\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\npublic class MyPartitioner extends Partitioner<MyWritable, NullWritable> {\n\n    @Override\n    public int getPartition(MyWritable key, NullWritable value, int numPartitions) {\n        return (key.hashCode() & Integer.MAX_VALUE) % numPartitions;\n    }\n}\n```\n\n##### MyGroup\n\n```java\nimport org.apache.hadoop.io.WritableComparable;\nimport org.apache.hadoop.io.WritableComparator;\n\npublic class MyGroup extends WritableComparator {\n    protected MyGroup() {\n        super(MyWritable.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable w1, WritableComparable w2) {\n        MyWritable ip1 = (MyWritable) w1;\n        MyWritable ip2 = (MyWritable) w2;\n        return MyWritable.compare(ip1.getFirst(), ip2.getFirst());\n    }\n}\n```\n\n##### SecondarySortMapper\n\n```java\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class SecondarySortMapper extends\n        Mapper<LongWritable, Text, MyWritable, NullWritable> {\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context)\n            throws IOException, InterruptedException {\n\n        String[] strs = value.toString().split(\" \");\n        String year = strs[0];\n        String Temperature = strs[1];\n        if (year == null || Temperature == null) {\n            return;\n        }\n\n        context.write(\n                new MyWritable(Integer.parseInt(strs[0]), Integer\n                        .parseInt(strs[1])), NullWritable.get());\n    }\n\n}\n```\n\n##### SecondarySortReducer\n\n```java\nimport java.io.IOException;\n\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Reducer;\n\npublic class SecondarySortReducer extends\n        Reducer<MyWritable, NullWritable, MyWritable, NullWritable> {\n\n    @Override\n    protected void reduce(MyWritable key, Iterable<NullWritable> values,\n                          Context context) throws IOException, InterruptedException {\n\n        for (NullWritable val : values) {\n            context.write(key, NullWritable.get());\n        }\n    }\n}\n```\n\n##### SecondarySortJob\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\npublic class SecondarySortJob {\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(SecondarySortJob.class);\n        job.setMapperClass(SecondarySortMapper.class);\n        job.setPartitionerClass(MyPartitioner.class);\n        job.setSortComparatorClass(MyComparator.class);\n        job.setGroupingComparatorClass(MyGroup.class);\n        job.setReducerClass(SecondarySortReducer.class);\n        job.setOutputKeyClass(MyWritable.class);\n        job.setOutputValueClass(NullWritable.class);\n        job.setNumReduceTasks(1);\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        job.waitForCompletion(true);\n    }\n}\n```\n\n#### 原始数据\n\n```\n2016 32\n2017 38\n2016 31\n2016 39\n2015 35\n2017 34\n2016 37\n2017 36\n2018 35\n2015 31\n2018 34\n2018 33\n```\n\n##### 排序后数据\n\n```java\n2015    35\n2015    31\n2016    39\n2016    37\n2016    32\n2016    31\n2017    38\n2017    36\n2017    34\n2018    35\n2018    34\n2018    33\n```\n\n## 连接\n\n\n\n### Reduce端连接\n\n\n\n#### TableBean\n\n```java\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class TableBean implements Writable {\n\n    private String orderId;\n    private String pid;\n    private int amount;\n    private String pname;\n    private String flag;\n\n    public TableBean() {\n        super();\n    }\n\n    public TableBean(String orderId, String pid, int amount, String pname, String flag) {\n        super();\n        this.orderId = orderId;\n        this.pid = pid;\n        this.amount = amount;\n        this.pname = pname;\n        this.flag = flag;\n    }\n\n\n    @Override\n    public String toString() {\n        return orderId + \"\\t\" + pname + \"\\t\" + amount;\n    }\n\n    public String getOrderId() {\n        return orderId;\n    }\n\n    public void setOrderId(String orderId) {\n        this.orderId = orderId;\n    }\n\n    public String getPid() {\n        return pid;\n    }\n\n    public void setPid(String pid) {\n        this.pid = pid;\n    }\n\n    public int getAmount() {\n        return amount;\n    }\n\n    public void setAmount(int amount) {\n        this.amount = amount;\n    }\n\n    public String getPname() {\n        return pname;\n    }\n\n    public void setPname(String pname) {\n        this.pname = pname;\n    }\n\n    public String getFlag() {\n        return flag;\n    }\n\n    public void setFlag(String flag) {\n        this.flag = flag;\n    }\n\n    @Override\n    public void write(DataOutput out) throws IOException {\n        out.writeUTF(orderId);\n        out.writeUTF(pid);\n        out.writeInt(amount);\n        out.writeUTF(pname);\n        out.writeUTF(flag);\n    }\n\n    @Override\n    public void readFields(DataInput in) throws IOException {\n        orderId = in.readUTF();\n        pid = in.readUTF();\n        amount = in.readInt();\n        pname = in.readUTF();\n        flag = in.readUTF();\n    }\n}\n```\n\n#### TableMapper\n\n```java\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.lib.input.FileSplit;\n\nimport java.io.IOException;\n\npublic class TableMapper extends Mapper<LongWritable, Text, Text, TableBean>{\n    Text k = new Text();\n    TableBean tableBean = new TableBean();\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //获取文件名\n        FileSplit split = (FileSplit) context.getInputSplit();\n        String name = split.getPath().getName();\n        //判断\n        String line = value.toString();\n        if (name.startsWith(\"order\")){\n            String[] fields = line.split(\"\\t\");\n            tableBean.setOrderId(fields[0]);\n            tableBean.setPid(fields[1]);\n            tableBean.setAmount(Integer.parseInt(fields[2]));\n            tableBean.setPname(\"\");\n            tableBean.setFlag(\"0\");\n            k.set(fields[1]);\n            context.write(k, tableBean);\n        }else {\n            String[] fields = line.split(\"\\t\");\n            tableBean.setOrderId(\"\");\n            tableBean.setPid(fields[0]);\n            tableBean.setAmount(0);\n            tableBean.setPname(fields[1]);\n            tableBean.setFlag(\"1\");\n            k.set(fields[0]);\n            context.write(k, tableBean);\n        }\n    }\n}\n```\n\n#### TableReducer\n\n```java\nimport org.apache.commons.beanutils.BeanUtils;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.lang.reflect.InvocationTargetException;\nimport java.util.ArrayList;\nimport java.util.List;\n\npublic class TableReducer extends Reducer<Text, TableBean, NullWritable, TableBean> {\n    //1001 01 1\n    //01   小米\n    @Override\n    protected void reduce(Text key, Iterable<TableBean> values, Context context) throws IOException, InterruptedException {\n        //用来接收order表的数据\n        List<TableBean> orderBeans = new ArrayList<>();\n        //用来接受pd表的数据\n        TableBean pBean = new TableBean();\n        for (TableBean value : values){\n            //order表\n            if (value.getFlag().equals(\"0\")){\n                TableBean oBean = new TableBean();\n                try {\n                    BeanUtils.copyProperties(oBean, value);\n                    orderBeans.add(oBean);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n            }else {\n                //pd.txt\n                try {\n                    BeanUtils.copyProperties(pBean, value);\n                } catch (IllegalAccessException e) {\n                    e.printStackTrace();\n                } catch (InvocationTargetException e) {\n                    e.printStackTrace();\n                }\n            }\n        }\n        for (TableBean orderbean : orderBeans){\n            orderbean.setPname(pBean.getPname());\n            context.write(NullWritable.get(),orderbean);\n        }\n    }\n}\n```\n\n#### TableDriver\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TableDriver {\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration configuration = new Configuration();\n        Job job = Job.getInstance(configuration);\n\n        // 2 指定本程序的jar包所在的本地路径\n        job.setJarByClass(TableDriver.class);\n\n        // 3 指定本业务job要使用的mapper/Reducer业务类\n        job.setMapperClass(TableMapper.class);\n        job.setReducerClass(TableReducer.class);\n\n        // 4 指定mapper输出数据的kv类型\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(TableBean.class);\n\n        // 5 指定最终输出的数据的kv类型\n        job.setOutputKeyClass(NullWritable.class);\n        job.setOutputValueClass(TableBean.class);\n\n        // 6 指定job的输入原始文件所在目录\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n\n    }\n}\n```\n\n##### 原始数据\n\n![Ffw9q1.png](https://s1.ax1x.com/2018/12/29/Ffw9q1.png)\n\n##### 结果\n\n```java\n1001    小米    1\n1002    华为    2 \n1003    格力    3\n```\n\n### Map端连接\n\n##### DistributedCacheMapper\n\n```java\nimport org.apache.commons.lang.StringUtils;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.BufferedReader;\nimport java.io.FileInputStream;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport java.net.URI;\nimport java.util.HashMap;\n\npublic class DistributedCacheMapper extends Mapper<LongWritable, Text, Text, NullWritable> {\n    Text k = new Text();\n    HashMap<String, String> map = new HashMap<>();\n    /**\n     * 先缓存pd表\n     * @param context\n     * @throws IOException\n     * @throws InterruptedException\n     */\n    @Override\n    protected void setup(Context context) throws IOException, InterruptedException {\n        URI[] cacheFiles = context.getCacheFiles();\n        //获取输入字节流\n        FileInputStream fis = new FileInputStream(cacheFiles[0].getPath());\n        //获取转换流\n        InputStreamReader isr = new InputStreamReader(fis, \"UTF-8\");\n        //获取缓存流\n        BufferedReader br = new BufferedReader(isr);\n        String line = null;\n        //开始一行一行读数据\n        while (StringUtils.isNotEmpty(line = br.readLine())) {\n            String[] fields = line.split(\"\\t\");\n            //将数据放入map中\n            map.put(fields[0],fields[1]);\n        }\n        fis.close();\n        isr.close();\n        br.close();\n    }\n\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //获取一行\n        String line = value.toString();\n        //通过pid获取pname\n        String[] fields = line.split(\"\\t\");\n        String pid = fields[1];\n         String pname = map.get(pid);\n        String str = line + \"\\t\" + pname;\n        k.set(str);\n        context.write(k, NullWritable.get());\n    }\n}\n```\n\n##### DistributedCacheDriver\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.NullWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.net.URI;\n\npublic class DistributedCacheDriver {\n    public static void main(String[] args) throws Exception {\n        // 1 获取job信息\n        Configuration configuration = new Configuration();\n        Job job = Job.getInstance(configuration);\n\n        // 2 设置加载jar包路径\n        job.setJarByClass(DistributedCacheDriver.class);\n\n        // 3 关联map\n        job.setMapperClass(DistributedCacheMapper.class);\n        // 4 设置最终输出数据类型\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(NullWritable.class);\n\n        // 5 设置输入输出路径\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        // 6 加载缓存数据写入内存 默认是本地路径\n        job.addCacheFile(new URI(args[2]));\n\n        // 7 map端join的逻辑不需要reduce阶段，设置reducetask数量为0\n        job.setNumReduceTasks(0);\n\n        // 8 提交\n        boolean result = job.waitForCompletion(true);\n        System.exit(result ? 0 : 1);\n\n    }\n}\n```\n\n这种方法的运行速度很快,输入数据进行Mapper计算前分了部分,启用了一个Mapper,而Reducer连接时启动了两个Mapper.但该种计算模式受到JVM种堆的分配限制,如果集群中的内存足够大,业务符合逻辑(如内连接和做外连接)时可以考虑这种计算模式,如果数据集很大,Reducer连接操作是一个很不错的选择.当然连接操作的思想还有一些其他的方法,可以根据业务不同,采取不同的编程思想。\n\n![Ff0BnA.png](https://s1.ax1x.com/2018/12/29/Ff0BnA.png)","tags":["MapReduce"],"categories":["大数据"]},{"title":"MapReduce高级编程","url":"/2018/12/28/MapReduce高级编程/","content":"\n {{ \"MapReduce 计数器、最值\"}}：<Excerpt in index | 首页摘要><!-- more --> \n\n计数器\n\n数据集在进行MapReduce运算过程中，许多时候，用户希望了解待分析的数据的运行的运行情况。Hadoop内置的计数器功能收集作业的主要统计信息，可以帮助用户理解程序的运行情况，辅助用户诊断故障。\n\n```\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n18/12/28 10:37:46 INFO client.RMProxy: Connecting to ResourceManager at datanode3/192.168.1.103:8032\n18/12/28 10:37:48 WARN mapreduce.JobResourceUploader: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\n18/12/28 10:37:50 INFO input.FileInputFormat: Total input paths to process : 2\n18/12/28 10:37:50 INFO mapreduce.JobSubmitter: number of splits:2\n18/12/28 10:37:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1545964109134_0001\n18/12/28 10:37:53 INFO impl.YarnClientImpl: Submitted application application_1545964109134_0001\n18/12/28 10:37:54 INFO mapreduce.Job: The url to track the job: http://datanode3:8088/proxy/application_1545964109134_0001/\n18/12/28 10:37:54 INFO mapreduce.Job: Running job: job_1545964109134_0001\n18/12/28 10:38:50 INFO mapreduce.Job: Job job_1545964109134_0001 running in uber mode : false\n18/12/28 10:38:50 INFO mapreduce.Job:  map 0% reduce 0%\n18/12/28 10:39:28 INFO mapreduce.Job:  map 100% reduce 0%\n18/12/28 10:39:48 INFO mapreduce.Job:  map 100% reduce 100%\n18/12/28 10:39:50 INFO mapreduce.Job: Job job_1545964109134_0001 completed successfully\n18/12/28 10:39:51 INFO mapreduce.Job: Counters: 49\n        File System Counters\n                FILE: Number of bytes read=78\n                FILE: Number of bytes written=353015\n                FILE: Number of read operations=0\n                FILE: Number of large read operations=0\n                FILE: Number of write operations=0\n                HDFS: Number of bytes read=258\n                HDFS: Number of bytes written=31\n                HDFS: Number of read operations=9\n                HDFS: Number of large read operations=0\n                HDFS: Number of write operations=2\n        Job Counters\n                Launched map tasks=2\n                Launched reduce tasks=1\n                Data-local map tasks=2\n                Total time spent by all maps in occupied slots (ms)=67297\n                Total time spent by all reduces in occupied slots (ms)=16699\n                Total time spent by all map tasks (ms)=67297\n                Total time spent by all reduce tasks (ms)=16699\n                Total vcore-milliseconds taken by all map tasks=67297\n                Total vcore-milliseconds taken by all reduce tasks=16699\n                Total megabyte-milliseconds taken by all map tasks=68912128\n                Total megabyte-milliseconds taken by all reduce tasks=17099776\n        Map-Reduce Framework\n                Map input records=8\n                Map output records=8\n                Map output bytes=78\n                Map output materialized bytes=84\n                Input split bytes=212\n                Combine input records=8\n                Combine output records=6\n                Reduce input groups=4\n                Reduce shuffle bytes=84\n                Reduce input records=6\n                Reduce output records=4\n                Spilled Records=12\n                Shuffled Maps =2\n                Failed Shuffles=0\n                Merged Map outputs=2\n                GC time elapsed (ms)=3303\n                CPU time spent (ms)=8060\n                Physical memory (bytes) snapshot=470183936\n                Virtual memory (bytes) snapshot=6182424576\n                Total committed heap usage (bytes)=261361664\n        Shuffle Errors\n                BAD_ID=0\n                CONNECTION=0\n                IO_ERROR=0\n                WRONG_LENGTH=0\n                WRONG_MAP=0\n                WRONG_REDUCE=0\n        File Input Format Counters\n                Bytes Read=46\n        File Output Format Counters\n                Bytes Written=31\n\n```\n\n这些记录了该程序运行过程的的一些信息的计数，如Map input records=8，表示Map有8条记录。可以看出来这些内置计数器可以被分为若干个组，即对于大多数的计数器来说，Hadoop使用的组件分为若干类。 \n\n### 计数器列表\n\n| 组别                                             | 名称/类别                                                    |\n| ------------------------------------------------ | ------------------------------------------------------------ |\n| MapReduce任务计数器（Map-Reduce Framework）      | org.apache.hadoop.mapreduce.TaskCounter                      |\n| 文件系统计数器（File System Counters）           | org.apache.hadoop.mapreduce.FiIeSystemCounter                |\n| 输入文件任务计数器（File Input Format Counters） | org.apache.hadoop.mapreduce.lib.input.FilelnputFormatCounter |\n| 输出文件计数器（File Output Format Counters）    | org.apache.hadoop.mapreduce.lib.output.FileOutputFormatCounter |\n| 作业计数器（Job Counters）                       | org.apache.hadoop.mapreduce.JobCounter                       |\n\n大部分的Hadoop都有相应的计数器，可以对其进行追踪，方便处理运行中出现的问题，这些信息从应用角度又分为任务计数器和作业计数器：\n\n#### 任务计数器\n\n##### 内置MapReduce任务计数器\n\n| 计数器名称                                          | 说明                                                         |\n| --------------------------------------------------- | ------------------------------------------------------------ |\n| map输人的记录数(MAP_INPUT_RECORDS）                 | 作业中所有map已处理的输人记录数。每次RecordReader读到一条记录并将其传给map的map()函数时，该计数器的值递增 |\n| 分片（split）的原始字节数(SPLIT_RAW_BYTES)          | 由map读取的输人分片对象的字节数。这些对象描述分片元数据（文件的位移和长度），而不是分片的数据自身，因此总规模是小的 |\n| map输出的记录数(MAP_OUTPUT_RECORDS)                 | 作业中所有map产生的map输出记录数。每次某一个map 的OutputCollector调用collect()方法时，该计数器的值增加 |\n| map输出的字节数(MAP_OUTPUT_BYTES)                   | 作业中所有map产生的耒经压缩的输出数据的字节数·每次某一个map的OutputCollector调用collect()方法时，该计数器的值增加 |\n| map输出的物化字节数（MAP_OUTPUT_MATERIALIZED_BYTES) | map输出后确实写到磁盘上的字节数；若map输出压缩功能被启用，则会在计数器值上反映出来 |\n| combine输人的记录数(COMBINE_INPUT_RECORDS)          | 作业中所有combiner(如果有）已处理的输人记录数。combiner的迭代器每次读一个值，该计数器的值增加。注意：本计数器代表combiner已经处理的值的个数，并非不同的键组数（后者并无实所意文，因为对于combiner而言，并不要求每个键对应一个组。 |\n| combine输出的记录数(COMBINE_OUTPUT_RECORDS)         | 作业中所有combiner（如果有）已产生的输出记录数。每当一个combiner的OutputCollector调用collect()方法时，该计数器的值增加 |\n| reduce输人的组（REDUCE_INPUT_GROUPS）               | 作业中所有reducer已经处理的不同的码分组的个数。每当某一个reducer的reduce()被调用时，该计数器的值增加。 |\n| reduce输人的记录数（REDUCE_INPUT_RECORDS)           | 作业中所有reducer已经处理的输人记录的个数。每当某个reducer的迭代器读一个值时，该计数器的值增加。如果所有reducer已经处理数完所有输人，則该计数器的值与计数器\"map输出的记录\"的值相同。 |\n| reduce输出的记录数（REDUCE_OUTPUT_RECORDS）         | 作业中所有map已经产生的reduce输出记录数。每当某个reducer的OutputCollector调用collect()方法时，该计数器的值增加。 |\n| reduce经过shuffle的字节数(REDUCE_SHUFFLE_BYTES)     | 由shuffle复制到reducer的map输出的字节数。                    |\n| 溢出的记录数(SPILLED_RECORDS)                       | 作业中所有map和reduce任务溢出到磁盘的记录数                  |\n| CPU毫秒(CPU_MILLISECONDS)                           | 一个任务的总CPU时间，以毫秒为单位，可由/proc/cpuinfo获取     |\n| 物理内存字节数（PHYSICAL_MEMORY_BYTES）             | 一个任务所用的物理内存，以字节数为单位，可由/proc/meminfo获取 |\n| 虚拟内存字节数(VIRTUAL_MEMORY_BYTES）               | 一个任务所用虚拟内存的字节数，由/proc/meminfo获取            |\n| 有效的堆字节数(COMMITTED_HEAP_BYTES)                | 在JVM中的总有效内存最（以字节为单位），可由Runtime. getRuntime().totalMemory()获取 |\n| GC运行时间毫秒数(GC_TIME_MILLIS)                    | 在任务执行过程中，垃圾收集器(garbage collection）花费的时间（以毫秒为单位），可由GarbageCollector MXBean. getCollectionTime()获取 |\n| 由shuffle传输的map输出数(SHUFFLED_MAPS)             | 由shume传输到reducer的map输出文件数。                        |\n| 失敗的shuffle数(FAILED_SHUFFLE)                     | shuffle过程中，发生map输出拷贝错误的次数                     |\n| 被合并的map输出数（MERGED_MAP_OUTPUTS）             | shuffle过程中，在reduce端合并的map输出文件数                 |\n\n##### 内置文件系统任务计数器\n\n| 计数器名称                                 | 说明                                                         |\n| ------------------------------------------ | ------------------------------------------------------------ |\n| 文件系统的读字节数（BYTES_READ）           | 由map任务和reduce任务在各个文件系统中读取的字节数，各个文件系统分别对应一个计数器，文件系统可以是local、 HDFS、S3等 |\n| 文件系统的写字节数(BYTES_WRITTEN）         | 由map任务和reduce任务在各个文件系统中写的字节数              |\n| 文件系统读操作的数量(READ_OPS)             | 由map任务和reduce任务在各个文件系统中进行的读操作的数量（例如，open操作，filestatus操作） |\n| 文件系统大规模读操作的数量(LARGE_READ_OPS) | 由map和reduce任务在各个文件系统中进行的大规模读操作（例如，对于一个大容量目录进行list操作）的数量 |\n| 文件系统写操作的数量(WRITE_OPS)            | 由map任务和reduce任务在各个文件系统中进行的写操作的数量（例如，create操作，append操作） |\n\n##### 内置的输入文件任务计数器\n\n| 计数器名称               | 说明                                     |\n| ------------------------ | ---------------------------------------- |\n| 读取的字节数(BYTES_READ) | 由map任务通过FilelnputFormat读取的字节数 |\n\n##### 内置输出文件任务计数器\n\n| 计数器名称                | 说明                                                         |\n| ------------------------- | ------------------------------------------------------------ |\n| 写的字节数(BYTES_WRITTEN) | 由map任务（针对仅含map的作业）或者reduce任务通过FileOutputFormat写的字节数 |\n\n#### 作业计数器\n\n##### 内置的作业计数器\n\n| 计数器名称                                 | 说明                                                         |\n| ------------------------------------------ | ------------------------------------------------------------ |\n| 启用的map任务数（TOTAL_LAUNCHED_MAPS）     | 启动的map任务数，包括以“推测执行”方式启动的任务。            |\n| 启用的reduce任务数(TOTAL_LAUNCHED_REDUCES) | 启动的reduce任务数，包括以“推测执行”方式启动的任务。         |\n| 启用的uber任务数(TOTAL_LAIÆHED_UBERTASKS)  | 启用的uber任务数。                                           |\n| uber任务中的map数(NUM_UBER_SUBMAPS)        | 在uber任务中的map数。                                        |\n| Uber任务中的reduce数(NUM_UBER_SUBREDUCES)  | 在任务中的reduce数。                                         |\n| 失败的map任务数（NUM_FAILED_MAPS）         | 失败的map任务数。                                            |\n| 失败的reduce任务数(NUM_FAILED_REDUCES)     | 失败的reduce任务数                                           |\n| 失败的uber任务数(NIN_FAILED_UBERTASKS)     | 失败的uber任务数。                                           |\n| 被中止的map任务数（NUM_KILLED_MAPS）       | 被中止的map任务数。                                          |\n| 被中止的reduce任务数(NW_KILLED_REDUCES)    | 被中止的reduce任务数。                                       |\n| 数据本地化的map任务数（DATA_LOCAL_MAPS）   | 与输人数据在同一节点上的map任务数。                          |\n| 机架本地化的map任务数（RACK_LOCAL_MAPS)    | 与输人数据在同一机架范围内但不在同一节点上的map任务数。      |\n| 其他本地化的map任务数（OTHER_LOCAL_MAPS）  | 与输人数据不在同一机架范围内的map任务数。由于机架之间的带宽资源相对较少，Hadoop会尽量让map任务靠近输人数据执行，因此该计数器值一般比较小。 |\n| map任务的总运行时间(MILLIS_MAPS)           | map任务的总运行时间，单位毫秒。包括以推测执行方式启动的任务。可参见相关的度量内核和内存使用的计数器(VCORES_MILLIS_MAPS和MB_MILLIS_MAPS） |\n| reduce任务的总运行时间(MILLIS_REDUCES)     | reduce任务的总运行时间，单位毫秒。包括以推滌执行方式启动的任务。可参见相关的度量内核和内存使用的计数器(VQES_MILLIS_REARES和t*B_MILLIS_REUKES) |\n\n#### 自定义计数器\n\n虽然Hadoop内置的计数器比较全面，给作业运行过程的监控带了方便，但是对于那一些业务中的特定要求(统计过程中对某种情况发生进行计数统计)MapReduce还是提供了用户编写自定义计数器的方法。\n\n##### 过程\n\n1. 定义一个Java的枚举类型(enum)，用于记录计数器分组，其枚举类型的名称即为分组的名称，枚举类型的字段就是计数器名称。\n2. 通过Context类的实例调用getCounter方法进行increment(long incr)方法，进行计数的添加。\n\n##### 案例\n\n###### ReportTest\n\n```java\npublic enum ReportTest {                //定义枚举\n    ErroWord, GoodWord, ReduceReport\t//写入要记录的计数器名称\n}\n```\n\n###### Mapper类\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class TxtMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n\n        String[] words = value.toString().split(\" \");\n\n        for (String word : words) {\n            if (word.equals(\"GoodWord\")) {\n                context.setStatus(\"GoodWord is coming\");\n                context.getCounter(ReportTest.GoodWord).increment(1);\n            } else if (word.equals(\"ErroWord\")) {\n                context.setStatus(\"BadWord is coming!\");\n                context.getCounter(ReportTest.ErroWord).increment(1);\n            } else {\n                context.write(new Text(word), new IntWritable(1));\n            }\n\n        }\n    }\n}\n\n```\n\n###### Reducer类\n\n```java\n\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\nimport java.util.Iterator;\n\npublic class TxtReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int sum = 0;\n\n        Iterator<IntWritable> it = values.iterator();\n        while (it.hasNext()) {\n            IntWritable value = it.next();\n            sum += value.get();\n        }\n        if (key.toString().equals(\"hello\")) {\n            context.setStatus(\"BadKey is comming!\");\n            context.getCounter(ReportTest.ReduceReport).increment(1);\n        }\n        context.write(key, new IntWritable(sum));\n    }\n}\n```\n\n###### ToolRunnerJS\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.conf.Configured;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Counters;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.input.TextInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;\nimport org.apache.hadoop.util.Tool;\n\npublic class ToolRunnerJS extends Configured implements Tool {\n    public static void main(String[] args) throws Exception {\n        ToolRunnerJS tool = new ToolRunnerJS();\n        tool.run(null);\n    }\n\n    public int run(String[] args0) throws Exception {\n        //Configuration:MapReduce的类,向Hadoop框架描述MapReduce执行工作\n        Configuration conf = new Configuration();\n        String output = \"jishuqi1\";\n\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(ToolRunnerJS.class);\n        job.setJobName(\"jishu\");               //设置Job名称\n\n        job.setOutputKeyClass(Text.class); //设置Job输出数据 K\n        job.setOutputValueClass(IntWritable.class); //设置Job输出数据 V\n\n        job.setMapperClass(TxtMapper.class);\n        job.setReducerClass(TxtReducer.class);\n\n        job.setInputFormatClass(TextInputFormat.class);\n        job.setOutputFormatClass(TextOutputFormat.class);\n\n        FileInputFormat.addInputPath(job, new Path(\"/input/counter/*\")); //为 Job设置输入路径\n        FileOutputFormat.setOutputPath(job, new Path(\"/output/counter_result\")); //为Job设置输出路径\n\n        job.waitForCompletion(true);\n        Counters counters = job.getCounters();\n        System.out.println(\"Counter getGroupNames:\"+counters.getGroupNames());\n        return 0;\n    }\n}\n```\n\n#### 查看\n\n![FWk6ln.png](https://s1.ax1x.com/2018/12/28/FWk6ln.png)\n\n\n\n\n\n通过Web界面也可以查看但是需要设置设置\n\n```xml\n <property>\n           <name>mapreduce.jobhistory.address</name>\n\t       <value>datanode1:10020</value>\n           <description>MapReduce  JobHistory Server IPC host:port</description>\n</property>\n\n<property>\n        <name>mapreduce.jobhistory.webapp.address</name>\n        <value>datanode1:19888</value>\n        <description>MapReduce JobHistory Server Web UI host:port</description>\n</property>\n```\n\n启动服务\n\n```shell\nmr-jobhistory-daemon.sh start historyserver\n```\n\nweb界面查看\n\n![FWk5Y4.png](https://s1.ax1x.com/2018/12/28/FWk5Y4.png)\n\n## 最值\n\n最大值、最小值、平均值、均方差、众数、中位数等都是统计学中经典的数值统计，也是常用的统计属性字段，如果想知道最大的10个数，最小的10个数，这涉及到Top N/Bottom N 问题。\n\n###  单一最值\n\n常用的统计属性的字段在MapReduce的求解过程中，由一个大任务分解成若干个Mapper任务，最后会进行Reducer合并，比传统计算求解略显复杂，在MaoReduce框架中，会以Key进行分区、分组、排序的操作，在进行这些数值的操作时哦，只要设定合理的key，整个问题也就简单化了。使用Combiner可以减少Shuffle到Reduce端中间的K V的数目，减轻网络和IO的目的。\n\n#### 求解最大值最小值\n\n数据\n\n```\n2017-10 300\n2017-10 100\n2017-10 200\n2017-11 320\n2017-11 200\n2017-11 280\n2017-12 290\n2017-12 270\n```\n\n#### MinMaxWritable\n\n```java\nimport org.apache.hadoop.io.Writable;\n\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class MinMaxWritable implements Writable {\n    private int min;//记录最大值\n    private int max;//记录最小值\n\n    public int getMin() {\n        return min;\n    }\n\n    public void setMin(int min) {\n        this.min = min;\n    }\n\n    public int getMax() {\n        return max;\n    }\n\n    @Override\n    public String toString() {\n        return min + \"\\t\" + max;\n    }\n\n    public void setMax(int max) {\n        this.max = max;\n    }\n\n    public void write(DataOutput out) throws IOException {\n        out.writeInt(max);\n        out.writeInt(min);\n    }\n\n    public void readFields(DataInput in) throws IOException {\n        min = in.readInt();\n        max = in.readInt();\n\n    }\n}\n```\n\n#### MinMaxMapper\n\n```java\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Mapper;\n\nimport java.io.IOException;\n\npublic class MinMaxMapper extends Mapper<Object, Text, Text, MinMaxWritable> {\n    private MinMaxWritable outTuple = new MinMaxWritable();\n\n    @Override\n    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {\n\n        String[] words = value.toString().split(\" \");\n        String data = words[0]; //定义记录的日期的自定义变量data\n        if (data == null) {\n            return;  //如果该日期为空，返回\n        }\n        outTuple.setMin(Integer.parseInt(words[1]));\n        outTuple.setMax(Integer.parseInt(words[1]));\n        context.write(new Text(data), outTuple);  //将结果写入到context\n    }\n}\n```\n\n#### MinMaxReducer\n\n```java\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Reducer;\n\nimport java.io.IOException;\n\npublic class MinMaxReducer extends Reducer<Text, MinMaxWritable, Text, MinMaxWritable> {\n    private MinMaxWritable result = new MinMaxWritable();\n\n    @Override\n    protected void reduce(Text key, Iterable<MinMaxWritable> values, Context context) throws IOException, InterruptedException {\n        result.setMax(0);\n        result.setMin(0);\n        //按照key迭代输出value的值\n        for (MinMaxWritable value : values) {\n            //最小值放入结果集\n            if (result.getMin() == 0 || value.getMin() < result.getMin()) {\n                result.setMin(value.getMin());\n            }\n            //最大值放入结果集\n            if (result.getMax() == 0 || value.getMax() > result.getMax()) {\n                result.setMax(value.getMax());\n            }\n        }\n        context.write(key, result);\n    }\n}\n```\n\n#### MinMaxJob\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\nimport org.apache.hadoop.util.GenericOptionsParser;\n\npublic class MinMaxJob {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n        if (otherArgs.length != 2) {\n            System.err.println(\"Usage:MinMaxMapper<in><out>\");\n            System.exit(2);\n        }\n        Job job = Job.getInstance(conf);\n        job.setJarByClass(MinMaxJob.class);\n        job.setMapperClass(MinMaxMapper.class);\n        //启用Combiner 减少网络传输的数据量 \n        job.setCombinerClass(MinMaxReducer.class);\n        job.setReducerClass(MinMaxReducer.class);\n\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(MinMaxWritable.class);\n\n        FileInputFormat.addInputPath(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n}\n```\n\n#### 计算过程\n\n![FfaTAK.png](https://s1.ax1x.com/2018/12/29/FfaTAK.png)\n\n在一个MapReduce计算的过程中，Mapper任务相对于Reduce任务是大量的，因此少量的Reducer处理大量数据的并不明智，所以通过在Shuffle阶段引入Combiner，并把Reducer作为它的计算类，大大减少了Reducer端数据的输入，整个计算过程变得合理可靠。\n","tags":["MapReduce"],"categories":["大数据"]},{"title":"MapReduce源码刨析","url":"/2018/12/25/MapReduce编程刨析/","content":"\n {{ \"MapReduce编程刨析\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## Map\n\nmap函数是对一些独立元素组成的概念列表(如单词计数中每行数据形成的列表)的每一个元素进行指定的操作(如把每行数据拆分成不同单词,并把每个单词计数为1),用户可以自定义一个把数据拆分成不同单词并把单词计数为1的映射map函数),事实上每个元素都是被独立操作的,而原始列表没有被修改,因为这里创建了一个新的列表来保存新的答案。\n\n```java\npublic class Mapper<KEYIN, VALUEIN, KEYOUT, VALUEOUT> {\n\n  \n  //设定Context传递给 {@link Mapper} 实现\n  public abstract class Context\n    implements MapContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {\n  }\n  //在任务开始的时候调用一次 为map方法提供预处理一些内容\n  protected void setup(Context context\n                       ) throws IOException, InterruptedException {\n  }\n\n // 对输入分片里的key/value对调用一次，进行处理。\n  @SuppressWarnings(\"unchecked\")\n  protected void map(KEYIN key, VALUEIN value, \n                     Context context) throws IOException, InterruptedException {\n    context.write((KEYOUT) key, (VALUEOUT) value);\n  }\n\n  //任务结尾调用一次，进行扫尾工作。\n  protected void cleanup(Context context\n                         ) throws IOException, InterruptedException {\n  }\n    \n  public void run(Context context) throws IOException, InterruptedException {\n    setup(context);\n    try {\n      while (context.nextKeyValue()) {\n        map(context.getCurrentKey(), context.getCurrentValue(), context); //对key/value进行处理。\n      }\n    } finally {\n      cleanup(context);\n    }\n  }\n}\n```\n\n编写MapReduce程序时，Map都要继承Mapper类，Mapper有4种泛型：KEYIN,VALUEIN,KEYOUT,VALUEOUT。KEYIN,VALUEIN是输入数据(key,value)的值，KEYOUT,VALUEOUT是输出数据(key,value)的值。因为它们经常在节点间进行网络传输，所以继承Writable接口被封闭类的驱动。\n\n首先run()方法执行Map作业中的setup方法，它只在作业开始的时候调用一次处理Map作业需要的一些初始化作业。\n\n然后，通过while循环遍历context里的(key,value)对 ，对每一组需要重写map方法以满足业务需求，在map中有3个参数。分别是key,value,context。key作为输入的关键字，value为输入的值。他们是MapReduce过程用于传值的(key,value)，数据的输入是一批（key,value）,从源码    context.write((KEYOUT) key, (VALUEOUT) value);可以看出生成结果也是一对(key,value),然后将其写入context。\n\n**因为MapReduce 是基于集群运算的框架，因此key和value的值为了满足集群之间的网络传输的规则，需要支持序列化和反序列化，而且整个MapRedcue过程会按照key进行排序分组，因此key必须实现WritableComparable接口，**保证MapReduce对数据输出的结果执行进行相应的排序操作。\n\n最后调用cleanup方法做最后的处理。它只在MapReduce进行结束的时候执行一次进行作业的扫尾工作。\n\n### 代码\n\n```java\npublic class WordCountMapper extends Mapper<LongWritable, Text, Text, IntWritable> {\n    @Override\n    protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n        //key记录的是数据的偏移位置，value是每次分片提供给我们的读取一行数据。\n        //Map读数据时按分片给的内容一行一行来读取的。\n        String[] words = value.toString().split(\" \"); //每一行数据拆按照“ ”拆分放入字符数组words\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n        for (String word : words) {\n            context.write(new Text(word), new IntWritable(1));  //每个单词当key，并赋值1\n        }\n\n    }\n```\n\n- 第1个参数类型**LongWritable**：输入key类型，记录数据分片的偏移量。\n- 第2个参数类型**Text**：输入value，对应分片中的文本数据。\n- 第3个参数**Text**：输出key，对应map方法计算的key值。\n- 第4个参数**IntWritable**：输出value，对应map计算的value值。\n\nMapper从分片后传出的上下文接收数据以LongWritable, Text为(key,value)接收，然后重写map方法，默认设置一行一行读取数据并以(key,value)的形式进行便利.最后经过context.write方法按照Mapper类中定义的输出格式(Text, IntWritable)写入上下文。给Mapper Redcuer 等支持Context传输程序使用。\n\n## Reduce\n\nReducer获取Mapper任务输出的已经完成的地址信息后，系统会启用复制程序，将需要的数据复制到本地存储空间，如果Mapper输出很小，会复制到Reducer的内存区域。否则会复制到磁盘上，随着复制内容的增加，Reduce作业批量地启动合并任务，执行合并操作，启动Reducer类后接收上下文地数据进行Reduce任务。\n\n```java\npublic class Reducer<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {\n\n  // 设定Context传递给{@link Reducer}实现，即获得Context的内容\n  public abstract class Context \n    implements ReduceContext<KEYIN,VALUEIN,KEYOUT,VALUEOUT> {\n  }\n\n   //任务开始调用一次，为reduce方法提供预处理的一些内容\n  protected void setup(Context context\n                       ) throws IOException, InterruptedException {\n  }\n   //对key/value进行处理\n  @SuppressWarnings(\"unchecked\")\n  protected void reduce(KEYIN key, Iterable<VALUEIN> values, Context context\n                        ) throws IOException, InterruptedException {\n    for(VALUEIN value: values) {//迭代获取context的数据\n      context.write((KEYOUT) key, (VALUEOUT) value); //将计算结果写入context\n    }\n  }\n\t//在任务结尾调用一次进行一次扫尾工作\n  protected void cleanup(Context context\n                         ) throws IOException, InterruptedException {\n\n  }\n//Reducer类的驱动方法\n  public void run(Context context) throws IOException, InterruptedException {\n    setup(context);\n    try {\n      while (context.nextKey()) {\n        reduce(context.getCurrentKey(), context.getValues(), context);\n        //如果使用备份存储，请将其重置\n        Iterator<VALUEIN> iter = context.getValues().iterator();\n        if(iter instanceof ReduceContext.ValueIterator) {\n          ((ReduceContext.ValueIterator<VALUEIN>)iter).resetBackupStore();        \n        }\n      }\n    } finally {\n      cleanup(context);  //扫尾\n    }\n  }\n}\n\n```\n\n任何一个Reduce任务都会继承Reducer类，有4个值分别是：KEYIN,VALUEIN,KEYOUT,VALUEOUT。\n\nKEYIN，VALUEIN是Reducer接收来自Mapper的输出，故Writable类型要和Mapper类中的KEYOUT、VALUEIN指定输出的key/value数据类型是一 一对应的。每个Reducer类接收的数据并不是Mapper传出的数据量，而是shuffle过程分区决定的，一般一个分区对应一个Reducer类，当只有一个Reducer类时，可以接收所有分区的数据。\n\nReducer的结构和Mapper源码结构十分相似，run方法的驱动Reducer的任务，执行顺序时setup→while→cleanup，其中setup与clean方法分别提供了对预执行和扫尾的操作和支持。分别在Reducer任务执行前执行一次，在Reducer任务后结尾执行一次。  while (context.nextKey())判断所在的Reducer类中（一般一个Reducer类对应一个，一个分区接收一组或多组由Map任务输出的key/value对的值）相同的key及相应的值一定在一个分区。）是否有下一个分区，如果有则会把相同key对应的值放到一块传给reduce方法进行处理。reducer有KEYIN key, Iterable <.VALUEIN./> values, Context context 共3个形式参数，其中 key时whiel条件判定的key，values就是与该vaues和key相同的key的所有值，然后会根据for循环把他们写入到上下文中。\n\nreduce方法将传过来的数据按照key进行排序。Reduce任务接收的数据来自Map任务的输出，中间经过shuffle分区、排序、分组，正式给reduce方法处理。\n\n### 代码\n\n```java\npublic class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n\t//reduce方法重写\n    @Override\n    protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n        int total = 0;  //初始化变量为0\n        for (IntWritable value : values) {\n            total += value.get();//将相同的单词对应的值加一起\n        }\n        context.write(key, new IntWritable(total));//结果写入上下文\n    }\n}\n```\n\n## Driver\n\n```java\npublic class WordCountDriver {\n    public static void main(String[] args) throws Exception {\n        Job job = Job.getInstance(new Configuration()); //获取环境变量\n\n        job.setJarByClass(WordCountDriver.class);  //指定驱动类\n\n        job.setMapperClass(WordCountMapper.class); //指定Map类\n        job.setMapOutputKeyClass(Text.class);\t//map K\n        job.setMapOutputValueClass(IntWritable.class);\t//map v\n\n        job.setReducerClass(WordCountReducer.class); //指定reducer类\n        job.setOutputKeyClass(Text.class);\t//reduce k\n        job.setOutputValueClass(IntWritable.class); //reduce v\n\n        FileInputFormat.setInputPaths(job, new Path(args[0])); //任务输如路径\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\t//任务输出路径\n\n        job.waitForCompletion(true);\n\n    }\n}\n\n```\n\n  首先获取Job的实例，并创建环境变量的实例conf赋值于Job的构造方法，在job作业中set方法只有作业被提交之后才起作用，之后他们将抛出一个IllegalStateException异常。通常，用户创建应用程序，通过Job描述作业各个方面，然后提交作业监视其进度。7-13行指定map和reduce的输入输出文件类型。FileInputFormat继承InputFormat类，主要完成输入路径的设置。FileOutputFormat继承OutputFormat类通过setOutputPath方法指定Job作业执行完成结果的输出路径，对于Shuffle过程默认的分区、分组、排序、如果不能满足任务要求，也可以自定义指定。\n\n## 过程\n\n1. 检查作业提交输入输出样式的细节。\n\n2. 为作业计算InputSplit值。\n\n3. 如果需要的话，为作业的DistributedCahe建立统计信息。\n\n4. 复制作业的jar包和配置文件到FileSystem上的MapReduce系统目录下。\n\n5. 提交作业到ResourceManager并且监控它的状态。\n\n### 作业Job输入\n\n1. 检查作业的有效性。\n\n2. 检查作业输入的有效性。\n\n3. 提供RecordReader的实现，这个RecordReadr从逻辑InputSplit中输入记录，这些记录将由Mapper处理。\n\n### 作业Job输出\n\n1. 检查作业的输出，检查路径是否已经存在\n2. 提供一个RecordWriter的实现，用来输出作业加过,TextOutputFormat是默认的OutputFormat，输出文件被保存在FileSystem上。\n\n## Mapper输入\n\nMapper的输入本质上来讲是源自于HDFS上存储的数据，这些数据进入Mapper计算之前有个分片的过程，它主要将HDFS上的Block在进行map之前重新划分，生成一组记录分片长度和一个记录数据位置的数组，进而内部形成记录数组位置的值key vakye扩及然后传给Mapper计算，这里 key和value的类型由一套默认的类型机制，同时也是向用户开放的。\n\n### setInputFormatClass\n\n```java\n  public void setInputFormatClass(Class<? extends InputFormat> cls\n                                  ) throws IllegalStateException {\n    ensureState(JobState.DEFINE);\n    conf.setClass(INPUT_FORMAT_CLASS_ATTR, cls, \n                  InputFormat.class);\n  }\n```\n\n这里有一个很重要的类InputFormat，它位于“package org.apache.hadoop.mapreduce”中，一共包含两种方法getSplits和createRecorecordReader\n\n```java\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic abstract class InputFormat<K, V> {\n\n   //对输入的数据进行分片\n  public abstract  List<InputSplit> getSplits(JobContext context ) throws IOException, InterruptedException;\n  \n  //获取分片中的数据\n  public abstract RecordReader<K,V> createRecordReader(InputSplit split,TaskAttemptContext context   ) throws IOException, InterruptedException;\n}\n```\n\ngetSplits对输入数据进行切片，最终获取一个InputSplit的返回列表。\n\n```java\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic abstract class InputSplit {\n\t//获取分片split的大小，以便分片按其排序，并返回分片的字节数据\n  public abstract long getLength() throws IOException, InterruptedException;\n\t//获取分片所在本地的命名列表(本地不需要序列化)，并返回一个新的节点数组\n  public abstract  String[] getLocations() throws IOException, InterruptedException;\n  //返回分片数据存储每一个位置的拆分信息列表，如果是空值表示所有的位置都有数组存储在磁盘上\n  @Evolving\n  public SplitLocationInfo[] getLocationInfo() throws IOException {\n    return null;\n  }\n}\n```\n\n createRecorder方法获得一个RecordReader的返回值源码信息如下\n\n```java\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {\n //初始化调用一次.\n  public abstract void initialize(InputSplit split, TaskAttemptContext context ) throws IOException, InterruptedException;\n//判断下一个key/value是否存在，如果存在则返回true\n  public abstract  boolean nextKeyValue() throws IOException, InterruptedException;\n  //获取当前的key。如果存在，则返回true\n  public abstract KEYIN getCurrentKey() throws IOException, InterruptedException;\n  //获取当前的值，返回读取的对象\n  public abstract  VALUEIN getCurrentValue() throws IOException, InterruptedException;\n  //记录 record reader通过数据的当前处理进度，返回0.0~1.0之间的数字。用于标记当前的进度。\n  public abstract float getProgress() throws IOException, InterruptedException;\n  //关闭recorde reader\n  public abstract void close() throws IOException;\n}\n```\n\nRecorder主要的功能是将数据拆分成KV对，然后传递给Map任务。\n\n### TextInputFormat\n\n输入采用的默认格式，如果Job对象不指定，系统默认会运行它，如果指定的话：\n\n```java\njob.setInputFormatClass(TextInputFormat.class);\n```\n\nTextInputFormat包含RecordReader和isSplitable两种方法位于package org.apache.hadoop.mapreduce.lib.input;\n\n```java\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic class TextInputFormat extends FileInputFormat<LongWritable, Text> {\n//定义文本的读取方式，是通过RecordReader返回的RecordReader<LongWritable, Text> 类实现的\n  @Override\n  public RecordReader<LongWritable, Text> createRecordReader(InputSplit split,TaskAttemptContext context) {\n    String delimiter = context.getConfiguration().get(\n        \"textinputformat.record.delimiter\");\n    byte[] recordDelimiterBytes = null;\n    if (null != delimiter)\n      recordDelimiterBytes = delimiter.getBytes(Charsets.UTF_8);//采用UTF_8编码\n    return new LineRecordReader(recordDelimiterBytes); //返回LineRecordReader实例\n  }\n//判断是否分片吗，如果分片返回true\n  @Override\n  protected boolean isSplitable(JobContext context, Path file) {\n  //根据文件后缀名来查找文件file的相关压缩编码器\n    final CompressionCodec codec =\n      new CompressionCodecFactory(context.getConfiguration()).getCodec(file);\n    if (null == codec) {\n      return true;// 没有压缩，返回true\n    }\n    //返回SplittableCompressionCodec的编码器实例。\n    return codec instanceof SplittableCompressionCodec;\n  }\n}\n```\n\nTextInputFormat以(longWrite，Text)形式继承了FileinputFormat类的逻辑，重写了isSplittable方法():\n\n```java\n  protected boolean isSplitable(JobContext context, Path filename) {\n    return true;\n  }\n```\n\n代码设定了默认分片的格式，在TextInputFormat类的isSplittable()方法，代码加入了压缩的判定，如果没有压缩，则设定为可分片，如果有压缩，返回的是分片压缩的解码器的实例。\n\ncreateRecorderReader()是定义文本文件读取方式，实际文件读取时通过它返回的RecordReader(LongWritable,Text)的字类LineRecordReader的实例位于(package org.apache.hadoop.mapreduce.lib.input;)在源码的207行-215行\n\n```java\n @Override\n  public LongWritable getCurrentKey() { //指定获取key类型 Mapper获取输入key的类型\n    return key;\n  }\n\n  @Override\n  public Text getCurrentValue() {  //指定获取value的类型也就是Mapper要获取输入value的类型\n    return value;\n  }\n```\n\n如果该条数据存在两个Block中\n\n```java\n public void initialize(InputSplit genericSplit, TaskAttemptContext context) throws IOException {\n    FileSplit split = (FileSplit) genericSplit;\n    Configuration job = context.getConfiguration();\n    this.maxLineLength = job.getInt(MAX_LINE_LENGTH, Integer.MAX_VALUE);\n    start = split.getStart();\n    end = start + split.getLength();\n    final Path file = split.getPath();\n\n    final FileSystem fs = file.getFileSystem(job);\n    fileIn = fs.open(file);\n    \n    CompressionCodec codec = new CompressionCodecFactory(job).getCodec(file);\n    if (null!=codec) {\n      isCompressedInput = true;\t\n      decompressor = CodecPool.getDecompressor(codec);\n      if (codec instanceof SplittableCompressionCodec) {\n        final SplitCompressionInputStream cIn =\n          ((SplittableCompressionCodec)codec).createInputStream(\n            fileIn, decompressor, start, end,\n            SplittableCompressionCodec.READ_MODE.BYBLOCK);\n        in = new CompressedSplitLineReader(cIn, job,\n            this.recordDelimiterBytes);\n        start = cIn.getAdjustedStart();\n        end = cIn.getAdjustedEnd();\n        filePosition = cIn;\n      } else {\n        in = new SplitLineReader(codec.createInputStream(fileIn,\n            decompressor), job, this.recordDelimiterBytes);\n        filePosition = fileIn;\n      }\n    } else {\n      fileIn.seek(start);\n      in = new UncompressedSplitLineReader(\n          fileIn, job, this.recordDelimiterBytes, split.getLength());\n      filePosition = fileIn;\n    }\n   //if的判断条件是start != 0,即从第二行开始读取数据，那么第一行数据去哪里么呢\n    if (start != 0) {\n      start += in.readLine(new Text(), 0, maxBytesToConsume(start));\n    }\n    this.pos = start;\n  }\n```\n\n为了保证数据的第一行被切断的时候正确读取，并没有判断数据是否被切断，而是一视同仁地除了第一个split，其他所有split都经过if的判定，全部从第二行开始读数据，当然到达split结尾时总是再多读一行，这样就避开了数据被切断的烦恼。如果最后一个split的结尾没有下一行了呢：\n\n```java\n public boolean nextKeyValue() throws IOException {\n    if (key == null) {\n      key = new LongWritable();\n    }\n    key.set(pos);\n    if (value == null) {\n      value = new Text();\n    }\n    int newSize = 0;\n    // 使用的判定条件计算当前位置小于或等于split的结尾位置，即当前已处于split的结尾位置时，while依然会再执行一次，那么结束，这样就解决了InputSplit读取的跨界问题。\n    // split limit i.e. (end - 1)\n    while (getFilePosition() <= end || in.needAdditionalRecordAfterSplit()) {\n      if (pos == 0) {\n        newSize = skipUtfByteOrderMark();\n      } else {\n        newSize = in.readLine(value, maxLineLength, maxBytesToConsume(pos));\n        pos += newSize;\n      }\n\n      if ((newSize == 0) || (newSize < maxLineLength)) {\n        break;\n      }\n\n      // line too long. try again\n      LOG.info(\"Skipped line of size \" + newSize + \" at pos \" + \n               (pos - newSize));\n    }\n    if (newSize == 0) {\n      key = null;\n      value = null;\n      return false;\n    } else {\n      return true;\n    }\n  }\n```\n\n\n\n### 优化策略\n\n作为Mapper输入，分片是一个很重要的环节，它主要将HDFS上的Block再进行Map计算之前进行逻辑划分，通常情况下分片大小和HDFS的Block块大小一样，也可以自定义。\n\n````java\n /**\n   *isSplitable方法确定文件是否分片\n   *如果文件可以拆分，此处设定分片为真\n   *否则如压缩文件不持支拆分的，则不进行拆分\n   */\nprotected boolean isSplitable(JobContext context, Path filename) {\n    return true;\n  }\n\n  public static void setInputPathFilter(Job job, Class<? extends PathFilter> filter) {\n    job.getConfiguration().setClass(PATHFILTER_CLASS, filter, PathFilter.class);\n  }\n\n  public static void setMinInputSplitSize(Job job,long size) {\n    job.getConfiguration().setLong(SPLIT_MINSIZE, size);\n  }\n  /**\n  *获取由格式强加的分片大小的下限，默认值是1。\n  */\n  public static long getMinSplitSize(JobContext job) {\n    return job.getConfiguration().getLong(SPLIT_MINSIZE, 1L);\n  }\n\n  public static void setMaxInputSplitSize(Job job, long size) {\n    job.getConfiguration().setLong(SPLIT_MAXSIZE, size);\n  }\n  //返回一个分片中最大的有效字符数\n  public static long getMaxSplitSize(JobContext context) {\n    return context.getConfiguration().getLong(SPLIT_MAXSIZE, Long.MAX_VALUE);\n  }\n\n  public static PathFilter getInputPathFilter(JobContext context) {\n    Configuration conf = context.getConfiguration();\n    Class<?> filterClass = conf.getClass(PATHFILTER_CLASS, null,\n        PathFilter.class);\n    return (filterClass != null) ?\n        (PathFilter) ReflectionUtils.newInstance(filterClass, conf) : null;\n  }\n\n\n  protected List<FileStatus> listStatus(JobContext job\n                                        ) throws IOException {\n    Path[] dirs = getInputPaths(job);\n    if (dirs.length == 0) {\n      throw new IOException(\"No input paths specified in job\");\n    }\n//用getSplits方法生成文件列表并将其制作成FileSplits\n  public List<InputSplit> getSplits(JobContext job) throws IOException {\n//返回getFormatMinSplitSize，getMinSplitSize的较大值。\n    StopWatch sw = new StopWatch().start();\n    long minSize = Math.max(getFormatMinSplitSize(), getMinSplitSize(job));\n    long maxSize = getMaxSplitSize(job);\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n for (FileStatus file: files) {\n \t//文件分片为真的话进行分片大小的计算\n      Path path = file.getPath();\n      long length = file.getLen();\n      if (length != 0) {\n        BlockLocation[] blkLocations;\n        if (file instanceof LocatedFileStatus) {\n          blkLocations = ((LocatedFileStatus) file).getBlockLocations();\n        } else {\n          FileSystem fs = path.getFileSystem(job.getConfiguration());\n          blkLocations = fs.getFileBlockLocations(file, 0, length);\n        }\n        //文件分片为真的话，进行分片大小的计算。\n        if (isSplitable(job, path)) {\n          long blockSize = file.getBlockSize();\n          long splitSize = computeSplitSize(blockSize, minSize, maxSize);\n\n          long bytesRemaining = length;\n          while (((double) bytesRemaining)/splitSize > SPLIT_SLOP) {\n            int blkIndex = getBlockIndex(blkLocations, length-bytesRemaining);\n            splits.add(makeSplit(path, length-bytesRemaining, splitSize,\n                        blkLocations[blkIndex].getHosts(),\n                        blkLocations[blkIndex].getCachedHosts()));\n            bytesRemaining -= splitSize;\n          }\n\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\n//分片大小的计算\n protected long computeSplitSize(long blockSize, long minSize,long maxSize) {\n    return Math.max(minSize, Math.min(maxSize, blockSize));\n  }\n````\n\n最小分片大小通常是1个字节，最大分片大小默认是由Java的long类型表示的最大值(Long.MAX_VALUE),只有把它的值设置成小于HDFS Block才有效果。computeSplitSize(blockSize, minSize, maxSize)计算分片的的大小。再默认情况下minSize<blockSize<maxSize。因此分片通常情况下就是HDFS的Block块的大小。\n\n这些值可以通过mapred.min.split.size、mapred.min.split.size、mapred.max.split.size和dfs.block.size，进行设定。\n\n默认情况下 TextInputformat 对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个 maptask，这样如果有大量小文件，就会产生大量的maptask，处理效率极其低下。 \n （1）最好的办法，在数据处理系统的最前端（预处理/采集），将小文件先合并成大文件，再上传到 HDFS 做后续分析。 \n （2）补救措施：如果已经是大量小文件在 HDFS 中了，可以使用另一种 InputFormat来做切片。（CombineTextInputFormat），它的切片逻辑跟 TextFileInputFormat 不同：它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个 maptask。 \n （3）优先满足最小切片大小，不超过最大切片大小 。\n\n```java\nCombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m   CombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m \n// 举例：0.5m+1m+0.3m+5m=2m + 4.8m=2m + 4m + 0.8m \n```\n\n```java\n//  如果不设置 InputFormat,它默认用的是 TextInputFormat.class \njob.setInputFormatClass(CombineTextInputFormat.class) CombineTextInputFormat.setMaxInputSplitSize(job, 4194304);// 4m \nCombineTextInputFormat.setMinInputSplitSize(job, 2097152);// 2m \n```\n\n## 分区\n\n分区是划分键值空间，Partitioner负责控制Map输出结果key的分隔，key(或者key子集)被用于产生分区，通常使用Hash函数。分区的数目与一个作业的Reduce任务的数目时一样的，因此，Partitioner控制将中间过程key（也就是这条记录）发送给$$m$$个Reduce任务中的哪一个来进行Reduce操作，它位于“org.apche.hadoop.mapreduce”中：\n\n```java\npublic abstract class Partitioner<KEY, VALUE> {\n  public abstract int getPartition(KEY key, VALUE value, int numPartitions);\n}\n```\n\n![F2jiuj.png](https://s1.ax1x.com/2018/12/27/F2jiuj.png)\n\n### HashPartitioner\n\n它位于“org.apache.hadoop.mapreduce.lib.partition”中式是默认的Partition。\n\n```java\npublic class HashPartitioner<K, V> extends Partitioner<K, V> {\n  public int getPartition(K key, V value, int numReduceTasks) {\n    return (key.hashCode() & Integer.MAX_VALUE) % numReduceTasks;\n  }\n}\n```\n\n其中key和value式Map的输出的（K,V）numReduceTasks式Reduce的任务数。Job的Reduce任务数可以指定：\n\n```java\njob.setNumberTask(3);//设定Reduce任务数量\n```\n\n此时getPartition的numReduceTasks值为3.\n\n用key.hashCode() 和 Integer.MAX_VALUE) 进行与操作，保证了数据的整数表达，再和numReduceTasks进行取余操作，保证了key值被大致分配给相应的Reduce任务，保证任务分配的均衡性。\n\n```java\njob.setPartitionerClass(HashPartitioner.class);\n```\n\n此代码不写，默认是HashPartitioner。\n\n### TotalOrderPartitioner\n\n分区过程通过从外部生成的源文件中读取分割点来影响总体顺序。这个类可以实现输出的全排序。这个类不是基于Hash的。他的getPartitione方法如下：\n\n```java\npublic int getPartition(K key, V value, int numPartitions) {\n  return partitions.findPartition(key);\n}\n```\n\n### KeyFieldBasedPartitioner\n\nKeyFieldBasedPartitioner是基于Hash的Partitioner，它提供了多个区间计算Hash，当区间数为0时，KeyFieldBasedPartitioner退化成HashPartitioner：\n\n````java\npublic int getPartition(K2 key, V2 value, int numReduceTasks) {\n    byte[] keyBytes;\n\n    List <KeyDescription> allKeySpecs = keyFieldHelper.keySpecs();\n    if (allKeySpecs.size() == 0) {\n      return getPartition(key.toString().hashCode(), numReduceTasks);\n    }\n\n    try {\n      keyBytes = key.toString().getBytes(\"UTF-8\");\n    } catch (UnsupportedEncodingException e) {\n      throw new RuntimeException(\"The current system does not \" +\n          \"support UTF-8 encoding!\", e);\n    }\n    // return 0 if the key is empty\n    if (keyBytes.length == 0) {\n      return 0;\n    }\n    \n    int []lengthIndicesFirst = keyFieldHelper.getWordLengths(keyBytes, 0, \n        keyBytes.length);\n    int currentHash = 0;\n    for (KeyDescription keySpec : allKeySpecs) {\n      int startChar = keyFieldHelper.getStartOffset(keyBytes, 0, \n        keyBytes.length, lengthIndicesFirst, keySpec);\n       // no key found! continue\n      if (startChar < 0) {\n        continue;\n      }\n      int endChar = keyFieldHelper.getEndOffset(keyBytes, 0, keyBytes.length, \n          lengthIndicesFirst, keySpec);\n      currentHash = hashCode(keyBytes, startChar, endChar, \n          currentHash);\n    }\n    return getPartition(currentHash, numReduceTasks);\n  }\n````\n\n### BinaryPartitioner\n\nBinaryPartitioner继承Partitioner<BinaryComparable, V> 是Partitioner的偏特化子类该类提供两个偏移量：\n\n```java\nmapreduce.partition.binarypartitioner.left.offset  //数组左偏移量（默认为0）\nmapreduce.partition.binarypartitioner.right.offset //数组右偏移量(默认为0)\nleftOffset = conf.getInt(LEFT_OFFSET_PROPERTY_NAME, 0);  \nrightOffset = conf.getInt(RIGHT_OFFSET_PROPERTY_NAME, -1);\n```\n\n在计算任何一个Reduce任务是仅仅对键值K的[rightOffset,leftOffset]这个区间区Hash。分区BinaryComparable键使用BinaryComparable键使用BinaryComparable.getBytes()返回的bytes数组的可配置部分。它的部分源码如下：\n\n```java\n  public int getPartition(BinaryComparable key, V value, int numPartitions) {\n    int length = key.getLength();\n    int leftIndex = (leftOffset + length) % length;\n    int rightIndex = (rightOffset + length) % length;\n    int hash = WritableComparator.hashBytes(key.getBytes(), \n      leftIndex, rightIndex - leftIndex + 1);\n    return (hash & Integer.MAX_VALUE) % numPartitions;\n  } \n}\n```\n\n### 自定义Partition\n\n```java\npublic class MyPartitioner extends Partitioner<Text, Text> { //定义分区名\n    @Override\n    public int getPartition(Text key, Text value, int numReduceTasks) {//重写分区防火阀\n        return (Integer.parseInt(key.toString()) & Integer.MAX_VALUE) % numReduceTasks;\n    }\n}\n```\n\njob中引用自定义分区\n\n```java\n job.setPartitionerClass(MyPartitioner.class);\n```\n\n## 排序\n\n排序Sort式MapReduce计算中的核心部分，默认按照字典排序，优势按照业务需求，就需要自定义排序，自定义排序编写排序时候要继承WritableComparator类，重写compare计算方法，对于接收key类型可以通过当前的构造方法super来指定。\n\n```java\npublic class MySort extends WritableComparator {\t\t\t\t//自定义排序名称\n    public MySort() {\n        super(IntWritable.class, true); //因为Shuffle过程是以key进行排序，这里指定keytWritablel类型\n    }\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) { //重写compare方法\n        IntWritable v1 = (IntWritable) a;\n        IntWritable v2 = (IntWritable) b;\n        return v2.compareTo(v1);\n    }\n}\n```\n\njob中引用自定义排序\n\n```java\njob.setSortComparatorClass(MySort.class);\n```\n\n## 分组\n\n默认情况下，reduce方法每次接收的是一组相同key的value值，所以每个reduce方法每次只能通过相同key所对应的值进行计算。但有时用户会期望不同的key所对应的value值能再一次reduce方法调用时进行操作。这样的期望与默认的行为不符合，此时需要用户进行自定义分组的操作。\n\n```java\npublic class MyGroupSort extends WritableComparator {\t//定义分组名称\n    public MyGroupSort() {\n        super(IntWritable.class, true); //指定key的writable类型\n    }\n    \n    @Override   //重写compare方法\n    public int compare(WritableComparable a, WritableComparable b) {\n        IntWritable v1 = (IntWritable) a;\n        IntWritable v2 = (IntWritable) b;\n\n        if (v1.get() > 10) {\n            return 0;   //表示同一数组\n        } else {\n            return -1; //代表不是同一数组\n        }\n    }\n}\n```\n\njob中引用自定义分组\n\n```java\njob.setGroupingComparatorClass(MyGroupSort.class);\n```\n\n## Combiner\n\nShuffle运行原理可以知道，启用Combiner可以减少磁盘和网络的IO，Combiner时相当于本地的Reduce进行计算，把相同 的key累加在一起，如果再RPC传输之前把相同的key进行规约。即不应先给最终的结果，又可以减轻网络传输压力。\n\n![FRFGOf.png](https://s1.ax1x.com/2018/12/27/FRFGOf.png)\n\nCombiner实现了在RPC传输之前对相同key的值进行了一次类似Reduce的计算操作，累加了值。然后把key和累加后的值作为k v通过RPC传输给Reduce。\n\n```java\npublic class WordCount_combiner_job {\n\n\n    public static class WordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            System.out.println(\"split:<\" + key + \":  \" + value + \">\");\n\n            String[] words = value.toString().split(\" \");\n            for (String word : words) {\n                System.out.println(\"split:<\" + key + \":  \" + word + \">\");\n                context.write(new Text(word), new IntWritable(1));\n\n            }\n        }\n    }\n\n    public static class WordCountReducer extends Reducer<Text, IntWritable, Text, IntWritable> {\n        @Override\n        protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int total = 0;\n            for (IntWritable value : values) {\n                total += value.get();\n            }\n            context.write(key, new IntWritable(total));\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n\n        Configuration conf = new Configuration();\n        Job job = Job.getInstance(conf); //获取环境变量\n\n        job.setJarByClass(WordCount_combiner_job.class); //设置jar包\n        job.setJobName(\"WordCount\");\n        job.setMapperClass(WordCountMap.class); //map作业\n        job.setMapOutputKeyClass(Text.class);\n        job.setMapOutputValueClass(IntWritable.class);\n\n        job.setCombinerClass(WordCountReducer.class);//在此处设置ombinerClass                                                                                                                                    \n        job.setReducerClass(WordCountReducer.class); //reduce作业\n        job.setOutputKeyClass(Text.class); // k\n        job.setOutputValueClass(IntWritable.class); //v\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.waitForCompletion(true);\n\n    }\n\n}\n```\n\n###  SVG业务\n\n有些业务在应用Combiner时必须仔细考虑一些问题，否则就会出错：\n\n```\navg1.txt                       avg2.txt\n20\t\t\t\t\t\t\t 25\n10\t\t\t\t\t\t\t 17\n3\t\t\t\t\t\t\t\n```\n\n### 错误代码\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TxSVG_Erro_job {\n    public static class TxSVGMapper extends Mapper<LongWritable, Text, IntWritable, IntWritable> {\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            context.write(new IntWritable(1), new IntWritable(Integer.parseInt(value.toString())));\n        }\n    }\n\n    public static class TxSVGReducer extends Reducer<IntWritable, IntWritable, IntWritable, IntWritable> {\n\n        @Override\n        protected void reduce(IntWritable key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n            int totoal = 0;\n            int count = 0;\n            for (IntWritable value : values) {\n                totoal += value.get();\n                count++;\n            }\n            context.write(new IntWritable(1), new IntWritable(totoal / count));\n        }\n    }\n\n    public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException {\n        Configuration conf = new Configuration(); //获取环境变量\n        Job job = Job.getInstance(conf);    //实例化任务\n\n        job.setJobName(\"AVG_ERRO\");  //设置任务名\n        job.setJarByClass(TxSVG_Erro_job.class); //设置指定jar\n\n        job.setOutputKeyClass(IntWritable.class);  //设置输出k\n        job.setOutputValueClass(IntWritable.class); //设置输出v\n\n        job.setMapperClass(TxSVGMapper.class); //设置map类\n\n        job.setCombinerClass(TxSVGReducer.class);\n        job.setReducerClass(TxSVGReducer.class);\n\n        FileInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n\n        job.waitForCompletion(true);\n    }\n```\n\n计算如图：\n\n![FReMtS.png](https://s1.ax1x.com/2018/12/27/FReMtS.png)\n\n正确的结果应该是(20+10+3+25+17)/5=15，\n\n但是WordCOunt应用Combiner的求法求SVG会出现错误，如果代码不变的情况下去掉        job.setCombinerClass(TxSVGReducer.class);可以提获得正确的结果，但是如果去掉COmbiner，整个数据都会全给一个Reduce计算，如果数据量大会导致Reduce任务所在的节点资源会出现宕机。\n\n### 思路\n\n1. 定义一个Writable用于存储数据量的值的平均值\n2. 计算总和，用Writable中的数据乘以品滚之来反推回总值。\n3. 计算平均值,平均值=综合/总共数据量。\n\n#### TxtSVG_Writable\n\n```java\nimport org.apache.hadoop.io.Writable;\nimport java.io.DataInput;\nimport java.io.DataOutput;\nimport java.io.IOException;\n\npublic class TxtSVG_Writable implements Writable {\n    private int count = 0;\n    private int average = 0;\n\n    public int getCount() {\n        return count;\n    }\n\n    public void setCount(int count) {\n        this.count = count;\n    }\n\n    public int getAverage() {\n        return average;\n    }\n\n    public void setAverage(int average) {\n        this.average = average;\n    }\n\n    @Override\n    public String toString() {\n        return count + \"\\t\" + average;\n    }\n\n    public void readFields(DataInput in) throws IOException {\n        count = in.readInt();\n        average = in.readInt();\n    }\n\n    public void write(DataOutput out) throws IOException {\n        out.writeInt(count);\n        out.writeInt(average);\n    }\n}\n```\n\n#### TxtSVG_TRUE_job\n\n```java\npublic class TxtSVG_TRUE_job {\n    public static class SVGMapper extends Mapper<LongWritable, Text, IntWritable, TxtSVG_Writable> {\n        private TxtSVG_Writable w = new TxtSVG_Writable();\n\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            w.setCount(1);\n            w.setAverage(Integer.parseInt(value.toString()));\n            context.write(new IntWritable(1), w);\n\n        }\n    }\n\n    public static class SVGReduce extends Reducer<IntWritable, TxtSVG_Writable, IntWritable, TxtSVG_Writable> {\n        private TxtSVG_Writable result = new TxtSVG_Writable();\n\n        @Override\n        protected void reduce(IntWritable key, Iterable<TxtSVG_Writable> values, Context context) throws IOException, InterruptedException {\n            int sum = 0;\n            int count = 0;\n            for (TxtSVG_Writable value : values) {\n                sum += value.getCount() * value.getAverage();\n                count += value.getCount();\n            }\n            result.setCount(count);\n            result.setAverage(sum / count);\n            context.write(key, result);\n        }\n    }\n\n    public static void main(String[] args) throws Exception {\n        Configuration configuration = new Configuration(); //获取环境变量\n        Job job = Job.getInstance(configuration); //实例化任务\n\n       job.setJobName(\"TRUE_AVG\");//设置任务名称\n        \n        job.setJarByClass(TxtSVG_TRUE_job.class);  //设置运行Jar类型\n        job.setOutputKeyClass(IntWritable.class); //输出Key格式\n        job.setOutputValueClass(TxtSVG_Writable.class);//设置Value格式\n\n        job.setMapperClass(SVGMapper.class);//设置mapper\n        job.setCombinerClass(SVGReduce.class);//Combiner在本地运行\n        job.setReducerClass(SVGReduce.class);//设置Reducer\n\n        FileInputFormat.setInputPaths(job, new Path(args[0])); //设置输入路径\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));//设置输出路径\n\n        job.waitForCompletion(true);\n    }\n\n}\n```\n\n平均值最麻烦的情况就是分子和计算的时候，一定用总和除以数据总量，因为错误的代码Combiner运行在本地，总体把 分子和分母的关系处理错了，导致结果出错，总之就是无论什么时候都要保证总和和总数量不能错。\n\n![FR3D3V.png](https://s1.ax1x.com/2018/12/27/FR3D3V.png)\n\n## OutPutFormat\n\n源码包位于包“package org.apache.hadoop.mapreduce;”中，是一个抽象类，能够设置文件的输出格式，完成输出规范检查，并未文件输出格式提供作业结果数据输出的功能。\n\n![FRtUfA.png](https://s1.ax1x.com/2018/12/27/FRtUfA.png)\n\n#### NullOutputFormat\n\n继承OutputFormat的类的一个抽象类，位于org.apache.hadoop.mapreduce.lib.output;\n\n```java\n/**\n * Consume all outputs and put them in /dev/null. \n */\n@InterfaceAudience.Public\n@InterfaceStability.Stable\npublic class NullOutputFormat<K, V> extends OutputFormat<K, V>\n```\n\n消耗所有的输出，并把键值对写入/dev/null。相当于舍弃他们。\n\n#### FileOutputFormat\n\n位于org.apache.hadoop.mapreduce.lib.output;是一个从FileSystem读取数据的基类。有子类MapFileOutputFormat，SequenceFileOutputFormat、TextOutputFormat。\n\n##### FileOutputFormat\n\n提供了若干静态方法，用户可以用他们进行设置输入路径设置、分块大小设置等全局设置。\n\n##### MapFileOutputFormat\n\n把MapFile作为输出。需要确保Reducer输出的key已经排好序。\n\n##### SequenceFileOutputFormat\n\nSequenceFileOutputFormat将他的输出写进一个二进制顺序文件。容易压缩，如果为后续MapReduce任务的输出，是很好的输出格式。\n\n##### TextOutputFormat\n\n在FileOutputFormat所有的子类中，TextOutputFormat类是默认的输出格式，将每条记录记录写成文本行。由于TextOutputFormat调用toString()方法把键和值转换成任意类型。\n\n#### FilterOutputFormat\n\n对OutputFormat的再一次封装，类似于Java的流的Filter方式\n\n对OutputFormat的输出可以自定义编写他的格式，自定义InputFormat类似首先要继承FileOutputFormat然后重写getRecordWrite方法，返回值类型是RecordeWriter。OutputFormat输出可以指定多路径。和Reduce任务数联系密切，当Reduce任务书为1时，分区数多于1也能运行，也就是说Reduce任务数大于1时，它于分区数必须时保持一致的。\n\n#### DBOutputFormat\n\n继承OutputFormat接收  K V 对，其中key的继承类DBWritable接口，OutputFormat将Reduce输出发送到SQL表。DBOutPutFormat返回的RecordWriter值使用批量SQL查询写入数据库。\n\n### 实例分区\n\n```java\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Partitioner;\n\n\npublic class MyPartitioner extends Partitioner<Text, IntWritable> {\n    public int getPartition(Text key, IntWritable value, int numPartitions) {\n        if (key.toString().equals(\"bye\")) { //key 为bye 进度第0分区\n            return 0;\n        } else if (key.toString().equals(\"hello\")) {  //key为hello进度第1分区\n            return 1;\n        } else if (key.toString().equals(\"hadoop\")) { //key为hadoop 进度第2分区\n            return 2;\n        } else {\n            return 3; //其他的进入第3分区\n        }\n    }\n}\n```\n\n```java\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IntWritable;\nimport org.apache.hadoop.io.LongWritable;\nimport org.apache.hadoop.io.Text;\nimport org.apache.hadoop.mapreduce.Job;\nimport org.apache.hadoop.mapreduce.Mapper;\nimport org.apache.hadoop.mapreduce.Reducer;\nimport org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\nimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n\nimport java.io.IOException;\n\npublic class TxtCounter_job {\n    public static class wordCountMap extends Mapper<LongWritable, Text, Text, IntWritable> {\n        @Override\n        protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {\n            String[] words = value.toString().split(\" \");\n\n            for (String word : words) {\n                context.write(new Text(word), new IntWritable(1));\n            }\n        }\n    }\n        public static class WordCountReduce extends Reducer<Text, IntWritable, Text, IntWritable> {\n            @Override\n            protected void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {\n                int sum = 0;\n                for (IntWritable value : values) {\n                    System.out.println(\"<\" + key + \":  \" + value + \">\");\n                    sum += value.get();\n                }\n                context.write(key, new IntWritable(sum));\n            }\n        }\n\n\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();  //获取环境变量\n        Job job = Job.getInstance(conf); //实例化任务\n\n        job.setJarByClass(TxtCounter_job.class); //设定jar类型\n\n        job.setOutputKeyClass(Text.class); //设置输出key格式\n        job.setOutputValueClass(IntWritable.class); //设置value格式\n\n        job.setMapperClass(wordCountMap.class); //设置Mapper类\n        job.setReducerClass(WordCountReduce.class); //设置reduce类\n\n        job.setPartitionerClass(MyPartitioner.class); //自定义分区\n\n        job.setNumReduceTasks(4); //设置reduce任务数量\n        FileInputFormat.setInputPaths(job, new Path(args[0])); //添加输入路径\n        FileOutputFormat.setOutputPath(job, new Path(args[1])); // 添加输出路径\n\n        job.waitForCompletion(true);\n    }\n}\n\n```\n\n\n\n\n\n\n\n\n\n ","tags":["MapReduce"],"categories":["大数据"]},{"title":"MapReduce的工作机制","url":"/2018/12/24/MapReduce的工作机制/","content":"\n {{ \"《Hadoop权威指南》中的MapReduce工作机制和Shuffle\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 框架\n\nHadoop2.x引入了一种新的执行机制MapRedcue 2。这种新的机制建议在Yarn的系统上，目前用于执行的框架可以通过mapreduce.framework.name属性进行设置，值“local“表示本地作业运行器，“classic”值是经典的MapReduce框架(也称MapReduce1，它使用一个jobtracker和多个tasktracker)，yarn表示新的框架。\n\n## MR工作运行机制\n\nHadoop的运行工作机制需要下面5个独立实体：\n\n客户端，提交Mapreduce工作。\n\n YARN资源管理器（ResourceManager），负责协调集群上计算机资源的分配。\n\nYARN节点管理(NodeManager)，负责启动和监视集群中机器上的计算容器(container)。\n\nMapReduce的application master（简写为MRAppMaster），负责协调运行MapReduce的作业，它 和MapReduce任务在容器中运行，这些容器由资源管理器节点管理器进行管理。\n\n分布式文件系统（一般为HDFS），用来与其他实体间共享作业文件。\n\n![Fc1h8S.png](https://s1.ax1x.com/2018/12/25/Fc1h8S.png)\n\n \n\n### 作业提交\n\n可以通过一个简单的方法调用来运行MapReduce作业：Job对象的submit()方法。注意也可以调用waitForCompletion()，它用于提交以前没有提交过的作业，并等待它的完成。submit()方法调用了封装了大量的处理细节。Job的submit()方法创建了一个内部的JobSummiter实例，并且调用其submitJobInternal()方法（参见上图步骤1），提交作业后，waitForCompletion()每秒轮询作业进度，如果发现自己上次报告由改变，便把进度报告到控制台。作业完成后，如果成功，就是显示出计数器；如果失败，则导致作业失败的错误会记录到控制台。\n\nJobSummiter 所实现的作业提交过程如下所述：\n\n- Client 向资源提管理器请求一个新应用ID，用于MapReduce作业ID。（参见步骤2）\n- 检查作业的输出说明，例如：如果没有指定目录或者输出目录已经存在，作业就不提交，错误抛回给MapReduce程序。\n- 计算作业的输入分片。如果分片无法计算，比如输入路径不存在，作业就不提交，错误返回MapReduce程序 。\n- 将运行作业所需要的资源（包括作业Jar文件、配置文件和计算所得的输入分片）复制到一个以作业ID命名的HDFS目录中(请参见步骤3a)。作业Jar的副本较多(由mapreduce.client.submit.file.relication属性控制。默认值：10)，因此在运行作业的任务时，集群中有很多个副本可供节点管理器访问。保存成功后，Client即可以从HDFS获取文件原信息(步骤3b)。\n- 通过调用资源管理器的submitApplication()方法提交作业。参见步骤4。\n\n### 作业初始化\n\n资源管理器收到调用它的submitApplication()消息后，便将请求传递给YARN调度器（scheduler）。调度器分配一个容器，然后资源管理器在节点管理器的管理下在容器中启动application master的进程。（步骤5a和5b）。\n\nMapReduce作业的application master是一个Java应用程序，它的主类是MRAppMaster。由于接接收来自任务的进度和完成报告（步骤6），因此application master对作业的初始化是通过创建多个簿记对象以保持对作业进度的追踪来完成的。接下来，它接收来自共享文件系统的、在客户端计算的输入分片（步骤7）。然后对每一个分片创建一个map对象以及由mapreduce.job.reduces属性（通过作业的setNumReduceTask()方法设置）确定的多个reduce任务对象。任务 ID（Task ID）在此时分配。\n\napplication master 必须决定如何运行构成MapReduce作业的各个任务。如果作业很小，就选择和自己在同一个JVM上运行任务。在一个节点顺序运行这些任务相比，当application master 判断在新的容器中分配和运行任务的开销大于并行运行它们的开销时，就会发生这一情况。这样的小作业称为uber任务。\n\n小作业：默认情况下小作业就是少于mapper且只有一个输入大小小于一个HDFS块的作业(通过设置mapreduce.job.ubertask.maxmaps)和mapreduce.job.ubertask.maxbytes可以改变这几个值）。必须明确启动uber任务（对于单个作业，或者是对整个集群），具体方案是将mapreduce.job.ubertask.enable设置成true。\n\n最后，在任何任务运行之前，application master调用setupJob()方法设置OutputCmmitter。FileOutputCmmitter为默认值，表示将建立作业的最终输出目录及任务输出的临时作业空间。\n\n### 任务分配\n\n如果作业不合适作为uber任务运行，那么applicationmaster就会为该作业中所有map任务和reduce任务请求容器（步骤8）.首先为Map任务发出请求，该请求优先级要高于reduce的任务请求，这是因为所有的map任务须在reduce的排序阶段能够启动（shuffle）。直到有5%的map任务已经完成时，为reduce任务的请求才会发出。\n\nreduce任务能够在集群中任意位置运行，但是map的任务请求有着数据本地化局限，这也是任务调度器所关注的，在理想的情况下，任务是数据本地化(data local)的，因为这在分片驻留的统一节点上运行。可选的情况是，任务可能是机架本地化(rack local)的，意味着任务在分片驻留的统一节点运行。即和分片在同一机架而非同一个节点上运行，有些任务既不是数据本地化的，也不是机架本地化的，他们会从别的机架，而不是运行所在的机架上获取自己的数据。对于一个特定作业运行，可以通过查看作业的计数器来确定每个本地化层次上运行的任务数量。\n\n请求也为任务制定了内存需求和CPU数。默认情况下，每个map任务和reduce任务都分配到1024MB的内存和一个虚拟的内核，这些值可以在每个作业的基础上进行配置：\n\n- mapreduce.map.memory.mb\n- mapreduce.reduce.memory.mb\n- mapreduce.map.cpu.vcores\n- mapreduce.map.cpu.vcores.memory.mb\n\n### 任务执行\n\n一旦资源管理器的调度器为任务分配了一个特定节点上的容器，application master就通过与节点管理器来启动容器（步骤9a 和 9b），该任务由主类为YarnChild的一个Java应用程序执行。在它运行任务之前，首先将任务需要的资源本地化，包括作业的配置、JAR文件和所有来自分布式缓存的文件(步骤10).最后，运行map任务和reduce任务(步骤 11)。\n\nYarnChild在指定的JVM中运行，因此用户定义的map和reduce函数(甚至是YarnChild)中的任何缺陷不会影响到节点管理器。例如导致其崩溃或挂起。\n\n每个任务都能够执行搭建(setup)和提交(commit)动作，他们和任务本身在同一个JVM中运行，并由作业的OutputCommitter确定。对于基于文件的作业，提交动作将任务输出由临时位置搬移到最终位置。提交协议确保当前推测(speculative execution)被启用时，只有一个任务副本被提交，其他的都被取消。\n\n### 进度和状态的更新\n\nMapReduce作业是长时间运行的批量作业，运行时间范围从数秒到数小时。可能是一个很长时间段，所以对于用户而言，能够得知关于作业进展的一些反馈时很重要的。一个作业和他的每个任务都有一个状态(status)，包括：作业或任务的状态(比如，运行中，成功完成，失败)、map和reduce的进度、作业计数器的值、状态消息或描述(可以由用户代码来设置)。这些状态在作业期间不断改变。\n\n任务在运行时，对其他进度(progress 即任务完成百分比) 保持追踪。对map任务，任务进度是已处理输入所占的比例。对于reduce，系统会估计已处理reduce输入的比例。整个部过程分为三部分，与shuffle的三个阶段对应（详情参见shuffle和排序）。比如如果任务已经执行reducer一般的输入，那么任务的进度是5/6,这是因为已经完成复制和排序阶段（每个占1/3），并且已经完成reduce阶段的一半1/6）。\n\n**MapReduce中的进度的组成：进度并不总是可测量的，但是虽然如此，它能告诉Hadoop有个任务正在做一些事情比如:正在些输出记录的任务是由进度的，即使此时这个进度不能用需要写的总量的百分比来表示（因为即便是产生这些输出的任务，也可能不知道需求写的总量。进度报告很重要，构成进度的所有操作如下：**\n\n- 读入一条输入记录(在mapper或reducer中)。\n- 写入一条输出记录(在mapper或reducer中)。\n- 设状态描述（通过Reporter或者TaskAttemptConetext的setStatus()方法）。\n- 增加计数器的值（使用Reporter的incrCounter()方法或Counter的increment()方法）。\n- 调用Reporter或TaskAttemptContext的progress()方法。\n\n任务也有一组计数器，负责对任务运行过程中各个事件进行计数，这些技术其要么内置于框架中，列入一些如的map输出记录数，要么由用户自己定义。\n\n当map任务或reduce任务运行时，子进程和自己的父application master通过 umbilical 接口通信。每隔3秒，任务通过这个umbilicali接口想自己的application master报告进度和状态(包括计数器),application master会形成一个作业的汇聚视图(aggregate view)。\n\n资源管理器的界面显示了所有运行中的应用程序，并且分别由链接指应用各自的application master的界面，这些界面展示了MapReduce作业的诸多细节，包括进度。\n\n在作业期间，客户端每秒轮询一次 application master以接收最新的状态(轮询时间间隔通过mapreduce.client.progressmonitor.polinterval设置)。客户端也可以用Job的getStatus()方法得到一个JobStatus的实例，后者包含作业的所有状态信息。\n\n![F64LfP.png](https://s1.ax1x.com/2018/12/24/F64LfP.png)\n\n### 作业完成\n\n当application master收到作业最后一个任务已完成的通知后，便把作业状态设置为“成功”。然后在Job轮询状态时，便一直到任务已经完成，于是Job打印一条消息告知用户，然后从waitForCompletion()方法返回。Job的统计信息和计数值也在这个时候输出到控制台。如果ApplicationMaster有相应的设置，也会发送一个HTTP做作业通知。希望收到回调指令的客户端可以通过mapreduce.job.end-notification.url属性来进行这项设置。\n\n最后，作业完成时，application master和任务管理器清理其工作状态(这样中间输出将被删除)，OutputCommitter的commitJob()方法会被调用，作业信息由历史服务器存档，以便日后用户需要时可以查询。\n\n## Shuffle 与 排序\n\nMapReduce确保每个redcuer的输入都是按键排序，将map的输出作为输入传给reducer的过程称为shuffle。学习shuffle时如何工作，因为它有助于我们了解工作机制方便我们优化MapReduce程序。shuffle属于不断被优化和改进的代码库的一部分，因此下面描述了一些细节（可能会随着版本升级而改变）。从许多方面来看，shuffle是MapReduce的“心脏”，是MapReduce最核心的部分。\n\n### map端\n\nmap函数开始产生输出时，并不是简单地将它写到磁盘。这个过程更复杂，它利用缓冲地方式写到内存并出于效率的考虑进行预排序。\n\n![F6jll9.png](https://s1.ax1x.com/2018/12/24/F6jll9.png)\n\n每个map任务都有一个环形内存缓冲区用于存储任务输出。在默认情况下，缓冲区为100M这个值可以通过改变mapreduce.task.io.sort.mb来调整。一旦缓冲区内容达到阈值(mapreduce.map.sort.spill.percent，默认时0.8或者80%)，一个后台线程便开始把内容溢写(split)到磁盘。在溢出些到磁盘的过程中，map输出继续写到缓冲区，但如果在此间缓存被填满，map会被阻塞直到写磁盘过程完成。溢写过程按轮询方式将缓冲区中的内容写道mapreduce.cluster.local.dir属性在作业特定子目录下指定目录中。\n\n在写磁盘之前，线程首先根据数据最终要传的reducer把数据分成相应的分区(partion)。在每个分区中，后台线程案件进行内存中排序，如果有一个combiner函数，它就在排序后的输出上运行。运行combiner函数使得map输出结果更紧凑，因此减少到写到磁盘的数据和传递给reducer的数据。\n\n每次内存缓冲区到达溢出的阈值，就会新建一个一溢出文件(splil file)，因此在map任务写完其最后一个输出记录之后，会有几个溢出文件。在任务完成之前。溢出文件被合并成一个已分区且已排序的输出文件。配置属性mapreduce.task.io.sort.factor控制一次最多能合并多少流，默认值是10。\n\n如果至少存在3个溢出文件(通过mapreduce.combine.minspills属性设置)时，则(combine.minspills属性设置)时，则combiner就会在输出文件写到磁盘之前再次运行，combiner可以在输入上反复运行，但并不影响最终结果。如果只有1或2个溢写文件，那么由于map输出规模减少，因而不值得调用combiner带来的开销，因此不会为该map输出再次运行combiner。\n\n在将压缩map输出写到磁盘的过程中对它进行压缩往往时很好的主意，因为这样会写磁盘的速度更快，节约磁盘空间，并且减少传给reducer的数据量，在默认的情况下输出是不压缩的，但只要将mapreduce.map.output.compress设置为true，就可以启用此功能。\n\nreducer通过HTTP得到输出文件的分区。用于文件分区的工作线程的数量由任务的mapreduce.shuffle.max.threads属性控制，此属性针对的时每一个节点管理器，而不是针对每个map任务，默认值是0将最大线程设置为机器处理器数量的两倍。\n\n### reducer端\n\nmap输出文件位于运行map任务的tasktracker的本地磁盘**（注意：尽管map输出经常写道maptasktracker的本地磁盘，但是reduce输出并不这样）**，现在tasktracker需要为分区文件运行reduce任务，并且reduce任务需要集群上若干map任务的map输出作为其特殊的分区文件，每一个map任务的完成时间可能不同，因此在每个任务完成时，reduce任务就开始复制其输出。默认值是5个线程，但是默认值可以通过设置mapreduce.reduce.shuffle.parallelcopies属性设置。\n\nreducer如何知道要从哪台机器取得map输出：\n\nmap任务完成后，他们会用心跳机制通知他们的application master。因此对于指定作业，application master知道map输出和主机位置之间的映射关系。reducer中的一个线程定期询问master以便获取map输出主机的位置，直到获得所有输出位置。\n\n由于第一个reducer可能失败，因此主机并没有在第一个reducer检测到输出时就立即从磁盘上删除他们，相反，主机会等待，直到application告诉他删除map的输出，这是作业完成后执行的。\n\n如果map的输出相当小，会被复制到reduce任务的JVM内存中（缓冲区大小由mapreduce.reduce.shuffle.input.buffer.percent属性控制，用于指定此用途的堆空间的百分比），否则，map输出被复制到磁盘。一旦内存缓冲区达到阈值大小（由mapreduce.reduce.merge.percent决定）或达到map输出阈值（由mapreduce.reduce.merge.inmem.threshold控制），则合并后溢出写到磁盘中。如果指定combiner，则在合并期间运行它以降低写入磁盘的数据量。\n\n随着磁盘上副本增多，后台线程会将他们合并为更大的、排好序的文件。这会为后面的合并节省一些时间，注意为了合并，压缩的map输出通过map任务都必须在内存中被解压缩。复制完所有map输出后，reduce任务进入排序阶段(更准确的说时合并阶段，而排序时在map端进行的)，这个阶段将合并map输出，维持其顺序排序。这是循环进行的。比如，如果有50个map输出，而合并因子是10（10默认设置，由mapreduce.task.io.sort.factor属性设置，与map的合并类似），合并将进行5趟。每趟将10个文件合并成一个文件，因此最后有5个中间文件。\n\n在最后阶段，即reduce阶段，直接把数据输入reduce函数，从而省略了一次磁盘往返的形成，并没有将这个5个文件合并成一个已经排序的文件作为最后一趟。最后的合并可以来及内存和磁盘片段。\n\n每趟合并的文件数实际上比实例中展示的有所不同，目标是合并最小数量的文件以便满足最后一趟的合并系数。因此如果有40个文件，我们不会在四趟中每趟合并10个文件从而得到4个文件，相反第一次只合并4个文件随后三趟合并完整的10个文件，在最后一趟中，4个已经合并的文件和与下来的6个（未合并的）文件共合计10个文件如图所述。                   \n\n![FcSwUP.png](https://s1.ax1x.com/2018/12/24/FcSwUP.png)\n\n\n\n这合并并没有改变合并次数，它只是一个优化措施，目的是尽量减少写到磁盘的数据量，因为最后一趟总是直接合并到reduce。\n\n在reduce阶段，对已经排序输出中的每个键调用reduce函数。此阶段的输出直接写到输出文件系统，一般为HDFS。如果采用HDFS，由于节点管理器也运行数据节点。所以第一个块副本将被写到本地磁盘文件。\n\n## 配置调优\n\n#### map端\n\n| 属性名称                         | 类型      | 默认值                                     | 说明                                                         |\n| -------------------------------- | --------- | ------------------------------------------ | ------------------------------------------------------------ |\n| mapreduce.task.io.sort.mb        | int       | 100                                        | 排序map输出是所使用的内存缓存缓冲区的大小，以兆字节为单位。  |\n| mapreduce.map.sort.spill.precent | float     | 0.80                                       | map输出内存缓冲和利用开始磁盘一些过程的边界索引，这两者使用比例的阈值。 |\n| mapreduce.task.io.sort.factor    | int       | 10                                         | 排序文件时，一次最多合并的流数。这个属性也在reduce中使用，将此值增加到100是很常见的。 |\n| mapreduce.map.combine.minspills  | int       | 3                                          | 运行combiner所需最少溢出文件数（如果已经指定combiner）。     |\n| mapreduce.map.out.put.compress   | boolean   | fasle                                      | 是否压缩map输出。                                            |\n| mapreduce.output.compress.codec  | classname | org.apache.hadoop.io.compress.DefaultCodec | 用于map输出的压缩解码器。                                    |\n| mapreduce.shuffle.max.threads    | int       | 0                                          | 每个节点管理器的工作线程数，用于将map输出到reducer。这是集群范围设置，这是集群的范围的设置，不能由单个作业设置，0表示使用Netty默认值，即两倍于可用的处理器数。 |\n\n总的原则是给shuffle过程尽量多提供内存空间。然而一有个平衡问题，也就是要确保map函数和reduce函数能够得到足够的内存来运行。这就是为什么写map函数和reduce函数时尽量少用内存的原因。他们不一般不应该无限使用内存(例如，应避免在map中堆积数据)。\n\n运行map任务和reduce任务的JVM，其内存大小由mapred.child.java.opts属性设置，任务节点上的内存应该尽可能设置的大些。\n\n 在map端，可以通过避免多次溢写磁盘来获得最佳性能；一次最佳的情况。如果能估算map输出大小，就可以合理地设置mapreduce.task.io.sort.*属性来尽可能减少溢写的次数，具体而言，如果可以，就要增加mapreduce.task.io.sort.mb的值。MapReduce设计计数器（“SPILLED_RECORDS”）计算在作业运行整个阶段中溢写磁盘的记录数，对于调优很有帮助。注意，这个计数器包括map和reduce两端的溢出写。\n\n在reduce端，中间数据全部驻留在内存时，就能够获得最佳性能，在默认情况下，这就是不可能发生的，因为所有内存一般都会预留给reduce函数。但如果reduce函数的内存需求不大，把mapreduce.reduce.merge.inmem.threshold设置为0，把mapreduce.reduce.input.buffer.percent设置为1.0或者更低的值。\n\n#### reduce端\n\n\n\n| 属性名称                                       | 类型  | 默认值 | 描述                                                         |\n| ---------------------------------------------- | ----- | ------ | ------------------------------------------------------------ |\n| mapreduce.reduces.shuffle.parallelcopies       | int   | 5      | 用于map输出复制到reduce的线程数。                            |\n| mapreduce.reduce.shuffle.maxfetchfailures      | int   | 10     | 在声明之前，reducer获取一个map输出花的最大时间。             |\n| mapreduce.task.shuffle.maxfetchfailures        | int   | 10     | 排序文件时一次最多合并流的数量，这个属性也在map端使用        |\n| mapreduces.reduce.shuffle.input.buffer.percent | float | 0.70   | 在shuffle的复制阶段，分配给map输出的缓冲区对空间的百分比。   |\n| mapreduce.reduce.shuffle.merge.percent         | float | 0.66   | map输出缓冲区(由map.job.shuffle.input.buffer.percent定义)的阈值使用比例。用于启动合并并输出和磁盘溢写出的过程。 |\n| mapreduce.reduce.merge.inmem.thresholld        | int   | 1000   | 启动合并输出和磁盘溢写的过程的map输出的阈值数。0或者更小的数意味着没有阈值限制，溢写行为由mapreduce.reduce.shuffle.percent单独控制。 |\n| mapreduce.reduce.input.buffer.percent          | float | 0.0    | 在reduce的过程中，在内存中保存map输出的空间占整个堆空间的比例。reduce阶段开始时，内存中的map输出大小不能大于这个值。默认情况下，在reduce任务开始之前，所有的map输出都合并到磁盘上，以便为reducer提供尽可能多的内存，然而，如果reducer需要的内存较少，可以增加此值来最小化访问磁盘的次数。 |\n\n​\t\t\t\t\t\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t参考资料:《Hadoop权威指南》","tags":["MapReduce"],"categories":["大数据"]},{"title":"MapReduce入门和优化方案","url":"/2018/12/22/Mapreduce/","content":"\n {{ \"MapReduce基本原理和高性能网络下优化\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Mapreduce概述\n\nMapreduce式谷歌开源的一项重要技术,是一个编程模型,用来进行大数据量的计算,对于大数据量的计算通常采用的处理方式式并行计算,对于许多开发者来说,自己完全实现一个并行计算程序难度太大.而MapReduce就是一种简化并行计算的模型,它使得那些没有多少并行计算经验的开发人员也可以开发出并行计算应用程序,通过简化编程模型,降低了开发并行应用程序的难度。\n\n## 工作原理\n\n并行计算模型通常从并行计算的设计和分析出发，将各种并行计算机(至少某以类并行计算机)的基本特征抽象出来，形成一个抽象的计算模型，广义上来讲，并行计算模型为并行计算提供了硬件和软件界面。在该界面的约定下，并行计算软件和硬件设计者可以开发对并行那个计算支持的机制，从而提高系统性能。由代表性的并行计算 模型包括PRAM模型、BSP模型、LogP模型、C^3模型。\n\n### 并行计算\n\n同时使用多种计算资源解决计算问题的过程，是提高计算机系统计算速度和处理能力的一种有效手段，基本思想是：用多个处理器来协同求求解同一问题，即将被求解的问题分解成若干部分，各部分均由一个独立的处理机来并行计算，并行计算系统可以是专门设计的、包含多个处理器的超级计算机，也可以式以某种方式互连的若干台独立计算机构成的集群。通过并行计算集群完成数据的处理，再将处理的结果返回给用户。\n\n### 并行计算处理系统\n\n同时执行多个任务、多条指令或同时对多个数据项进行处理的计算机系统。主要包括以下两种类型的计算机：\n\n1. 同时能够多条指令或同时处理多个数据项的单中央处理器计算机。\n\n2. 多处理联机系统。\n\n并行计算处理计算机的结构特点主要包括两个方面：\n\n1. 单机处理机内广泛采用各种并行措施。\n2. 由单处理机发展成各种不同耦合度的多处理系统。并行处理的主要目的式提高系统的处理能力。有些类型的并行处理计算机系统(如多处理机系统)还可以提高系统的可靠性。由于器件的发展，并行处理计算机系统具有良好的性能性价比，而且还有进一步提高的趋势。\n\n## Mapreduce工作原理\n\nMapReduce是一个编程模型是处理和生成超大数据集的算法模型的相关实现。用于首先用一个Map函数处理一个基于键值对的数据集合，输出中间的基于键值对的数据集合；然后创建一个Reduce函数来合并所有的相同中间键(key)和中间值(value)对。MapReduce架构的程序能够在大量普通配置的计算机上实现并行化处理。这个系统运行时只关心：**如何分隔输入数据，在大型计算机组成的集群上的调度，集中处理计算机的错误处理，管理集群中计算机之间的必要的通信 **采用MapReduce架构，可以使那些没有并行计算和分布式处理系统开发经验的程序员有效利用分布式的资源。\n\n设计这个抽象模型的灵感来自于Lisp和许多函数时语言的Map和Reduce原语：在输入数据的“逻辑”上应用Map操作得出一个中间键值对(key-value)集合，然后把所有的具有相同键值(key)的中间值(value)应用到Reduce操作，从而达到合并中间的数据得到想要的结果目的。使用MapReduce模型 ，结合用户自身需求实现Map和Reduce函数，可以十分容易实现大规模的并行化计算，MapReduce自带“再次执行”（re-execution）功能。也提供了初级的灾备实现方案，MapReduce框架模型让我们很容易地通过简单的接口来实现自动地并行化和大规模地分布式计算，通过MapReduce接口，实现大量普通的PC进行高效地并行计算。\n\n通常计算节点和存储节点是在一起地。MapReduce框架和分布式文件系统HDFS运行在相同地节点上，这种配置允许框架在那些已经存好数据的节点上高效地调度，这可以使整个集群的网络带宽被非常高效地利用。\n\n##  MapReduce计算模型\n\nMapReuce编程框架适用于大数据计算：大数据管理、大数据分析、大数据清洗、大数据预处理。\n\nMapReduce是在HDFS上将一个大文件切分割成若干个小文件分别存储在不同节点的基础上，尽量在数据节点上完成小任务计算再合并成最终结果。其中一个大任务分解成一个小任务的过程就是典型的合并计算过程。以尽量快地完成海量数据的计算。\n\n![Fsvx5F.md.png](https://s1.ax1x.com/2018/12/22/Fsvx5F.md.png)\n\n 一个MapReduce作业(job)通常会把输入的数据集切分为若干独立的数据Block，用Map任务(task)以完全并行的方式处理它们。框架首先对Map输出先进行排序，然后结果输出给Reduce任务。典型的作业的输入输出都会被存储在文件系统中。整个框架负责任务的调度、监控、已失效的任务重新执行。 MapReduce在计算时分为几个步骤：\n\n### HDFS\n\n存储大文件切分后的数据块，Block块大小为128M。\n\n### 分片 \n\n进行Map计算之前，MapReduce会根据输入文件计算输入分片（input split），每个输入分片只针对一个Map任务，输入分片存储的给数据本身，而是一个分片长度和一个记录数据的位置的数组。\n\n![FszaFO.md.png](https://s1.ax1x.com/2018/12/22/FszaFO.md.png)\n\nblock分片并非完全是1:1关系，Hadoop1.$$x$$默认的块大小是64M，Hadoop2.0之后默认的块大小是128M，可以再hdfs-site.xml中设置dfs.block.size，单位是Byte。minSplitSize的大小默认是1B，MaxSplitSize的大小默认是Long.MAX_VALUE=9223372036854775807。\n\n分片大小的计算公式如下\n\n```java\nminsize =max{minSplitSize,mapred.min.split.size}\nmaxSize=mappred.max.split.size\nsplitSize=max{minSize,min{maxSize,blockSize}}\n```\n\n### Map\n\n将输入键值对(key/value pair)映射到一组中间格式的键值对集合。Map的数目通常是由输入数据的大小决定的，一般就是所有输入文件的总块数(Block)。官网给参考值Map正常的并行规模是10~100个Map，对于CPU消耗比较小的Map任务可以设置到300个左右。由于每个任务初始化需要一定的事件，因此比较合理的情况是Map执行的事件至少超过1分钟。可以通过setNumMapTasks(Int)将这个数值设置得更高，Map任务数与Block对应关系会改变。\n\n### Shuffle\n\nHadoop的核心是MapReduce，而Shuffle是MapReduce的核心。Shuffle的主要任务是从Map结束到Reduce开始之间的过程。Shuffle阶段完成了数据的分区、分组、排序的工作，\n\n### Reduce\n\n将与一个key关联的一组中间数值集归约(Reduce)为一个更小的数据集。Reducer时输入的是Mapper已经排序的输出。这个过程和排序两个阶段时同时进行的：Map的输出也是一边被取回。一边被合并的。Reducer的输出是没有被排序的。Reduce的数目建议是0.95或者1.78乘以（no. of nodes $$*$$ no. of maximum containers per node ）。系数为0.95的时候所有Reduce可以再Map已完成就立即启动，开始计算Map的输出结果，系数是1.75的时候，速度快的节点可以再完成第一轮的Reduce任务后，立即开始第二轮，这样可以得到比较好的负载均衡效果。增加Reduce数目会增加整个框架的开销，但是可以改善负载均衡,降低由于执行失败带来的负面影响,上述比例因子比整体数目稍小一些,是为了给框架中的推测任务或失败任务预留一些Reduce资源,无Reducer时,*** 如果没有规约要求进行,那么设置Reduce的数目为0是合法的.这种情况下Map任务的输出会直接写入由setOutputPath(Path)指定的输出路径，框架再把他们写入FileSystem之前并没有对他们进行排序 ***。\n\n输出：记录MapReduce结果的输出。\n\n## MapReduce经典案例 \n\n从HDFS上获取指定的Block进行分片后，将分片结果输如Mapper进行Map处理，讲解过输出给Shuffle完成分区，排序、分组的计算过程，之后计算结果会被Reducer指定业务进一步处理。\n\n![Fy84X9.png](https://s1.ax1x.com/2018/12/22/Fy84X9.png)\n\n为了方便框架执行排序操作，key类必须实现WritableComparable接口。MapReduce输入和输出的类型描述。\n\n(input) (k1,v1)  →map→(k2,v2)→(combine → (k2,v2) →reduce →(k3,v3)(output)\n\n## MapReduce应用场景\n\nMapReduce框架透明性强，易于编程、易拓展；对于宕机等原因导致的错误，容错性很强；能够处理PB级以上的海量数据的离线处理。单由于它的工作机制有延迟性，对于类似于Mysql这种毫秒级或者秒级内返回结果的情况达不到要求。MapReduce**自身的设计特点决定了数据源必须必须时静态的，**故不能处理动态变化的数据，如流式计算等。\n\n由于在MapReduce的计算过程中，大量的数据需要经过Map到Reduce端的传送，故在进行一些如两张表或者多张表的Join计算时，通过Map任何分别将两表的文件记录处理成（Join Key, Value），然后按照Join Key做Hash分区后，送到不同的Reduce任务去处理。Reduce任务一般使用Nested Loop方式递归左表的数据，并便利右表的每一行，对于相等的Join Key，处理Join结果并输出。由于大量数据需要经过Map到Reduce端的传送,Join计算性能并不高。华为的FusionInsight擦产品对HDFS进行了文件快集中分布式的加强，使需要做关联和汇总计算的两张表FileA和FileB，通过或指定一个分布ID，使其所有的Block分布在一起，不需要再跨节点读取数据就能完成计算，极大地提高了MR Join地性能。\n\n## LATE:算法\n\nHadoop一个最大的优点是能够自动处理失败。如果一个节点坏了，Hadoop会将此节点的任务放到另一个节点上执行。同样地，工作但速度慢的节点称之为掉队节点,Hadoop会将此节点上的任务随机复制到另外一台机器上进行更快速的处理。此行为称之为推测执行。推测执行带来了如下几个难题：\n\n1. 推测执行要占用资源,如网络等。\n2. 选择节点和选择任务同样重要。\n3. 异构性能环境中，很难确定哪一个节点的速度慢。\n\nHadop被广泛应用用于短小任务中，其中应答时间是一个很重要的因素。Hadoop的性能与任务调度程序有着很大的关系，任务调度程序假定集群的节点均匀并且任务是线性执行的，根据这些假设决定何时重新执行掉队任务。事实上，集群并不一定是均匀的，如在虚拟数据中心就不具备均匀性。Hadop的任务在异构环境容易引起严重的性能退化。在亚马逊EC2的一个具有200个虚拟机的集群中，LATE调度算法可以缩短Hadoop的应答时间至原来的一半。\n\n### Hadoop的推测执行\n\n当一个节点空闲时，Hadoop从3种任务中选择一个执行；第一种时失败任务，第二种是没有被执行任务，第三种是随机地选择一个任务，为了推测执行任务，Hadoop用一个进度得分来衡量任务的进度，最小值为0，最大值为1。对于Map来说，进度得分就是输入数据中被读入比例。对于Reduce来说，执行被分为3部分，任务读取Map结果，Map结果排序。以及Reduce任务执行。每部分为1/3，在每部分中，分数为被处理的数据比例。\n\nHadoop根据每类任务的平均得分来定义一个阈值，每当一个任务的进度得分小于该类任务的平均得分减去0.2，并且该任务运行了至少一分钟，那些任务可被看成掉队任务。调度程序保证每个任务最多只有一个被推测执行。\n\n当运行多个任务时，Hadoop利用先进先出的规则，运行最早提交的任务，同样有一个优先级系统来提高任务的优先级。\n\n### Hadoop调度程序前提\n\n每个节点的工作速率相同。一个任务执行速率始终保持一致。将一个任务复制到另一个节点时没有花销。一个任务的进度得分等于它被完成的比例。特别地，在一个Reduce阶段中，复制，排序、执行各占总时间的1/3。任务是成波浪型结束的，因此一个进度的得分低的任务更可能时掉队任务。同一种任务的工作量大致相等。\n\n### LATE调度程序的基本思想\n\n每次选取最晚结束的任务来推测执行，从而极大地减少响应时间，有很多方法能够评估任务的剩余时间。可以 用一个简单的方法，此方法经过实验验证具有很好的效果。假设任务是以恒定的速度来执行，那么一个任务的进度根据公式$$ProgressScore/T$$来估计，即每分钟处理的任务进度是多少。剩余时间根据公式$$(1-ProgressScore)/ProgressRate$$来估计。\n\n每次推测任务复制到快速执节点上运行，并且通过一个简单的机制来实现，即不要把任务复制到进度总分在阈值以下的节点中，其中进度总分为该节点所有已经完成及正在运行的任务的进度得分相加。 \n\n为了解决推测执行需要消耗资源的事实，用两个启发式方法来增强LATE算法。\n\n1. 每次同时能够执行的推测任务数设定一个上限，叫做SpeculativeCap。\n2. 一个任务的进度率需要与SlowTaskThreshold比较来决定其是否足够慢。这防止了全部任务均是快速任务，产生无意义的推测执行。\n\n综上所述,LATE算法的工作流程如下。\n\n当一个节点申请任务，并且执行的推测任务数小于上限，则：当此节点的总进程得分小于SlowNodeThreshold时，忽视请求；根据剩余时间将当前正在运行的非推测任务排序；将进度率低于SlowNodeThreshold的任务中排名最高的任务，复制到此节点。\n\n与Hadoop类似，LATE算法仅仅对于执行至少一分钟的任务进行判断。LATE算法是一个简单、适应性强的程序调度算法、其有一系列的优点：首先，适合异构环境：其次,LATE算法在决定哪个节点来运行推测任务时考虑了异构节点；最后，通过强制估计剩余时间而不是进度率，LATE算法只推测执行能够缩短总时间的的任务。此算法通过估计任务剩余时间来推测地执行任务，并且能够最大限度地缩短响应时间。\n\n## Mantri：MapReduce 异常处理\n\nMapReduce作业通常有无法预测的性能表现。一个作业由一个分阶段执行任务集合构成，这些任务会有前后依赖的关系——有的任务依赖于其他任务的计算结果。很多任务是并行的，如果一个任务的执行时间超过相似任务的时间，则依赖这个任务的结果的任务就都会被延迟。在一个作业中，少数几个这样的异常可以组织作业剩余任务的执行，甚至可以使作业执行时间增加34%。\n\nMantri 是一个能够监视任务并根据导致异常的原因来剔除异常的系统。它主要采用了以下的技术：\n\n1. 重启已经认识到资源约束和工作不均匀的异常任务；\n\n2. 根据集群网络环境情况安排任务；\n\n3. 根据开销-收益分析结果来保护任务的输出结果。\n\n###  Mantri设计\n\n![FydbSP.png](https://s1.ax1x.com/2018/12/22/FydbSP.png)\n\n\n\n如果一个任务因为争抢资源而延后，重启或者复制一份该任务已经完成的结果可以加快任务进度。不要在低宽带条件下进行跨机架的数据传送，如果这些无法避免，则需要一个系统的配置避免热点。为了加速那些正在等待已经丢失的输入的任务，使用一些方法来保护输出。最后，当遇到任务量分布不均匀的问题时，Mantri优先执行大的任务，以避免再快要完成作业时卡在大的任务上。\n\n### 估计 $ t_{rem}$和  $ t_{new}$\n\n $t_{rem}$表示完成一个正在运行中的任务所需要的剩余时间，$t_{new}$表示重新运行该任务所需的时间。\n\n##### 计算$ t_{rem}$的模型：\n\n$ t_{rem}=t_{elaspsed} \\frac{d}{d_{read}}+t_{wrapup} $\n\n##### 参数标识\n\n###### $ d_{read}$\n\n表示一个任务已经读取的数据量； \n\n###### $d$\n\n表示这个任务总数需要处理的数据量；\n\n###### $t_{elaspsed}$\n\n代表加工$$d_{read}$$所需的时间，所以前半部分代表加工剩余所需的时间；\n\n###### $t_{wrapup}$\n\n表示代表所有数据已经读取进来之后需要计算时间，这个时间时根据之前的任务进行评估的。\n\n###### $ t_{new}  $模型\n\n计算模型：$$ t _ {new}=processRate*locationFactor*d+schedLag $$ \n\n### Mantri的实现方法\n\n实现重启的方法：Mantri使用两种不同的方法来实现重启。第一种杀死一个进程中的任务并在别的地方重新运行，第二种制作一个这个任务的副本。执行这两种方法均需要一个条件，及P<sub>{tnew< trem}</sub> 比较大。\n\n配置任务的方法：已知任务链接情况和输入数据的位置，使用一个中心调度机制可以最优地对所有任务进行配置，但是这要求掌握实时地网络信息和中心调度机，进行大量地协调计算。Mantri没有采用这种方法，而是采用了以中近似最优的算法，这种算法时根据集群网络情况配置任务的，既不需要知道实时网络情况，也不需要进行作业间的协调工作。\n\n避免重新计算的方法：为了减轻重新计算造成的作业完成时间的增加，Mantri通过复制任务的输出结果来避免数据丢失的问题。\n\nMantri最根本的优势在于它将静态的MapReduce作业的结构和在运行过程中动态获取的信息整合在一个完整的框架里。这个框架可以依据整合的信息发现异常，对可以采取的针对性措施、可以获得的收益和消耗进行衡量，如果值得采取，就实施针对性措施。\n\n## SKewTune：MapReduce中数据偏斜处理\n\n对于很多运行在该平台的应用来说：数据偏斜是一个显著的挑战。当数据偏斜发生的时候，一些分区的操作在处理输入数据时，比其他分区花费的时间更长，造成了整个计算时间变长。一个MapReduce工作主要由Map阶段和Reduce阶段组成。在每个阶段，输入数据的一个子集被计算机集群进行分布式任务处理。Map任务完成后通知Reduce任务接收新的可用数据，这个转换过程被称为重分配，直到所有的Map任务完成后，Reduce阶段的重分配才完成，然后开始Reduce操作，负载不均衡可能发生在Map阶段，也可以发生在Reduce阶段，数据偏斜会显著增加作业执行时间，以及降低吞吐率 。\n\n### Map阶段\n\n高代价记录,Map任务一个接一个处理由键值对组成的一系列记录.理想情况下处理事件在不同的记录之间的差距不大. 然而根据不同的应用,有些记录可能需要更多的CPU和内存.这些高代价记录可能比其他的记录大,也可能是Map算法的运行时间取决于记录的值。\n\n异构的Map，MapReduce是一个一元运算符，但是也可以通过逻辑级联多个数据集，作为一个单一的输入和模拟实现$$n$$元运算。每个结果集可能需要不同的处理，从而不导致运行时间是多峰分布。\n\n### Reduce阶段\n\n分区偏移。在MapReduce中，Map任务的输出会通过默认的散列分区或者用户自定义分区逻辑分布在不同的Reduce任务。默认散列分区通常是足够的均匀分布数据。然而散列分区不保证均匀分布。\n\n高代价的关键字组。在MapReduce中Reduce任务处理(关键字、值域)对序列，被称为关键字组。类似于Map任务处理高代价记录一样，高代价关键字会造成Reduce运行时间的偏移。\n\n数据偏斜是一个众所周知的问题，已经在数据库管理系统、自适应和流处理系统中被广泛研究。\n\n以往的解决方案都会产生其他的代价或者有较为严格的要求，而SkewTune不要求来自用户的输入，它被广泛使用，因为它并不是探究是什么原因导致数据偏斜发生的，而是观察工作的执行，重新均衡负载，使资源变得可用。当集群中的一个节点空闲时，SkewTune通过最大预期的剩余处理时间来标识任务，然后这个任务的未处理输入数据会主动的地重新分区，重分区会充分利用集群中的节点，并且会保留输入数据的顺序，使原始输出可以通过串联来重建。\n\nSkewTune是一个共享集群，SkewTune假定一个用户可以访问集群中的所有的资源，在共享集群中和有两种方法来使用SkewTune：\n\n1. 用一个任务调度程序为每个用户定义一个资源集。\n2. 通过实施SkewTune 感知调度程序来优化缓存器。\n\n![Fy7WX8.png](https://s1.ax1x.com/2018/12/23/Fy7WX8.png)\n\nSkewTune 设计用于设计MapReduce的引擎，特征在于通过基于磁盘的处理和面向记录的数据模型。通过拓展Hadoop并行数据处理系统来实现SkewTune技术。SkewTune通过优化在减小数据偏斜的同时，还能保持容错和MapReduce的可扩展性。\n\n### 特点\n\n1.  SkewTune能够减少两种非常常见的数据偏斜：由于操作分区的数据不均匀造成的数据偏斜和由于一些数据子集执行花费时间比其他的数据子集长造的数据偏斜。\n\n2. SkewTune可以优化为修改的MapReduce程序；程序员不需要改变代码。\n\n3. SkewTune保持其他UDO（User Defined Operations,用户自定义操作）的互操作性，它保证没有在SkewTune下执行时，数据以相同的排序序列出现在每个分区内，并且操作的输出由同样数的分区组成。\n\n4. SkewTune与流水线优化兼容，它不要求连续的操作之前的同步屏障。\n\n### 设计理念\n\n1. 开发者透明：让MapReduce开发者开发出高性能的产品更加容易。研究者希望SkewTune是一个改进版的运行更快的Hadoop。不用开发者使用任何特殊的模板来完成任务。或者需要他们进行类似于代价函数这样的输入。\n2. 解决方案透明：设计者希望使用SkewTune与不使用SkewTune的输出是一样，包括同样数量的文件和同样的顺序。\n3. 最大的适用性： 在MapReduce中或者其他的并行数据数据处理系统中，很多因素可以导致UDO的数据倾斜。设计者希望SkewTune能够处理各种不同的数据倾斜，而不是针对一种数据倾斜，它监控执行，并且在数据倾斜发生时发出通知，做出相应的响应，而不是纠结是什么原因导致任务变慢了。\n4. 没有同步屏障：并行数据处理系统尽量减少全局同步的屏障，以确保高性能并能够增量产出。即使在MapReduce种，Reducer也可以在Mapper执行完之前开始复制数据。此外新的MapReduce扩展力图促进流水线操作。因此,Skew Tune避免任何设计方案要求阻塞。\n\nSkewTune假设MapReduce工作遵循API的规定：每个map()和reduce()函数调用时独立的。这个假设使SkewTune自动缓解数据偏斜技术。因为重新在Map函数和Reduce函数调用边界分区输入数据使安全的，不会破坏程序的逻辑性。\n\nSkewTune实现方法简介：SkewTune以Hadoop的任务作为输入，将任务Map和Reduce阶段看作是独立的，SkewTune的数据偏斜环节技术被设计用于MapReduce的类型的数据处理引擎。\n\n这些引擎在数据偏斜处理时有以下的重要的特征：\n\n1. 一个协调 —— 工作架构：其中协调节点做出调度决策，而工作节点运行分配给他们的任务。一个任务完成后，工作节点从协调节点哪里获取新的任务。\n2. 去耦执行：一个操作符不会对它的前一个操作符产生反馈，二者彼此独立。\n3. 独立处理记录：执行一个UDO时每条记录都是独立的。\n4. 任务进度估计：估计剩余时间，然后工作节点定期传给协调节点。\n5. 任务统计：跟踪一些基本的统计，例如处理过的或者为处理过的数据大小或记录数。\n\nSkewTune工作原理如图：\n\n![FyLgUg.png](https://s1.ax1x.com/2018/12/23/FyLgUg.png)\n\n（a）显示了在没有SkewTune时，发生数据偏斜后的运行时间有最慢的任务T<sub>2</sub>决定；\n\n（b）显示了在使用SkewTune后，在任务T<sub>1</sub>完成后检测到了数据偏斜，此时标记T<sub>2</sub>为落后者，然后重新分区T<sub>2</sub>种未处理完的数据。系统会根据剩余的数据全部分给T<sub>1</sub>、T<sub>2</sub>、T<sub>3</sub>者三个节点而不仅仅时分配给了T<sub>1</sub>、T<sub>2</sub>两个节点。这样T<sub>3</sub>结束后也可以继续工作，从图中也可以看出，重新分配的目的是保证任务是同时完成的。重新分配后的子任务T<sub>2a</sub>、T<sub>2b</sub>、T<sub>2c</sub>被称作mitigators。SkewTune重复这个检测-减少循环。当检到T<sub>4</sub>的剩余未处理数据重新分区。检测时间太早会导致任务拆分，从而造成不必要的开销，太晚会错过最佳时间，从而使减缓数据偏斜的效果变得更差。\n\n对于整个过程有如下的解释：\n\n1. 后数据偏斜检测：由于任务在连续的阶段使彼此分离的，也就是说，Map任务尽可能地输入数据，产生输出结果，不会因为等待Reduce任务取走输出结果而阻塞。同时Reduce任务也不会因为连续的Map任务中阻塞。因此SkewTune采用延缓数据偏斜矫正、直到有空闲的节点处理的策略，类似于MapReduce种的预测推测执行机制。重新分配开销只发生在有闲置资源时，从而降低了误报率，同时通过立即给空闲节点分配资源来避免漏报。\n\n2. 识别落后者：一次识别并重分配一个落后任务是最有利的。SkewTune通过估计最大剩余时间来落后任务。\n\n3. 标记数据偏斜的原则：剩余处理时间的一半大于重分区的开销，即 $$\\frac{Tremain}{2}>w$$,即式中$$w$$式在30s的量级上的，也就是说任务剩余时间在1min以上才会出发重分区。\n\n    因而我们得到了下属数据的偏斜检测算法：\n\n    算法1：数据偏斜检测算法。\n\n    输入：R是正在执行的任务集；W未为调度等待中的任务及；inProgress为全局表示，指导数据偏斜减缓。\n\n    输出：一个任务调度。\n\n    ```java\n    task<-null  \n    if w ≠ φ then\n    task<-chooseNextTask(W)  //ChoseNextTask()为普通任务调度函数\n    else if inProgress ≠ 0 ≠ φ then\n    \ttask<-argmax time_remain(task) //其中task∈R，这就是要找到的最大的剩余时间任务\n    \tif task ≠ φ && time_remain(task)>2*w then //时间满足上式公式\n    \t\tstopAndMitigate(task) //通知被选中的任务停止任务，并提交处理玩的输出。\n    \t\ttask=NULL\n    \t\tinProgress<-true;\n    \tend if \n    end if      \n    \n    \n    ```\n\n    SkewTune实现数据偏斜减缓的原则如下：尽量减少重分区次数以减少重分区开销(通过标识一个落后者)；尽量减少重分区的可见副作用以实现透明度(使用范围分区)；尽量减少总的开销（使用一种廉价的、线性时间的启发式算法）。\n\n算法步骤：\n\n停止落后的算法：协调者通知落后者停止运算，并且记录下最后处理的位置，然后对剩余数据重分区。如果发生落后者处在不能停下来或者很难停下来的状态时，协调者选择另外一个落后者，或者如果该落后者是最后一个任务，重分区该后者的整个输入。\n\n扫描剩余数据：为了保证数据倾斜减缓的透明性，SkewTune使用范围分区分配工作给mitigator，这样能够保证数据顺序保持不败你。如果使用Hash函数的方式不能保证均匀分派。范围分配落后者的剩余输入数据需要收集剩余数据的一些信息，SkewTune采用了对输入数据的压缩摘要来收集，摘要使用了一些列关键字间隔的方式，每个间隔大致相等。\n\n选择间隔大小|S|标识集群的总节点数，Δ标识未处理的字节数。由于SkewTune想要分配不均匀的工作量给不同的mitigator，所以生成了 $$k*|S|$$ 个间隔，实现更细粒度的数据分配，但他们也通过增加数量增加了开销。\n\n​\t\t\t\t\t\t $$  间隔s= \\lfloor \\frac{Δ}{k*S|} \\rfloor $$\t\t\n\n算法2：间隔生成算法。\n\n输入：$$I$$为有序间隔流；b为初始单位间隔字节大小，在本地扫描中被设置为s；s为目标单位间隔字节大小，也就是阈值；k为间隔的最小值，在本地扫描中被忽略。\n\n输出：间隔列表。\n\n```java\nresult = NULL\ncur = new_interval() (当前间隔)\nfor all i ∈  I do\n    if i.bytes>b || cur.bytes>=b then\n    \tif b<s then\n    \t\tresult.appendIfnotEmpty(cur)\n            if  |result|>=2*k then\n            \tb=min{2*b,s}\n\t\t\t   result=GenerateIntervals(result,b,b,k)\n            end if\n        else \n            result.appendInNotEmpty(cur)\n     \tend if\n        cur = i\n    else \n        cur.updateStat(i)\n        cur.end=i.end\n   end if\nend for\nresult.appendIfnotEmpty(cur)\nreturn result        \n```\n\n选择本地扫描或者并行扫描；SkewTune会比较两种选择的开销。对于本地扫描，Δ标识剩余输入数据的字节，$$\\beta$$标识本地磁盘带宽，时间为  Δ/$$\\beta$$，对于并行扫描，时间为安排一个额外的MapReduce任务和该任务完成需要时间之和。后者等于最慢的节点(设为$$n$$)需要的时间，即$$ \\frac {\\sum\\nolimits_{o∈O_n} o.bytes} {\\beta}$$ ,$$O_n$$ 标识所有节点$$n$$的所有Map输出。\n\n最后按照关系比较大小：Δ/$$\\beta$$ > $$ \\frac {\\sum\\nolimits_{o∈O_n} o.bytes |n\\epsilon N} {\\beta}+\\rho$$\n\nSkewTune实现方法的规划减缓器；目标是找到一个连续的、保持顺序的分配间隔给mitigator由于规划算法在减缓数据偏斜流程的关键路径上，为此要求速度足够快。因此人们提出了具有线性时间复杂度的启发式算法。\n\n该算法分为两个阶段，第一个阶段计算最快的完成时间opt(假设能够对剩余的工作完美分隔)，当一个节点的分配工作小于2 x $$w$$时，停止以防止产生任意小碎片。在第二个阶段中，算法依次给最早可用的减缓器分配的间隔值，不但重复，直到分配完所有的间隔。时间复杂度为$$O(|I|+|S|+log|S|)$$,其中$$I$$是间隔数目，$$S$$是集群中的节点数。\n\nSkewTune能够显著地减少作业在偏斜状态下的工作时间，并且对没有偏斜的开销增加很小，SkewTune同样能够保证输出 和初始未优化的顺序和分区特性，使平台和现有的代码兼容。\n\n## 基于RDMA的MapReduce设计：\n\n由于Map和Reduce过程得到的数据可能分布在不同聚类中，网络性能在考察使用Hadoop MapReduce框架实现的数据密集的应用程序的性能上起到关键作用。由于大数据应用的数据集常有TB或者PB量级，基于Hadoop MapReduce 框架中默认的基于Socket通信模型的数据传输方式成为系统性能的瓶颈。为了避免这种潜在的瓶颈，企业数据中心的开发者开始使用高性能互联网技术（如InfiniBand）网络来提升其大数据应用的性能和规模。虽然这样的系统正在被使用。当前的Hadoopn中间层组件还不能完全利用由InfiniBand所提供的高性能通信功能，研究分析指出，在InfiBand网络使用不同的云计算中间件，可带来性能上的巨大地飞跃。所以当一个高性能网络存在时，人们不得不重新思考Hadoop系统地设计方案。由此，人们提出了一种在InfiniBand网络上基于RDMA的Hadoop MapReduce的设计。这种InfiniBand网络基于MapReduce新的设计方案及检索中间数据的高效预取和高速缓存机制表现良好。\n\n![F6iXJP.png](https://s1.ax1x.com/2018/12/23/F6iXJP.png)\n\n上图是实现Hadoop MapReduce框架的两种设计方案（a）是通常的设计方案,(b)是通常的设计方案，是基于RDMA的设计方案。在图(a)中默认的MapReduce工作流程中，映射输出被保存在本地磁盘上的文件，可以通过TaskTracker访问，在运行ReduceTask、Shuffle、Readuc的过程中，系统通过HTTP提供的这些映射输出。（b）是基于RDMA的MapReuce设计，在经历Shuffle阶段，该设计修改了TaskTracker和ReduceTask两者，以充分发挥RDMA的好处，而不是使用HTTP协议。\n\n### 关键技术\n\n#### 基于RMDA的shuffle设计\n\n在重排阶段，为了实现RDMA的好处，修改了TaskTracker和ReduceTask。\n\nTaskTracker增加以下的心组件。\n\n##### RDMA监听器\n\n每个TaskTracker创建的时候有一个RDMA监听器。Tasker中的RDMA监听器从ReduceTask发来连接请求，并预先建立的队列中添加一个链接，在有需要的情况下启动RDMA接收器。UCR作为一个建立在终端的连接库，一个终端与一个Socket连接相似。RDMA监听器使用UCR连接终端并未每个终端分配响应的缓存。\n\n##### RDMA接收器\n\n每个RMDA接收器对应来及ReduceTask的请求做出响应，RDMA接收器获得终端列表，并从终端接收请求。当收到请求时，其把请求放入DataRequest队列中。\n\n##### DataRequest队列\n\nDataRequest队列用来保存ReduceTask中得到的所有请求。其中从RDMA接收获取并保存储存这些请求，直到RMDA相应其开始进一步处理。\n\n##### RMDA 响应器\n\nRDMA响应器从属于一个等待从DataRequest队列输入请求的线程池。每当DataRequest队列获得一个新的请求，RDMA响应器的一个会对请求做出响应，其是一个轻量级的线程，在发送响应后，如果没有其他请求，就会进入等待状态。\n\n##### ReduceTask增加了RDMA备份器：\n\n在原来的MapReduce设计中，备份线程响应TaskTracker的请求数据并未Merge操作排序数据。同样的，RDMACopier请求发给TaskerTrackers，并把数据存储到DataToMerge队列等候合并操作。\n\n### 快速Merge设计\n\n在MapReduce默认设计中，每个HTTP响应由整个Map的输出文件组成，并将其划分到多个数据包中，在使用高性能网络后，通信时间显著降低，这使一次完成向多个通信步骤传输Map输出文件成为可能。这样，就可以在一些键值对从Map输出到归约器的瞬间合并。因此，在设计中RMDA响应器的一侧TaskTracker发送一定数量的键值对，其发生在Map输出的开始部分而不是整个Map。在接收来自所有Map位置的键值对的同时，一个ReduceTask合并所有这些数据，建立一个优先队列。然后，它不断从优先队列中提取按顺序排序的键值对，并把这些数据写入到一个先入先出的结构体DataToReduce队列中。当每个Map输出文件已经再Map中排序时，ReduceTask的合并操作只有从优先级队列中提取数据，直到来自特定Map的键值对数目减少到零。此时，它需要从特定Map任务得到下一组键值对，并写入到优先队列中。\n\n### 中间数据预提取和缓存\n\n为了确保TaskTracker更快地响应，本地磁盘中地映射输出文件地中间数据要有一个高效地缓存机制。 \n\nTaskTracker在缓存时添加了MapOutput预提取器，MapOutput预提器。MapOutput预提器时用于尽可能快速地缓存那些中间映射输出地守护线程池。完成一个任务映射后，守护进程地一个进程开始从该映射输出数据。并缓存再本地预提取缓存中。这个缓存的新功能就是它可以根据数据获得的难易程度和必要性调整缓存、可以更迅速地根据Reduce Task需求任务优先进行缓存。\n\n\n\n## 论文\n\n[SkewTune:Mitigating Skew in Mapreduce Applications.pdf ](http://note.youdao.com/noteshare?id=24e3f99cf2b1ffadc794b7e662ea3cec&sub=97CF5B93EC8343429399A1B5903B8960)\n\n[High-Performance Design of Hadoop RPC with RDMA over InfiniBand.pdf ](http://note.youdao.com/noteshare?id=b084a3faf3159900dc3346d4e53c5bde&sub=7DE8BC5B10994647A40D2CACDB3779BC)\n\n\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t     参考资料：《Hadoop集群程序设计与开发》\n\n\n\n\n","tags":["MapReduce"],"categories":["大数据"]},{"title":"Hadoop的RPC工作原理","url":"/2018/12/20/Hadoop的RPC/","content":"\n {{ \"RPC远程过程调用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\nHadoop的远程过程调用(Remote Procedure\tCall,RPC)是Hadoop中核心通信机制，RPC主要通过所有Hadoop的组件元数据交换，如MapReduce、Hadoop分布式文件系统(HDFS)和Hadoop的数据库(Hbase)。RPC是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议，RPC假定某些协议如(TCP和UDP)存在，为通信程序之间携带信息数据。\n\n**TCP**\n\nTCP（Transmission Control Protol,传输文本协议）是一种面向连接、可靠的、基于字节流的传输层的通信信息，由IETF的RFC793定义。简化的计算网络OSI模型中，它完成第四层(传输层)所指定的功能，用户数据报协议(UDP)是同一层内另一个重要的传输协议，在因特网协议簇(Internel Protocol Suite)中，TCP基于IP层之上、应用层之下的中间层，不同主机的应用层之间经常需要可靠的、像管道一样的连接，但是IP层不提供这样的流机制，而是提供不可靠的包交换的。\n\n当应用向TCP层发送用于网间传输的、用字节流表示的数据流时，TCP则把数据流分割成适当长度的报文段，最大传输段大小(MSS)通常受该计算机连接网络的数据链路层的最大传送单元(MTU) 限制。之后TCP把数据包传到IP层、由它通过网络将包传送给接收端实体的TCP层。TCP为了保证报文的可靠性，给每个包一个序列号，同时该序列号也保证了传送到接收端实体的包的按序接收。然后接收端实体对已经成功收到的字节发回一个相应的确认(ACK)；如果发送端实体在合理的往返时延(RTT)内未收到去确认，则对应的数据包就被假设为以丢失，将会被重传。\n\n**UDP**\n\nUDP（User Datagram Protocol，用户数据报协议）是OSI(Open System Interconnection,开放式系统互联)参考模型中一种无连接的传输层协议，提供面向事务的简单不可靠信息传送服务，也就是说UDP不提供数据包分组、组装，并且不能对数据包进行排序，当报文发送之后，是无法得知其是否安全完整到达的。IETF RFC 768是UDP的正式规范。\n\n与熟悉的TCP传输控制协议一样，UDP直接位于IP(网络协议)的上层，根据OSI（开放系统互连）参考模型，UDP和TCP都属于传输层协议。UDP的主要作用是将网络数据流量压缩成数据的形式。一个典型的数据包就是一个二进制数据的传输单位，每个数据包的前8个字节用来包含报头信息，剩余字节则用来包含具体的传输数据。\n\n**HTTP**\n\nHTTP（Hyper Text Transfer Protoco;超文本传输协议）是互联网上应用最为广泛的一种网络协议，所有的WWW文件都必须最受这个标准，设计HTTP最初的目的是提供一种发布和接收HTML页面的方法。\n\nHTTP是客户端和服务器请求和应答的的标准。客户端是终端用户，服务器是终端用户，服务器是网站，通过使用Web浏览器、网络爬虫或者其他工具，客户端发起一个服务器到指定端口(默认80)的HTTP请求。这个客户端被称为用户代理(User Agent)。应答服务器上存储着(一些资源)，如HTML的文件和图像。这个应答服务器被称为源服务器(Origin Server)，在用户代理和源服务器中化建可能存在多个中间件，如代理、网关或者隧道(Tunnels)。尽管TCP/IP是互联网上最流行的应用，HTTP并没有规定必须用它和（基于）它支持的层。但事实上，HTTP可以在任何互联网协议上或者其他网络实现。HTTP并没有规定必须使用它和(基于)它支持的层。事实上，HTTP可以在任何互联网协议上或者其他网络上实现。HTTP只假定(其下层协议提供)可靠的传输，任何能提供这种保证的协议都可以被其使用。HTTP使用TCP而不是UDP的原因在于(打开)一个网页必须需要传送很多数据，而TCP提供传输控制，按顺序组织数据和纠正错误。\n\n## RPC简介\n\nRPC的主要功能目标是构建分布式计算（应用场景）更容易，在提供强大的远程调用服务能力时。不损失本地调用的语义整洁性。\n\nRPC由以下特点：\n\n1. 透明性：远程调用其他机器上的程序，对用户来说就像调用本地的方法一样。\n\n2. 高性能：RPC Server能够并发处理多个来自Client的请求。\n\n3. 可控性：JDK中已经提供了一个RPC框架-------RMI,但是该RPC框架过于重量级并且可控制指出比较少，所以Haoop实现了自定义的RPC框架。\n\n    实现RPC程序包括5个部分，即User、User-stub、RPCRuntime、Server-stub、Server\n\n    这里的User就是Client端，当User想发起一个远程调用的时候，它实际时通过本地调用User-stub。User-stub负责将调用的接口、方法和参数通过约定的协议规范进行编码并通过本地的RPCRuntime实例传输到远端的实例。远端RPCRuntime实例收到请求后交给Server-stub进行解码后，发起本地端调用，调用结果再返回给User。\n\n[![Fr2wVK.png](https://s1.ax1x.com/2018/12/20/Fr2wVK.png)](https://imgchr.com/i/Fr2wVK)\n\n## HadoopRPC简介\n\nHadoop中RPC的运行机制\n\n同其他RPC框架一样，HadoopRPC分为4个部分\n\n1. 序列化从层：Client与Server端通信传递的信息采用了Hadoop提供的序列化或自定义的Writable类型。\n2. 函数调用层：HadoopRPC通过动态代理以及Java发射实现函数调用。\n3. 网络传输层：Hadoop RPC采用了基于TPC/IP的Socket机制。\n4. 服务端框架层：RPC Server利用Java NIO及采用了事件驱动的I/O模型，提高自己的并发处理能力。\n\nHadoop RPC 再整个Hadoop应用中十分 的广泛、Client、DataNode、NameNode之间的通信全靠它。例如人们平时操作HDFS的时候使用的时FileSystem类，它内部就有一个DFSClient对象，这个对象负责与 NameNode打交道。再运行时DFSClient在本地创建一个NameNode的代理。然后就操作这个代理，这个代理就会通过网络，远程调用NameNode方法，当然它也能返回值。\n\nHadoop RPC 默认设计时基于Java套接字通信，基于高性能网络的通信并不能达到最大的性能，也会是一个性能瓶颈，当一个调用要被发送到服务器时,RPC客户端首先分配DataOutputBuffer，它包含了一个常见的Java版本具有32字节的默认内部缓存器 。该缓冲区可以保存所有序列化的数据，然而一个很大的序列化数据不能保存在较小的内部缓存中。\n\n## 设计艺术\n\n**动态代理**\n\n动态代理可以提供另一个对象的访问，同时隐藏实际情况的具体事务，因为代理对象对客户隐藏了实际的多项，目前Java开发包提供对动态代理的支持，但是现在只支持对接口的实现。\n\n**反射  -------- 动态加载类 ** \n\n反射机制时在运行状态中，对与任意一个类，都能够知道这个类的所有属性和方法：对于任意一个对象，都能够调用它的任意一个方法和属性。\n\n使用反射类来动态加载类的步骤：定义一个接口，让所有功能类都实现该接口，通过类名来加载类，\t得到该类的类型，命令为\n\n```java\nClass mClass = Class.forName(“ClassName”):\n```\n\n使用newInstance()方法获取该类的一个对象,命令为\n\n```java\nmClass.newInstance()；\n```\n\n将得到的对象转换成接口类型，命令为 \n\n```java\nInterfaceName = objectName(InterfaceName)mClass.newInstance()；\n```\n\n通过该类的对象来调用其中的方法。\n\n**序列化**\n\n序列化是把对象转换成字节序列的过程，反过来说就是反序列化。它有两方面的应用：一方面是存储对象，可以是永久存储在硬盘的一个文件上，也就是存储在redis支持序列化存储的一个容器中；另外一方面是远程传输对象。\n\n**非阻塞异步IO**\n\n非阻塞异步IO是指用户调用读写方法是不堵塞的，立即返回，而且用户不需要关注读写，需要提供回调操作，内核线程在完成读写后回调用户提供的callback。\n\n## 使用RPC的步骤\n\n1. 定义RPC协议\n\n    RPC协议是客户端与服务端之间的通信接口，它定义了服务器端对外提供的服务接口。\n\n2. 实现RPC协议\n\n    Hadoop RPC协议通常是一个Java接口，需要用于实现该接口。\n\n3. 构造和启动RPC Server\n\n    直接使用静态类Builder构造一个RPC Server，并调用函数start()启动该 Server()。\n\n4. 构造RPC Client 并发送请求\n\n    使用静态方法getProxy构造客户端代理对象，直接通过代理对象调用远程端的方法。\n\n\n##  应用实例\n\n**定义一个RPC协议**\n\n下面定义一个IProxyProtocol通信接口，声明一个Add()方法。\n\n```java\npackage rpc;\n\nimport org.apache.hadoop.ipc.VersionedProtocol;\n\n\npublic interface IRPCInterface extends VersionedProtocol {\n    long versionID = 1;    //版本号，默认情况下不同版本号之间的server和client不能通信。\n\n    int add(int a, int b);\n}\n```\n\n注意：\n\n1. Hadoop中所有自定义RPC接口都需要继承VersionedProtocol接口，它描述了协议的版本信息。\n2. 默认情况下，不同版本号的RPC Client 和Server之间不能相互通信，因为客户端和服务器端通过版本号标识。  \n\n**实现RPC协议**\n\nHadoop RPC 协议通常是一个Java接口，用户需要实现该接口。对IProxyProtocol接口进行简单的实现：\n\n```java\npackage rpc;\n\nimport org.apache.hadoop.ipc.ProtocolSignature;\n\nimport java.io.IOException;\n\nclass IRPCInterfaceImpl implements IRPCInterface {\n\n    public int add(int a, int b) {\n        return a + b;\n    }\n\n    public long getProtocolVersion(String s, long l) throws IOException {\n        return IRPCInterface.versionID;\n    }\n\n    public ProtocolSignature getProtocolSignature(String s, long l, int i) throws IOException {\n        return new ProtocolSignature(IRPCInterface.versionID, null);\n    }\n}\n```\n\n这里实现Add方法很简单就是一个加法操作。\n\n**构建RPC Server 并启动服务**\n\n通过RPC静态方法getServer来获得Server对象\n\n```java\npackage rpc;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.ipc.RPC;\nimport org.apache.hadoop.ipc.RPC.Server;\n\nimport java.io.IOException;\n\npublic class RpcServer {\n    public static void main(String[] args) throws IOException {\n        Server server = new RPC.Builder(new Configuration()).\n                setBindAddress(\"127.0.0.1\").setPort(8080).\n                setInstance(new IRPCInterfaceImpl()).\n                setProtocol(IRPCInterface.class).build();\n\n        server.start();\n    }\n}\n\n```\n\n启动：\n\n![FsCJWF.md.png](https://s1.ax1x.com/2018/12/21/FsCJWF.md.png)\n\n![FsCYz4.png](https://s1.ax1x.com/2018/12/21/FsCYz4.png)\n\n\n\n**构造RPC Client 并发送请求**\n\n这里使用静态方法getPRoxy或者waitForProxy构造客户端代理对象，直接通过代理对象调用远程端的方法，具体代码如下\n\n```java\npackage rpc;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.ipc.RPC;\n\nimport java.io.IOException;\nimport java.net.InetSocketAddress;\n\npublic class RpcClient {\n    public static void main(String[] args) throws IOException {\n        IRPCInterface proxy = RPC.getProxy(IRPCInterface.class, 1,\n                new InetSocketAddress(\"127.0.0.1\", 8080), new Configuration());\n        int result = proxy.add(2018, 1);\n        System.out.println(\"result = \" + result);\n    }\n}\n\n```\n\n![FsCaLR.md.png](https://s1.ax1x.com/2018/12/21/FsCaLR.md.png)\n\n经过上面的四步，我们建立起了一个非常高效的客户机-------服务端网络模型\n\n## RPC的改进 \n\n目前的Hadoop RPC设计也存在性能瓶颈，人们通过InfiniBand提出了一个超过Hadoop，RPC的、高性能设计的RPCoIB，然后也提出了一个JVM——旁路缓冲管理方案，并在观察消息的大小，以免多重记忆分配、数据序列化和反序列化副本。这种新的设计可以在Hadoop中很容易地与HDFS、Hbase及MapReduce框架进行很好地继承。此外，RPCoIB还对外应用程序提高了很好的灵活性。\n\n### 基本思想\n\nRPCoIB使用与现在有的用于基于套接字的 Hadoop RPC 由相同的接口。它的基本思想是：\n\n目前的Hadoop RPC 是基于Java 的InputStream 、OutputStream 及SocketChannel,InputStream和OutputStream主要用于客户端,SocketChannel由服务器使用,他们之间基本的读写操作都很相似.为了更好的权衡性能和向后兼容,要设计一条基于RDMA的Java IO 的接口兼容类这些类包括RDMAInputStream|、RDMAOutputStream及RDMAChanael。\n\n### JVM ——路缓冲管路\n\n当RDMAOutStream或RDMAInputStream构建地时候，它们将获得一个从本地缓冲得到的本地缓冲区。当RPCoIB库加载时，这些缓冲区为RDMA操作进行预分配和预先登记。得到的缓存开销是非常小的，并且分配将所有的调用进行摊销。这些本地缓冲区将被包装在Java的DirectByteBuffer，它可以由Java层和本地IO层进行访问。在Java层的所有串行化数据可以直接存储在本地缓冲区，以避免中间缓冲区JVM中的堆空间，当缓冲区被填满时，意味着序列化的空间太大而被保存在当前缓冲区，当被请求调用时，所存储的数据将通过实时的JNI RDMA库被送出去。该库通过第一次直接缓冲区操作得到本地缓冲区地址，然后使用RDMA操作来访问数据。在接收和反序列化过程中，接收器也可以用RDMAInputStream，以避免额外的内存复制。\n\n![FsiFC8.md.png](https://s1.ax1x.com/2018/12/21/FsiFC8.md.png)\n\n​\t\t\t\t\tJVM  旁路缓冲区和通信管理中的RDMA输入和输出流 \n\n基于历史两级缓冲池：首先定义了一种元组<portocol,method>。protocol时方法的类名，它需要Hadoop的RPC中注册。在整个MapReduce作业期间，许多类型的调用会保持很可靠的大小进行执行。然而。一些请求似乎有着不规则的消息大小。从图中可以看到JT_heartbeat 和NN_getFileInfo。\n\n![Fsi6rd.md.png](https://s1.ax1x.com/2018/12/21/Fsi6rd.md.png)\n\n这表明，当得到一个缓冲区（具有适当尺寸）以处理当前调用时，缓冲区有较高的可能性利用与下一个调用相同的元组<protocol,method>,这种现象被称为 Message Size Locality 因此人们提出可上图的缓冲池设计。\n\n两级缓冲池模型如下：一个是本地内存池，另外一个是在JVM层的阴影池。阴影吃的内存缓冲来来自本地缓冲池，它指向本地缓冲区并提供Java层的DirectBuffer对象。\n\n![FsFyWT.md.png](https://s1.ax1x.com/2018/12/21/FsFyWT.md.png)\n\n​\t\t\t\t\t     \t\t\t\t      基于历史的两级缓存池\n\n在默认Hadoop的RPC中，客户端客户端有两个主要线程一个是调用程序线程，它负责获取或创建连接，调用发送和等待来及RPC服务器的结果，另外一个是Connection进程。当RPC客户端希望使用远程过程调用时，首先与RPC服务器交换结束点信息。由于设计RDMAInputStream和RDMAOutputStream需要在Connection中修改setupIOStreams以提供基于RDMA的IO流。有了这个设计，可以看到，并不需要对现有的RPC并不惜要对RPC接口进行任何改变，这样可以与默认的Hadoop RPC 保持相同的接口语义和机制，因此，上层应用程序可以透明的使用这样的设计。\n\n InfiniBand架构是一种支持多并发连接的“转换线缆”技术，在这种技术中，每种连接都可以2.5Gbit/s的运行速度。这种架构在一个连接的时候是500Mbit/s，4个链接的时候速度是2Gbit/s的运行速度，12个链接的时候速度可以达到6Gbit/s。\n\n与当前计算机的I/O子系统不同，InfiniBand是一个功能完善的通信系统。InfiniBand贸易组织把这种新的总线结构称为I/O网络，并把它比作开关，因为所给信息寻求目的路径是由控制矫正信息决定的。InfiniBand使用的是IPv6的128位地址空间，因此它具有近乎无限量的设备拓展性。\n\n## 论文\n\n[High-Performance Design of Hadoop RPC with RDMA over InfiniBand*.pdf](http://note.youdao.com/noteshare?id=ecd3a7342bb7ab646a0cadb8f0b4e4b6&sub=CC0CC98308B74200A15C51D9B91FBB41)\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n​\t\t\t\t\t\t\t\t\t\t\t\t\t参考资料：《Hadoop集群程序设计与开发》","tags":["RPC"],"categories":["大数据"]},{"title":"Hadoop的I/O操作","url":"/2018/12/20/Hadoop的IO操作/","content":"\n {{ \"Hadoop的IO\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\nHadoop自带的一条原子操作作用域数据I/O操作,其中有一些技术比Hadoop更常用,如数据完整性保持和压缩在处理好几个TB级别的数据集时值得关注.\n\n## 数据完整性\n\nHadoop用户不希望在存储和处理数据时丢失或损坏任何数据，但是当系统中需要处理数据量达到Hadoop处理极限时，数据被损坏不可避免。\n\n检验数据是否损坏常见的措施是：在第一次数据被引入系统时计算校验和(checksum)并在数据通过一个不可靠的通道进行传输时再次计算校验和，如果再次计算的校验和和原来的校验和不匹配，我们可以认为数据已坏，该技术只是检测无法恢复，因此推荐使用ECC内存（实现错误检查和纠正技术的内存条），当然校验和也有可能是损坏的，但是几率比较低。\n\n最常用的错误检测码时CRC-32（32位循环冗余校验）任何大小的数据计算得到一个32位的整数校验和。HadoopCheckFileSysterm使用CRC-32计算校验和。HDFS用于校验和计算的则是一个更有效的变体CRC-32C。\n\n## HDFS数据完整性\n\nHDFS对写入的数据计算校验和，读取数据时验证，它追对每个由dfs.bytes-per-checksum指定字节的数据计算校验和。默认情况下位512个字节，由于CRC-32是4个字节，所以存储校验和的额外开销低于1%。\n\ndatanode负责收到数据后存储该数据及其校验和之前的数据进行验证，它在收到客户端的数据或复制其他datanode的数据时执行这个操作。正在写数据的客户端以及校验和发送到由一系列datanode组成的管线，管线最后一个datanode负责校验和，每个datanode都保存用于验证校验日志和的日志(persistent log of checksum verification)，所以它知道每个数据块的最后一次验证时间，客户端验证一个数据块后，就会告诉这个datanode，datanode由此更新日志，保存这些统计信息对于检测损坏的磁盘很有价值。\n\n不只是客户端读取数据的时候会验证校验和，每个datanode也会有个后台线程运行DataBlockScanner，从而定期验证存储在datanode上的所有数据块。\n\nHDFS是多副本存储，因此可以通过数据副本来修复损坏的数据块，从而获取完好的、新的副本，如果检查到错误，先向Namenode报告这个数据块副本标记为已损坏以及正在尝试读操作的这个datanode抛出ChecksumExcepion，这样它不将客户端请求资源直接发送到这个节点。或者尝试将这个副本复制到其他datanode。之后安排这个数据块的一个副本复制到另外一个datanode，如此以来数据块的复制因子(replication factor)又回到了期望水平。此后一个损坏的数据块副本便被删除了。\n\n在使用open()方法读取文件之前，将false值传递给FileSystem对象的setVeryfyChecksum()方法，既可以禁用校验和验证。在命令解释器中使用-get选项中的-ignoreCrc命令或者使用等价的-copyToLocal命令，也可以达到同样的效果，如果一个损坏的文件需要检查并决定如何处理，这个特性是十分有用的。例如你在删除该文件之前尝试看看是否能够恢复部分数据。\n\n可以hadoop的命令 fs -checksum 来检查一个文件的校验和。\n\n## 压缩\n\n文件压缩有两大好处；减少存储文件所需要的磁盘空间，加速数据在网络和磁盘上的传输，在处理大量数据的时候相当重要。\n\n### 格式\n\n| 压缩格式 | 工具  | 算法    | 文件拓展名 | 可切分    |\n| -------- | ----- | ------- | ---------- | --------- |\n| DEFLATE  | 无    | DEFLATE | .deflate   | 否        |\n| gzip     | gzip  | DEFLATE | .gz        | 否        |\n| bzip2    | bzip2 | bzip2   | .bz2       | 是        |\n| LZO      | lzop  | LZO     | .lzo       | 是:被索引 |\n| LZ4      | 无    | LZ4     | .lz4       | 否        |\n| Snappy   | 无    | Snappy  | .snappy    | 否        |\n\n### 性能\n\n```\n测试环境:\n8 core i7 cpu \n8GB memory\n64 bit CentOS\n1.4GB Wikipedia Corpus 2-gram text input\n```\n\n![FrZhtI.md.png](https://s1.ax1x.com/2018/12/20/FrZhtI.md.png)\n\n可以看出压缩比越高，压缩时间越长，压缩比：Snappy < LZ4 < LZO < GZIP < BZIP2\n\n`gzip: `\n优点：压缩比在四种压缩方式中较高；hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；有hadoop native库；大部分linux系统都自带gzip命令，使用方便。 \n缺点：不支持split。\n\n`lzo压缩 `\n优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；需要在linux系统下自行安装lzop命令，使用方便。 \n缺点：压缩率比gzip要低；hadoop本身不支持，需要安装；lzo虽然支持split，但需要对lzo文件建索引，否则hadoop也是会把lzo文件看成一个普通文件（为了支持split需要建索引，需要指定inputformat为lzo格式）。\n\n`snappy压缩 `\n优点：压缩速度快；支持hadoop native库。 \n缺点：不支持split；压缩比低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。\n\n`bzip2压缩 `\n优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。 \n\n缺点：压缩/解压速度慢；不支持native。\n\n## 使用\n\nMapReduce 可以在三个阶段中使用压缩。\n\n输入压缩文件。如果输入的文件是压缩过的，那么在被 MapReduce 读取时，它们会被自动解压。\n\nMapReduce 作业中，对 Map 输出的中间结果集压缩。实现方式如下：\n\n可以在 core-site.xml 文件中配置，代码\n\n```xml\n<property>\n  \t   <name>mapred.compress.map.output</name>\n       <value>true</value>\n</property>\n```\n\nJava代码\n\n```java\nconf.setCompressMapOut(true);\nconf.setMapOutputCompressorClass(GzipCode.class);\n```\n\n最后一行代码指定 Map 输出结果的编码器。\n\nMapReduce 作业中，对 Reduce 输出的最终结果集压。实现方式如下：\n\n可以在 core-site.xml 文件中配置，代码如下\n\n```xml\n<property>\n  \t   <name>mapred.output.compress</name>\n       <value>true</value>\n</property>\n```\n可以在 core-site.xml 文件中配置，代码如下\n\n```java\nconf.setBoolean(“mapred.output.compress”,true);\nconf.setClass(“mapred.output.compression.codec”,GzipCode.class,CompressionCodec.class);\n```\n最后一行同样指定 Reduce 输出结果的编码器。\n\n## 压缩框架\n\n前面已经提到过关于压缩的使用方式，其中第一种就是将压缩文件直接作为入口参数交给 MapReduce 处理，MapReduce 会自动根据压缩文件的扩展名来自动选择合适解压器处理数据。那么到底是怎么实现的呢？如下图所示：\n\n![FrmJZ4.png](https://s1.ax1x.com/2018/12/20/FrmJZ4.png)\n\n[以下引自](http://www.cnblogs.com/xuxm2007/archive/2012/06/15/2550996.html)[xuxm2007](http://www.cnblogs.com/xuxm2007/archive/2012/06/15/2550996.html):\n\n**CompressionCodec对流进行压缩和解压缩**\n\nCompressionCodec有两个方法可以用于轻松地压缩或解压缩数据。要想对正在被写入一个输出流的数据进行压缩，我们可以使用 createOutputStream(OutputStreamout)方法创建一个CompressionOutputStream（未压缩的数据将 被写到此)，将其以压缩格式写入底层的流。相反，要想对从输入流读取而来的数据进行解压缩，则调用 createInputStream(InputStreamin)函数，从而获得一个CompressionInputStream,，从而从底层的流 读取未压缩的数据。CompressionOutputStream和CompressionInputStream类似干 java.util.zip.DeflaterOutputStream和java.util.zip.DeflaterInputStream，前两者 还可以提供重置其底层压缩和解压缩功能，当把数据流中的section压缩为单独的块时，这比较重要。比如SequenceFile。\n\n**下例中说明了如何使用API来压缩从标谁输入读取的数据及如何将它写到标准输出：**\n\n```java\npublic class StreamCompressor \n{\n     public static void main(String[] args) throws Exception \n     {\n          String codecClassname = args[0];\n          Class<?> codecClass = Class.forName(codecClassname); // 通过名称找对应的编码/解码器\n          Configuration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);\n // 通过编码/解码器创建对应的输出流\n          CompressionOutputStream out = codec.createOutputStream(System.out);\n // 压缩\n          IOUtils.copyBytes(System.in, out, 4096, false);\n          out.finish();\n     }\n} \n```\n\n用CompressionCodecFactory方法来推断CompressionCodecs\n\n阅读一个压缩文件时，我们通常可以从其扩展名来推断出它的编码/解码器。以.gz结尾的文件可以用GzipCodec来阅读，如此类推。每个压缩格式的扩展名如第一个表格;\n\nCompressionCodecFactory提供了getCodec()方法，从而将文件扩展名映射到相应的CompressionCodec。此方法接受一个Path对象。下面的例子显示了一个应用程序，此程序便使用这个功能来解压缩文件。\n\n```java\npublic class FileDecompressor {\n    public static void main(String[] args) throws Exception {\n       String uri = args[0];\n       Configuration conf = new Configuration();\n       FileSystem fs = FileSystem.get(URI.create(uri), conf);\n       Path inputPath = new Path(uri);\n       CompressionCodecFactory factory = new CompressionCodecFactory(conf);\n       CompressionCodec codec = factory.getCodec(inputPath);\n       if (codec == null) {\n           System.err.println(\"No codec found for \" + uri);\n           System.exit(1);\n       }\n       String outputUri =\n       CompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension());\n       InputStream in = null;\n       OutputStream out = null;\n       try {\n           in = codec.createInputStream(fs.open(inputPath));\n           out = fs.create(new Path(outputUri));\n           IOUtils.copyBytes(in, out, conf);\n       } finally {\n           IOUtils.closeStream(in);\n           IOUtils.closeStream(out);\n       }\n    }\n}\n```\n\n编码/解码器一旦找到，就会被用来去掉文件名后缀生成输出文件名（通过CompressionCodecFactory的静态方法removeSuffix()来实现）。这样，如下调用程序便把一个名为file.gz的文件解压缩为file文件:\n\n```\n% hadoop FileDecompressor file.gz\n```\n\nCompressionCodecFactory 从io.compression.codecs配置属性定义的列表中找到编码/解码器。默认情况下，这个列表列出了Hadoop提供的所有编码/解码器 (见表4-3)，如果你有一个希望要注册的编码/解码器(如外部托管的LZO编码/解码器)你可以改变这个列表。每个编码/解码器知道它的默认文件扩展 名，从而使CompressionCodecFactory可以通过搜索这个列表来找到一个给定的扩展名相匹配的编码/解码器(如果有的话)。\n\n| 属性名                | 类型           | 默认值                                                       |\n| :-------------------- | -------------- | ------------------------------------------------------------ |\n| io.compression.codecs | 逗号分隔的类名 | org.apache.hadoop.io.compress.DefaultCodec, org.apache.hadoop.io.compress.GzipCodec, |\n|                       |                | org.apache.hadoop.io.com                                     |\n\n**本地库**\n\n考虑到性能，最好使用一个本地库（native library）来压缩和解压。例如，在一个测试中，使用本地gzip压缩库减少了解压时间50%，压缩时间大约减少了10%(与内置的Java实现相比 较)。表4-4展示了Java和本地提供的每个压缩格式的实现。井不是所有的格式都有本地实现(例如bzip2压缩)，而另一些则仅有本地实现（例如 LZO）。\n\n| 压缩格式 | Java实现 | 本地实现 |\n| -------- | -------- | -------- |\n| DEFLATE  | 是       | 是       |\n| gzip     | 是       | 是       |\n| bzip2    | 是       | 否       |\n| LZO      | 否       | 是       |\n\nHadoop带有预置的32位和64位Linux的本地压缩库，位于库/本地目录。对于其他平台，需要自己编译库，具体请参见Hadoop的维基百科http://wiki.apache.org/hadoop/NativeHadoop。\n\n本地库通过Java系统属性java.library.path来使用。Hadoop的脚本在bin目录中已经设置好这个属性，但如果不使用该脚本，则需要在应用中设置属性。\n\n默认情况下，Hadoop会在它运行的平台上查找本地库，如果发现就自动加载。这意味着不必更改任何配置设置就可以使用本地库。在某些情况下，可能 希望禁用本地库，比如在调试压缩相关问题的时候。为此，将属性hadoop.native.lib设置为false，即可确保内置的Java等同内置实现 被使用(如果它们可用的话)。\n\nCodecPool(压缩解码池)\n\n如果要用本地库在应用中大量执行压缩解压任务，可以考虑使用CodecPool，从而重用压缩程序和解压缩程序，节约创建这些对象的开销。\n\n下例所用的API只创建了一个很简单的压缩程序，因此不必使用这个池。此应用程序使用一个压缩池程序来压缩从标准输入读入然后将其写入标准愉出的数据：\n\n```java\npublic class PooledStreamCompressor \n {\n    public static void main(String[] args) throws Exception \n    {\n        String codecClassname = args[0];\n        Class<?> codecClass = Class.forName(codecClassname);\n        Configuration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf);\n        Compressor compressor = null;\n        try {\ncompressor = CodecPool.getCompressor(codec);//从缓冲池中为指定的CompressionCodec检索到一个Compressor实例\nCompressionOutputStream out = codec.createOutputStream(System.out, compressor);\n            IOUtils.copyBytes(System.in, out, 4096, false);\n            out.finish();\n        } finally\n        {\n            CodecPool.returnCompressor(compressor);\n        }\n      }\n} \n```\n\n我 们从缓冲池中为指定的CompressionCodec检索到一个Compressor实例，codec的重载方法 createOutputStream()中使用的便是它。通过使用finally块，我们便可确保此压缩程序会被返回缓冲池，即使在复制数据流之间的字 节期间抛出了一个IOException。\n\n**压缩和输入分割**\n\n在考虑如何压缩那些将由MapReduce处理的数据时，考虑压缩格式是否支持分割是很重要的。考虑存储在HDFS中的未压缩的文件，其大小为1GB. HDFS块的大小为128MB ，所以文件将被存储为8块，将此文件用作输入的MapReduce会创建8个输入分片(split，也称为\"分块\")，每个分片都被作为一个独立map任务的输入单独进行处理。\n\n现在假设，该文件是一个gzip格式的压缩文件，压缩后的大小为1GB。和前面一样，HDFS将此文件存储为 8块。然而，针对每一块创建一个分块是没有用的因为不可能从gzip数据流中的任意点开始读取，map任务也不可能独立于其分块只读取一个分块中的数据。gZlp格式使用DEFLATE来存储压缩过的数据，DEFLATE 将数据作为一系列压缩过的块进行存储。问题是，每块的开始没有指定用户在数据流中任意点定位到下一个块的起始位置，而是其自身与数据流同步。因此，gzip不支持分割(块)机制。在这种情况下，MapReduce不分割gzip格式的文件，因为它知道输入的是gzip格式(通过文件扩展名得知)，而gzip压缩机制不支持分割机制。这样是以牺牲本地化为代价:一个map任务将处理8个HDFS块，大都不是map的本地数据。与此同时，因为map任务少，所以作业分割的粒度不够细，从而导致运行时间变长。\n\n在我们假设的例子中，如果是一个LZO格式的文件，我们会碰到同样的问题，因为基本压缩格式不为reader 提供方法使其与流同步。但是，bzip2格式的压缩文件确实提供了块与块之间的同步标记(一个48位的π近似值) 因此它支持分割机制。对于文件的收集，这些问题会稍有不同。ZIP是存档格式，因此t可以将多个文件合并为一个ZIP文件。每个文单独压缩，所有文档的存储位置存储在ZIP文件的尾部。这个属性表明Z l P 文件支持文件边界处分割，每个分片中包括ZIP压缩文件中的一个或多个文件。\n\nZIP格式文件的结构如下图：ZIP文件结构请查看[wiki](https://en.wikipedia.org/wiki/Zip_%28file_format%29#File_headers)\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601164323.png)\n\n**在MapReduce 中使用压缩**\n\n如前所述，如果输入的文件是压缩过的.那么在被MapReduce 读取时，它们会被自动解压，根据文件扩展名来决定应该使用哪一个压缩解码器。如果要压缩MapReduce作业的输出.请在作业配置文件中将mapred.output.compress属性设置为true，将mapred.output.compression.codec属性设置为自己打算使用的压缩编码/解码器的类名。\n\n```java\n public class MaxTemperatureWithCompression {\n \n        public static void main(String[] args) throws Exception {\n            if (args.length != 2) {\n                System.err.println(\"Usage: MaxTemperatureWithCompression <input path> \"\n                        + \"<output path>\");\n                System.exit(-1);\n            }\n            Job job = new Job();\n            job.setJarByClass(MaxTemperature.class);\n            FileInputFormat.addInputPath(job, new Path(args[0]));\n            FileOutputFormat.setOutputPath(job, new Path(args[1]));\n            job.setOutputKeyClass(Text.class);\n            job.setOutputValueClass(IntWritable.class);\n            FileOutputFormat.setCompressOutput(job, true);\n            FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class);\n            job.setMapperClass(MaxTemperatureMapper.class);\n            job.setCombinerClass(MaxTemperatureReducer.class);\n            job.setReducerClass(MaxTemperatureReducer.class);\n            System.exit(job.waitForCompletion(true) ? 0 : 1);\n        }\n    }\n```\n\n我们使用压缩过的输入来运行此应用程序(其实不必像它一样使用和输入相同的格式压缩输出)，如下所示:\n\n% hadoop MaxTemperatur eWithCompression input/ ncdc/sample.txt.gz\n\noutput\n\n量生终输出的每部分都是压缩过的.在本例中，只有一部分:\n\n% gunzip -c output/part-OOOOO.gz\n\n1949 111\n\n1950 22\n\n如果为输出使用了一系列文件， 可以设置mapred.output.compresson.type 属性来控制压缩类型。默认为RECORD，它压缩单独的记录。将它改为BLOCK，可以压缩一组记录，由于有更好的压缩比，所以推荐使用。\n\n**map 作业输出结果的压缩**\n\n即使MapReduce 应用使用非压缩的数据来读取和写入，我们也可以受益于压缩map阶段的中阔输出。因为map作业的输出会被写入磁盘并通过网络传输到reducer节点，所以如果使用LZO之类的快速压缩，能得到更好的性能，因为传输的数据量大大减少了.表4-5显示了启用map输出压缩和设置压缩格式的配置属性.\n\n![](https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/大数据/Spark/SQL/20190601164358.png)\n\n下面几行代码用于在map 作业中启用gzip 格式来压缩输出结果:\n\n```\nConfiguration conf = new Configuration();\nconf.setBoolean(\"mapred.compress.map.output\", true);\nconf.setClass(\"mapred.map.output.compression.codec\", GzipCodec.class,\nCompressionCodec.class);\nJob job = new Job(conf);\n```\n\n旧的API要这样配置:\n\n```java\nconf.setCompressMapOutput(true);\nconf.setMapOutputCompressorClass(GzipCodec.class);\n```\n\n压缩就到此为止了。总之编码和解码在hadoop有着关键的作用。","tags":["Hadoop"],"categories":["大数据"]},{"title":"Yarn","url":"/2018/12/19/Yarn/","content":"\n {{ \"Yarn的基本介绍和模式\"}}：<Excerpt in index | 首页摘要><!-- more -->\n<The rest of contents | 余下全文>\n\n## YARN的介绍\n\nApache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。\n\nYARN是再MRv1发展过来的，它克服了NRv1的各种限制，因此我们先了解一下MRv1\n\n### MRv1\n\n![FBb5dA.md.png](https://s1.ax1x.com/2018/12/19/FBb5dA.md.png)\n\nJobTracker：用户程序提交了一个Job，任务（job）会发给JobTracker，JobTracker是Map-Reduce框架中心，它负责把任务分解成map和reduce的作业（task）；需要与集群中的机器定时心跳(heartbeat)通信；需要管理那些程序应该跑在那些机器上；需要管理所有job失败、重启操作。\n\nTasktracker是JobTracker和Task之间的桥梁，Tasktracker可以配置map和reduce的作业操(task slot)。TaskTracker通过心跳告知JobTracker自己还有空闲的作业Slot时，JobTrackr会向其分派任务。它将接收并执行JobTracker的各种命令:启动任务、提交任务、杀死任务。\n\nTaskScheduler工作再JobTracker上。根据JobTracker获得的slot信息完成具体的分配工作，TaskScheduler支持多种策略以提高集群工作效率。\n\n#### 局限性\n\n1. 扩展局限性：JobTracker同时要兼顾资源管理和作业控制的两个功能，成为系统的一个瓶颈，制约Hadoop集群计算能力的拓展。（4000节点任务数400000）\n2. MRv1采用master/slave结构，其中JobTracker作为计算管理的master存在单点问题，它容易出现故障整个MRv1不可用。\n3. 资源利用率局限。MRv1采用了基于作业槽位（slot）的资源分配模型，槽位时一个粗粒度的资源分配单位，通常一个作业不会用完槽位对应的物理资源，使得其他作业也无法再利用空闲资源。对于一些IO密集，CPU空闲,当作业槽占满之后其他CPU密集型的也无法使用集群。MRv1将槽分为Map和Reduce两种slot，且不允许他们之间共享，这样做会导致另一种槽位资源紧张，另外一种资源闲置。\n4. 无法支持多种计算框架，随着数据处理要求越来越精细，Mapreduce这种基于磁盘的离线计算框架已经满足不了各方需求，从而出现了新的计算框架，如内存计算框架、流式计算框架、迭代计算资源框架、而MRv1不支持多种计算框架并存。\n\n### YARN\n\n随着互联网的高速发展，新的计算框架不断出现，如内存计算框架、流式计算框架、迭代计算资源框架、这几种框架通常都会被用到考虑到资源的利用率运维和数据共享等因素，企业通常希望将所有的计算框架部署到一个 公共集群中，让他们共享集群的计算资源，并对资源进行同意使用，同时又能采用简单的资源隔离方案，这样便催生了轻量弹性计算平台需求Yarn的设计是一个弹性计算平台，不仅仅支持Mapreduce计算框架而是朝着对多种计算框架进行统一管理方向发展。\n\n![FBvbzq.md.png](https://s1.ax1x.com/2018/12/19/FBvbzq.md.png)\n\n#### 优点\n\n1. 资源利利用率高，按照框架角度进行资源划分，往往存在应用程序数据和计算资源需求的不均衡性，使得某段时间内计算资源紧张，而另外一种计算方式的资源空闲，共享集群模式则通过框架共享全部的计算资源，使得集群中的资源更加充分合理的利用。\n\n2. 运维成本低，如果使用：”一个框架一个集群“的模式，运维人员需要独立管理多个框架，进而增加运维的难度，共享模式通常只需要少数管理员可以完成多个框架的管理。\n\n3. 数据共享，随着数据量的增加，跨集群之间的数据不仅增加了硬件成本，而且耗费时间，共享集群模式可以共享框架和硬件资源，大大降低了数据移动带来的成本。\n\n#### 组件\n\nYARN采用了Master/Slave结构，在整个资源管理框架中ResourceManager为master，NodeManager为Slave。ResourceManager负责对各个 NodeManager上的资源进行统一的管理和调度，当用户提交一个应用程序时，需要生成以一个用于追踪和管理这个程序即ApplicationMaster，它负责向ResourceManager申请资源，并要求NodeManager启动可以占用一定的资源任务，不同的ApplicationMaster会被分不到不同的节点上，他们之间是相互独立的。\n\n[![FDCkZV.md.png](https://s1.ax1x.com/2018/12/19/FDCkZV.md.png)](https://imgchr.com/i/FDCkZV)\n\n##### ResourceManager(RM)\n\nResourceManager负责整个系统的资源分配和管理，是一个全局的资源管理器。主要由两个组件构成：调度器和应用管理器。\n\n1. 调度器(Scheduler)：调度器根据队列、容器等限制条件（每个队列配给  一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各自正在运行的程序。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象的概念”资源容器“(Container)从而限定每个使用的资源量，容器用于执行的特定应用的进程每个容器都有所有资源限制（CPU 内存 等）一个容器可以是一个Unix进程也可以是一个Linux cgroup(Linux内核提升的一种限值，记录隔离进程并且使用的物理资源 CPU 内存 IO 等 谷歌提出)调度器相应应用程序的资源请求，是可插拔的，如Capcity  Scheduler、Fair Scheduler。\n2. 应用程序管理器（Applications Manager）：应用程序管理器负责管理整个系统的所有应用程序，包括应用程序提交，与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动等，追踪分给的Container的情况。\n\n##### ApplicationMaster(AM)\n\nApplicationMaster是一个详细的框架库，它结合了ResourceManager获取资源和NodeManager协同工作来运行和监控任务，用户提交的每一个应用程序军包含一个AM，主要的功能是：\n\n1. 与ResourceManager调度器协商以获取抽象资源(Container);\n2. 负责应用的监控，追踪应用执行状态重启任务失败等。\n3. 与NodeManager协同工作完成Task的执行和监控。\n\nMRappMaster是Mapreduce的ApplicationMaster实现，它是的Mapreduce应用程序可以直接在YARN平台之上，MRApplication负责Mappreduce应用的生命周期、作业管理资源申请再分配、container启动与释放、作业恢复等。其他的如计算框架如 Spark on YARN ,Storm on Yarn。如果需要可以自己写一个符合规范的YARN的应用模型。\n\n##### NodeManager(NM)\n\nNM是每个节点上的资源和任务管理器，他会定时地向RM汇报 本节点上地资源使用情况和各个Container地运行状态；同时会接受AM的Container启动/停止等请求。\n\n##### Container\n\nYARN 资源抽象 ，封存了某个节点是的多维度资源，如内存、CPU、磁盘、网络等。当AM向RM申请资源时，RM为AM返回资源是用Container表示的 。YARN为每一个任务分配给一个Container且该任务只能读取该Container中描述的资源。Container不同于MRv1的slot，他是一个动态划分单位，根据应用程序的需求动态生成的。\n\n##### Yarn的工作流程\n\n![FDkeTx.md.png](https://s1.ax1x.com/2018/12/19/FDkeTx.md.png)\n\n1. 用户向YARN提交应用程序，其中还包含了ApplicationMaster程序，启动AM的命令，命令参数和用户程序等等；本步骤需要准确描述运行AplicationMaster的unixs进程的所有信息。通常由YarnClient完成。\n2. Resource Manager为该应用程序分配一个Container，和对应的NameNode进行通信，要求他在Container中启动ApplicationMaster；\n3. ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过RM查看程序的运行状态。然后它将为各个任务申请资源，并监控他的运行状态，知道运行结束，重复4~7；\n4. ApplicationMaster采用轮询的方式通过RPC方式向ResourceManager申请和领取资源，资源的协调通过异步方式完成。\n5. ApplicationMaster一旦申请资源后，便开始与对应的NodeManager通信，要求他们启动作业。\n6. NodeManager为作业设置好运行环境（环境变量，jar包，二进制程序）将任务写道一个脚本中，并通过运行该脚本启动作业。\n7. 各个作业通过协议RPC方式向ApplicationMaster同步汇报自己当前的进度和状态，AM随时掌握各个作业的运行状态，从而在作业失败的时候重启作业。\n8. 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭。\n\n##### YARN资源模型\n\n一个应用程序可以通过ApplicationMaster请求特定的资源需求来满足它的资源需要。调度器会被分配给一个Container来相应资源需求、用于满足ApplicationMaster在ResourceRequst中提出的要求。Container包含5类信息：优先级、期望资源所在节点、资源量、container数目、松弛本地性(是否没有满足本地资源时，选择机架本地资源)\n\nResourceRequst包含物种信息:\n\n- 资源名称：期望所在的主机名、机架、用*表示没有特殊要求。\n- 优先级： 程序内部请求的优先级，用来调整程序内部各个ResourceRequst的次序。\n- 资源需求：需要的资源量，表示成内存里，CPU核数的元组(目前YARN仅支持内存和CPU两种资源)\n- container数：表示 需要container数量，它决定了用该ResourceRequst指定的Container总数。\n- 是否松弛本地性：在本机架资源剩余量无法满足要求是改为只要运行在服务器所在的机架上进行就行。\n\n本质上Container是一种资源分配的形式，是ResourceManager为ResourceRequst成功分配资源的结果。Container授予节点在机器上使用资源（如内存、CPU）的权利，YARN允许的不仅仅是Java的应用程序，概念上Container在YARN分配任务分为两层：第一层为ResourceManage调度器结果分配给用用程序ApplicationMaster，第二层，作为运行环境用ApplicationMaster分配给作业资源。\n\n![FDJTAK.md.png](https://s1.ax1x.com/2018/12/19/FDJTAK.md.png)\n\n​    \t\t\t\t\t\t\t\t\tYARN启动应用程序流程\n\n首先，客户端通知ResourceManager它要提交一个应用程序。随后ResourceManager在应答中返回一个ApplicationId以及必要的客户端请求资源的集群容量信息，资源请求分为两步：1.提交Application请求 2.返回ApplicationID。\n\n3.客户端使用“Application submission Contex”发起响应，Application subminssion上下文包含了ApplicationID、客户名、队列以及其他启动Application所需要的信息。“ContainerLaunch Context”(CLC)也会发送给ResourceManager,CLC提供资源需求、作业文件、安全令牌以及在节点上启动ApplicationMaster所需要的其他信息。应用程序被提交之后，客户端可以向ResourceManager请求结束这个应用或者提供这个应用程序的状态。\n\n4.当ResourceManager接收到客户端的应用提交上下文，它会为ApplcationMaster调度一个可用的Container来启动AM。如果合适的Container，Resource会联系相应的NodeManager并启动ApplicationMaster。如果没有合适的Container，则请求必须等待。ApplicationMaster会发送注册请求到ResourceManager，RM可以通过RPC的方式监控应用程序的状态。在注册请求中，ResourceManager会发送集群的最大和最小的容器容量(第5步)，供ApplicationMaster决定如何使用当前集群的可用资源。\n\n基于ResourceMangaer的可用资源信息，ApplicationMaster会请求一定数量的Container（第6步），ResourceManager根据调度政策，尽量为Application分配资源，最为资源请求应答发送给ApplicationMaster（第7步）。\n\n应用启动之后，ApplicationMaster将心跳和进度信息通过心跳发送给ResourceManager，这些心跳中，ApplicationMaster可以请求或释放Container。应用结束时ApplicationMaster发送给结束信息到ResourceManager以注销自己。\n\n![FDU9N6.md.png](https://s1.ax1x.com/2018/12/19/FDU9N6.md.png)\n\nResourceManager已经将分配的NodeManager的控制权移交给ApplicationMaster。ApplicationMaster将独立联系其指定的节点管理器，并提供Container Launch Context，CLC包括环境便利，远程存储依赖文件、以及环境依赖文件，数据文件都被拷贝到节点的本地存储上。同一个节点的依赖文件都可以被同一应用程序Container共享。\n\n一旦所有的Container都启动完成，ApplicationMaster就可以检查他们的状态。ResourceManager不参与应用程序的执行，只处理调度和监控其他资源。ResourceManager可以命令NodeManager杀死Container，比如ApplicationMaster通知ResourceManager自己的任务结束或者时ResourceManager需要为其他应用程序抢占资源。或者Container超出资源限制时都可能发生杀死Container事件。当Container被销毁后，NodeManager会清理它的本地工作目录。\t应停用程序结束后，ApplicationMaster会发送结束信号通知ResourceManager，然后ResourceManager通知NodeManager收集日志并清理Container专用文件。如果Container还没退出，NodeManager也可以接收指令去杀死剩余的Container。\n\n#### YARN 中的调度\n\n#### FIFO\n\n![FDdr7R.png](https://s1.ax1x.com/2018/12/19/FDdr7R.png)\n\n简单易懂，不需要任何配置(FIFO Scheduler),容量调度器(Capcity Scheduler)和公平调度器（Fair Scheduler）。FIFO调度器将应用放置在一个队列中然后按照提交的顺序(先进先出)运行应用，首先为队列中的第一个任务请求分配资源。第一个应用的请求被满足后再一次为队列的下一条进行服务。\n\n#### Capacity\n\n![FDdDB9.png](https://s1.ax1x.com/2018/12/19/FDdDB9.png)\n\n一个独立的专门队列保证小作业提交就可以启动，由于队列容量是为了那个队列中的作业所保留的。这意味这与使用FIFO调度器相比，大作业执行的事件要长。容量调度器允许多个组织共享一个Hadoop集群，每个组织可以分配到全部集群资源的一部分。每个组织都被配置一个专门的队列，每个队列都被配置为可以使用一定的集群资源。队列可以进一步按层次划分，这样每个组织内的不同用户能够共享该组织队列所分配的资源。在一个队列内，使用FIFO调度政策对应用进行调度。单个作业使用的资源不会超过其队列容量然而，如果队列中由多个作业，并且队列资源不够用，如果仍有可用的空闲资源，那么容量调度器可能会被空余的资源分配给队列的作业，哪怕这超出队列容量。这称之为为“弹性队列”。\n\n#### Fair\n\n![FDwZDJ.png](https://s1.ax1x.com/2018/12/19/FDwZDJ.png)\n\nFair调度器是一个队列资源分配方式，在整个时间线上，所有的Job平均的获取资源。默认情况下，Fair调度器知识对内存资源做公平的调度和分配。当集群中只有一个任务运行时，那么此任务会占用整个集群的资源。当其他的任务提交后，那些释放的资源就会被分配给新的Job，所以每个任务最终都能够获取几乎一样多的资源。\n\n#### 对比\n\n| 调度器        | FIFO                             | Capcity                                                      | Fair                                                         |\n| ------------- | -------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| 设计目的      | 最简单的调度器，易于理解和上手。 | 多用户的情况下，最大化集群的吞吐和利用率。                   | 多用户的情况下，强调用户公平地贡献资源。                     |\n| 队列 组织方式 | d单队列                          | 树状组织队列，无论夫队列还是子队列都会由资源参数限制，子队列地资源限制计算是基于父队列地。应用提交到叶子队列。 | 树状组织队列。但是父队列和子队列没有参数继承关系。父队列地资源限制对于子队列没有影响。应用提交到叶子队列。 |\n| 资源限制      | 无                               | 父子队列之间有容量关系。每个队列限制了资源使用了，全局最大资源使用了，最大活跃应用数量。 | 每个叶子队列有最小共享量，最大资源量和最大活跃应用了。用户有最大活跃应用数量地全局被指。 |\n| 队列ACL限制   | 可以限制应用提交权限             | 可以限制应用提交权限和队列开关继承，父子队列间地ACL会继承    | 可以限制应用提交权限，父子队列间地ACL会继承。但是由于支持客户端动态创建队列，需要限制默认队列地应用数量。 |\n| 队列排序算法  | 无                               | 按照队列地资源使用了最小的优先                               | 可以限制应用提交权限，父子队列间的ACL会继承。但是由于支持客户端动态创建队列，需要限制默认队列的应用数量。目前，还看不到关闭动态创建队列的选项。 |\n| 应用选择算法  | 先进先出                         | 先进先出                                                     | 公平排序算法排序                                             |\n| 本地优先分配  | 支持                             | 支持                                                         | 支持                                                         |\n| 延迟调度      | 不支持                           | 不支持                                                       | 支持                                                         |\n| 资源抢占      | 不支持                           | 不支持                                                       | 支持                                                         |\n\n总结：\n\n- FIFO： 最简单的调度器，按照先进先出的方式处理应用。只有一个队列可提交应用，所有的用户提交到这个队列。可以针对这个队列设置ACL，没有应用优先级可以配置。\n- Capcity：可以看作是FIFO的多队列版本，每个队列可以限制资源使用量。但是队列间的资源分配 以使用量安培作为依据，使得容量小的队列有竞争优势，集群整体吞较大。延迟调度机制使得应用可以放弃，跨机器或者跨机架的调度机会，争取本地调度。\n- Fair：多队列，多用户共享资源。特有的客户端创建队列的特性，使得权限控制不太完美，根据队列设定的最小共享量或者权重等参数，按照比例共享资源。延迟调度机制跟Capcity的牡蛎类似但是实现方式稍微不同，资源抢占特性，是指调度容器能够依据公平共享算法，计算每个队列应得的资源，将超额资源的队列的部分容器释放掉的特性。\n\n## 总结\n\n![FD0qOg.png](https://s1.ax1x.com/2018/12/19/FD0qOg.png)","tags":["Hadoop"],"categories":["大数据"]},{"title":"HDFS高级功能","url":"/2018/12/19/HDFS高级功能/","content":"\n {{ \"HDFS的六大高级特性\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 安全模式\n\n安全模式是HDFS所处的一种特殊状态，在这种状态下，文件系统只接受读数据请求，而不接受删除、修改等变更请求。在NameNode主节点启动时，HDFS首先进入安全模式，DataNode在启动的时候会向namenode汇报可用的block等状态，让NameNode得到块的位置信息,并对每一个文件对应的数据块副本进行统计,当最小副本条件满足时HDFS自动离开安全模式。如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，如果HDFS出于安全模式下，则文件block不能进行任何的副本复制操作，因此达到最小的副本数量要求是基于datanode启动时的状态来判定的，启动时不会再做任何复制（从而达到最小副本数量要求）\n\n假设我们设置的副本数即(参数dfs.replication是5),那么在datanode上应该有5个副本存在,假设只存在3个副本那么比例就是3/5=0.6,默认的最小的副本率是0.999.因此系统会自动复制副本到其他的dataNode,使得副本率不小于0.99.\n\n安全模式的相关命令:\n\n查看当前状态\n\n```shell\nhdfs dfsadmin   -safemode get\n```\n\n进入安全模式\n\n```shell\nhdfs dfsadmin   -safemode enter\n```\n\n强制离开安全模式\n\n```java\nhdfs dfsadmin -safemode  leave\n```\n\n一直等待直到安全模式结束\n\n```\nhdfs dfsadmin -safemode wait\n```\n\nWeb查看启动过程 端口50070\n\n![FBIseI.md.png](https://s1.ax1x.com/2018/12/19/FBIseI.md.png)\n\n## 回收站\n\nHDFS会为每一个用户创建一个回收站目录：/user/用户名/.Trash/，每一个被用户通过Shell删除的文件/目录，在系统回收站中都一个周期，也就是当系统回收站中的文件/目录在一段时间之后没有被用户恢复的话，HDFS就会自动的把这个文件/目录彻底删除，之后，用户就永远也找不回这个文件/目录了。在HDFS内部的具体实现就是在NameNode中开启了一个后台线程Emptier，这个线程专门管理和监控系统回收站下面的所有文件/目录，对于已经超过生命周期的文件/目录，这个线程就会自动的删除它们，不过这个管理的粒度很大。另外，用户也可以手动清空回收站，清空回收站的操作和删除普通的文件目录是一样的，只不过HDFS会自动检测这个文件目录是不是回收站，如果是，HDFS当然不会再把它放入用户的回收站中了。\n\n设置trashtrash选项\n\nhadoop中的trash选项默认关闭,如果生效,需要更改conf中的core-site.xml\n\n```xml\n<property>\n    <name>fs.trash.interval</name>\n    <value>1440</value>\n</property>\n<property>\n    <name>fs.trash.checkpoint.interval</name>\n    <value>1440</value>\n</property>\n```\n\n\n\n执行 Hadoop fs -rm -f /文件路径，将文件删除时，文件并不是直接消失在系统中，而是被从当前目录move到了所属用户的回收站中。保留时间（1440分钟=24小时），24小时内，用户可以去回收站找到这个文件，并且恢复它。24小时过后，系统会自动删除这个文件，于是这个文件彻底被删除了。\n\nfs.trash.checkpoint.interval则是指的是检查垃圾回收的检查间隔,应该是小于或者等于fs.trash.checkpoint.。想要恢复数据只需要把数据移动到要恢复的数据即可。\n\n## 快照\n\nHadoop 2.x HDFS新特性\n\n基于某时间点的数据的备份复制.利用快照,可以针对基本的目录或者整个文件系统,让HDFS在数据损坏时恢复到过去一个正确的时间点,快照比较常见的应用场景书数据备份,以防止一些用户错误或者灾难.\n\n快照功能默认禁用,开启或者禁用快照功能需要针对目录进行操作命令如下\n\n```shell\nhdfs dfsadmin -allowSnapshot<snapshotDir>\t\t\t#开启快照\nhdfs dfsadmin -disallowSnapshot<snapshotDir>\t\t #取消快照\nhdfs dfs -createSnapshot<snapshotDir> [<snapshotName>] # 创建快照\nhdfs dfs -deleteSnapshot<snapshotDir> [<snapshotName>] # 删除快照\nhdfs dfs -renameSnapshot<snapshotDir> <oldName> <newName> #重命名快照\nhdfs snapshotDiff <snapshodir> <snapshoName> <snapshoName>  #比较快照不同\nhdfs lsSnapshottableDir                                      ##查看快录\n```\n\n开启快照\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfsadmin -allowSnapshot /input\nAllowing snaphot on /input succeeded\n```\n\n创建快照\n\n```shell\nhdfs dfs -createSnapshot /input input_2018-12-19\nCreated snapshot /input/.snapshot/input_2018-12-19\n```\n\n上传文件\n\n```\n[hadoop@datanode1 ~]$ hadoop fs -put bigtable /input\n```\n\n创建快照\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfs -createSnapshot /input input_2018-12-19_2\nCreated snapshot /input/.snapshot/input_2018-12-19_2\n```\n\n对比快照\n\n```shell\n[hadoop@datanode1 ~]$ hdfs snapshotDiff /input input_2018-12-19 input_2018-12-19_2\nDifference between snapshot input_2018-12-19 and snapshot input_2018-12-19_2 under directory /input:\nM       .\n+       ./bigtable\n```\n\nWeb查看\n\n![FBoJXj.png](https://s1.ax1x.com/2018/12/19/FBoJXj.png)\n\n恢复数据\n\n恢复数据的方法就是拷贝\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfs -cp /input/.snapshot/input_2018-12-19 /input\n[hadoop@datanode1 ~]$ hdfs dfs -ls /input/input_2018-12-19\nFound 2 items\n-rw-r--r--   3 hadoop supergroup         51 2018-12-19 10:17 /input/input_2018-12-19/test\n-rw-r--r--   3 hadoop supergroup         12 2018-12-19 10:17 /input/input_2018-12-19/write.txt\n```\n\n## 配额\n\n- setQuota 针对HDFS中的某个目录设置文件和目录数量之和的最大值\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfsadmin -setQuota 5 /input  # 该目录下目录数之和不超过5\n#超过5个情况下put文件\n#put: The NameSpace quota (directories and files) of directory /input is exceeded: quota=5 file count=8\n```\n\n- setSoaceQuota\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfsadmin -setSpaceQuota 134217728 /input #设置input的存储空间为128M \n```\n\n- 清除配额命令\n\n```shell\n[hadoop@datanode1 ~]$ hdfs dfsadmin -clrQuota /input\n[hadoop@datanode1 ~]$ hdfs dfsadmin -clrSpaceQuota /input\n```\n\n## 联邦\n\nFederation即为“联邦”，该特性允许一个HDFS集群中存在多个NameNode同时对外提供服务，这些NameNode分管一部分目录（水平切分），彼此之间相互隔离，但共享底层的DataNode存储资源。\n\n【单机namenode的瓶颈大约是在4000台集群，而后则需要使用联邦机制】\n\n什么是Federation机制 \n\nFederation是指HDFS集群可使用多个独立的NameSpace(NameNode节点管理)来满足HDFS命名空间的水平扩展 ,这些NameNode分别管理一部分数据，且共享所有DataNode的存储资源。\n\nNameSpace之间在逻辑上是完全相互独立的(即任意两个NameSpace可以有完全相同的文件名)。在物理上可以完全独立(每个NameNode节点管理不同的DataNode)也可以有联系(共享存储节点DataNode)。一个NameNode节点只能管理一个Namespace\n\nFederation机制解决单NameNode存在的以下几个问题 \n\n（1）HDFS集群扩展性。每个NameNode分管一部分namespace，相当于namenode是一个分布式的。 \n\n（2）性能更高效。多个NameNode同时对外提供服务，提供更高的读写吞吐率。 \n\n（3）良好的隔离性。用户可根据需要将不同业务数据交由不同NameNode管理，这样不同业务之间影响很小。 \n\n（4）Federation良好的向后兼容性，已有的单Namenode的部署配置不需要任何改变就可以继续工作。\n\nFederation是简单鲁棒的设计 \n\n鲁棒性（健壮和强壮）：在输入错误、磁盘故障、网络过载或有意攻击情况下，能否不死机、不崩溃\n\n由于联盟中各个Namenode之间是相互独立的：Federation整个核心设计大部分改变是在Datanode、Config和Tools，而Namenode本身的改动非常少，这样Namenode原先的鲁棒性不会受到影响。比分布式的Namenode简单，虽然扩展性比真正的分布式的Namenode要小些，但是可以迅速满足需求。\n\n另外一个原因是Federation良好的向后兼容性，可以无缝的支持目前单Namenode架构中的配置。已有的单Namenode的部署配置不需要任何改变就可以继续工作。\n\nFederation不足之处 \n\nHDFS Federation并没有完全解决单点故障问题。虽然namenode/namespace存在多个，但是从单namenode/namespace看，仍然存在单点故障。因此 Federation中每个namenode配置成HA高可用集群，以便主namenode挂掉一下，用于快速恢复服务。\n\n![FBTt2D.png](https://s1.ax1x.com/2018/12/19/FBTt2D.png)\n\n## HA\n\n通常一个集群只有一个NameNode,所有的元数据由唯一的NameNode负责管理,如果机器或者进程变得不可用,整个集群都会无法工作。知道NameNode恢复工作为止。影响集群的可用性，HDFS高可用通过提供同一集群中运行两个NameNode的方法来解决问题，一台保持活跃（Active）状态对外提供服务，一台处于备用（Standby）状态，两个节点保持数据同步。为了实时同步两个NameNode上的元数据需要一个共享存储系统，可以是NFS QJM 或者Zookeeper,Active的NameNode将共享数据写入到共享系统中去，而Standby监听该系统与ActiveNameNode基本保持一致因此在一个NameNode不能对外提供服务的情况下，可以快速故障转移到另外一个NameNode。\n\n![FBHwNt.png](https://s1.ax1x.com/2018/12/19/FBHwNt.png)","tags":["HDFS"],"categories":["大数据"]},{"title":"HDFS的操作SHELL和API","url":"/2018/12/18/HDFS的操作SHELL和API/","content":" {{ \"HDFS的shell操作和JavaAPI的使用\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## WEB\n\n- WEB端口50090查看SecondaryNameNode信息。可以查看Hadoop的版本，NameNode的IP，Checkpoint等信息。\n\n![FBatc4.md.png](https://s1.ax1x.com/2018/12/18/FBatc4.md.png)\n\n- WEB端口50070可以查看HDFS的信息和目录结构\n\n    ![FBay9O.md.png](https://s1.ax1x.com/2018/12/18/FBay9O.md.png)\n\n![FBacge.png](https://s1.ax1x.com/2018/12/18/FBacge.png)\n\n## SHELL\n\n####  查看\n\n```\nhdfs dfs -ls [-d][-h][-R] <paths>\n[-d]:返回path\n[-h]:按照KMG数据大小单位显示文件大小，默认B\n[-R]:级联显示paths下文件\n```\n\n#### 创建文件夹\n\n```shell\nhdfs dfs -mkdir [-p]<paths>\n```\n\n#### 新建文件\n\n```shell\nhdfs dfs -touchz<paths>\n```\n\n#### 查看文件\n\n```sh\nhdfs dfs -cat/[-ignoreCrc] [src]\nhdfs dfs -text -ignoreCrc /input/test  #忽略校验\nhdfs dfs -cat -ignoreCrc /input/test\n```\n\n#### 追写文件\n\n```shell\nhdfs dfs --appendToFile <localsrc> <dst>\n如果localsrc为\"-\"表示数据来自键盘输入用\"Ctrc+C\"取消输入\n```\n\n#### 上传下载\n\n```shell\nhdfs dfs -put [-f][-p]<localsrc> <dst>   # 上传到指定目录\nhdfs dfs -get [-p]<src>   <localhost>    # 现在到本地\n```\n\n#### 删除文件\n\n```shell\nhdfs dfs -rm [-f] [-r] <src>\n-f 如果要删除的文件不存在,不显示错误信息\n-r/R 级联删除目录下所有文件和子目录文件\n```\n\n#### 磁盘空间\n\n```shell\nhdfs dfs -du[-s][-h]<path>\n[-s]显示指定目录所占空间大小\n[-h]按照K M G 数据显示文件大小\n```\n\n## JAVA API\n\n### 步骤\n\n1. 实例化Configuration\n\nConfiguration封装了客户端或服务器的配置，Confiuration实例会自动加载HDFS的配置文件core-site.xml，从中获取Hadoop集群中的配置信息。因此我们要把集群的配置core-site.xml放在Maven项目的resources目录下。\n\n```java\nConfiguration conf = new Configuration();\n```\n\n2. 实例化FileSystem\n\nFileSystem类是客户端访问文件系统的入口，是一个抽象的文件系统。DistributedFileSystem类是FileSystem的一个具体实现。实例化FileSystem并发挥默认的文件系统代码如下：\n\n```java\nFileSystem fs = FileSystem.get(conf);\n```\n\n3. 设置目标对象的路径\n\nHDFS API 提供了Path类来封装文件路径。PATH类位于org.apache.hadoop.fs.Path包中，代码如下：\n\n```java\nPath path = new Path(\"/input/write.txt\");\n```\n\n### 执行文件操作\n\n得到Filesystem实例后，就可以使用该实例提供的方法成员执行相应的操作。如果：打开文件，创建文件、重命名文件、删除文件。\n\n​\t\t\t\t\t\t\t\t\t\tFileSystem类常用成员函数\n\n| 方法名称及参数                        | 返回值             | 功能                     |\n| ------------------------------------- | ------------------ | ------------------------ |\n| create(Path f)                        | FSDataOutputStream | 创建一个文件             |\n| open(Path f)                          | FSDatatInputStream | 打开指定的文件           |\n| delete(Path f)                        | boolean            | 删除指定文件             |\n| exsits(Path f)                        | boolean            | 检查文件是否存在         |\n| getBlockSize(Path f)                  | long               | 返回指定的数据块的大小   |\n| getLength(Path f)                     | long               | 返回文件长度             |\n| mkdir(Path f)                         | boolean            | 检查文件是否存在         |\n| copyFromLocalFile(Path src, Path dst) | void               | 从本地磁盘复制文件到HDFS |\n| copyToLocalFile(Path src, Path dst)   | void               | 从HDFS复制文件到本地磁盘 |\n| ...........                           | ..........         | ................         |\n\n#### 上传文件\n\n```java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\n\n\npublic class PutFile {\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();\n\n        FileSystem fs = FileSystem.get(URI.create(\"hdfs://datanode1:9000\"),conf,\"hadoop\");\n\n        //本地文件\n        Path src = new Path(\"D:\\\\上传文件.txt\");\n\n        //HDFS 路径\n        Path dst = new Path(\"/input/上传文件.txt\");\n\n        fs.copyFromLocalFile(src, dst);\n        fs.close();\n        System.out.println(\"文件上传成功\");\n    }\n}\n\n```\n\n#### 创建文件\n\n```java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\n\npublic class CreateFile {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n\n        FileSystem fs = FileSystem.get(URI.create(\"hdfs://datanode1:9000\"), conf, \"hadoop\");\n        Path dfs = new Path(\"/input/上传文件.txt\");\n        FSDataOutputStream newFile = fs.create(dfs, true);  //是否覆盖文件 true 覆盖 false 追加\n        newFile.writeBytes(\"this is a create file tes\");\n        System.out.println(\"创建文件成功\");\n\n    }\n}\n\n```\n\n#### 文件详情\n\n```java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileStatus;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\npublic class SeeInfo {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n\n        FileSystem fs = FileSystem.get(URI.create(\"hdfs://datanode1:9000\"), conf, \"hadoop\");\n\n        // HDFS文件\n        Path fpath = new Path(\"/input/上传文件.txt\");\n\n        FileStatus fileLinkStatus = fs.getFileLinkStatus(fpath);\n        //获得块大小\n        long blockSize = fileLinkStatus.getBlockSize();\n        System.out.println(\"blocksize:    \" + blockSize);\n\n        //获得文件大小\n        long len = fileLinkStatus.getLen();\n        System.out.println(\"Filesize:    \" + len);\n\n        //获得文件所有者\n        String owner = fileLinkStatus.getOwner();\n        System.out.println(\"FileOwner:     \" + owner);\n\n        //获得创建时间\n        SimpleDateFormat formater = new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\");\n        long accessTime = fileLinkStatus.getAccessTime();\n        System.out.println(\"access time:   \" + formater.format(new Date(accessTime)));\n\n        //获得修改时间\n        long modificationTime = fileLinkStatus.getModificationTime();\n        System.out.println(\"modify time:    \" + formater.format(new Date(modificationTime)));\n\n    }\n}\n```\n\n![FBsDLF.png](https://s1.ax1x.com/2018/12/18/FBsDLF.png)\n\n\n\n#### 下载文件\n\n```java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.io.IOUtils;\n\nimport java.io.FileOutputStream;\nimport java.io.InputStream;\nimport java.io.OutputStream;\nimport java.net.URI;\n\npublic class GetFIle {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n\n        FileSystem fs = FileSystem.get(URI.create(\"hdfs://datanode1:9000\"), conf, \"hadoop\");\n        // HDFS 文件\n        InputStream in = fs.open(new Path(\"/input/上传文件.txt\"));\n\n        //保存到本地位置\n        OutputStream out = new FileOutputStream(\"D://下载文件.txt\");\n        IOUtils.copyBytes(in, out, 4096, true);\n\n        System.out.println(\"下载文件成功\");\n        fs.close();\n\n    }\n}\n```\n\n#### 删除文件\n\n```javapackage hdfs;\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\n\npublic class DeleteFile {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n\n        FileSystem fs = FileSystem.get(URI.create(\"hdfs://datanode1:9000\"), conf, \"hadoop\");\n\n        Path path = new Path(\"/input/上传文件.txt\");\n        fs.delete(path);\n        System.out.println(\"删除成功\");\n    }\n}\n```\n\n\n\n\n\n\n\n","tags":["HDFS"],"categories":["大数据"]},{"title":"Hadoop分布式文件系统HDFS","url":"/2018/12/17/Hadoop分布式文件系统HDFS/","content":"\n {{ \"HDFS的探究\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## HDFS\n\nHDFS是 Hadoop Distribute File System的缩写，是谷歌GFS分布式文件系统的开源实现，Apache Hadoop的一个子项目，HDFS基于流数据访问模式的分布式文件系统，支持海量数据的存储，允许用户将百千台组成存储集群，HDFS运行在低成本的硬件上，提供高吞吐量，高容错性的数据访问。\n\n### 优点\n\n- 可以处理超大文件(TB、PB)。\n- 流式数据访问 一次写入多次读取，数据集一旦生成，会被复制分发到不同存储节点上，响应各种数据分析任务请求。\n- 商用硬件  可以运行在低廉的商用硬件集群上。\n- 异构软硬件平台的可移植性，HDFS在设计的时候就考虑到了平台的可以移植性，这种特性方便了 HDFS在大规模数据应用平台的推广。\n\n###  缺点\n\n- 低延迟的数据访问，要求地时间延迟数据访问的应用不是在HDFS上运行，可以用Hbase，高吞吐量的同时会以提高时间延迟为代价。\n- 大量小文件 由于namenode将文件的元数据存储在内存中，因此受制于namenode的内存容量，一般来说目录块和数据库的存储信息约占150个字节也就是说如果有100万个文件，每一个文件占一个数据块，则需要300MB内存，但是存储十亿个就超出了当前的硬件能力了。\n- 多用户写入，任意修改文件。HDFS中的文件写入只支持一个用户，且只能以添加的方式正在文件末尾添加数据，不支持多个写操作，不支持文件的任意位置修改。\n\n## 数据块\n\n HDFS默认的块大小是128M，与单一磁盘上的文件系统相似,HDFS也被划分为块大小的多个分块。HDFS中小于一个块大小的文件不会占据整个块的空间，当一个1M的文件存储在一个128M的块中时，文件只占1M的磁盘空间，而不是128M。HDFS的块比磁盘的块大，目的时为了最小化的寻址开销，假设寻址时间约为10ms，传输速度为100M/s，为了使寻址时间仅占传输时间的1%，我们要将块设置成100MB，默认块的大小为128M，这个块的大小也不能设置得过大。MapReduce中的map任务通常一次只处理一个数据块的数据，因此如果任务数量太少了（少于集群中的节点数量），作业的运行速度就会比较慢。\n\n## NameNode\n\n元数据节点（NameNode)的作用是管理分布式文件的命名空间，一个集群只有一个NameNode节点，主要负责HDFS文件系统的管理工作包括命名空间管理和文件Block管理，在HDFS内部，一个文件被分成为一个或者多个Block的所有元数据信息，主要包括“文件名 —>>数据块的映射“，”数据块 —>>DataNode“的映射列表，该列表通过DataNode上报给NameNode建立，NameNode决定文件数据块到具体的DataNode节点的映射。\n\nNameNode管理文件系统的命名空间(namespace)，它维护这文件系统树以及文件树中所有的文件（文件夹）的元数据。管理这些信息的文件有两个，分别是命名空间镜像文件(namespace image)和操作日志文件(edit log),这些信息被缓存在RAM中，也会持被持久化存储在硬盘。\n\n为了周其性地将元数据节点地镜像文件fsimage和日志edits合并，以防止日志文件过大，HDFS定义了辅助元数据节点（Secondary NameNode）上也保存了一份fsimage文件，以确保在元数据文件中地镜像文件失败时可以恢复。\n\n### 容错机制\n\n#### NFS\n\n备份组成文件系统元数据持久状态的文件，将本地持久状态写入磁盘地同时，写入一个远程挂载地文件系统（NFS）\n\n#### 辅助节点\n\n运行一个辅助的Namenode，不作为Namenode使用，定期合并编辑日志和命名空间镜像，一般在一台单独的物理计算机上运行，因为它需要占用大量CPU时间，并且需要与namenode一样多的内存来执行合并操作，会保存Namenode合并后的命名空间和镜像文件，一般会滞后于NameNode节点的。\n\n#### RAID\n\n使用磁盘陈列NameNode节点上冗余备份Namenode的数据。\n\n\n\n## DataNode\n\n数据节点只负责存储数据，一个Block会在多个DataNode中进行冗余备份，一个块在一个DataNode上最多只有一个备份，DataNode上存储了数据块ID和数据块内容以及他们的映射关系。\n\nDataNode定时和NameNode通信，接收NameNode的指令，默认的超时时长为10分钟+30s。 NameNode上不永久保存DataNode上有哪些数据块信息，通过DataNode上报的方式更新NameNode上的映射表，DataNode和NameNode建立连接后，会不断和 NameNode保持联系，包括NameNode第DataNode的一些命令，如删除数据或把数据块复制到另一个DataNode上等，DataNode通常以机架的形式组织，机架通过一个交互机将所有的系统链接起来。机架内部节点之间传输速度快于机架间节点的传输速度。\n\nDataNode同时作为服务器接收客户端的范围呢，处理数据块的读、写请求。DataNode之间还会相互他通信，执行数据块复制任务，在数据块复制任务，在客户端执行写操作时，DataNode之间需要相互通信配合，保持写操作的一致性，DataNode的功能包括：\n\n1. 保存Block，每个块对应原数据信息文件，描述这个块属于那个文件，第几个块等信息。\n2. 启动DataNode线程，向NameNode定期汇报Block信息。\n3. 定期向NameNode发送心跳保持联系。如果10分钟没有接收到心跳，则认为其lost，将其上的Block复制到其他DataNode节点上。\n\n## SecondaryNameNode\n\nSecondaryNameNode定期地创建命名空间地检查点。首先从NameNode下载fsimage和edit.log然后在本地合并他们，将合并后地fsimage和edit.log上传回到NameNode，这样减少了NameNode重新启动NameNode时合并fsimage和edits.log花费地时间，定期合并fsimage和edit.log文件，使得edits.log大小保持在限定返回内并起到*冷备份*的作用，在NameNode失效的时候，可以恢复fsimage。SecondaryNameNode与NameNode通常运行在不同的机器上，内存呢与NameNode的内存一样大。\n\n参数dfs.namenode.secondary.http-address设置SecondaryNamenode 的通信地址，SecondaryNamenode上的检查点进程开始由两个配置参数控制，第一个参数dfs.namenode.checkpoint.period，指定两个连续检查点之间的时间差默认1小时。dfs.namenode.checkpoint.txns，定义NameNode上新增事务的数量默认1百万条。即使没有到达设定的时间也会启动fsimage和edits的合并。\n\n### 工作流程\n\n1. SecondaryNameNode会定期和NameNode通信，请求其停止使用edits文件，暂时将更新的操作写道一个新的文件edits.new上来。这个操作时瞬间完成的，上层写日志的函数时完全赶不到差别。\n\n2. SecondaryNameNode通过HTTP GET的方式从NameNode上获取fsimage和edits.log文件,并下载到相应的目录下。\n\n3. SecondaryNameNode将下载下来的fsimage载入到内存，然后一条一条的执行edits文件中的哥哥更新操作，使内存中的fsimage保持最新；这个过程就是edits和fsimage文件合并。\n\n4. SecondaryNameNode 执行完3操作后，会通过POST的方式新的fsimage文件发送到NameNode节点上。\n\n5. Namenode从SecondaryNameNode接收到更新的fsimage替换旧的fsimage文件，同时将edits.new更名为edits文件，这个过程中edits就变小了。\n\n    ![F0h1j1.png](https://s1.ax1x.com/2018/12/17/F0h1j1.png)\n\n## HDFS工作机制\n\n### 机架感知\n\n在HDFS中，数据块被复制成副本，存放在不同的节点上，副本的存放是HDFS可靠性和性能的关键，优化副本存放策略使HDFS区分于其他大部分分布式文件系统的共同要特征，HDFS采用了一种称为机架感知的（rck-aware）的策略来改进数据的可靠性，可用性和网络带宽的利用率。HDFS实例一般运行在多个机架的计算机组成的集群上，不同机架上的两台机器之间的通信时通过交互机。在多数情况下，一个机架上的两台机器间的带宽会比不同机架的两台机器的带宽大。\n\n通过一个机架感知的过程,NameNode可以确定每一个Datanode所属的机架id，一个简单的策略是将副本放在不同的机架，可以有效防止一个机架失效时数据的丢失，并且允许读数据的时候充分利用各个机架的带宽，这种策略设置可以将副本均匀分布在集群中，有利于当组件失效的情况下的负载均衡，但是 ，因为这种策略的一个写操作需要传输数据块到多个机架这样增加了写的代价。\n\n多数情况下副本系数时3，HDFS的存放策略时将一个副本存放在本地的机架上，一个副本放在同一机架的另外一个节点上 ，最后一个副本放在不同机架的节点上，这种策略减少了机架间的数据传输，提高了写操作的效率。机架间的错误远比节点的错误少，这种策略提减少了读取数据时需要的网络传输总带宽。这种策略下，副本并不是均匀分布在同一个机架上。三分之一的副本在一个节点，三分之二的副本在一个机架上，其他副本均匀分布在剩下的机架中，这种策略在不损害数据可靠性和可读性能的情况下改进了写的性能。\n\n![F0hvvR.png](https://s1.ax1x.com/2018/12/17/F0hvvR.png)\n\n\n\n### 分配原理\n\n- 有了机架感知，NameNode就可以画出下图所示的datanode网络拓扑图,\n\n![F04CVK.png](https://s1.ax1x.com/2018/12/17/F04CVK.png)\n\n- 最底层是Hx是 datanode, 则H1的rackid=/D1/R1/H1，H1的parent是R1，R1的是D1，有了这些rackid信息就可以计算出任意两台datanode之间的距离\n\n```\ndistance(/D1/R1/H1,/D1/R1/H1)=0  相同的datanode\ndistance(/D1/R1/H1,/D1/R1/H2)=2  同一rack下的不同datanode\ndistance(/D1/R1/H1,/D1/R1/H4)=4  同一IDC下的不同datanode\ndistance(/D1/R1/H1,/D2/R3/H7)=6  不同IDC下的datanode\n```\n\n- 写文件时根据策略输入 dn 节点列表，读文件时按与client由近到远距离返回 dn 列表\n\n### 文件读取\n\n1. HDFS Client 通过FileSystem对象的Open方法打开要读取的文件。\n\n2. DistributeFileSystem负责向远程的元数据（NameNode）发起RPC调用，得到文件的数据信息。返回数据块列表，对于每个数据块，NameNode返回该数据块的DataNode地址。\n\n3. DistributeFileSystem返回一个 FSDataInputSteam对象给客户端，客户端调用FSdataInputSteam对象的read()方法开始读取数据。\n\n4. 通过对数据流反复调用read()方法，把数据从数据节点传输到客户端。\n\n5. 当数据读取完毕时，DFSInputSteam对象会关闭此数据节点的链接，连接此文件下一个数据块的最近数据节点。\n\n6. 当客户端读取完数据时，调用FSDataInputSteam对象的close()关闭输入流。\n\n    ![FB9AJJ.md.png](https://s1.ax1x.com/2018/12/18/FB9AJJ.md.png)\n\n#### API\n\n| 方法名                                         | 返回值 | 说明                                                         |\n| ---------------------------------------------- | ------ | ------------------------------------------------------------ |\n| read(ByteBuffer buf)                           | int    | 读取数据放到buf缓冲区,返回所读取的字节数。                   |\n| read(long pos,byte[] buf, int offset ,int len) | int    | 输入流的指定位置开始把数据读取到缓冲区中，pos指定从 输入文件读取的位置，offset数据写入缓冲区位置的（偏移量）len读操作的最大的字节数。 |\n| readFully(long pos,byte[] buff[])              | void   | 从指定位置，读取所有数据到缓冲区                             |\n| seek(long set)                                 | void   | 指向输入流的第offset字节                                     |\n| relaseBuffer(ByteBuffer buff)                  | void   | 删除                                                         |\n\n#### 代码\n\n```java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataInputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\n\n\npublic class DataInputStream {\n    public static void main(String[] args) throws Exception {\n        Configuration conf = new Configuration();\n        FileSystem fs = FileSystem.get(new URI(\"hdfs://192.168.1.101:9000\"),conf,\"hadoop\");\n        Path src = new Path(\"/input/test\");\n        FSDataInputStream dis = fs.open(src);\n        String str = dis.readLine();\n        while (str.length() > 0) {\n            System.out.println(str);\n            str = dis.readLine();\n            if (str == null) break;\n        }\n        dis.close();\n    }\n}\n\n```\n\n### 文件写入\n\n1. 客户端调用DistributedFileSystem对象的create方法创建一个文件输出流对象\n2. DistributedFileSystem向远程的NameNode节点但发起一次RPC调用，NameNode检查该文件是否存在，如果存在将会覆盖写入，以及客户端是否有权限新建文件。\n3. 客户端调用的FSDataOutputStream对象的write()方法写数据，首先数据先被写入到缓冲区，在被切分为一个个数据包。\n4. 每个数据包发送到用NameNode节点分配的一组数据节点的一个节点上，在这组数据节点组成的管线上依次传输数据包。\n5. 管线上的数据接按节点顺序反向发挥确认信息(ack)最终由管线中的第一个数据节点将整条管线确认信息发给客户端。\n\n![FBPdPg.md.png](https://s1.ax1x.com/2018/12/18/FBPdPg.md.png)\n\n#### 代码\n\n```Java\npackage hdfs;\n\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.fs.FSDataOutputStream;\nimport org.apache.hadoop.fs.FileSystem;\nimport org.apache.hadoop.fs.Path;\n\nimport java.net.URI;\n\npublic class DataOutputStream {\n    public static void main(String[] args) throws Exception {\n\n        Configuration conf = new Configuration();\n        //设置 hdfs 地址 conf  用户名称\n        FileSystem fs = FileSystem.get(new URI(\"hdfs://192.168.1.101:9000\"), conf, \"hadoop\");\n\n        //文件路径\n        Path path = new Path(\"/input/write.txt\");\n\n        //字符\n        byte[] buff = \"hello world\".getBytes();\n        FSDataOutputStream dos = fs.create(path);\n        dos.write(buff);\n        dos.close();\n        fs.close();\n\n    }\n}\n```\n\n## 数据容错\n\n### 数据节点\n\n每个DataNode节点定期向NameNode发送心跳信号，网络割裂会导致DataNode和NameNode失去联系，NameNode通过心跳信号的缺失来检测DataNode是否宕机。当DataNode宕机时不再将新的I/O请求发给他们。DataNode的宕机会导致数据块的副本低于指定值，NameNode不断检测这些需要复制的数据块，一旦发现低于设定副本数就启动复制操作。在某个DataNode节点失效，或者文件的副本系数增大时都可能需要重新复制。\n\n### 名称节点\n\n名称节点保存了所有的元数据信息，其中最核心的两大数据时fsimage和edits.logs，如果这两个文件损坏，那么整个HDFS实例失效。Hadoop采用了两种机制来确保名称节点安全。\n\n1. 把名称节点上的元数据同步到其他的文件系统比如（远程挂载的网络文件系统NFS）\n2.  运行一个第二名称节点SecondaryNameNode当名称节点宕机后，可以使用第二名称节点元数据进行数据恢复，但会丢失一部分数据。\n\n因此会把两种方式结合起来一起使用，当名称节点发生宕机的时候，首先到远程挂载的网络文件系统中获取备份的元数据信息，放到第二名称节点上进行恢复并把第二名称节点作为名称节点来使用。\n\n### 数据出错\n\n某个DataNode获取的数据块可能是损坏的，损坏可能是由DataNode的存储设备错误，网络错误或者软件bug造成的。HDFS使用校验和判断数据块是否损坏。当客户端创建一个新的HDFS会计算这个每个数据块的校验和。并将校验和作为一个单独的隐藏文件保存在同一个HDFS命名空间下。当客户端获取文件内容后，它会检验从DataNode获取的数据和对应的校验是否匹配，如果不匹配客户端从其他Datanode获取该数据块的副本。HDFS的每个DataNode还保存了检查校验和日志，客户端的每一次检验都会记录到日志中。\n\n\n\n\n\n\n\n\n\n","tags":["Hadoop"],"categories":["大数据"]},{"title":"Hadoop简介与分布式安装","url":"/2018/12/17/Hadoop简介与分布式安装/","content":"\n {{ \"Hadoop的基本概念和分布式安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Hadoop\n\n### 简介\n\nHadoop 是Apache Lucene创始人道格·卡丁(Doug Cutting)创建的,Lucene是一个应用广泛的文本搜索库,Hadoop起源于开源网络搜索引擎Apache Nutch,后者是Lucene项目的一部分. \n\nApache Hadoop项目的目标是可靠的、可拓展的分布式计算开发开源软件。\n\nApache Hadoop平台本质是一个计算存储框架，允许使用简单的编程模型跨计算机集群地处理大型数据集，将计算存储操作从单个服务器拓展到数千台服务器（小型机）每台服务器提供本地计算和存储。平台本身不是依靠提升硬件来提高高可用的，而是在应用层检测和处理故障。从而在一组计算机上提供高性能的服务，每个计算机都可能出现故障，Hadoop中所有的模块。都基于一个假设，即硬件故障是常见事件，应由框架自动处理。\n\nHadoop是一个用Java编写的Apache开放源代码框架，它允许使用简单的编程模型在计算机集中环境分布式处理大型数据集。Hadoop框架式应用程序在跨计算机集群提供分布式存储在计算集群提供的存储和计算环境中工作，Hadoop旨在从单个服务器扩展到数千台机器，每台机器提供了本地计算和存储。\n\n其核心构成分别为 HDFS（分布式文件系统）、MapReduce（分布式计算系统）、Yarn（资源管理系统）\n\n###  HDFS\n\nHDFS是Google发布于2003年10月的《*Google* FS》的开源实现。\n\nHadoop分布式文件系统（HDFS）能够提供对数据访问的高吞吐量，适用于大数据场景的数据存储，因为HDFS提高了高可靠性（主要通过多副本实现）、高拓展性（通过添加机器来达到线性拓展）、和高吞吐率的数据存储服务，Hadoop是被设计成能够运行在通用的硬件上的分布式文件系统，因此可以使用廉价的通用机器。大大减少了公司的成本。\n\nHDFS的基本原理是将数据文件以指定大小拆分成数据块，并将数据块以副本的方式存储到多台机器上，即使其中某一个节点出现故障，那么该节点上的数据块副本丢失还有其对应的其他节点的数据副本，但是前提是你的副本系数大于1，HDFS将数据文件拆分、容错、负载均衡等透明化（用户感知不到整个过程，只知道上传了一个文件到HDFS上其中数据的切分、存储在那些机器上是感知不到的）我们可以把HDFS看成是一个容量巨大的、具有高容错的磁盘，在使用的时候完全可以当作本地的磁盘进行使用，所以说HDFS是适用于海量数据的可靠性存储。\n\n### Mapreduce\n\nMapreduce是一个分布式、并发处理的编程模型，用于进行大数据量的计算，MapReduce的名字源于模型中的两个操作：Map（映射）和Reduce（归纳）。Mapreduce是一种简化并进行应用程序开发的编程模型，能够让没有多少并行应用经验的开发人员可以进行快速地学会并行应用开发，而且不需要去关注并行计算中地一些底层问题，按照Mapreduce API的编程模型实现业务逻辑的开发即可。\n\n一个Mapreduce作业通常会把输入的结果集切分成若干个独立的数据块，由map任务以并行处理的方式，对map的输出先进行排序，然后把结果输入给reduce任务由reduce任务来完成最终的统一处理。通常Mapreduce作业的输入和输出都是用HDFS进行存储的，也就是说Mapreduce框架处理数据的输入源和输出目的地大部分场景都是储存在HDFS上。\n\n在部署Hadoop集群时，通常是将计算节点和存储节点部署在同一个节点上，这样做的原因是允许计算框架在任务调度时，可以先将作业优先调度到那些已经存有数据节点上进行数据计算，这样可以使整个集群的网络带宽十分高效地利用，这也是大数据中十分著名地话“移动计算而不是移动数据”。\n\n### Yarn\n\nYarn的全成是 Yarn Another Resource Negotiator，是一个同源资源管理系统，可以为运行在YARN之上的分布式程序提供统一的资源管理和调度。在Yarn我们可以运行不同类型的作业，如：Mapreduce、Spark、TEZ等不同的计算框架\n\nYarn是随着Hadoop发展而催生的新框架，Yarn的基本思想是将Hadoop1.x中的Mapreduce架构中的JobTracker的资源管理和作业调度监控功能进行分离，解决了Hadoop1.x只能运行 Mapreduce框架的限制。\n\n## 安装\n\n### 机器\n\n准备3台linux机器\n\n本教程ip配置如下\n\n| hostname  | ip            | 角色                        |\n| --------- | ------------- | --------------------------- |\n| datanode1 | 192.168.1.101 | NameNode  Datanode          |\n| datanode2 | 192.168.1.102 | SecondaryNameNode  Datanode |\n| datanode3 | 192.168.1.103 | ResourceManager   DataNode  |\n\n### 修改主机名\n\n```shell\nvim /etc/sysconfig/network\nETWORKING=yes\nHOSTNAME=datanode1\n#其他机器依次执行\n```\n\n### SSH\n\n设置master节点和两个slave节点之间的双向ssh免密通信，下面以master节点ssh免密登陆slave节点设置为例，进行ssh设置介绍（以下操作均在master机器上操作）：\n\n```shell\n首先生成master的rsa密钥：ssh-keygen -t rsa \n设置全部采用默认值进行回车\n将生成的rsa追加写入授权文件：cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n给授权文件权限：chmod 600  ~/.ssh/authorized_keys\n进行本机ssh测试：ssh datasnode1 正常免密登陆后所有的ssh第一次都需要密码，此后都不需要密码\n将master上的authorized_keys传到datanode1\nsudo scp ~/.ssh/id_rsa.pub hadoop@datanode1:~/   \n登陆到slave1操作：ssh slave1输入密码登陆  \ncat ~/id_rsa.pub >> ~/.ssh/authorized_keys\n修改authorized_keys权限：chmod 600  ~/.ssh/authorized_keys\n退出slave1：exit\n进行免密ssh登陆测试：ssh slave1\n```\n\n### JAVA \n\n1.解压\n\n```shell\ntar -zxvf jdk-8u162-linux-x64.tar.gz -C /opt/module/\n```\n\n2.配置\n\n```shell\n# 修改 /etc/profile\n#JAVA_HOME\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\nexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH\nexport PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH\n```\n\n3.更新\n\n```shell\nsource /etc/profile\n```\n\n### NTP 时间同步\n\n修改配置\n\n```shell\nvim /etc/ntp.conf\n```\n\n主机配置\n![F0EkqI.png](https://s1.ax1x.com/2018/12/17/F0EkqI.png)\n\n从机配置\n![F0mrcD.md.png](https://s1.ax1x.com/2018/12/17/F0mrcD.md.png)\n\n 从节点同步时间\n\n```shell\nservice ntpd restart\nchkconfig ntpd on  # 开机启动\nntpdate -u datanode1\ncrontab -e\n* */1 * * * /usr/sbin/ntpdate datanode1     #每一小时同步一次\n```\n\n### 同步脚本\n\n```shell\n#!/bin/bash\n#1 获取输入参数个数，如果没有参数，直接退出\npcount=$#\nif((pcount==0)); then\necho no args;\nexit;\nfi\n\n#2 获取文件名称\np1=$1\nfname=`basename $p1`\necho fname=$fname\n\n#3 获取上级目录到绝对路径\npdir=`cd -P $(dirname $p1); pwd`\necho pdir=$pdir\n\n#4 获取当前用户名称\nuser=`whoami`\n\n#5 循环\nfor((host=1; host<4; host++)); do\n        #echo $pdir/$fname $user@datanode$host:$pdir\n        echo --------------- datanode$host ----------------\n        rsync -rvl $pdir/$fname $user@datanode$host:$pdir\ndone\n```\n\n### 解压文件\n\n```shell\ntar -zxvf hadoop-2.7.2.tar.gz -C /opt/module/\nmv hadoop-2.7.2 hadoop\n```\n\n### 修改配置\n\n#### core-site\n\n```xml\n<configuration>\n\t<!-- 指定HDFS中NameNode的地址 -->\n        <property>\n                <name>fs.defaultFS</name>\n    \t\t   <value>hdfs://datanode1:9000</value>\n        </property>\n        <!-- 指定hadoop运行时产生文件的存储目录 -->\n        <property>\n                <name>hadoop.tmp.dir</name>\n                <value>/opt/module/hadoop/data</value>\n        </property>\n         <property>\n              <!-- 指定垃圾回收时间每隔60分钟 -->\n                <name>fs.trash.interval </name>\n                <value>60</value>\n        </property>\n</configuration>\n```\n\n#### hdfs-site\n\n```xml\n<configuration>\n        <property>\n          <!--指定副本数-->\n                <name>dfs.replication</name>\n                <value>3</value>\n        </property>\n        <property>\n             <!-- 指定 secondaryNamenode -->\n        <name>dfs.namenode.secondary.http-address</name>\n        <value>datanode2:50090</value>\n    </property>\n</configuration>\n\n```\n\n#### yarn-site\n\n```xml\n<configuration>\n<!-- reducer获取数据的方式 -->\n    <property>\n        <name>yarn.nodemanager.aux-services</name>\n        <value>mapreduce_shuffle</value>\n    </property>\n<!-- 指定YARN的ResourceManager的地址 -->\n        <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>datanode3</value>\n        </property>\n  <!--开启历史查看任务-->\n  <property>\n        <name>yarn.resourcemanager.recovery.enabled</name>\n        <value>true</value>\n    </property>\n</configuration>\n\n```\n\n#### hadoop-env\n\n```shell\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n```\n\n#### yarn-env\n\n```shell\n#some Java parameters\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n```\n\n#### mapred-env\n\n```shell\nexport JAVA_HOME=/opt/module/jdk1.8.0_162\n```\n\n####  mapred-site\n\n```xml\n<configuration>\n<!-- 指定mr运行在yarn上 -->\n        <property>\n                <name>mapreduce.framework.name</name>\n                <value>yarn</value>\n        </property>\n</configuration>\n```\n\n#### 分发\n\n```shell\n[hadoop@datanode1 module]$ xsync hadoop/\n[hadoop@datanode1 module]$ xsync jdk1.8.0_162/\n```\n\n#### 格式化hdfs\n\n```\nhdfs namenode -format\n```\n\n![F0Kifx.md.png](https://s1.ax1x.com/2018/12/17/F0Kifx.md.png)\n\n### 启动\n\n```shell\n[hadoop@datanode1 hadoop]$ start-dfs.sh\n[hadoop@datanode1 hadoop]$ jps\n51235 NameNode\n51356 DataNode\n52111 Jps\n51919 NodeManager\n[hadoop@datanode3 hadoop]$ start-yarn.sh\n[hadoop@datanode3 hadoop]$ jps\n22260 ResourceManager\n22090 DataNode\n22384 NodeManager\n23013 Jps\n```\n\n###  界面\n\n![F01qQP.md.png](https://s1.ax1x.com/2018/12/17/F01qQP.md.png)\n\n![F01OL8.md.png](https://s1.ax1x.com/2018/12/17/F01OL8.md.png)\n\n\n\n\n\n\n","tags":["Hadoop"],"categories":["大数据"]},{"title":"Elasticsearch的JavaAPI","url":"/2018/12/16/Elasticsearch的JavaAPI/","content":"\n {{ \"Elasticsearch的JavaAPI\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n获取客户端对象\n\n```java\npublic class App {\n\n    private TransportClient client;\n\n    //获取客户端对象\n    @Before\n    public void getClinet() throws UnknownHostException {\n        Settings settings = Settings.builder().put(\"cluster.name\", \"my-application\").build();\n\n        //获得客户端对象\n        client = new PreBuiltTransportClient(settings);\n\n        client.addTransportAddress(new InetSocketTransportAddress(InetAddress.getByName(\"192.168.1.11\"), 9300));\n        System.out.println(client.toString());\n\n    }\n```\n\n\t## 创建索引\n\n```java\n//创建索引\n@Test\npublic void createIndex() {\n    client.admin().indices().prepareCreate(\"blog1\").get();\n    client.close();\n}\n\n//删除索引\n@Test\npublic void deleteIndex() {\n    client.admin().indices().prepareDelete(\"blog\").get();\n    client.close();\n}\n```\n\n## 新建文档\n\n```java\n//新建文档\n@Test\npublic void createIndexByJson() {\n    //创建文档内容\n\n    String json = \"{\" + \"\\\"id\\\":\\\"1\\\",\" + \"\\\"title\\\":\\\"基于Lucene的搜索服务器\\\",\"\n            + \"\\\"content\\\":\\\"它提供了一个分布式多用户能力的全文搜索引擎，基于RESTful web接口\\\"\" + \"}\";\n\n    //创建\n    IndexResponse response = client.prepareIndex(\"blog\", \"article\", \"1\").setSource(json).execute().actionGet();\n    //打印返回值\n    System.out.println(\"索引\" + response.getIndex());\n    System.out.println(\"类型\" + response.getType());\n    System.out.println(\"id\" + response.getId());\n    System.out.println(\"结果\" + response.getResult());\n    client.close();\n}\n\n//创建文档以hashmap\n@Test\npublic void createIndexBymap() {\n    HashMap<String, Object> json = new HashMap<String, Object>();\n\n    json.put(\"id\", \"2\");\n    json.put(\"title\", \"hph\");\n    json.put(\"content\", \"博客 hph.blog\");\n    IndexResponse response = client.prepareIndex(\"blog\", \"article\", \"2\").setSource(json).execute().actionGet();\n    //打印返回值\n    System.out.println(\"索引\" + response.getIndex());\n    System.out.println(\"类型\" + response.getType());\n    System.out.println(\"id\" + response.getId());\n    System.out.println(\"结果\" + response.getResult());\n    client.close();\n}\n//创建文档以hashmap\n    @Test\n    public void createIndexBymap() {\n        HashMap<String, Object> json = new HashMap<String, Object>();\n\n        json.put(\"id\", \"2\");\n        json.put(\"title\", \"hph\");\n        json.put(\"content\", \"博客 hph.blog\");\n        IndexResponse response = client.prepareIndex(\"blog\", \"article\", \"2\").setSource(json).execute().actionGet();\n        //打印返回值\n        System.out.println(\"索引\" + response.getIndex());\n        System.out.println(\"类型\" + response.getType());\n        System.out.println(\"id\" + response.getId());\n        System.out.println(\"结果\" + response.getResult());\n        client.close();\n\n    }\n\n    //创建文档已bulder\n    @Test\n    public void createIndexbyBulider() throws IOException {\n        XContentBuilder builder = XContentFactory.jsonBuilder().startObject().field(\"id\", \"4\").field(\"title\", \"博客\").field(\"content\", \"hphblog.cn\").endObject();\n        IndexResponse response = client.prepareIndex(\"blog\", \"article\", \"3\").setSource(builder).execute().actionGet();\n        //打印返回值\n        System.out.println(\"索引\" + response.getIndex());\n        System.out.println(\"类型\" + response.getType());\n        System.out.println(\"id\" + response.getId());\n        System.out.println(\"结果\" + response.getResult());\n        client.close();\n    }\n```\n\n## 查询索引\n\n```java\n//单个索引查询\n@Test\npublic void queryIndex() {\n    //查询\n    GetResponse response = client.prepareGet(\"blog\", \"article\", \"2\").get();\n\n    //打印\n    System.out.println(response.getSourceAsString());\n\n    //关闭资源\n    client.close();\n\n}\n\n//多个索引查询\n@Test\npublic void queryMultiIndex() {\n\n    //查询\n    MultiGetResponse response = client.prepareMultiGet().add(\"blog\", \"article\", \"3\")\n            .add(\"blog\", \"article\", \"1\")\n            .add(\"blog\", \"article\", \"2\").get();\n\n    for (MultiGetItemResponse multiGetItemResponse : response) {\n        GetResponse response1 = multiGetItemResponse.getResponse();\n\n        //判断是否存在\n        if (response1.isExists()) {\n            System.out.println(response1.getSourceAsString());\n        }\n    }\n}\n\n//更改数据\n@Test\npublic void update() throws ExecutionException, InterruptedException, IOException {\n    UpdateRequest updateRequest = new UpdateRequest(\"blog\", \"article\", \"2\");\n    updateRequest.doc(XContentFactory.jsonBuilder().startObject().field(\"id\", \"2\")\n            .field(\"title\", \"hphblog\").field(\"content\", \"大数据博客学习技术分享\").endObject()\n    );\n\n    UpdateResponse response = client.update(updateRequest).get();\n\n    //打印返回值\n    System.out.println(\"索引\" + response.getIndex());\n    System.out.println(\"类型\" + response.getType());\n    System.out.println(\"id\" + response.getId());\n    System.out.println(\"结果\" + response.getResult());\n    client.close();\n\n}\n```\n\n## 更新文档\n\n```java\n//更新文档updaset\n@Test\npublic void upsert() throws IOException, ExecutionException, InterruptedException {\n    //没有就创建\n    IndexRequest indexRequest = new IndexRequest(\"blog\", \"article\", \"5\");\n    indexRequest.source(XContentFactory.jsonBuilder().startObject().field(\"id\", \"5\")\n            .field(\"title\", \"大数据技术分享的学习\").field(\"content\", \"大数据技术的分享与学习希望和大家多多交流\").endObject\n                    ());\n    //有文档内容就更新\n    UpdateRequest updateRequest = new UpdateRequest(\"blog\", \"article\", \"5\");\n    updateRequest.doc(XContentFactory.jsonBuilder().startObject().field(\"id\", \"5\")\n            .field(\"title\", \"hphblog\").field(\"content\", \"技术分享、技术交流、技术学习\").endObject()\n    );\n\n    updateRequest.upsert(indexRequest);\n\n    UpdateResponse response = client.update(updateRequest).get();\n\n    //打印返回值\n    System.out.println(\"索引\" + response.getIndex());\n    System.out.println(\"类型\" + response.getType());\n    System.out.println(\"id\" + response.getId());\n    System.out.println(\"结果\" + response.getResult());\n    client.close();\n\n\n}\n```\n\n## 删除文档\n\n```java\n//删除文档\n@Test\npublic void delete() {\n    client.prepareDelete(\"blog\", \"article\", \"5\").get();\n    client.close();\n\n}\n```\n\n## 查询所有\n\n```java\n//查询所有\n@Test\npublic void matchAllquery() {\n    //1.执行查询\n    SearchResponse response = client.prepareSearch(\"blog\").setTypes(\"article\").setQuery(QueryBuilders.matchAllQuery()).get();\n\n    SearchHits hits = response.getHits();\n\n    System.out.println(\"查询的结果为\" + hits.getTotalHits());\n\n    Iterator<SearchHit> iterator = hits.iterator();\n    while (iterator.hasNext()) {\n        SearchHit next = iterator.next();\n        System.out.println(next.getSourceAsString());\n    }\n\n    client.close();\n}\n\n//分词查询\n@Test\npublic void query() {\n    //1.执行查询\n    SearchResponse response = client.prepareSearch(\"blog\").setTypes(\"article\").setQuery(QueryBuilders.queryStringQuery(\"学习全文\")).get();\n\n    SearchHits hits = response.getHits();\n\n    System.out.println(\"查询的结果为\" + hits.getTotalHits());\n\n    Iterator<SearchHit> iterator = hits.iterator();\n    while (iterator.hasNext()) {\n        SearchHit next = iterator.next();\n        System.out.println(next.getSourceAsString());\n    }\n\n    client.close();\n}\n```\n\n## 通配符查询\n\n```java\n//通配符查询\n@Test\npublic void wildcardQuery() {\n    //1.执行查询\n    SearchResponse response = client.prepareSearch(\"blog\").setTypes(\"article\").setQuery(QueryBuilders.wildcardQuery(\"content\", \"分\")).get();\n\n    SearchHits hits = response.getHits();\n\n    System.out.println(\"查询的结果为\" + hits.getTotalHits());\n\n    Iterator<SearchHit> iterator = hits.iterator();\n    while (iterator.hasNext()) {\n        SearchHit next = iterator.next();\n        System.out.println(next.getSourceAsString());\n    }\n\n    client.close();\n}\n```\n\n## 模糊查询\n\n```java\n//模糊查询\n@Test\npublic void fuzzyQuery() {\n    //1.执行查询\n    SearchResponse response = client.prepareSearch(\"blog\").setTypes(\"article\").setQuery(QueryBuilders.fuzzyQuery(\"title\", \"LuceNe\")).get();\n\n    SearchHits hits = response.getHits();\n\n    System.out.println(\"查询的结果为\" + hits.getTotalHits());\n\n    Iterator<SearchHit> iterator = hits.iterator();\n    while (iterator.hasNext()) {\n        SearchHit next = iterator.next();\n        System.out.println(next.getSourceAsString());\n    }\n\n    client.close();\n}\n\n//映射相关的操作\n@Test\npublic void createMapping() throws Exception {\n\n    // 1设置mapping\n    XContentBuilder builder = XContentFactory.jsonBuilder()\n            .startObject()\n            .startObject(\"article\")\n            .startObject(\"properties\")\n            .startObject(\"id1\")\n            .field(\"type\", \"string\")\n            .field(\"store\", \"yes\")\n            .endObject()\n            .startObject(\"title2\")\n            .field(\"type\", \"string\")\n            .field(\"store\", \"no\")\n            .endObject()\n            .startObject(\"content\")\n            .field(\"type\", \"string\")\n            .field(\"store\", \"yes\")\n            .endObject()\n            .endObject()\n            .endObject()\n            .endObject();\n```\n\n## 添加mapping\n\n```java\n    // 2 添加mapping\n    PutMappingRequest mapping = Requests.putMappingRequest(\"blog1\").type(\"article\").source(builder);\n\n    client.admin().indices().putMapping(mapping).get();\n\n    // 3 关闭资源\n    client.close();\n}\n```","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"Elasticsearch分布式机制探究","url":"/2018/12/16/Elasticsearch分布式机制探究/","content":"\n {{ \"Elasticsearch的分布式秘密\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### 复杂分布式机制\n\nElasticsearch是一套分布式的系统，分布式是为了应对大数据量隐藏了复杂的分布式机制\n\n分片机制\n\n集群发现机制\n\nshard负载均衡\n\nshard副本，请求路由，集群扩容，shard重分配\n\n### 分片机制\n\n#### 规则\n\nshard = hash(routing) % number_of_primary_shards\n\nRouting值可以是一个任意的字符串，默认情况下，它的值为存数数据对应文档 _id 值，也可以是用户自定义的值。Routing这个字符串通过一个hash的函数处理，并返回一个数值，然后再除以索引中主分片的数目，所得的余数作为主分片的编号，取值一般在0到number_of_primary_shards - 1的这个范围中。通过这种方法计算出该数据是存储到哪个分片中。\n\n正是这种路由机制，导致了主分片的个数为什么在索引建立之后不能修改。对已有索引主分片数目的修改直接会导致路由规则出现严重问题，部分数据将无法被检索。\n\n### 集群发现机制\n\nThe discovery module is responsible for discovering nodes within a cluster, as well as electing a master node.\n\nNote, Elasticsearch is a peer to peer based system, nodes communicate with one another directly if operations are delegated / broadcast. All the main APIs (index, delete, search) do not communicate with the master node. The responsibility of the master node is to maintain the global cluster state, and act if nodes join or leave the cluster by reassigning shards. Each time a cluster state is changed, the state is made known to the other nodes in the cluster (the manner depends on the actual discovery implementation).\n\n![FdA6C4.md.png](https://s1.ax1x.com/2018/12/16/FdA6C4.md.png)\n\ndiscovery.zen.ping_timeout (默认3秒）配置允许对选举的时间进行调整，用来处理缓慢或拥挤的网络。当一个节点请求加入主节点，它会发送请求信息到主节点，请求的超时时间配置为discovery.zen.join timeout,这个时间比较长，是discovery.zen.ping timeout 时间的20倍。当主节点发生问题的时候，现有的节点又会通过ping来重新选举一个新的主节点。当 discovery.zen.masterelection.filter_client 设置为 true 的时候，在选举主节点的时候从客户端节点（node.client为true,或者node.data和node.master同时为(false)的ping 操作将被忽略，该参数默认为 true当 discovery.zen.master election.filter data 为true时，在选举主节点的时候从数据节点（node.data为ture, node.mas..ter同时为false)的ping操作将被忽略，默认为falsec主节点配置为true的节点一直都有选举的资格当节点node.master设置为false或者node.client设置为true的时候，它们将自动排除成为主节点的可能性。\n\ndiscovery.zen.minimum  master   nodes设置需要加入一个新当选的主节点的最小节点数B，或者接收它作为主节点的最小节点数：：如果不满足这一要求，主节点会下台，重新开始新的选举。\n\n###  shard负载均衡\n\n\"Shard\" 这个词英文的意思是\"碎片\"，而作为数据库相关的技术用语，称之为\"分片\",是水平扩展(Scale Out，亦或横向扩展、向外扩展)的解决方案，其主要目的是为突破单节点数据库服务器的 I/O 能力限制，解决数据库扩展性问题。\n\n\n\n### shard副本\n\n- index包含多个shard\n- 每个shard都是一个最小工作单元，承载部分数据，lucene实例，完整的建立索引和处理请求的能力\n- 增减节点时，shard会自动在nodes中负载均衡\n- primary shard和replica shard，每个document肯定只存在于某一个primary shard以及其对应的replica shard中，不可能存在于多个primary shard\n- replica shard是primary shard的副本，负责容错，以及承担读请求负载\n- primary shard的数量在创建索引的时候就固定了，replica shard的数量可以随时修改\n- primary shard的默认数量是5，replica默认是1，默认有10个shard，5个primary shard，5个replica shard primary shard不能和自己的replica shard放在同一个节点上（否则节点宕机，primary shard和副本都丢失，起不到容错的作用），但是可以和其他primary shard的replica shard放在同一个节点上\n\n### 集群扩容\n\n![Fdeco6.md.png](https://s1.ax1x.com/2018/12/16/Fdeco6.md.png)\n\n\n\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"Elasticsearch聚合分析","url":"/2018/12/16/Elasticsearch聚合分析/","content":"\n {{ \"Elasticsearch聚合分析\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n##  预先设置\n\n在进行聚合分析的是皇后首先把文本的field的fielddata属性设置为true\n\n```js\nPUT /ecommerce/_mapping/product\n{\n  \"properties\": {\n    \"tags\": {\n      \"type\": \"text\",\n      \"fielddata\": true\n    }\n  }\n}\n```\n\n## 计算每个tag下的商品数量\n\n```json\nGET /ecommerce/product/_search\n{\n  \"aggs\": {\n    \"group_by_tags\": {\n      \"terms\": { \"field\": \"tags\" }\n    }\n  }\n}\n```\n\n结果\n\n```json\n{\n  \"took\": 11,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 3,\n    \"max_score\": 1,\n    \"hits\": [\n  ......\n  \"aggregations\": {\n    \"group_by_tags\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"fangzhu\",\n          \"doc_count\": 2\n        },\n        {\n          \"key\": \"meibai\",\n          \"doc_count\": 1\n        },\n        {\n          \"key\": \"qingxin\",\n          \"doc_count\": 1\n        }\n      ]\n    }\n  }\n}\n```\n\n## 包含yagao的商品，计算每个tag下的商品数量\n\n```json\nGET /ecommerce/product/_search\n{\n  \"size\": 0,\n  \"query\": {\n    \"match\": {\n      \"name\": \"yagao\"\n    }\n  },\n  \"aggs\": {\n    \"all_tags\": {\n      \"terms\": {\n        \"field\": \"tags\"\n      }\n    }\n  }\n}\n```\n\n## 先分组，再算每组的平均值，计算每个tag下的商品的平均价格\n\n```json\nGET /ecommerce/product/_search\n{\n    \"size\": 0,\n    \"aggs\" : {\n        \"group_by_tags\" : {\n            \"terms\" : { \"field\" : \"tags\" },\n            \"aggs\" : {\n                \"avg_price\" : {\n                    \"avg\" : { \"field\" : \"price\" }\n                }\n            }\n        }\n    }\n}\n```\n\n结果\n\n```json\n{\n  \"took\": 22,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 3,\n    \"max_score\": 0,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"group_by_tags\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"fangzhu\",\n          \"doc_count\": 2,\n          \"avg_price\": {\n            \"value\": 27.5\n          }\n        },\n        {\n          \"key\": \"meibai\",\n          \"doc_count\": 1,\n          \"avg_price\": {\n            \"value\": 30\n          }\n        },\n        {\n          \"key\": \"qingxin\",\n          \"doc_count\": 1,\n          \"avg_price\": {\n            \"value\": 40\n          }\n        }\n      ]\n    }\n  }\n}\n```\n\n\n\n## 计算每个tag下的商品的平均价格，并且按照平均价格降序排序\n\n```json\nGET /ecommerce/product/_search\n{\n    \"size\": 0,\n    \"aggs\" : {\n        \"all_tags\" : {\n            \"terms\" : { \"field\" : \"tags\", \"order\": { \"avg_price\": \"desc\" } },\n            \"aggs\" : {\n                \"avg_price\" : {\n                    \"avg\" : { \"field\" : \"price\" }\n                }\n            }\n        }\n    }\n}\n```\n\n结果\n\n````json\n{\n  \"took\": 26,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n  \"hits\": {\n    \"total\": 3,\n    \"max_score\": 0,\n    \"hits\": []\n  },\n  \"aggregations\": {\n    \"all_tags\": {\n      \"doc_count_error_upper_bound\": 0,\n      \"sum_other_doc_count\": 0,\n      \"buckets\": [\n        {\n          \"key\": \"qingxin\",\n          \"doc_count\": 1,\n          \"avg_price\": {\n            \"value\": 40\n          }\n        },\n        {\n          \"key\": \"meibai\",\n          \"doc_count\": 1,\n          \"avg_price\": {\n            \"value\": 30\n          }\n        },\n        {\n          \"key\": \"fangzhu\",\n          \"doc_count\": 2,\n          \"avg_price\": {\n            \"value\": 27.5\n          }\n        }\n      ]\n    }\n  }\n}\n````\n\n## 按照指定的价格范围区间进行分组，然后在每组内再按照tag进行分组，最后再计算每组的平均价格\n\n```js\nGET /ecommerce/product/_search\n{\n  \"size\": 0,\n  \"aggs\": {\n    \"group_by_price\": {\n      \"range\": {\n        \"field\": \"price\",\n        \"ranges\": [\n          {\n            \"from\": 0,\n            \"to\": 20\n          },\n          {\n            \"from\": 20,\n            \"to\": 40\n          },\n          {\n            \"from\": 40,\n            \"to\": 50\n          }\n        ]\n      },\n      \"aggs\": {\n        \"group_by_tags\": {\n          \"terms\": {\n            \"field\": \"tags\"\n          },\n          \"aggs\": {\n            \"average_price\": {\n              \"avg\": {\n                \"field\": \"price\"\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n```\n\n\n\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"Elasticsearch增删改查","url":"/2018/12/16/Elasticsearch增删改查/","content":"\n {{ \"Elasticsearch的增删改查\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### 面向文档\n\ndocument数据格式 \n\n1. 应用系统的数据结构都是面向对象的，复杂的\n2. 对象数据存储到数据库中，只能拆解开来，变为扁平的多张表，每次查询的时候还得还原回对象格式，相当麻烦\n3. ES是面向文档的，文档中存储的数据结构，与面向对象的数据结构是一样的，基于这种文档数据结构，es可以提供复杂的索引，全文检索，分析聚合等功能\n4. es的document用json数据格式来表达\n\n### Java数据\n\n```java\npublic class Employee {\n\n  private String email;\n  private String firstName;\n  private String lastName;\n  private EmployeeInfo info;\n  private Date joinDate;\n\n}\n\nprivate class EmployeeInfo {\n  \n  private String bio; // 性格\n  private Integer age;\n  private String[] interests; // 兴趣爱好\n\n}\nEmployeeInfo info = new EmployeeInfo();\ninfo.setBio(\"curious and modest\");\ninfo.setAge(30);\ninfo.setInterests(new String[]{\"bike\", \"climb\"});\n\nEmployee employee = new Employee();\nemployee.setEmail(\"zhangsan@sina.com\");\nemployee.setFirstName(\"san\");\nemployee.setLastName(\"zhang\");\nemployee.setInfo(info);\nemployee.setJoinDate(new Date());\n```\n\n### 数据库数据\n\n#### employee\n\n| id   | email            | first_name | last_name | join_date  |\n| ---- | ---------------- | ---------- | --------- | ---------- |\n| 001  | hangsan@sina.com | san        | zhang     | 2017/01/01 |\n\n#### employee_info\n\n| employee_id | bio                | age  | interests   |\n| :---------- | ------------------ | ---- | ----------- |\n| 001         | curious and modest | 30   | bike, climb |\n\n### Json数据\n\n```json\n{\n    \"email\":      \"zhangsan@sina.com\",\n    \"first_name\": \"san\",\n    \"last_name\": \"zhang\",\n    \"info\": {\n        \"bio\":         \"curious and modest\",\n        \"age\":         30,\n        \"interests\": [ \"bike\", \"climb\" ]\n    },\n    \"join_date\": \"2017/01/01\"\n}\n```\n\n### 集群管理\n\n```json\nGET /_cat/health?v \n```\n\ngreen：每个索引的primary shard和replica shard都是active状态的\nyellow：每个索引的primary shard都是active状态的，但是部分replica shard不是active状态，处于不可用的状态\nred：不是所有索引的primary shard都是active状态的，部分索引有数据丢失了\n\n现在只启动动了一个es进程，相当于就只有一个node。现在es中有一个index，就是kibana自己内置建立的index。由于默认的配置是给每个index分配5个primary shard和5个replica shard，而且primary shard和replica shard不能在同一台机器上（为了容错）。现在kibana自己建立的index是1个primary shard和1个replica shard。当前就一个node，所以只有1个primary shard被分配了和启动了，但是一个replica shard没有第二台机器去启动。只要启动第二个es进程，就会在es集群中有2个node，然后那1个replica shard就会自动分配过去，然后cluster status就会变成green状态。\n\n### 新增\n\n```json\n#语法\nPUT /index/type/id\n{\n  \"json数据\"\n}\n# 添加商品1\nPUT /ecommerce/product/1\n{\n    \"name\" : \"gaolujie yagao\",                        #商品名称\n    \"desc\" :  \"gaoxiao meibai\",                       #商品描述\n    \"price\" :  30,\t\t\t\t\t\t\t\t   #商品价格\n    \"producer\" :      \"gaolujie producer\",            #生厂厂家\n    \"tags\": [ \"meibai\", \"fangzhu\" ]                   #产品标签\n}\n#添加商品2\nPUT /ecommerce/product/2\n{\n    \"name\" : \"jiajieshi yagao\",\n    \"desc\" :  \"youxiao fangzhu\",\n    \"price\" :  25,\n    \"producer\" :      \"jiajieshi producer\",\n    \"tags\": [ \"fangzhu\" ]\n}\n#添加商品3\nPUT /ecommerce/product/3\n{\n    \"name\" : \"zhonghua yagao\",\n    \"desc\" :  \"caoben zhiwu\",\n    \"price\" :  40,\n    \"producer\" :      \"zhonghua producer\",\n    \"tags\": [ \"qingxin\" ]\n}\n\n```\n\nes会自动建立index和type，不需要提前创建，而且es默认会对document每个field都建立倒排索引，让其可以被搜索\n\n### 查询\n\n```\n#语法\nGET /index/type/id\nGET /ecommerce/product/1\n```\n\n```json\n{\n  \"_index\": \"ecommerce\",\n  \"_type\": \"product\",\n  \"_id\": \"1\",\n  \"_version\": 1,\n  \"found\": true,\n  \"_source\": {\n    \"name\": \"gaolujie yagao\",\n    \"desc\": \"gaoxiao meibai\",\n    \"price\": 30,\n    \"producer\": \"gaolujie producer\",\n    \"tags\": [\n      \"meibai\",\n      \"fangzhu\"\n    ]\n  }\n}\n```\n\n### 修改\n\n```json\nPUT /ecommerce/product/1\n{\n    \"name\" : \"jiaqiangban gaolujie yagao\",\n    \"desc\" :  \"gaoxiao meibai\",\n    \"price\" :  30,\n    \"producer\" :      \"gaolujie producer\",\n    \"tags\": [ \"meibai\", \"fangzhu\" ]\n}\n```\n\n### 删除\n\n```\nDELETE /ecommerce/product/1\n```\n\n\n\n## 查询\n\n### query string search\n\nquery string search的由来:因为search参数都是以http请求的query string来附带的\n\n```json\n{\n  \"took\": 3,\n  \"timed_out\": false,\n  \"_shards\": {\n    \"total\": 5,\n    \"successful\": 5,\n    \"failed\": 0\n  },\n    \"hits\": {\n    \"total\": 3,\n    \"max_score\": 1,\n    \"hits\": \n  ......\n  {\n        \"_index\": \"ecommerce\",\n        \"_type\": \"product\",\n        \"_id\": \"3\",\n        \"_score\": 1,\n        \"_source\": {\n          \"name\": \"zhonghua yagao\",\n          \"desc\": \"caoben zhiwu\",\n          \"price\": 40,\n          \"producer\": \"zhonghua producer\",\n          \"tags\": [\n            \"qingxin\"\n          ]\n       ......\n}\n```\n\ntook：耗费了几毫秒\ntimed_out：是否超时，这里是没有\n_shards：数据拆成了5个分片，所以对于搜索请求，会打到所有的primary shard（或者是它的某个replica shard）\nhits.total：查询结果的数量，3个document\nhits.max_score：score的含义，就是document对于一个search的相关度的匹配分数，越相关，就越匹配，分数也高\nhits.hits：包含了匹配搜索的document的详细数据\n\n按售价降序排列\n\n````json\nGET /ecommerce/product/_search?q=name:yagao&sort=price:desc\n````\n\n#### 适用场景\n\n适用于临时的在命令行使用一些工具，比如curl，快速的发出请求，来检索想要的信息；如果查询请求很复杂，是很难去构建的在生产环境中，几乎很少使用query string search\n\n### query DSL\n\nDSL：Domain Specified Language，特定领域的语言\nhttp request body：请求体，可以用json的格式来构建查询语法，比较方便，可以构建各种复杂的语法，比query string search肯定强大多了\n\n#### 查询所有\n\n```json\nGET /ecommerce/product/_search\n{\n  \"query\": { \"match_all\": {} }\n}\n```\n\n#### 条件查询\n\n查询名称包含yagao的商品，同时按照价格降序排序\n\n````json\nGET /ecommerce/product/_search\n{\n    \"query\" : {\n        \"match\" : {\n            \"name\" : \"yagao\"\n        }\n    },\n    \"sort\": [\n        { \"price\": \"desc\" }\n    ]\n}\n````\n\n#### 分页查询\n\n```json\nGET /ecommerce/product/_search\n{\n  \"query\": { \"match_all\": {} },\n  \"from\": 1,\n  \"size\": 1\n}\n```\n\n#### 指定查询\n\n更加适合生产环境的使用，可以构建复杂的查询\n\n```json\nGET /ecommerce/product/_search\n{\n  \"query\": { \"match_all\": {} },\n  \"_source\": [\"name\", \"price\"]\n  }\n```\n\n###  query filter\n\n#### 过滤查询\n\n搜索商品名称包含yagao，而且售价大于25元的商品\n\n```json\nGET /ecommerce/product/_search\n{\n    \"query\" : {\n        \"bool\" : {\n            \"must\" : {\n                \"match\" : {\n                    \"name\" : \"yagao\" \n                }\n            },\n            \"filter\" : {\n                \"range\" : {\n                    \"price\" : { \"gt\" : 25 } \n                }\n            }\n        }\n    }\n}\n```\n\n### full-text search（全文检索）\n\n```\nGET /ecommerce/product/_search\n{\n    \"query\" : {\n        \"match\" : {\n            \"producer\" : \"yagao producer\"\n        }\n    }\n}\n```\n\nproducer这个字段，会先被拆解，建立倒排索引\n\n| special   |         | 4    |\n| --------- | ------- | ---- |\n| yagao     |         | 4    |\n| producer  | 1,2,3,4 |      |\n| gaolujie  | 1       |      |\n| zhognhua  | 3       |      |\n| jiajieshi | 2       |      |\n\nyagao producer 会被拆解为 yagao和producer\n\n### phrase search（短语搜索）\n\n跟全文检索相对应，相反，全文检索会将输入的搜索串拆解开来，去倒排索引里面去一一匹配，只要能匹配上任意一个拆解后的单词，就可以作为结果返回\nphrase search，要求输入的搜索串，必须在指定的字段文本中，完全包含一模一样的，才可以算匹配，才能作为结果返回\n\n```\nGET /ecommerce/product/_search\n{\n    \"query\" : {\n        \"match_phrase\" : {\n            \"producer\" : \"yagao producer\"\n        }\n    }\n}\n```\n\n### highlight search（高亮搜索结果）\n\n```\nGET /ecommerce/product/_search\n{\n    \"query\" : {\n        \"match\" : {\n            \"producer\" : \"producer\"\n        }\n    },\n    \"highlight\": {\n        \"fields\" : {\n            \"producer\" : {}\n        }\n    }\n}\n```\n\n搜索结果会有一个<em> 标签 类似于这种效果\n\n![Fd9gg0.md.png](https://s1.ax1x.com/2018/12/16/Fd9gg0.md.png)\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"ElasticSearch索引","url":"/2018/12/15/ElasticSearch索引/","content":"\n {{ \"Elasticsearch索引\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 简介\n\n索引是具有相同结构的文档集合。在Elasticsearch中索引是个非常重要的内容，对Elasticsearch的大部分操作都是基于索引来完成的。同时索引可以类比关系型数据库Mysql中的数据库database\n\n### 创建索引\n\n创建索引的时候可以通过修改number of shards和 number of replicas参数的数量来修改分片和副本的数量。在默认的情况下分片的数量是5个，副本的数量是1个\n\n例如创建三个主分片两个副本分片的索引\n\n```json\nPUT /secisland\n{\n  \"settings\": {\n    \"index\":{\"number_of_shards\":3,\"number_of_replicas\":2}\n  }\n}\n#参数可以简写为\n{\"settings\": {\"number_of_shards\":3,\"number_of_replicas\":2}}\n```\n\n![FacGMF.md.png](https://s1.ax1x.com/2018/12/15/FacGMF.md.png)\n\n### 修改索引\n\n```json\nPUT /test_index/_settings\n{\n  \"number_of_replicas\":1\n}\n```\n\n![Fac0G6.md.png](https://s1.ax1x.com/2018/12/15/Fac0G6.md.png)\n\n对于任何Elasticsearch文档而言，一个文档会包括一个或者多个字段，任何字段都要有自己的数据类型，例如string、integer、date等。Elasticsearch中是通过映射来进行字段和数据类型对应的。在默认的情况下Elasticsearch会自动识别字段的数据类型。同时Elasticsearch提供了 mappings参数可以显式地进行映射。\n\n```json\nPUT /test_index1\n{\n\"settings\": {\"number_of_shards\":3,\"number_of_replicas\":2},\n\"mappings\": {\"secilog\": {\"properties\": {\"logType\": {\"type\": \"string\",\n\"index\": \"not_analyzed\"}}}}\n}\n```\n\n![FagQwd.md.png](https://s1.ax1x.com/2018/12/15/FagQwd.md.png)\n\n### 删除索引\n\n```json\nDELETE /test_index\n```\n\n![FagdmQ.md.png](https://s1.ax1x.com/2018/12/15/FagdmQ.md.png)\n\n### 获取索引\n\n```json\nGET /test_index1\n{\n  \"test_index1\": {\n    \"aliases\": {},\n    \"mappings\": {\n      \"secilog\": {\n        \"properties\": {\n          \"logType\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    },\n    \"settings\": {\n      \"index\": {\n        \"creation_date\": \"1544886365769\",\n        \"number_of_shards\": \"3\",\n        \"number_of_replicas\": \"2\",\n        \"uuid\": \"Iz8evLbCQ1CS85owEbKsgQ\",\n        \"version\": {\n          \"created\": \"5020299\"\n        },\n        \"provided_name\": \"test_index1\"\n      }\n    }\n  }\n}\n```\n\n### 过滤查询\n\n```json\n索引的settings和mappings属性。可配置的属性包括 settings、 mappingswarmers 和 aliases。\nGET /test_index1/_settings,_mapping, 如果索引不存在则返回404\n{\n  \"test_index1\": {\n    \"settings\": {\n      \"index\": {\n        \"creation_date\": \"1544886365769\",\n        \"number_of_shards\": \"3\",\n        \"number_of_replicas\": \"2\",\n        \"uuid\": \"Iz8evLbCQ1CS85owEbKsgQ\",\n        \"version\": {\n          \"created\": \"5020299\"\n        },\n        \"provided_name\": \"test_index1\"\n      }\n    },\n    \"mappings\": {\n      \"secilog\": {\n        \"properties\": {\n          \"logType\": {\n            \"type\": \"keyword\"\n          }\n        }\n      }\n    }\n  }\n}\n\n```\n\n### 关闭索引\n\n![Fa2YNR.md.png](https://s1.ax1x.com/2018/12/15/Fa2YNR.md.png)\n\n![Fa2R8P.md.png](https://s1.ax1x.com/2018/12/15/Fa2R8P.md.png)\n\n### 映射管理\n\n#### 增加映射\n\n```json\nPUT /test_index2\n{\n  \"mappings\": {\n    \"log\": {\n      \"properties\": {\n        \"message\": {\n          \"type\": \"string\"\n        }\n      }\n    }\n  }\n#以上接口添加索引名为test_index2，文档类型为log，其中包含字段message,字段类型是字符串：\n PUT test_index2/_mapping/user\n{\n  \"properties\": {\n    \"name\":{\"type\": \"string\"}\n  }\n}\n# 添加文档类型为user，包含字段name,字段类型是字符串。\n#设置多个索引映射时\nPUT /{index}/_mapping/{type}\n{index}可以有多种方式，逗号分隔：比如testl,test2，test3。\nall表示所有索引3通配符*表示所有。test*表示以test开头。\n{type}需要添加或更新的文档类型。\n{body}需要添加的字段或字段类型\n```\n\n#### 获取映射\n\n系统同时支持获取多个索引和类型的语法。\n获取文档映射接口一次可以获取多个索引或文档映射类型。该接口通常是如下格式:\nhost:port/{index}/ mapping/{type}, {index}和{type}可以接受逗号（，）分隔符，也可以使用\n\n```html\nGET  test_index2/_mapping/{string}\nGET /test_index2/_mapping/log,user\n```\n\n#### 判断存在\n\n健查索引或文档类型是否存在  存在返回200 不存在返回404\n\n```\nHEAD test_index2/secilog\n```\n\n### 索引别名\n\nElasticsearch可以对一个或多个索引指定别名,通过别名可以查询一个或多个索引内容,在内部Elasticsearch会把别名映射在索引上,别名不可以和索引名其他索引别名重复\n\n```json\nPOST /_aliases\n{\n  \"actions\":{\"add\":{\"index\":\"test_index1\",\"aliases\":\"othername1\"}}\n}\n#给test_index增加索引别名othername1\n```\n\n#### 修改别名\n\n别名没有修改的语法,当需要修改时,先删除 ,在添加\n\n```json\nPOST /_aliases\n{\n  \"actions\":{\"remove\":{\"index\":\"test_index1\",\"aliases\":\"othername2\"}}\n}\n```\n\n#### 删除别名\n\n```\nDELETE  http://{host}:{port}/{index}/_alias/{name}\n```\n\n#### 查询别名\n\n```\nGET  http://{host}:{port}/{index}/_alias/{name}\n```\n\n\n\n\n\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"Elasticsearch简介与安装","url":"/2018/12/15/Elasticsearch简介与安装/","content":"\n {{ \"Elasticsearch简介和安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## 搜索\n\n就是在任何场景下，找寻你想要的信息，这个时候，会输入一段你要搜索的关键字，然后就期望找到这个关键字相关的有些信息\n\n### 垂直搜索\n\n站内搜索\n\n### 互联网搜索\n\n电商网站，招聘网站，新闻网站，各种app\n\n### IT系统的搜索\n\nOA软件，办公自动化软件，会议管理，日程管理，项目管理，员工管理，搜索“张三”，“张三儿”，“张小三”；有个电商网站，卖家，后台管理系统，搜索“牙膏”，订单，“牙膏相关的订单”\n\n数据都是存储在数据库里面的，比如说电商网站的商品信息，招聘网站的职位信息，新闻网站的新闻信息，如果说从技术的角度去考虑，如何实现如说，电商网站内部的搜索功能的话，就可以考虑，去使用数据库去进行搜索。\n\n1、每条记录的指定字段的文本，可能会很长，比如说“商品描述”字段的长度，有长达数千个，甚至数万个字符，这个时候，每次都要对每条记录的所有文本进行扫描，来判断说，你包不包含我指定的这个关键词（比如说“牙膏”）\n2、还不能将搜索词拆分开来，尽可能去搜索更多的符合你的期望的结果，比如输入“生化机”，就搜索不出来“生化危机”\n\n用数据库来实现搜索，是不太靠谱的。通常来说，性能会很差的。\n\n## Lucene\n\nLucene是Apache软件基金会中一个开放源代码的全文搜索引擎工具包，是一个全文搜索引擎的架构，提供了完整的查询引擎和索引引擎，部分文本分析引擎。Lucene的目的是为软件开发人员提供一个简单易用的工具包，以方便在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文搜索引擎。\n\n[全文检索](https://baike.baidu.com/item/%E5%85%A8%E6%96%87%E6%A3%80%E7%B4%A2/8028630)是指计算机索引程序通过扫描文章中的每一个词，对每一个词建立一个索引，指明该词在文章中出现的次数和位置，当用户查询时，检索程序就根据事先建立的索引进行查找，并将查找的结果反馈给用户的检索方式。这个过程类似于通过字典中的检索字表查字的过程。全文搜索搜索引擎数据库中的数据。\n\n倒排索引源于实际应用中需要根据属性的值来查找记录。这种索引表中的每一项都包括一个属性值和具有该属性值的各记录的地址由于不是由记录来确定属性值，而是由属性值来确定记录的位置，因而称为倒排索引（inverted index)。带有倒排索引的文件我们称为倒排索引文件，简称倒排文件（inverted file)。倒排索引中的索引对象是文档或者文档集合中的单词等，用来存储这些单词在一个文档或者一组文档中的存储位置，是对文档或者文档集合的一种最常用的索引机制。搜索引擎的关键步骤就是建立倒排索引，倒排索引一般表示为一个关键词，然后是它的频度（岀现的次数)、位置（出现在哪一篇文章或网页中，及有关的日期，作者等信息)，好比一本书的目录、标签一般。读者想看哪一个主题相关的章节，直接根据目录即可找到相关的页面。不必再从书的第一页到最后一页，一页一页地查找。\n\n###  评分公式\n\n文档得分:它是一个刻画文档与査询匹配程度的参数。Apache Lucene的默认评分机制：TF/IDF (词频/逆文档频率）算法\n\n当一个文档经Lucene返回，则意味着该文档与用户提交的查询是匹配的。在这种情况下，每个返回的文档都有一个得分。得分越高，文档相关度更高，同一个文档针对不同查询的得分是不同的，比较某文档在不同查询中的得分是没有意义的。同一文档在不同查询中的得分不具备可比较性，不同查询返回文档中的最高得分也不具备可比较性。这是因为文档得分依赖多个因子，除了权重和查询本身的结构，还包括匹配的词项数目，词项所在字段，以及用于查询规范化的匹配类型等。在一些比较极端的情况下，同一个文档在相似查询中的得分非常悬殊，仅仅是因为使用了自定义得分查询或者命中词项数发生了急剧变化。\n\n#### 参考因子\n\n- 文档权重（document boost):索引期赋予某个文档的权重值。\n- 字段权重（field boost):查询期赋予某个字段的权重值。\n- 协调因子（coord):基于文档中词项命中个数的协调因子，一个文档命中了查询中的词项越多，得分越高。\n- 逆文档频率（inverse document frequency): —个基于词项的因子，用来告诉评分公式该词项有多么罕见。逆文档频率越低，词项越罕见。评分公式利用该因子为包含罕见词项的文档加权。\n- 长度范数（length nomi):每个字段的基于词项个数的归一化因子（在索引期计算出来并存储在索引中）。一个字段包含的词项数越多，该因子的权重越低，这意味着 Apache Lucene评分公式更“喜欢”包含更少词项的字段。\n- 词频（term frequency): —个基于词项的因子，用来表示一个词项在某个文档中出现了多少次。词频越高，文档得分越高。\n- 查询范数（query norm): —个基于查询的归一化因子，它等于查询中词项的权重平方和。查询范数使不同查询的得分能相互比较，尽管这种比较通常是困难且不可行的。\n\n#### TF/IDF评分公式\n\n$$score(q,d)=coord(q,d)*queryBoost(q)*\\frac{V(q)* {V(d)}}{|V(q)|}lengthNorm(d)*docBoost(d)$$\n\n上面公式糅合了布尔检索模型和向量空间检索模型。想了解更多请百度。\n\n#### 实际公式\n\n$$score(q,d)=coord(q,d)*queryBoost(q)*\\sum_{i \\ \\ in \\ \\ q}(tf ( t \\ \\  in \\ \\ d) *idf(t)^2*boost(t)*norm(t,d))$$\n\n得分公式是一个关于査询q和文档d的函数，有两个因子coord和queryNorm并不直接依赖查询词项，而是与查询词项的一个求和公式相乘。求和公式中每个加数由以下因子连乘所得：词频、逆文档频率、词项权重、范数。范数就是之前提到的长度范数。\n\n基本规则：\n\n- 越多罕见的词项被匹配上，文档得分越高。\n- 文档字段越短（包含更少的词项)，文档得分越高。\n- 权重越高（不论是索引期还是査询期赋予的权重值)，文档得分越高。\n\n## Elasticsearch\n\nElasticsearch (ES)是一个基于Lucene构建的开源、分布式、[RESTful](https://baike.baidu.com/item/RESTful/4406165?fr=aladdin)接口全文搜索引擎Elasticsearch还是一个分布式文档数据库，其中每个字段均是被索引的数据且可被搜索，它能够扩展至数以百计的服务器存储以及处理PB级的数据。它可以在很短的时间内存储、搜索和分析大量的数据。它通常作为具有复杂搜索场景情况下的核心发动机。Elasticsearch就是为高可用和可扩展而生的。可以通过购置性能更强的服务器来完成，称为垂直扩展或者向上扩展（Vertical Scale/Scaling Up)，或增加更多的服务器来完成，称为水平扩展或者向外扩展（Horizontal Scale/Scaling Out)尽管ES能够利用更强劲的硬件，垂直扩展毕竟还是有它的极限。真正的可扩展性来自于水平扩展，通过向集群中添加更多的节点来分担负载，增加可靠性。在大多数数据库中，水平扩展通常都需要你对应用进行一次大的重构来利用更多的节点。而ES天生就是分布式的：它知道如何管理多个节点来完成扩展和实现高可用性。这也意味着你的应用不需要做任何的改动。\n\n### 评分规则\n\n是ElasticSearch使用了Lucene的评分功能，但好在我们可以替换默认的评分算法，ElasticSearch使用了Lucene的评分功\n能但不仅限于Lucene的评分功能。用户可以使用各种不同的查询类型以精确控制文档评分的计算（custom_boost_factor 查询、constant_score 査询、custom_score 查询等），还可以通过使用脚本（scripting)来改变文档得分，还可以使用ElasticSearch 0.90中岀现的二次评分功能，通过在返回文档集之上执行另外一个查询，重新计算前N个文档的文档得分。\n\n- Okapi BM25模型：这是一种基于概率模型的相似度模型，可用于估算文档与给定査询匹配的概率。为了在ElasticSearch中使用它，你需要使用该模型的名字，BM25。一般来说，Okapi BM25模型在短文本文档上的效果最好，因为这种场景中重复词项对文档的总体得分损害较大。\n- 随机偏离（Divergence from randomness)模型：这是一种基于同名概率模型的相似度模型。为了在ElasticSearch中使用它，你需要使用该模型的名字，DFR。一般来说，随机偏离模型在类似自然语言的文本上效果较好。\n- 基于信息的（Information based)模型：这是最后一个新引人的相似度模型，与随机偏离模型类似。为了在ElasticSearch中使用它，你需要使用该模型的名字，IB。同样，IB模型也在类似自然语言的文本上拥有较好的效果。\n\n### 用途\n\n- 分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索。\n- 实时分析的分布式搜索引擎。\n- 可以扩展到上百台服务器，处理PB级别的结构化或非结构化数据。\n\n### 应用场景\n\n#### 国外\n\n- 维基百科，类似百度百科，牙膏，牙膏的维基百科，全文检索，高亮，搜索推荐\n- The Guardian（国外新闻网站），类似搜狐新闻，用户行为日志（点击，浏览，收藏，评论）+社交网络数据（对某某新闻的相关看法），数据分析，给到每篇新闻文章的作者，让他知道他的文章的公众反馈（好，坏，热门，垃圾，鄙视，崇拜）\n- Stack Overflow（国外的程序异常讨论论坛），IT问题，程序的报错，提交上去，有人会跟你讨论和回答，全文检索，搜索相关问题和答案，程序报错了，就会将报错信息粘贴到里面去，搜索有没有对应的答案\n- GitHub（开源代码管理），搜索上千亿行代码\n- 电商网站，检索商品\n- 日志数据分析，logstash采集日志，ES进行复杂的数据分析（ELK技术，elasticsearch+logstash+kibana）\n- 商品价格监控网站，用户设定某商品的价格阈值，当低于该阈值的时候，发送通知消息给用户，比如说订阅牙膏的监控，如果高露洁牙膏的家庭套装低于50块钱，就通知我，我就去买\n- BI系统，商业智能，Business Intelligence。比如说有个大型商场集团，BI，分析一下某某区域最近3年的用户消费金额的趋势以及用户群体的组成构成，产出相关的数张报表，**区，最近3年，每年消费金额呈现100%的增长，而且用户群体85%是高级白领，开一个新商场。ES执行数据分析和挖掘，Kibana进行数据可视化\n\n#### 国内\n\n- 国内：站内搜索（电商，招聘，门户，等等），IT系统搜索（OA，CRM，ERP，等等），数据分析（ES热门的一个使用场景）\n\n###  特点\n\n1. 可以作为一个大型分布式集群（数百台服务器）技术，处理PB级数据，服务大公司；也可以运行在单机上，服务小公司\n2. Elasticsearch不是什么新技术，主要是将全文检索、数据分析以及分布式技术，合并在了一起，才形成了独一无二的ES；lucene（全文检索），商用的数据分析软件（BI），分布式数据库（mycat）\n3. 对用户而言，是开箱即用的，非常简单，作为中小型的应用，直接3分钟部署一下ES，就可以作为生产环境的系统来使用了，数据量不大，操作不是太复杂\n4. 数据库的功能面对很多领域是不够用的（事务，还有各种联机事务型的操作）；特殊的功能，比如全文检索，同义词处理，相关度排名，复杂数据分析，海量数据的近实时处理；Elasticsearch作为传统数据库的一个补充，提供了数据库所不不能提供的很多功能\n\n### 核心概念\n\n（1）Near Realtime（NRT）：近实时，两个意思，从写入数据到数据可以被搜索到有一个小延迟（大概1秒）；基于es执行搜索和分析可以达到秒级\n\n（2）Cluster：集群，包含多个节点，每个节点属于哪个集群是通过一个配置（集群名称，默认是elasticsearch）来决定的，对于中小型应用来说，刚开始一个集群就一个节点很正常\n（3）Node：节点，集群中的一个节点，节点也有一个名称（默认是随机分配的），节点名称很重要（在执行运维管理操作的时候），默认节点会去加入一个名称为“elasticsearch”的集群，如果直接启动一堆节点，那么它们会自动组成一个elasticsearch集群，当然一个节点也可以组成一个elasticsearch集群\n\n（4）Document&field：文档，es中的最小数据单元，一个document可以是一条客户数据，一条商品分类数据，一条订单数据，通常用JSON数据结构表示，每个index下的type中，都可以去存储多个document。一个document里面有多个field，每个field就是一个数据字段。\n\n```js\n#product document\n{\n  \"product_id\": \"1\",\n  \"product_name\": \"高露洁牙膏\",\n  \"product_desc\": \"高效美白\",\n  \"category_id\": \"2\",\n  \"category_name\": \"日化用品\"\n}\n```\n\n\n\n（5）Index：索引，包含一堆有相似结构的文档数据，比如可以有一个客户索引，商品分类索引，订单索引，索引有一个名称。一个index包含很多document，一个index就代表了一类类似的或者相同的document。比如说建立一个product index，商品索引，里面可能就存放了所有的商品数据，所有的商品document。\n（6）Type：类型，每个索引里都可以有一个或多个type，type是index中的一个逻辑数据分类，一个type下的document，都有相同的field，比如博客系统，有一个索引，可以定义用户数据type，博客数据type，评论数据type。\n\n商品index，里面存放了所有的商品数据，商品document\n\n但是商品分很多种类，每个种类的document的field可能不太一样，比如说电器商品，可能还包含一些诸如售后时间范围这样的特殊field；生鲜商品，还包含一些诸如生鲜保质期之类的特殊field\n\ntype，日化商品type，电器商品type，生鲜商品type\n\n日化商品type：product_id，product_name，product_desc，category_id，category_name\n电器商品type：product_id，product_name，product_desc，category_id，category_name，service_period\n生鲜商品type：product_id，product_name，product_desc，category_id，category_name，eat_period\n\n```json\n#每一个type里面，都会包含一堆document\n{\n  \"product_id\": \"2\",\n  \"product_name\": \"长虹电视机\",\n  \"product_desc\": \"4k高清\",\n  \"category_id\": \"3\",\n  \"category_name\": \"电器\",\n  \"service_period\": \"1年\"\n}\n\n{\n  \"product_id\": \"3\",\n  \"product_name\": \"基围虾\",\n  \"product_desc\": \"纯天然，冰岛产\",\n  \"category_id\": \"4\",\n  \"category_name\": \"生鲜\",\n  \"eat_period\": \"7天\"\n}\n```\n\n\n\n（7）shard：单台机器无法存储大量数据，es可以将一个索引中的数据切分为多个shard，分布在多台服务器上存储。有了shard就可以横向扩展，存储更多数据，让搜索和分析等操作分布到多台服务器上去执行，提升吞吐量和性能。每个shard都是一个lucene index。\n（8）replica：任何一个服务器随时可能故障或宕机，此时shard可能就会丢失，因此可以为每个shard创建多个replica副本。replica可以在shard故障时提供备用服务，保证数据不丢失，多个replica还可以提升搜索操作的吞吐量和性能。primary shard（建立索引时一次设置，不能修改，默认5个），replica shard（随时修改数量，默认1个），默认每个索引10个shard，5个primary shard，5个replica shard，最小的高可用配置，是2台服务器。\n\n### 对比\n\n| 关系型数据库（比如Mysql） | 非关系型数据库（Elasticsearch） |\n| ------------------------- | ------------------------------- |\n| 数据库Database            | 索引Index                       |\n| 表Table                   | 类型Type                        |\n| 数据行Row                 | 文档Document                    |\n| 数据列Column              | 字段Field                       |\n| 约束 Schema               | 映射Mapping                     |\n\n| HTTP方法 | 数据处理 | 说明                                               |\n| -------- | -------- | -------------------------------------------------- |\n| POST     | Create   | 新增一个没有ID的资源                               |\n| GET      | Read     | 取得一个资源                                       |\n| PUT      | Update   | 更新一个资源。或新增一个含ID的资源（如果1D不存在） |\n| DELETE   | Delete   | 删除一个资源                                       |\n\n\n\n## ES安装\n\nElasticsearch官网： <https://www.elastic.co/products/elasticsearch>\n\n1）解压elasticsearch-5.2.2.tar.gz到/opt/module目录下\n\n```shell\n tar -zxvf elasticsearch-5.2.2.tar.gz -C /opt/module/\n```\n\n\n\n2）在/opt/module/elasticsearch-5.2.2路径下创建data和logs文件夹\n\n```shell\nmkdir data\nmkdir logs\n```\n\n\n\n3）修改配置文件/opt/module/elasticsearch-5.2.2/config/elasticsearch.yml\n\n```shell\n# ---------------------------------- Cluster -----------------------------------\ncluster.name: my-application\n# ------------------------------------ Node ------------------------------------\nnode.name: node-102\n# ----------------------------------- Paths ------------------------------------\npath.data: /opt/module/elasticsearch-5.2.2/data\npath.logs: /opt/module/elasticsearch-5.2.2/logs\n# ----------------------------------- Memory -----------------------------------\nbootstrap.memory_lock: false\nbootstrap.system_call_filter: false\n# ---------------------------------- Network -----------------------------------\nnetwork.host: 192.168.1.11   #自己ip\n# --------------------------------- Discovery ----------------------------------\ndiscovery.zen.ping.unicast.hosts: [\"elasticsearch\"] #自己主机名\n\n#注意\n（1）cluster.name如果要配置集群需要两个节点上的elasticsearch配置的cluster.name相同，都启动可以自动组成集群，这里如果不改cluster.name则默认是cluster.name=my-application，\n（2）nodename随意取但是集群内的各节点不能相同\n（3）修改后的每行前面不能有空格，修改后的“：”后面必须有一个空格\n```\n\n4)配置Linux系统\n\n切换到root用户，\n\n```shell\n#编辑/etc/security/limits.conf添加类似如下内容\n* soft nofile 65536\n* hard nofile 131072\n* soft nproc 2048\n* hard nproc 4096\n```\n\n```\n #编辑 /etc/security/limits.d/90-nproc.conf\n * soft nproc 1024\n#修改为\n* soft nproc 4096\n```\n\n```shell\n#编辑 vi /etc/sysctl.conf \nvm.max_map_count=655360\nfs.file-max=655360\n```\n\n```shell\nsysctl -p\n重新启动elasticsearch，即可启动成功。\n```\n\n```shell\n#测试集群\ncurl http://elasticsearch:9200\n{\n  \"name\" : \"node-1\",\n  \"cluster_name\" : \"my-application\",\n  \"cluster_uuid\" : \"mdLmu1rOS7qPNfToNOXzKA\",\n  \"version\" : {\n    \"number\" : \"5.2.2\",\n    \"build_hash\" : \"f9d9b74\",\n    \"build_date\" : \"2017-02-24T17:26:45.835Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"6.4.1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n\n```\n\n## 插件安装\n\n### 下载插件\n\n<https://github.com/mobz/elasticsearch-head>                  elasticsearch-head-master.zip\n\n<https://nodejs.org/dist/>            node-v6.9.2-linux-x64.tar.xz\n\n### 安装环境\n\n```shell\ntar -zxvf node-v6.9.2-linux-x64.tar.gz -C     /opt/module/\nvi /etc/profile\nexport NODE_HOME=/opt/module/node-v6.9.2-linux-x64\nexport PATH=$PATH:$NODE_HOME/bin\nsource /etc/profile \n```\n\n### node和npm\n\n```shell\nnode -v\nv6.9.2 \nnpm -v\n3.10.9 \n```\n\n### 插件配置\n\n```shell\nunzip elasticsearch-head-master.zip -d /opt/module/\n```\n\n### 换源\n\n```shell\nnpm config set registry https://registry.npm.taobao.org\nnpm config list / npm config get registery  #检查是否替换成功\n```\n\n### 安装插件\n\n```shell\nnpm install -g cnpm --registry=https://registry.npm.taobao.org\n```\n\n### 安装grunt:\n\n```shell\nnpm install -g grunt-cli\n```\n\n### 编辑Gruntfile.js\n\n```shell\n#文件93行添加hostname:'0.0.0.0'\noptions: {\n        hostname:'0.0.0.0',\n        port: 9100,\n        base: '.',\n        keepalive: true\n      }\n```\n\n### 检查\n\n```shell\n#检查head根目录下是否存在base文件夹\n#没有：将 _site下的base文件夹及其内容复制到head根目录下\nmkdir base\ncp base/* ../base/\n```\n\n### 启动grunt server：\n\n```shell\ngrunt server -d\n#如果提示grunt的模块没有安装：\nLocal Npm module “grunt-contrib-clean” not found. Is it installed? \nLocal Npm module “grunt-contrib-concat” not found. Is it installed? \nLocal Npm module “grunt-contrib-watch” not found. Is it installed? \nLocal Npm module “grunt-contrib-connect” not found. Is it installed? \nLocal Npm module “grunt-contrib-copy” not found. Is it installed? \nLocal Npm module “grunt-contrib-jasmine” not found. Is it installed? \nWarning: Task “connect:server” not found. Use –force to continue. \n\n#执行以下命令： \nnpm install grunt-contrib-clean -registry=https://registry.npm.taobao.org\nnpm install grunt-contrib-concat -registry=https://registry.npm.taobao.org\nnpm install grunt-contrib-watch -registry=https://registry.npm.taobao.org \nnpm install grunt-contrib-connect -registry=https://registry.npm.taobao.org\nnpm install grunt-contrib-copy -registry=https://registry.npm.taobao.org \nnpm install grunt-contrib-jasmine -registry=https://registry.npm.taobao.org\n#最后一个模块可能安装不成功，但是不影响使用。\n```\n\n###  Web访问\n\n![FULpvj.md.png](https://s1.ax1x.com/2018/12/15/FULpvj.md.png)\n\n### 集群无法访问\n\n```shell\n在/opt/module/elasticsearch-5.2.2/config路径下修改配置文件elasticsearch.yml，在文件末尾增加\nhttp.cors.enabled: true\nhttp.cors.allow-origin: \"*\"\n#重启ElasticSearch\n#重启插件\n```\n\n","tags":["Elasticsearch"],"categories":["数据库"]},{"title":"MongoDB进阶","url":"/2018/12/12/MongoDB进阶/","content":"\n {{ \"MongoDB 复制集\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### 核心组件\n\nMongoDB作为一个分布式文件存储数据库,我们要了解他的架构方式和重要的组件,分别是 :mongod(数据库核心程序)\n、mongos(他是用于分片集群的控制器和查询路由器)、mongo(他是交互式的MongoDB shell)\n\n### mongod\n\n此程序会处理所有的数据请求,管理数据格式并且执行于后台的管理操作.无参数运行默认的数据目录为/data/db.默认端口27017会此端口侦听socket的请求链接.\n\nmongod程序启动时确保该目录数据存在,并且当前用户拥有对该目录的写权限.如果该目录不存在或者没有写权限,程序启动失败,默认端口27017被占用,服务启动失败.\n\nMongod还有一个web服务器,侦听端口为28017,使用web昂问可以获取数据库的相关信息,\n\n\n\n### Mongo\n\nMongo为研发人员提供了交互式的JS API这样方便再数据库上直接做测试查询和操作,并且也可用于系统管理员对数据库进行有效的管理,可以通过命令的方式来处理.当mongo shell 启动后,默认链接连接到test数据库实例\n\n### Mongos\n\nMongos被用于MongoDB的分片.相当于一种路由服务,他对于来自应用层的查询请求进行并行处理并判断所请求的数据位于分片集群的具体位置\n\n### 复制\n\nMongoDB复制是将数据同步再多个服务器的过程.\n\n复制提供了数据用于备份.并且再多个服务器上存储副本,提高了数据的可用性,并保证数据的安全性.复制还允许您从硬件故障和服务中断中回复数据.\n\n#### 复制目标\n\n\n\n1. Failover(故障转移,故障切换,故障恢复):系统中其中一项设备或者服务失效无法运作时,另一项设备或服务自动接收原失效系统所指定的工作.\n2. Redundancy(数据用于):再一个数据集合中重复的数据.\n3. 避免单点,用于灾难时回复,报表处理,提升数据可用性。\n4. 读写分离，分单读压力。\n5. 对用户透明的系统维护升级\n\n#### 复制基础\n\n实现复制功能需要建立副本集\n\n副本集是由自动故障恢复功能的主从集群，有一个Primary节点和一个或者多个Seconday节点组成。\n\n![FY6jCF.png](https://s1.ax1x.com/2018/12/12/FY6jCF.png)\n\nmongodb的复制至少需要两个节点，一个是主节点处理用户的客户端请求，其余都是从节点，复制主节点上的数据，常见的模式由：一主一从，一主多从。主节点所有的操作oplog从节点定期轮询主节点获取这些操作，然后对自己的数据副本执行这些操作，从而保证从节点的数据于主节点一致。\n\n副本集的特征：\n\nN 个节点的集群\n\n任何节点可作为主节点\n\n所有写入操作都在主节点上\n\n自动故障转移\n\n自动恢复\n\n#### 实现复制集\n\n启动\n\n```\nmongo --nodb\n```\n\n设置副本集\n\n```json\nreplicaSet = new ReplSetTest({\"node\":3})\n```\n\n启动mongod服务器\n\n```json\n> replicaSet.startSet()\nReplSetTest starting set\n[ ]\n```\n\n配置复制功能\n\n\n\n","tags":["MongoDB"],"categories":["数据库"]},{"title":"MongoDB聚合","url":"/2018/12/11/MongoDB聚合/","content":"\n {{ \"MongoDB 除了基本的查询,还有强大的聚合工具\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## distinct\n\ndistinct用来找出给定键的所有不同的值。使用时必须指定集合和键。\n\n```json\n元数据\n> db.user.find()\n{ \"_id\" : ObjectId(\"5c0df99fbc6d47cbcdb55fd0\"), \"name\" : \"jack\", \"age\" : 19 }\n{ \"_id\" : ObjectId(\"5c0df9abbc6d47cbcdb55fd1\"), \"name\" : \"rose\", \"age\" : 20 }\n{ \"_id\" : ObjectId(\"5c0df9b0bc6d47cbcdb55fd2\"), \"name\" : \"jack\", \"age\" : 18 }\n{ \"_id\" : ObjectId(\"5c0df9c3bc6d47cbcdb55fd3\"), \"name\" : \"tony\", \"age\" : 21 }\n{ \"_id\" : ObjectId(\"5c0df9cdbc6d47cbcdb55fd4\"), \"name\" : \"adam\", \"age\" : 18 }\n{ \"_id\" : ObjectId(\"5c0e0824bc6d47cbcdb55fd5\"), \"age\" : 2, \"name\" : 1 }\n{ \"_id\" : ObjectId(\"5c0e0826bc6d47cbcdb55fd6\"), \"age\" : 2, \"name\" : 2 }\n{ \"_id\" : ObjectId(\"5c0e0828bc6d47cbcdb55fd7\"), \"age\" : 2, \"name\" : 3 }\n{ \"_id\" : ObjectId(\"5c0f3476478f8e67a82bc840\"), \"name\" : \"jack\", \"age\" : 19 }\n```\n\n```json\n> db.runCommand({distinct:\"user\",\"key\":\"age\"})\n{ \"values\" : [ 2, 18, 19, 20, 21 ], \"ok\" : 1 }\n```\n\n## group\n\ngroup做的聚合稍复杂一些。先选定分组所依据的键，而后MongoDB就会将集合依据选定键值的不同分成若干组。然后可以通过聚合每一组内的文档，产生一个结果文档。\n\n```\n元数据\n{\"day\":\"2010/10/03\",\"time\":\"10/3/2010 03:57:01 GMT-400\",\"price\":4.23}\n{\"day\":\"2010/10/04\",\"time\":\"10/4/2010 11:45:01 GMT-400\",\"price\":4.27}\n{\"day\":\"2010/10/05\",\"time\":\"10/5/2010 05:43:01 GMT-400\",\"price\":4.11}\n{\"day\":\"2010/10/06\",\"time\":\"10/6/2010 06:56:01 GMT-400\",\"price\":4.01}\n\n```\n\ngroup查询语句\n\n````json\n> db.runCommand({\n...     \"group\": {\n...         \"ns\": \"stocks\",\n...         \"key\": \"day\",\n...         \"inital\": {\n...             \"time\": 0\n...         },\n...         \"$reduce\": function(doc, prev) {\n...             if (doc.time > prev.time) {\n...                 prev.price = doc.price;\n...                 prev.time = doc.time;\n...             }\n...         }\n...     }\n... })\n{\n        \"ok\" : 0,\n        \"errmsg\" : \"initial has to be an object\",\n        \"code\" : 2,\n        \"codeName\" : \"BadValue\"\n}\n\n````\n\n  \"ns\": \"stocks\"       指定进行分组的集合\n\n   \"key\": \"day\",        指定文档分组一句的键,这里就是\"day\",所有\"day\"值相同的w文档被划分到了一组,\n\n \"inital\": { \"time\": 0   }   每一组reduce函数调用的初始时间,会作为初始文档,传递给后续过程,每一个组员的所有成员都是用这个累加器,所以改变会保留住.\n\n \"$reduce\": function(doc, prev)每个文档都对应一次这个调用。系统会传递两个参数：当前文档和累加器文档(本组当前的结果）。本例中，想让reduce函数比较当前文档的时间和累加器的时间。如果当前文档的时间更近，则将累加器的日期和价格替换成当前文档的值。别忘了，每一组都有一个独立的累加器，所以不必担心不同的日斯使用同一个累加器。\n\n如果只要最近30天的股价可以使添加condition\n\n```json\n> db.runCommand({\n...     \"group\": {\n...         \"ns\": \"stocks\",\n...         \"key\": \"day\",\n...         \"inital\": {\n...             \"time\": 0\n...         },\n...         \"$reduce\": function(doc, prev) {\n...             if (doc.time > prev.time) {\n...                 prev.price = doc.price;\n...                 prev.time = doc.time;\n...             }\n...         },\n... \"condition\":{\"day\":{$gt:\"2010/09/30\"}}\n...\n...     }\n... })\n```\n\n这里每组的\"price\"都是显式设置的，\"time\"先由初始化器设置，然后也是主动更新。\"day\"是默认被加进去的，因为分组依据的键默认被加入到每个\"retval\"内嵌文档中。要是不想返回这个键，可以用完成器把累加器文档变成任意形态，甚\n至变换成非文档（例如数字或字符串）。\n\n## aggregate \n\naggregate 提供的是类似SQL（结构化查询语言）的聚合操作，例如每个操作符都可以找到对应的sql关键字\n\n| MySQL     | MongoDB             |\n| --------- | ------------------- |\n| WHERE     | $match              |\n| GROUP BY  | $group              |\n| HAVING    | $match              |\n| SELECT    | $project            |\n| ORDERY BY | $sort               |\n| LIMIT     | $limit              |\n| SUM       | $sum                |\n| COUNT()   | $sum   $SortByCount |\n| join      | $lookup             |\n\nsql语句 与对应的聚合函数\n![FYGv1U.md.png](https://s1.ax1x.com/2018/12/11/FYGv1U.md.png)\n\n## MapReduce\n\n\n\n![FYJJgS.md.png](https://s1.ax1x.com/2018/12/11/FYJJgS.md.png)\n\nPython 脚本导入数据\n\n```python\nfrom pymongo import MongoClient\nfrom random import randint\nimport datetime\n\nclient = MongoClient()\ndb = client.get_database('taobao')\n\norder = db.order_info\n\nstatus = ['A', 'B', 'C']\n\ncust_id = ['A123', 'B123', 'C123']\n\nprice = [500, 250, 200, 300]\n\nsku = ['mmm', 'nnn']\n\nfor i in range(1, 100):\n    items = []\n    items_count = randint(2, 6)\n    for n in range(items_count):\n        # sku 库存量  qty 数量\n        items.append({\"sku\": sku[randint(0, 1)], \"qty\": randint(1, 10), \"price\": randint(0, 5)})\n    new = {\n        \"status\": status[randint(0, 2)],\n        \"cust_id\": cust_id[randint(0, 2)],\n        \"price\": price[randint(0, 3)],\n        \"ord_date\": datetime.datetime.utcnow(),\n        \"items\": items\n    }\n    print(new)\n    order.insert_one(new)\n    print(i)\nprint(order.estimated_document_count())\n```\n\n数据格式\n\n```json\n{\n        \"_id\" : ObjectId(\"5c0f1bc52a3cde1260163371\"),\n        \"status\" : \"B\",\n        \"cust_id\" : \"C123\",\n        \"price\" : 300,\n        \"ord_date\" : ISODate(\"2018-12-11T02:07:01.598Z\"),\n        \"items\" : [\n                {\n                        \"sku\" : \"nnn\",\n                        \"qty\" : 2,\n                        \"price\" : 5\n                },\n                {\n                        \"sku\" : \"mmm\",\n                        \"qty\" : 1,\n                        \"price\" : 4\n                }\n        ]\n}\n```\n\n查询每个cust_id 的所有price总和MapReduce\n\n```json\n> #定义 map函数\n> var mapFunction1 = function() {\n...                        emit(this.cust_id, this.price);\n...                    };\n\n> #定义reduce函数\n> var reduceFunction1 = function(keyCustId, valuesPrices) {\n...                           return Array.sum(valuesPrices);\n...                       };\n\n> #执行mapreduce，输出到当前db的map_reduce_example集合中\n> db.order_info.mapReduce(\n...                      mapFunction1,\n...                      reduceFunction1,\n...                      { out: \"map_reduce_example\" }\n...                    )\n{\n        \"result\" : \"map_reduce_example\",\n        \"timeMillis\" : 284,\n        \"counts\" : {\n                \"input\" : 99,\n                \"emit\" : 99,\n                \"reduce\" : 3,\n                \"output\" : 3\n        },\n        \"ok\" : 1\n}\n```\n\n查看结果\n\n```json\n> db.map_reduce_example.find()\n{ \"_id\" : \"A123\", \"value\" : 8350 }\n{ \"_id\" : \"B123\", \"value\" : 9150 }\n{ \"_id\" : \"C123\", \"value\" : 12800 }\n>\n```\n\n**聚合管道操作命令**\n\n```json\n> db.order_info.aggregate({ $group: { _id: \"$cust_id\", total: { $sum: \"$price\" }}})\n{ \"_id\" : \"A123\", \"total\" : 8350 }\n{ \"_id\" : \"B123\", \"total\" : 9150 }\n{ \"_id\" : \"C123\", \"total\" : 12800 }\n>\n```\n\n计算所有items 的平均库存 Mapreduce\n\n````json\n# map函数\n> var mapFunction2 = function() {\n...                        for (var idx = 0; idx < this.items.length; idx++) {\n...                            var key = this.items[idx].sku;\n...                            var value = {\n...                                          count: 1,\n...                                          qty: this.items[idx].qty\n...                                        };\n...                            emit(key, value);\n...                        }\n...                     };\n>\n\n    \n#reduce函数\n\n> var reduceFunction2 = function(keySKU, countObjVals) {\n...                      reducedVal = { count: 0, qty: 0 };\n...\n...                      for (var idx = 0; idx < countObjVals.length; idx++) {\n...                          reducedVal.count += countObjVals[idx].count;\n...                          reducedVal.qty += countObjVals[idx].qty;\n...                      }\n...\n...                      return reducedVal;\n...                   };\n\n#finalize函数\n\n> var finalizeFunction2 = function (key, reducedVal) {\n...\n...                        reducedVal.avg = reducedVal.qty/reducedVal.count;\n...\n...                        return reducedVal;\n...\n...                     };\n>\n# 执行mapreduce\n> db.order_info.mapReduce( mapFunction2,\n...                      reduceFunction2,\n...                      {\n...                        out: { merge: \"map_reduce_example\" },\n...                        finalize: finalizeFunction2\n...                      }\n...                    )\n{\n        \"result\" : \"map_reduce_example\",\n        \"timeMillis\" : 121,\n        \"counts\" : {\n                \"input\" : 99,\n                \"emit\" : 406,\n                \"reduce\" : 2,\n                \"output\" : 5\n        },\n        \"ok\" : 1\n}\n````\n\n查看\n\n```js\n> db.map_reduce_example.find()\n{ \"_id\" : \"A123\", \"value\" : 8350 }\n{ \"_id\" : \"B123\", \"value\" : 9150 }\n{ \"_id\" : \"C123\", \"value\" : 12800 }\n{ \"_id\" : \"mmm\", \"value\" : { \"count\" : 211, \"qty\" : 1135, \"avg\" : 5.37914691943128 } }\n{ \"_id\" : \"nnn\", \"value\" : { \"count\" : 195, \"qty\" : 1016, \"avg\" : 5.21025641025641 } }\n```\n\n**聚合管道操作命令实现，计算其所有items 的平均库存,要求输出结果包含count和qty；**\n\n```json\n> db.order_info.aggregate({$unwind:\"$items\"},{$group:{_id:\"$items.sku\",count:{$sum:1},totallyqty:{\"$sum\":\"$items.qty\"},avgsku:{\"$avg\":\"$items.qty\"}}})\n{ \"_id\" : \"nnn\", \"count\" : 195, \"totallyqty\" : 1016, \"avgsku\" : 5.21025641025641 }\n{ \"_id\" : \"mmm\", \"count\" : 211, \"totallyqty\" : 1135, \"avgsku\" : 5.37914691943128 }\n```\n\n**用聚合管道操作命令实现：根据cust_id，仓库编号进行分组，计算其所有items 的平均库存；**\n\n```json\n> db.order_info.aggregate({$unwind:\"$items\"},{$group:{_id:{cust_id:'$cust_id',skunn:'$items.sku'},avgsku:{\"$avg\":\"$items.qty\"}}})\n{ \"_id\" : { \"cust_id\" : \"B123\", \"skunn\" : \"mmm\" }, \"avgsku\" : 5.283783783783784 }\n{ \"_id\" : { \"cust_id\" : \"B123\", \"skunn\" : \"nnn\" }, \"avgsku\" : 5.121212121212121 }\n{ \"_id\" : { \"cust_id\" : \"C123\", \"skunn\" : \"nnn\" }, \"avgsku\" : 5.216216216216216 }\n{ \"_id\" : { \"cust_id\" : \"A123\", \"skunn\" : \"nnn\" }, \"avgsku\" : 5.3090909090909095 }\n{ \"_id\" : { \"cust_id\" : \"A123\", \"skunn\" : \"mmm\" }, \"avgsku\" : 5.508196721311475 }\n{ \"_id\" : { \"cust_id\" : \"C123\", \"skunn\" : \"mmm\" }, \"avgsku\" : 5.368421052631579 }\n```\n\n","tags":["MongoDB"],"categories":["数据库"]},{"title":"MongoDB索引","url":"/2018/12/10/MongoDB索引/","content":"\n {{ \"MongoDB索引的介绍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n##  简介\n\n索引就是用来加速查询的。数据库索引与书籍的索引类似：有了索引就不需要翻遍整本书，数据库则可以直接在索引中査找，使得查找速度能提高几个数量级。在索引中找到条目以后，就可以直接跳转到目标文档的位置。让这个比喻走个极端，可以说创建数据库索引就像确定如何组织书的索引一样。但你的优势是知道今后会做何种査询，以及哪些内容需要快速査找。索引能够提升查询效率.没有索引,MongoDB必须扫描集合中的所有文档才能找到匹配查询语句文档,索引是一种特殊的数据结构,将一小块数据集保存为容易遍历的形式.索引能够存储某种特殊字段或字段的值,按照索引指定的方式可以将字段值进行排序,对于添加的每一个索引,每一次插入更新,删除都将消耗更多的时间,因为数据发生改变的时候不仅仅是更新文档,还有更新集合上的所有索引.MongoDB限制每个集合上的最多有**64**个索引,使用索引键对文档进行排序时可以提升排序效率,索引时按照一定顺序排列的,,索引才有效果,索引类型有很多包括 单字段索引、复合索引、多支索引、地理空间索引、文本索引等等.\n\n## 创建索引\n\n```js\n> db.collection.ensureIndex(<keys>,<options>)  #3.0版本\n```\n\nkeys 希望创建索引的名称和排序方式1表示升序,-1表示降序排列,格式{key:1, key-1}如果时基于多个索引键进行排序,要特别注意索引的排序方向,相同的排序方向可以提升排序效率,单个键排序时没有影响。\n\noptions：可选参数，表示建立索引的设置：\n\n| 参数                    | 数据类型 | 默认值 | 功能                                                         |\n| ----------------------- | -------- | ------ | ------------------------------------------------------------ |\n| background              | Boolean  | false  | 后台建立索引，以便创建索引时不阻止其他数据库活动             |\n| unique                  | Boolean  | false  | 创建唯一索引                                                 |\n| name                    | String   |        | 指定索引名称，如果未指定，MongoDB会生成一个索引字段的名称和排序顺序串联 |\n| partialFilterExpression | document |        | 如果指定MongoDB指挥满足过滤表达式的记录                      |\n| sparse                  | Boolean  | false  | 对文档中不的存在的字段数据不启用索引                         |\n| expireAfterSeconds      | integer  |        | 指定索引的过期时间                                           |\n| storageEngine           |          |        | document类型允许用户配置索引的额存储引擎                     |\n\n### 删除索引\n\n##### 方式一\n\n删除当前集合中的所有索引（_id上默认索引除外）\n\n```js\ndb.collection.dropIndexes()\n```\n\n##### 方式二\n\n```\ndb.collection.dropIndex(index)\n```\n\n在索引上找到条目之后，就可以直接跳到目标文档的位置\n\n在user中有5个文档，其中存储信息如下\n\n```json\n{ \"_id\" : ObjectId(\"5c0df99fbc6d47cbcdb55fd0\"), \"name\" : \"jack\", \"age\" : 19 }\n{ \"_id\" : ObjectId(\"5c0df9abbc6d47cbcdb55fd1\"), \"name\" : \"rose\", \"age\" : 20 }\n{ \"_id\" : ObjectId(\"5c0df9b0bc6d47cbcdb55fd2\"), \"name\" : \"jack\", \"age\" : 18 }\n{ \"_id\" : ObjectId(\"5c0df9c3bc6d47cbcdb55fd3\"), \"name\" : \"tony\", \"age\" : 21 }\n{ \"_id\" : ObjectId(\"5c0df9cdbc6d47cbcdb55fd4\"), \"name\" : \"adam\", \"age\" : 18 }\n```\n\n\n\n如果有个查询\n\n```json\n> db.user.find({age:18})\n```\n\n这个时候需要遍历所有的文档，查询所有的文档，根据位置信息读出文档，对比age字段是否为18。当然入股偶只有5个文档，全表扫描的开销并不大，简单如果集合文档数量到百万，千万或者上亿的时候，对集合进行去哪表扫描的开销时非常大的，一个查询耗费数十秒甚至几分钟都可能。\n\n如果想加速\n\n```json\n> db.user.find({age:18})\n```\n\n可以对user表的age字段孙银\n\n按照age字段创建升序索引\n\n```json\n> db.user.createIndex({age:1})\n{\n        \"createdCollectionAutomatically\" : false,\n        \"numIndexesBefore\" : 1,\n        \"numIndexesAfter\" : 2,\n        \"ok\" : 1\n}\n```\n\n建立索引之后，MongoDB会额外存储一份按照ae字段升序排序的索引数据，索引通常采用类似于*btree*的结构持久化存储\n\n以保证从索引里快速找出某个age值对应的位置信息，然后根据位置信息就能读取出对应的文档。\n\n简单的说，索引就是将文档按照某个（或者某些）字段顺序组织起来，以便能根据该字段高效的查询。\n\n### 唯一索引\n\nMongoDB默认会为插入的文档生成_id字段（如果应用本身没有指定该字端），_id时文档的唯一表示，为了保证能够根据文档id快速查询文档，MongoDB默认会为集合创建_id的字段索引，唯一索引可以确保集合的每一个文档的指定键都有唯一值。如果想保留文档的“name”键都有不一样的值，创建一个唯一索引就好了：\n\n```json\ndb.user.ensureIndex({name:1},{unique:true})\n```\n\n由于insert并不健查文档时否查如果，为了避免插入的文档包含与唯一键重复i的值，可能要用安全插入才能满足要求。这样，在插入文档时会看到存在重复键的错误的提示。\n\n##### 消除重复\n\n当为已有的索引创建索引，可能有些之已经有重复了。这时候创建所以会失败，我们希望将所有包含重复值的文档都删掉。dropDups选项就可以奥六发现的第一个文档，二删除接下来有重复的键值文档，如“name”键重复的索引值\n\n```json\n db.user.ensureIndex({name:1},{unique:true,dropDups:true})\n```\n\n通常做开发不建议删除操作。\n\n\n\n### 复合唯一索引\n\n符合索引就是建立在多个字段上的索引 例如:\n\n```json\n> db.user.ensureIndex({age:1,name:1})\n```\n\n上述符合索引由“age” “name” 组成的索引\n\n复合唯一索引就是允许单个的键的值相同，但是所有键的组合之组合起来的不同索引\n\n````\n{ \"age\" : 2, \"name\" : 1 }\n{ \"age\" : 2, \"name\" : 2 }\n{ \"age\" : 2, \"name\" : 3 }\n````\n\n所有的“age”键都相同，但是“name“键的值不同，如果尝试再次插入\n\n```\n{ \"age\" : 2, \"name\" : 1 }\n```\n\n数据库会体时存在重复键的错误。\n\n### 索引管理\n\n索引的元信息存储在每个数据库的system.indexes集合中。这是一个保留集合不能对其插入或者删除文档。操作只能通过ensureIndex或者droplndexes进行。\n\n#### 查询索引明细\n\n```json\n> db.user.getIndexes()\n[\n        {\n                \"v\" : 2,\n                \"key\" : {\n                        \"_id\" : 1\n                },\n                \"name\" : \"_id_\",\n                \"ns\" : \"han.user\"\n        },\n        {\n                \"v\" : 2,\n                \"key\" : {\n                        \"age\" : 1\n                },\n                \"name\" : \"age_1\",\n                \"ns\" : \"han.user\"\n        },\n        {\n                \"v\" : 2,\n                \"key\" : {\n                        \"age\" : 1,\n                        \"name\" : 1\n                },\n                \"name\" : \"age_1_name_1\",\n                \"ns\" : \"han.user\"\n        },\n        {\n                \"v\" : 2,\n                \"key\" : {\n                        \"age\" : 2,\n                        \"name\" : 1\n                },\n                \"name\" : \"age_2_name_1\",\n                \"ns\" : \"han.user\"\n        }\n]\n```\n\n#### 查询索引的大小\n\n```\n> db.user.totalIndexSize()\n118784(单位：字节)\n>\n```\n\n#### 修改索引\n\nMongoDB没有单独的修改索引方法，如果要修改某个索引，需要先删除旧的索引，再创建新的索引，\n\n随着数据量的增长你会发现某个collection需要修改索引或者增加索引，此时创建索引就会很费力，同时也比较消耗性能。创建索引时MongoDB默认是阻塞式，会让索引建立更快，任何请求都不能相应。可以使用{ background:True } 选项再后台完成，同也可能正常处理请求。不过这种方式也会造成请求的响应很慢，如果再非紧急的情况下，最好晚上业务量少的时候统一处理。\n\n### 地理空间索引\n\n还有一种査询变得越来越流行（尤其是随着移动设备的出现）：找到离当前位置最近的N个场所。MongoDB为坐标平面査询提供了专门的索引，称作地理空间索引。假设要找到给定经纬度坐标周围最近的咖啡馆，就需要创建一个专门的索引来提髙这种査询的效率，这是因为这种査询需要搜索两个维度。地理空间索引同样可以由ensurelndex来创建，只不过参数不是1或者-1，而是2d\":\n\n```json\ndb.map.ensurelndex ({ \"gps\":\"2d\"})\n```\n\n“gps”键的值必须是某种形式的一对值：一个包含两个元素的数组或是包含两个键的内嵌文档。下面这些都是有效的：\n\n```json\n{ \"gps\" : [0, 100 ] }\n{ \"gps\" : { \"X\" : -30, \"y\" : 30 } }\n{ \"gps，1 : { \"latitude” : -180, \"longitude” : 180 } }\n```\n\n至于键名可以随意，例如\n\n```json\n\"gps\" : {\"foo\" : 0, \"bar\" : 1}}\n```\n\n也是可以的。\n\n默认情况下，地理空间索引假设值的范围是-180 180 (对经纬度来说很方便）。要是想用其他值，可以通过ensurelndex的选项来指定最大最小值：\n\n```json\ndb.star.trek. ensurelndex ({\" light-years” : ,,2d,'}, {\"min\" : -1000,\"max\" : 1000})\n```\n\n这样就创建了一个2000光年见方的空间索引。\n地理空间査询以两种方式进行，即普通査询（用find)或者使用数据库命令。find的方式与一般的查询差别不大，只不过用了\"$near”。需要两个目标值的数组作为参数：\n\n```json\ndb.map.find{ {\"gps1':{”$near\" : [40,-73]}})\n```\n\n这会按照离点（40, -73)由近及远的方式将map集合的所有文档都返回。在没有指定limit的值时，默认是**100**个文档。要是不需要这么多结果，就应该设置一个少点的值以节约资源。例如，下面的例子将返回离（40, -73)最近的10个文档。\n\n```json\ndb.map.find({ngps\" : {\"$near\" :[40, -73]}}).limit(10)\n```\n\n也可以用geoNear来完成相同的操作：\n\n```json\ndb.runCommand({geoNear : \"map\", near : [40, -73], num : 10});\n```\n\ngeoNear还会返回每个文档到査询点的距离。这个距离是以你插入的数据为单位的，如果按照经纬度的角度插入，则距离就是经纬度。find与”$near”的组合不会给出距离，但若是结果大于4MB，这是唯一的选择\n\nMongoDB不但能找到靠近一个点的文档，还能找到指定形状内的文档。做法就是将原来的n\\$nearn换成\"\\$within\"。n$within\"获取数量不断增加的形状作为参数。若查看地理空间索引的联机帮助文档(http://www.mongodb.org/display/DOCS/Geospatial+ Indexing),可以找到最新的形状列表。有两个选项：你可以査询矩形和圆形内的所有点。\n\n对于矩形，使用\"$box\"选项:\n\n```json\ndb.map.find({HgpsM : {\"$withinn : {\"$box\" : [[10, 20], [15, 30]]}}})\n```\n\n”$box\"参数是两个元素的数组，第一个元素指定了左下角的坐标，第二个指定右上角的坐标。\n\n同样，也可以用“”$center\"来找到圆形内部的所有点，只不过参数变成了圆心和半径：\n\n```json\ndb.map.find ({\"gps\" : {\"$within” : {\"$center\" : [[12,25], 5] }}})\n```\n\n#### 复合地理空间索引\n\n应用经常要找的东西往往不只是一个地点。例如，用户要找出周围所有的咖啡店或者披萨店。将地理空间索引与普通索引组合起来就可以满足这种需求。例如，要査询\"location”和\"descn，就可以这样创建索引：\n\n```json\ndb.ensurelndex ({ \" location11 : \"2d\", \"desc\" : 1})\n```\n\n然后就能很快找到最近的咖啡馆了：\n\n```json\n> db.map.find {{location: {\"$near\" : [-70, 30]}, \"desc\":\n\"coffeeshop”}).limit(1)\n{\n\t\"_id” : Objectld(”4c0dl348928a815a720a0000\"),\n\t\"name\" : \"Mud\",\n\t\"location\" : [x, y],\n\t\"desc\" : [\"coffee\",\"coffeeshop\", \"muffins\", \"espresso\"]\n}\n```\n\n\n\n#### 地球不是二维平面\n\nMongoDB的地理空间索引假设索引内容是在一个平面上的。这就意味着对于球体，比如地球，它并不是十分精确，尤其是在极地区域。具体来说，两条经线之间纬线的长度在赤道和在育空地区1是大不一样的，后者要短得多。可以用不同的投影手段将地球映射到二维平面，当然它们的精度和相应的复杂度各不相同。","tags":["MongoDB"],"categories":["数据库"]},{"title":"MongoDB查询","url":"/2018/12/09/MongoDB查询/","content":"\n {{ \"MongoDB的查询语法\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## find查询\n\nMongoDB中使用find来进行查询。査询就是返回一个集合中文档的子集，子集合的范围从0个文档到整个集合。find的第一个参数决定了要返回哪些文档，其形式也是一个文档，说明要执行的査询细节。\n\n### 查询所有数据\n\n空的査询文档{}会匹配集合的全部内容。要是不指定査询文档，默认就是{}。\t\n\n```json\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cd66df792e1bdd43e96f6\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cd67ef792e1bdd43e96f7\"),\n        \"username\" : \"hello\",\n        \"nickname\" : \"Hello\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cd68bf792e1bdd43e96f8\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n\n```\n\n当我们查询文档中加入键值对的时候,就意味着限定了查询的条件,对于绝大部分的数据类型来说,这种方式简单明了,整数匹配整数,布尔匹配布尔类型,字符串匹配字符串,查询简单类型,只要制定好要查找的值就行,如果你想查找所\"username\"为“foo”的文档,直接可以把键值对写入查询文档.\n\n```json\n> db.user.find({username:\"foo\"}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cd66df792e1bdd43e96f6\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cd68bf792e1bdd43e96f8\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n```\n\n可以向查询文档加入多个键值对来将多个查询条件组合到一起,如查询username为hello  password为\"123\" 的用户\n\n```json\n> db.user.find({ \"username\" : \"hello\",password:\"123\"}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cd67ef792e1bdd43e96f7\"),\n        \"username\" : \"hello\",\n        \"nickname\" : \"Hello\",\n        \"password\" : \"123\"\n}\n```\n\n### 指定返回值\n\n通常情况下我们查询数据的时候,并不需要将文档中所有的键值都返回.这种情况下我们可以通过find的第二个参数来指定想要返回的键.这样做机会节省传输数据量,又能节省客户端解码文档的时间和内存消耗。如果我们只想要username的相关数据,不要_id、nickname、password 可以这么写\n\ndb.文档.find({条件},{要查询的键})\n\n```json\n> db.user.find({},{_id:0,username:1})\n第一个参数是一个空对象,表示从全部的数据中查询,我们可以看到这里没有写nickname=o和password=0但是也没有显示出来,如果_id:0不屑会显示出来_id的信息的\n{ \"username\" : \"foo\" }\n{ \"username\" : \"hello\" }\n{ \"username\" : \"foo\" }\n```\n\n### 查询条件\n\n| 运算符 | 对应到mysql的运算符          |\n| ------ | ---------------------------- |\n| $gt    | >                            |\n| $gte   | >=                           |\n| $in    | in                           |\n| $lt    | <                            |\n| $lte   | <=                           |\n| $ne    | !=                           |\n| $nin   | not   in                     |\n| $all   | 无对应项,指数组所有单元匹配. |\n\n### 元数据\n\n```json\n[{\"goods_id\":1,\"cat_id\":4,\"goods_name\":\"KD876\",\"goods_number\":1,\"click_count\":7,\"shop_price\":1388.00,\"add_time\":1240902890},{\"goods_id\":4,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\\u539f\\u88c5\\u5145\\u7535\\u5668\",\"goods_number\":17,\"click_count\":0,\"shop_price\":58.00,\"add_time\":1241422402},{\"goods_id\":3,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a\\u539f\\u88c55800\\u8033\\u673a\",\"goods_number\":24,\"click_count\":3,\"shop_price\":68.00,\"add_time\":1241422082},{\"goods_id\":5,\"cat_id\":11,\"goods_name\":\"\\u7d22\\u7231\\u539f\\u88c5M2\\u5361\\u8bfb\\u5361\\u5668\",\"goods_number\":8,\"click_count\":3,\"shop_price\":20.00,\"add_time\":1241422518},{\"goods_id\":6,\"cat_id\":11,\"goods_name\":\"\\u80dc\\u521bKINGMAX\\u5185\\u5b58\\u5361\",\"goods_number\":15,\"click_count\":0,\"shop_price\":42.00,\"add_time\":1241422573},{\"goods_id\":7,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\\u539f\\u88c5\\u7acb\\u4f53\\u58f0\\u8033\\u673aHS-82\",\"goods_number\":20,\"click_count\":0,\"shop_price\":100.00,\"add_time\":1241422785},{\"goods_id\":8,\"cat_id\":3,\"goods_name\":\"\\u98de\\u5229\\u6d669@9v\",\"goods_number\":1,\"click_count\":9,\"shop_price\":399.00,\"add_time\":1241425512},{\"goods_id\":9,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aE66\",\"goods_number\":4,\"click_count\":20,\"shop_price\":2298.00,\"add_time\":1241511871},{\"goods_id\":10,\"cat_id\":3,\"goods_name\":\"\\u7d22\\u7231C702c\",\"goods_number\":7,\"click_count\":11,\"shop_price\":1328.00,\"add_time\":1241965622},{\"goods_id\":11,\"cat_id\":3,\"goods_name\":\"\\u7d22\\u7231C702c\",\"goods_number\":1,\"click_count\":0,\"shop_price\":1300.00,\"add_time\":1241966951},{\"goods_id\":12,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9A810\",\"goods_number\":8,\"click_count\":13,\"shop_price\":983.00,\"add_time\":1245297652}]\n\n[{\"goods_id\":13,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a5320 XpressMusic\",\"goods_number\":8,\"click_count\":13,\"shop_price\":1311.00,\"add_time\":1241967762},{\"goods_id\":14,\"cat_id\":4,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a5800XM\",\"goods_number\":1,\"click_count\":6,\"shop_price\":2625.00,\"add_time\":1241968492},{\"goods_id\":15,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9A810\",\"goods_number\":3,\"click_count\":8,\"shop_price\":788.00,\"add_time\":1241968703},{\"goods_id\":16,\"cat_id\":2,\"goods_name\":\"\\u6052\\u57fa\\u4f1f\\u4e1aG101\",\"goods_number\":0,\"click_count\":3,\"shop_price\":823.33,\"add_time\":1241968949},{\"goods_id\":17,\"cat_id\":3,\"goods_name\":\"\\u590f\\u65b0N7\",\"goods_number\":1,\"click_count\":2,\"shop_price\":2300.00,\"add_time\":1241969394},{\"goods_id\":18,\"cat_id\":4,\"goods_name\":\"\\u590f\\u65b0T5\",\"goods_number\":1,\"click_count\":0,\"shop_price\":2878.00,\"add_time\":1241969533},{\"goods_id\":19,\"cat_id\":3,\"goods_name\":\"\\u4e09\\u661fSGH-F258\",\"goods_number\":12,\"click_count\":7,\"shop_price\":858.00,\"add_time\":1241970139},{\"goods_id\":20,\"cat_id\":3,\"goods_name\":\"\\u4e09\\u661fBC01\",\"goods_number\":12,\"click_count\":14,\"shop_price\":280.00,\"add_time\":1241970417},{\"goods_id\":21,\"cat_id\":3,\"goods_name\":\"\\u91d1\\u7acb A30\",\"goods_number\":40,\"click_count\":4,\"shop_price\":2000.00,\"add_time\":1241970634},{\"goods_id\":22,\"cat_id\":3,\"goods_name\":\"\\u591a\\u666e\\u8fbeTouch HD\",\"goods_number\":1,\"click_count\":15,\"shop_price\":5999.00,\"add_time\":1241971076}]\n\n[{\"goods_id\":23,\"cat_id\":5,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN96\",\"goods_number\":8,\"click_count\":17,\"shop_price\":3700.00,\"add_time\":1241971488},{\"goods_id\":24,\"cat_id\":3,\"goods_name\":\"P806\",\"goods_number\":100,\"click_count\":35,\"shop_price\":2000.00,\"add_time\":1241971981},{\"goods_id\":25,\"cat_id\":13,\"goods_name\":\"\\u5c0f\\u7075\\u901a\\/\\u56fa\\u8bdd50\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":48.00,\"add_time\":1241972709},{\"goods_id\":26,\"cat_id\":13,\"goods_name\":\"\\u5c0f\\u7075\\u901a\\/\\u56fa\\u8bdd20\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":19.00,\"add_time\":1241972789},{\"goods_id\":27,\"cat_id\":15,\"goods_name\":\"\\u8054\\u901a100\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":95.00,\"add_time\":1241972894},{\"goods_id\":28,\"cat_id\":15,\"goods_name\":\"\\u8054\\u901a50\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":0,\"click_count\":0,\"shop_price\":45.00,\"add_time\":1241972976},{\"goods_id\":29,\"cat_id\":14,\"goods_name\":\"\\u79fb\\u52a8100\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":0,\"click_count\":0,\"shop_price\":90.00,\"add_time\":1241973022},{\"goods_id\":30,\"cat_id\":14,\"goods_name\":\"\\u79fb\\u52a820\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":9,\"click_count\":1,\"shop_price\":18.00,\"add_time\":1241973114},{\"goods_id\":31,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9E8 \",\"goods_number\":1,\"click_count\":5,\"shop_price\":1337.00,\"add_time\":1242110412},{\"goods_id\":32,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\",\"goods_number\":4,\"click_count\":9,\"shop_price\":3010.00,\"add_time\":1242110760}]\n\n```\n\n```json\n> db.goods.insert([{\"goods_id\":1,\"cat_id\":4,\"goods_name\":\"KD876\",\"goods_number\":1,\"click_count\":7,\"shop_price\":1388.00,\"add_time\":1240902890},{\"goods_id\":4,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\\u539f\\u88c5\\u5145\\u7535\\u5668\",\"goods_number\":17,\"click_count\":0,\"shop_price\":58.00,\"add_time\":1241422402},{\"goods_id\":3,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a\\u539f\\u88c55800\\u8033\\u673a\",\"goods_number\":24,\"click_count\":3,\"shop_price\":68.00,\"add_time\":1241422082},{\"goods_id\":5,\"cat_id\":11,\"goods_name\":\"\\u7d22\\u7231\\u539f\\u88c5M2\\u5361\\u8bfb\\u5361\\u5668\",\"goods_number\":8,\"click_count\":3,\"shop_price\":20.00,\"add_time\":1241422518},{\"goods_id\":6,\"cat_id\":11,\"goods_name\":\"\\u80dc\\u521bKINGMAX\\u5185\\u5b58\\u5361\",\"goods_number\":15,\"click_count\":0,\"shop_price\":42.00,\"add_time\":1241422573},{\"goods_id\":7,\"cat_id\":8,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\\u539f\\u88c5\\u7acb\\u4f53\\u58f0\\u8033\\u673aHS-82\",\"goods_number\":20,\"click_count\":0,\"shop_price\":100.00,\"add_time\":1241422785},{\"goods_id\":8,\"cat_id\":3,\"goods_name\":\"\\u98de\\u5229\\u6d669@9v\",\"goods_number\":1,\"click_count\":9,\"shop_price\":399.00,\"add_time\":1241425512},{\"goods_id\":9,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aE66\",\"goods_number\":4,\"click_count\":20,\"shop_price\":2298.00,\"add_time\":1241511871},{\"goods_id\":10,\"cat_id\":3,\"goods_name\":\"\\u7d22\\u7231C702c\",\"goods_number\":7,\"click_count\":11,\"shop_price\":1328.00,\"add_time\":1241965622},{\"goods_id\":11,\"cat_id\":3,\"goods_name\":\"\\u7d22\\u7231C702c\",\"goods_number\":1,\"click_count\":0,\"shop_price\":1300.00,\"add_time\":1241966951},{\"goods_id\":12,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9A810\",\"goods_number\":8,\"click_count\":13,\"shop_price\":983.00,\"add_time\":1245297652}])\n> db.goods.insert([{\"goods_id\":13,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a5320 XpressMusic\",\"goods_number\":8,\"click_count\":13,\"shop_price\":1311.00,\"add_time\":1241967762},{\"goods_id\":14,\"cat_id\":4,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9a5800XM\",\"goods_number\":1,\"click_count\":6,\"shop_price\":2625.00,\"add_time\":1241968492},{\"goods_id\":15,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9A810\",\"goods_number\":3,\"click_count\":8,\"shop_price\":788.00,\"add_time\":1241968703},{\"goods_id\":16,\"cat_id\":2,\"goods_name\":\"\\u6052\\u57fa\\u4f1f\\u4e1aG101\",\"goods_number\":0,\"click_count\":3,\"shop_price\":823.33,\"add_time\":1241968949},{\"goods_id\":17,\"cat_id\":3,\"goods_name\":\"\\u590f\\u65b0N7\",\"goods_number\":1,\"click_count\":2,\"shop_price\":2300.00,\"add_time\":1241969394},{\"goods_id\":18,\"cat_id\":4,\"goods_name\":\"\\u590f\\u65b0T5\",\"goods_number\":1,\"click_count\":0,\"shop_price\":2878.00,\"add_time\":1241969533},{\"goods_id\":19,\"cat_id\":3,\"goods_name\":\"\\u4e09\\u661fSGH-F258\",\"goods_number\":12,\"click_count\":7,\"shop_price\":858.00,\"add_time\":1241970139},{\"goods_id\":20,\"cat_id\":3,\"goods_name\":\"\\u4e09\\u661fBC01\",\"goods_number\":12,\"click_count\":14,\"shop_price\":280.00,\"add_time\":1241970417},{\"goods_id\":21,\"cat_id\":3,\"goods_name\":\"\\u91d1\\u7acb A30\",\"goods_number\":40,\"click_count\":4,\"shop_price\":2000.00,\"add_time\":1241970634},{\"goods_id\":22,\"cat_id\":3,\"goods_name\":\"\\u591a\\u666e\\u8fbeTouch HD\",\"goods_number\":1,\"click_count\":15,\"shop_price\":5999.00,\"add_time\":1241971076}])\n> db.goods.insert([{\"goods_id\":23,\"cat_id\":5,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN96\",\"goods_number\":8,\"click_count\":17,\"shop_price\":3700.00,\"add_time\":1241971488},{\"goods_id\":24,\"cat_id\":3,\"goods_name\":\"P806\",\"goods_number\":100,\"click_count\":35,\"shop_price\":2000.00,\"add_time\":1241971981},{\"goods_id\":25,\"cat_id\":13,\"goods_name\":\"\\u5c0f\\u7075\\u901a\\/\\u56fa\\u8bdd50\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":48.00,\"add_time\":1241972709},{\"goods_id\":26,\"cat_id\":13,\"goods_name\":\"\\u5c0f\\u7075\\u901a\\/\\u56fa\\u8bdd20\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":19.00,\"add_time\":1241972789},{\"goods_id\":27,\"cat_id\":15,\"goods_name\":\"\\u8054\\u901a100\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":2,\"click_count\":0,\"shop_price\":95.00,\"add_time\":1241972894},{\"goods_id\":28,\"cat_id\":15,\"goods_name\":\"\\u8054\\u901a50\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":0,\"click_count\":0,\"shop_price\":45.00,\"add_time\":1241972976},{\"goods_id\":29,\"cat_id\":14,\"goods_name\":\"\\u79fb\\u52a8100\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":0,\"click_count\":0,\"shop_price\":90.00,\"add_time\":1241973022},{\"goods_id\":30,\"cat_id\":14,\"goods_name\":\"\\u79fb\\u52a820\\u5143\\u5145\\u503c\\u5361\",\"goods_number\":9,\"click_count\":1,\"shop_price\":18.00,\"add_time\":1241973114},{\"goods_id\":31,\"cat_id\":3,\"goods_name\":\"\\u6469\\u6258\\u7f57\\u62c9E8 \",\"goods_number\":1,\"click_count\":5,\"shop_price\":1337.00,\"add_time\":1242110412},{\"goods_id\":32,\"cat_id\":3,\"goods_name\":\"\\u8bfa\\u57fa\\u4e9aN85\",\"goods_number\":4,\"click_count\":9,\"shop_price\":3010.00,\"add_time\":1242110760}])\nBulkWriteResult({\n        \"writeErrors\" : [ ],\n        \"writeConcernErrors\" : [ ],\n        \"nInserted\" : 10,\n        \"nUpserted\" : 0,\n        \"nMatched\" : 0,\n        \"nModified\" : 0,\n        \"nRemoved\" : 0,\n        \"upserted\" : [ ]\n})\n\n\n> db.goods.find().count()\n31\n```\n\n#### 查询区间\n\n#####  $in\n\n```json\n查询商品价格3000-5000的\n> db.goods.find({shop_price:{$gte:3000,$lte:5500}}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0ce0c1bde03aeb84cf5a9c\"),\n        \"goods_id\" : 23,\n        \"cat_id\" : 5,\n        \"goods_name\" : \"诺基亚N96\",\n        \"goods_number\" : 8,\n        \"click_count\" : 17,\n        \"shop_price\" : 3700,\n        \"add_time\" : 1241971488\n}\n{\n        \"_id\" : ObjectId(\"5c0ce0c1bde03aeb84cf5aa5\"),\n        \"goods_id\" : 32,\n        \"cat_id\" : 3,\n        \"goods_name\" : \"诺基亚N85\",\n        \"goods_number\" : 4,\n        \"click_count\" : 9,\n        \"shop_price\" : 3010,\n        \"add_time\" : 1242110760\n}\n>\n```\n\n\\$in可以用来查询一个键的多个值,对于但意见要是由多个值与其匹配的话就要用\\$in \n\n```json\n# 查询商品是编号是4和3的\n> db.goods.find({goods_number:{$in:[4,3]}},{_id:0,goods_number:1,goods_name:1,shop_price:1}).pretty()\n{ \"goods_name\" : \"诺基亚E66\", \"goods_number\" : 4, \"shop_price\" : 2298 }\n{ \"goods_name\" : \"摩托罗拉A810\", \"goods_number\" : 3, \"shop_price\" : 788 }\n{ \"goods_name\" : \"诺基亚N85\", \"goods_number\" : 4, \"shop_price\" : 3010 }\n\n### 查询商品编号不是4和3的\n\n> db.goods.find({goods_number:{$nin:[4,3]}},{_id:0,goods_number:1,goods_name:1,shop_price:1})\n{ \"goods_name\" : \"KD876\", \"goods_number\" : 1, \"shop_price\" : 1388 }\n{ \"goods_name\" : \"诺基亚N85原装充电器\", \"goods_number\" : 17, \"shop_price\" : 58 }\n{ \"goods_name\" : \"诺基亚原装5800耳机\", \"goods_number\" : 24, \"shop_price\" : 68 }\n{ \"goods_name\" : \"索爱原装M2卡读卡器\", \"goods_number\" : 8, \"shop_price\" : 20 }\n{ \"goods_name\" : \"胜创KINGMAX内存卡\", \"goods_number\" : 15, \"shop_price\" : 42 }\n{ \"goods_name\" : \"诺基亚N85原装立体声耳机HS-82\", \"goods_number\" : 20, \"shop_price\" : 100 }\n{ \"goods_name\" : \"飞利浦9@9v\", \"goods_number\" : 1, \"shop_price\" : 399 }\n{ \"goods_name\" : \"索爱C702c\", \"goods_number\" : 7, \"shop_price\" : 1328 }\n{ \"goods_name\" : \"索爱C702c\", \"goods_number\" : 1, \"shop_price\" : 1300 }\n{ \"goods_name\" : \"摩托罗拉A810\", \"goods_number\" : 8, \"shop_price\" : 983 }\n{ \"goods_name\" : \"诺基亚5320 XpressMusic\", \"goods_number\" : 8, \"shop_price\" : 1311 }\n{ \"goods_name\" : \"诺基亚5800XM\", \"goods_number\" : 1, \"shop_price\" : 2625 }\n{ \"goods_name\" : \"恒基伟业G101\", \"goods_number\" : 0, \"shop_price\" : 823.33 }\n{ \"goods_name\" : \"夏新N7\", \"goods_number\" : 1, \"shop_price\" : 2300 }\n{ \"goods_name\" : \"夏新T5\", \"goods_number\" : 1, \"shop_price\" : 2878 }\n{ \"goods_name\" : \"三星SGH-F258\", \"goods_number\" : 12, \"shop_price\" : 858 }\n{ \"goods_name\" : \"三星BC01\", \"goods_number\" : 12, \"shop_price\" : 280 }\n{ \"goods_name\" : \"金立 A30\", \"goods_number\" : 40, \"shop_price\" : 2000 }\n{ \"goods_name\" : \"多普达Touch HD\", \"goods_number\" : 1, \"shop_price\" : 5999 }\n{ \"goods_name\" : \"诺基亚N96\", \"goods_number\" : 8, \"shop_price\" : 3700 }\nType \"it\" for more\n> it\n{ \"goods_name\" : \"P806\", \"goods_number\" : 100, \"shop_price\" : 2000 }\n{ \"goods_name\" : \"小灵通/固话50元充值卡\", \"goods_number\" : 2, \"shop_price\" : 48 }\n{ \"goods_name\" : \"小灵通/固话20元充值卡\", \"goods_number\" : 2, \"shop_price\" : 19 }\n{ \"goods_name\" : \"联通100元充值卡\", \"goods_number\" : 2, \"shop_price\" : 95 }\n{ \"goods_name\" : \"联通50元充值卡\", \"goods_number\" : 0, \"shop_price\" : 45 }\n{ \"goods_name\" : \"移动100元充值卡\", \"goods_number\" : 0, \"shop_price\" : 90 }\n{ \"goods_name\" : \"移动20元充值卡\", \"goods_number\" : 9, \"shop_price\" : 18 }\n{ \"goods_name\" : \"摩托罗拉E8 \", \"goods_number\" : 1, \"shop_price\" : 1337 }\n\n```\n\n#####  $OR\n\nMongoDB中由两种方式进行OR查询.“\\$sin”和(\\$nin)可以用来查询多个键值\\$or()更通用一些,用来完成多个任意给定的值,如查询商品价格大于5000 或者商品id为3的商品\n\n```json\n> db.goods.find({$or:[{shop_price:{$gte:5000}},{goods_number:3}]},{_id:0,goods_name:1,goods_number:1,shop_price:1})\n{ \"goods_name\" : \"摩托罗拉A810\", \"goods_number\" : 3, \"shop_price\" : 788 }\n{ \"goods_name\" : \"多普达Touch HD\", \"goods_number\" : 1, \"shop_price\" : 5999 }\n```\n\n##### $not\n\n\\$not是元条件句,即可以用在其他条件之上(正则和文档).\\$not和\\$nin 的区别是\\$not可以用在任何地方\\$nin只能用到几何上.例如查询商品名有没有诺基亚的\n\n```json\n> db.goods.find({goods_name:{$not:/诺基亚/}},{_id:0,goods_name:1})\n{ \"goods_name\" : \"KD876\" }\n{ \"goods_name\" : \"索爱原装M2卡读卡器\" }\n{ \"goods_name\" : \"胜创KINGMAX内存卡\" }\n{ \"goods_name\" : \"飞利浦9@9v\" }\n{ \"goods_name\" : \"索爱C702c\" }\n{ \"goods_name\" : \"索爱C702c\" }\n{ \"goods_name\" : \"摩托罗拉A810\" }\n{ \"goods_name\" : \"摩托罗拉A810\" }\n{ \"goods_name\" : \"恒基伟业G101\" }\n{ \"goods_name\" : \"夏新N7\" }\n{ \"goods_name\" : \"夏新T5\" }\n{ \"goods_name\" : \"三星SGH-F258\" }\n{ \"goods_name\" : \"三星BC01\" }\n{ \"goods_name\" : \"金立 A30\" }\n{ \"goods_name\" : \"多普达Touch HD\" }\n{ \"goods_name\" : \"P806\" }\n{ \"goods_name\" : \"小灵通/固话50元充值卡\" }\n{ \"goods_name\" : \"小灵通/固话20元充值卡\" }\n{ \"goods_name\" : \"联通100元充值卡\" }\n{ \"goods_name\" : \"联通50元充值卡\" }\nType \"it\" for more\n> it\n{ \"goods_name\" : \"移动100元充值卡\" }\n{ \"goods_name\" : \"移动20元充值卡\" }\n{ \"goods_name\" : \"摩托罗拉E8 \" }\n```\n\n\n\n### 特定类型的查询\n\n####  null\n\nnull查询有点奇怪,null表示空,使用努力了查询不仅仅匹配自身,而且还会匹配\"不存在多大\",所以,这种匹配还会返回缺少这个键的文档,下面我们距离说明\n\n元数据\n\n```json\n{\n        \"_id\" : ObjectId(\"5c0cf5a2bde03aeb84cf5aaa\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5aabde03aeb84cf5aab\"),\n        \"username\" : \"hello\",\n        \"nickname\" : \"HELLO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5aebde03aeb84cf5aac\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5b8bde03aeb84cf5aad\"),\n        \"username\" : null,\n        \"nickname\" : \"JSON\",\n        \"password\" : \"123\"\n}\n```\n\n第四个文档的\"username\"的值是null,若是我们查询集合user中\"usrname\"的值为null:\n\n```json\n> db.user.find({username:null}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cf5b8bde03aeb84cf5aad\"),\n        \"username\" : null,\n        \"nickname\" : \"JSON\",\n        \"password\" : \"123\"\n}\n```\n\n集合中没有\"age\"这个键,如果我们将上面的查询语句username改成age结果会返回所有的文档。\n\n```json\n> db.user.find({age:null}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cf5a2bde03aeb84cf5aaa\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5aabde03aeb84cf5aab\"),\n        \"username\" : \"hello\",\n        \"nickname\" : \"HELLO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5aebde03aeb84cf5aac\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5b8bde03aeb84cf5aad\"),\n        \"username\" : null,\n        \"nickname\" : \"JSON\",\n        \"password\" : \"123\"\n}\n>\n```\n\n#### 正则表达式\n\n```json\n> db.user.find({username:/foo/i}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cf5a2bde03aeb84cf5aaa\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0cf5aebde03aeb84cf5aac\"),\n        \"username\" : \"foo\",\n        \"nickname\" : \"FOO\",\n        \"password\" : \"123\"\n}\n```\n\n\n\n#### 查询数组\n\n查询数组中的元素也是很简单的，每个元素都是整个键（数组的键）的值。例如：如果数组是一个水果清单\n\n```json\n{\n        \"_id\" : ObjectId(\"5c0cfe68bde03aeb84cf5aae\"),\n        \"fruit\" : [\n                \"apple\",\n                \"banana\",\n                \"peach\"\n        ]\n}\n```\n\n查询语句\n\n```json\n> db.food.find({fruit:\"banana\"}).pretty()\n{\n        \"_id\" : ObjectId(\"5c0cfe68bde03aeb84cf5aae\"),\n        \"fruit\" : [\n                \"apple\",\n                \"banana\",\n                \"peach\"\n        ]\n}\n```\n\n数组查询方法\n\n#### $all\n\n多个元素匹配数组，用$all匹配一组元素\n\n```json\n#元数据\n> db.food.find()\n{ \"_id\" : ObjectId(\"5c0cfe68bde03aeb84cf5aae\"), \"fruit\" : [ \"apple\", \"banana\", \"peach\" ] }\n{ \"_id\" : ObjectId(\"5c0d00b6bde03aeb84cf5aaf\"), \"fruit\" : [ \"apple\", \"kumquat\", \"orange\" ] }\n{ \"_id\" : ObjectId(\"5c0d00cbbde03aeb84cf5ab0\"), \"fruit\" : [ \"cherry\", \"banana\", \"apple\" ] }\n\n#查询由 apple和banana的文档\n> db.food.find({fruit:{$all:[\"apple\",\"banana\"]}})\n{ \"_id\" : ObjectId(\"5c0cfe68bde03aeb84cf5aae\"), \"fruit\" : [ \"apple\", \"banana\", \"peach\" ] }\n{ \"_id\" : ObjectId(\"5c0d00cbbde03aeb84cf5ab0\"), \"fruit\" : [ \"cherry\", \"banana\", \"apple\" ] }\n```\n\n##### key.index\n\n数组中的索引可以作为键使用，需要key.index语法指定数组的下标\n\ndb.food.find({})\n\n```json\n> db.food.find({\"fruit.2\":\"orange\"})\n{ \"_id\" : ObjectId(\"5c0d00b6bde03aeb84cf5aaf\"), \"fruit\" : [ \"apple\", \"kumquat\", \"orange\" ] }\n```\n\n数组的小标从0开始\n\n#### \\$size\n\nsize在查询语法中可以指定查询数组的大小（数组长度+1）\n\n````js\n> db.food.find({\"fruit\":{$size:3}})\n{ \"_id\" : ObjectId(\"5c0cfe68bde03aeb84cf5aae\"), \"fruit\" : [ \"apple\", \"banana\", \"peach\" ] }\n{ \"_id\" : ObjectId(\"5c0d00b6bde03aeb84cf5aaf\"), \"fruit\" : [ \"apple\", \"kumquat\", \"orange\" ] }\n{ \"_id\" : ObjectId(\"5c0d00cbbde03aeb84cf5ab0\"), \"fruit\" : [ \"cherry\", \"banana\", \"apple\" ] }\n>\n````\n\n\n\n### 游标\n\n#### 简介\n\nMongoDB中的游标与关系型数据库中的游标在功能上大同小异,游标相当于C语言中的指针,可以定位到某一条记录,在MongoDB中,则是文档,因此在mongoDB中游标也有定义生命,打开,读取,关闭这几个流程.客户通过游标,能够实现对最终结构进行有效的控制,例如限制结果输了,跳过该部分结果或者根据.\n\n游标不是查询结果,可以理解为数据在便利过程中内部指针,其返回值是一个资源,或者说数据读取接口,客户端通过对游标进行一些设置就能够对查询结果进行有效的控制如可以限制查询得到的结果数量,跳过部分结果,或者对结果集进行任意键进行排序等.\n\n直接对一个集合进行调用find()方法是,我们会发现如果查询的结果超过二十条,指挥返回20条结果,这是因为MingoDB会自动递归find()返回的游标\n\n#### 介绍\n\ndb.collection.find()方法返回一个游标,对于文档的访问,我们需要进行游标迭代,MongoDB的游标与关系型数据库SQL中的游标类似,可以通过对游标进行(如限制查询的结果数,跳过结果数设置来控制查询结果).游标会消耗内存和相关系统资源,游标使用完后应尽快释放资源.\n\n在mongo shell 中,如果返回的游标结束为指定给某个var定义的变量,则游标自动迭代20次,输出前20个文档,超出20的情形则需要输入it来法爷.本文内容描述手动方式实现索引迭代过程如下\n\n###### 声明游标\n\n```js\nvar cusor = db.collectionName.find(query,projection)\n```\n\n###### 打开游标\n\n````js\ncusor.hasNext()\n#判断游标是否已经取到头了\n````\n\n###### 读取数据\n\n```js\ncusor.Next()\n#取出游标的下一个文档\n```\n\n\n###### 关闭游标\n\n\n```js\ncusor.Next()\n#此步骤可以省略通常是自动关闭,也可以显示关闭.\n```\n\n我们用while来实现便利游标的示例:\n\n```js\nvar mycursor =db.user.find({})\nwhile(mycursor.hasNext()){\n    printjson(mycursor.next())\n}\n```\n\n#### 输出游标结果集\n\n\n\n1. print\n2. printjson\n\n\n\n1. 使用print输出游标的结果集:\n\n    ```js\n    var cursor = db.user.find(){}\n    \twhile(curwsor.hasNext()){\n    \tprint(tojson(myCurson.next()))    \n    }\n    ```\n2. 是用printjson输出游标的结果集\n    ```js\n    var cursor = db.user.find(){}\n    \twhile(curwsor.hasNext()){\n    \tprintjson(tojson(myCurson.next()))    \n    }\n    ```\n\n#### 迭代\n\n游标还有一个迭代函数,允许我们自定义回调函数来逐个处理每个单元.cursor.forEach(回调函数)步骤如下\n\n1. 定义回调函数\n2. 打开游标\n3. 迭代\n\n\n\n##### 元数据\n\n```js\n> db.food.find()\n{ \"_id\" : ObjectId(\"5c0d23c7bb8749abe43ce55c\"), \"food\" : [ \"apple\", \"banana\", \"peach\" ] }\n{ \"_id\" : ObjectId(\"5c0d23dfbb8749abe43ce55d\"), \"food\" : [ \"apple\", \"kumquat\", \"orange\" ] }\n{ \"_id\" : ObjectId(\"5c0d23f3bb8749abe43ce55e\"), \"food\" : [ \"cherry\", \"banana\", \"apple\" ] }\n>\n```\n\n##### 定义函数\n\n```json\n> var getFuntion = function(obj){print(obj.food)}\n```\n\n##### 打开游标\n\n```js\n> var cursor = db.food.find()\n```\n\n##### 迭代\n\n`````js\n> cursor.forEach(getFuntion)\napple,kumquat,orange\ncherry,banana,apple\ncherry,banana,apple\n`````\n\n##### 数组迭代\n\n````js\n> var cursor = db.food.find()\n> var documentArray=cursor.toArray()\n> printjson(documentArray)\n[\n        {\n                \"_id\" : ObjectId(\"5c0d250634103d4c5c927ac8\"),\n                \"food\" : [\n                        \"apple\",\n                        \"kumquat\",\n                        \"orange\"\n                ]\n        },\n        {\n                \"_id\" : ObjectId(\"5c0d251734103d4c5c927ac9\"),\n                \"food\" : [\n                        \"cherry\",\n                        \"banana\",\n                        \"apple\"\n                ]\n        },\n        {\n                \"_id\" : ObjectId(\"5c0d252534103d4c5c927aca\"),\n                \"food\" : [\n                        \"cherry\",\n                        \"banana\",\n                        \"apple\"\n                ]\n        }\n]\n````\n\n##### 注意 \n\n 不要随意使用toArray()因为这样会把所有的行立即以对象的形式存到内存里,可以用于取出少数的几行.\n\n####  游标解析\n\n看待游标有两种角度：客户端的游标以及客户端游标表示的数据库游标。前面讨论的都是客户端的游标，接下来简要看看服务器端发生了什么。在服务器端，游标消耗内存和其他资源。游标遍历尽了结果以后，或者客户端发来消息要求终止，数据库将会释放这些资源。释放的资源可以被数据库换作他用，这是非常有益的，所以要尽量保证尽快释放游标（在合理的前提下)。还有一些情况导致游标终止（随后被清理）。首先，当游标完成匹配结果的迭代时，它会清除自身。另外，当游标在客户端已经不在作用域内了，驱动会向服务器发送专门的消息，让其销毁游标。最后，即便用户也没有迭代完所有结果，并且游标也还在作用域中，10分钟不使用，数据库游标也会自动销毁。这种“超时销毁”的行为是我们希望的：极少有应用程序希望用户花费数分钟坐在那里等待结果。然而，的确有些时候希望游标持续的时间长一些。若是如此的话，多数驱动程序都实现了一个叫immortal的函数，或者类似的机制，来告知数据库不要让游标超时。如果关闭了游标的超时时间，则一定要在迭代完结果后将其关闭，否则它会一直在数据库中消耗服务器资源。\n\n\n\n","tags":["MongoDB"],"categories":["数据库"]},{"title":"MongoDB基础命令","url":"/2018/12/09/MongoDB基础命令/","content":"\n {{ \"MongoDB的增删改\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## MongoDB 入门命令\n\n### 查看当前数据库\n\n```sql\n> show dbs\nadmin   0.000GB\nconfig  0.000GB\nlocal   0.000GB\n>\n\n-- use databaseName 选库\n> use test\nswitched to db test\n>\n\n-- show tables/collections 查看当前库下的collection\n> show tables\n> show collections\n>\n```\n\n### 基础操作\n\nMongodb的库是隐式创建,你可以use 一个不存在的库然后在该库下创建collection,即可创建库\n\n```sql\n--创建collection\n--db.createCollection(‘collectionName’)  \n\n> db.createCollection('test')\n{ \"ok\" : 1 }\n>\n> show tables\ntest\n>\n\n--collection允许隐式创建\n--Db.collectionName.insert(document);\n> db.stu.insert({stu:'001',name:'xiaoming'})\nWriteResult({ \"nInserted\" : 1 })\n> show tables\nstu\ntest\n\n-- 删除collection\ndb.collectionName.drop()\n\n-- 删除database\ndb.dropDatabase();\n> db.dropDatabase()\n{ \"dropped\" : \"test\", \"ok\" : 1 }\n>\n```\n\n\n\n### 增\n\n#### 插入数据\n\n```json\n> db.stu.insert({sid:\"10\"})\nWriteResult({ \"nInserted\" : 1 })\n> db.stu.insert({sid:\"11\"})\nWriteResult({ \"nInserted\" : 1 })\n```\n\n```json\n> db.stu.find()\n{ \"_id\" : ObjectId(\"5c0c8a0b31a9b3cbb9df1d4f\"), \"sid\" : \"10\" }\n{ \"_id\" : ObjectId(\"5c0c8ac731a9b3cbb9df1d50\"), \"sid\" : \"11\" }\n```\n\n添加数据时不添加任何主键,会制动生成一个主键,主键不会像关系型数据库那样自动递增(为了分布式考虑),使用的是时间戳+机器编号+进程编号+序列号来生成,保证每个id都是唯一的.id为5c0c8a0b31a9b3cbb9df1d4f,可以分解为  \n5c0c8a0b 31a9b3  cbb9   df1d4f   (5c0c8a0b)表示时间戳, 31a9b3  表示机器号, cbb9   表示进程编号, df1d4f  表示序列号\n\n我们也可以手动指定ID\n\n```json\n> db.stu.insert({_id:\"001\",\"sid\":\"12\"})\nWriteResult({ \"nInserted\" : 1 })\n> db.stu.find()\n{ \"_id\" : ObjectId(\"5c0c8a0b31a9b3cbb9df1d4f\"), \"sid\" : \"10\" }\n{ \"_id\" : ObjectId(\"5c0c8ac731a9b3cbb9df1d50\"), \"sid\" : \"11\" }\n{ \"_id\" : \"001\", \"sid\" : \"12\" }\n>\n```\n\n#### 批量插入\n\n```json\n> db.user.insert([\n... {username:\"xiaoming\",nickname:\"XM\",passwd:\"123\"} ,\n... {username:\"xiaogang\",nickname:\"XG\",passwd:\"111\"},\n... {username:\"xiaohua\",nickname:\"XH\",passwd:\"123\"}\n... ])\nBulkWriteResult({\n        \"writeErrors\" : [ ],\n        \"writeConcernErrors\" : [ ],\n        \"nInserted\" : 3,\n        \"nUpserted\" : 0,\n        \"nMatched\" : 0,\n        \"nModified\" : 0,\n        \"nRemoved\" : 0,\n        \"upserted\" : [ ]\n})\n# 查看数据\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0c8f3531a9b3cbb9df1d51\"),\n        \"username\" : \"xiaoming\",\n        \"nickname\" : \"XM\",\n        \"passwd\" : \"123\"\n}\n{\n        \"_id\" : ObjectId(\"5c0c8f3531a9b3cbb9df1d52\"),\n        \"username\" : \"xiaogang\",\n        \"nickname\" : \"XG\",\n        \"passwd\" : \"111\"\n}\n{\n        \"_id\" : ObjectId(\"5c0c8f3531a9b3cbb9df1d53\"),\n        \"username\" : \"xiaohua\",\n        \"nickname\" : \"XH\",\n        \"passwd\" : \"123\"\n}\n\n```\n\n执行插入数据的时候,驱动程序会把数据转换成为BSON格式,然后将数据输入数据库,数据库会解析BSON,并检验是否含有“_id”键,因为用户如果没有自定义\"_id\",会自动生成,而且每次插入的文档不能超过16M(插入文档的大小跟MongoDB版本有关系)\n\n### 删除文档\n\n##### 方式一\n\ndb.user.deleteMany({})\n\n```json\n> db.user.deleteMany({})\n{ \"acknowledged\" : true, \"deletedCount\" : 3 }\n>\n> db.user.remove({})\nWriteResult({ \"nRemoved\" : 3 })\n```\n\n上述命令会删除user所有的文档,不删除集合本身,原有的索引也会保留,remove函数可以接收一个查询文档作为可选参数给定参数后,可以删除指定符条件的文档。\n\n#####  方式二\n\n```json\n> db.user.remove({passwd:\"123\"})\nWriteResult({ \"nRemoved\" : 2 })\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0c91d131a9b3cbb9df1d5b\"),\n        \"username\" : \"xiaogang\",\n        \"nickname\" : \"XG\",\n        \"passwd\" : \"111\"\n}\n\n```\n\n删除数据是永久性的不可以撤销也不能恢复。\n\n### 更新文档\n\n在MongoDB中更新单个文档的操作是原子性的，默认情况下如果一个update操作多个文doc，那么对于每个doc的更新是原子性的,但是对于整个update操作而言,不是原子性的可能存在前面的doc更新成功,而后面的文档更新失败,由于更新单个文档doc的操作是原子性的,如果两个更新同时发生,那么一个更新操作会足协另外一个,doc的最终结果的值是由事件靠后的更新操作删除决定的.\n\n#### 格式\n\ndb.collection.update(critera,objNew,upset,multi)\n\ncritera:查询条件\n\nobjNew:update对象和一些更新操作符\n\nupset:如果存在update记录,是否插入objNew这个新文档,true为插入,默认为false\n\nmulti:默认是false,值更新找到的第一条记录,如果是true,按照条件查询出看来的记录全部更新\n\n#### save\n\n另一个更新命令是save 格式如下\n\ndb.collection.save(object)\n\nobj表示要更新的对象,如果内部已经存在一个和obj相同的\"_id\"的记录纸Mongodb会把obj对象替换集合内已存在的记录,如果不存在,则会插入obj对象.\n\n#### 文档替换\n\n用于一个新文档完全替代匹配的文档,这种方法先把数据读出来,之后对对象的方式完成修改,这种方式一般用在修改较大的情况下:\n\n```json\ndb.user.insertOne({ \n\tname:\"foo\",\n\tnickname:\"bar\",\n\tfriends:12,\n\tenemies:2\n})\n```\n\n我们希望把数据修改成为\n\n````json\ndb.user.insertOne({ \n\tname:\"foo\",\n\tnickname:\"bar\",\n\trelations:{\n    friends:12,\n\tenemies:2\n\t}\n})\n````\n\n\n\n##### 步骤\n\n查询对象存储到u中:\n\n```json\nvar u = db.user.findOne({name:\"foo\"})\n```\n\n设置relations的值:\n\n```json\n u.relations = {friends:u.friends,enemies:u.enemies}\n```\n\n修改username的值:\n\n```json\n> u.username = u.name\nfoo\n```\n\n删除friends:\n\n```json\n> u.username = u.name\nfoo\n```\n\n删除enmies:\n\n```json\n> delete u.enemies\ntrue\n```\n\n删除name:\n\n```json\n delete u.name\n```\n\n替换对象\n\n```json\n> db.user.update({name:\"foo\"},u)\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n```\n\n查询\n\n```json\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cab14d22a51c6ef9cdcee\"),\n        \"nickname\" : \"bar\",\n        \"relations\" : {\n                \"friends\" : undefined,\n                \"enemies\" : undefined\n        },\n        \"username\" : \"foo\"\n}\n\n```\n\n这种替换基于编程思想来进行的这种方式对单个对象傅咋修改比较适用\n\n#### 使用修改器\n\n修改文档只修改文章的部分,而不是全部,这个时候我们可以使用修改器对文档进行更新,他的主要思想是通过*$*符号来进行修改这些操作\n\n##### 增加和减少\n\ninc可以对数据进行增加和减少,这个操作只针对数字类型,小数或者整数.\n\n添加一条数据:\n\n```json\n> db.topic.insertOne({title:\"first\",version:108})\n{\n        \"acknowledged\" : true,\n        \"insertedId\" : ObjectId(\"5c0cb1ba422725fda4bd5746\")\n}\n> db.topic.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cb1ba422725fda4bd5746\"),\n        \"title\" : \"first\",\n        \"version\" : 108\n}\n\n```\n\n将数字减少3\n\n```json\n> db.topic.update({\"title\" : \"first\"},{$inc:{version:-3}})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n> db.topic.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cb1ba422725fda4bd5746\"),\n        \"title\" : \"first\",\n        \"version\" : 105\n}\n```\n\n#### $set修改器\n\n使用  set 可以完成的顶的需求修改\n\n```\n原始数据\n> db.author.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cb444422725fda4bd5747\"),\n        \"name\" : \"foo\",\n        \"age\" : 20,\n        \"gender\" : \"male\",\n        \"intro\" : \"student\"\n}\n```\n\n将intro 修改为 teacher\n\n```json\n> db.author.update({name:\"foo\"},{$set:{intro:\"teacher\"}})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n> db.author.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cb444422725fda4bd5747\"),\n        \"name\" : \"foo\",\n        \"age\" : 20,\n        \"gender\" : \"male\",\n        \"intro\" : \"teacher\"\n}\n>\n```\n\n\n\n#### $push修改器\n\n使用push可以完成数组的插入,会在最后一条插入,如果没有这个key会自动创建一长条插入\n\n```json\n> db.post.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cc527422725fda4bd5748\"),\n        \"title\" : \"a blog\",\n        \"content\" : \"...\",\n        \"author\" : \"foo\"\n}\n#s使用push插入数组\n\ndb.post.update({title:\"a blog\"},{$push:{comments:{name:\"lina\",email:\"lina@email.com\",content:\"lina replay\"}}})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n> db.post.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cc527422725fda4bd5748\"),\n        \"title\" : \"a blog\",\n        \"content\" : \"...\",\n        \"author\" : \"foo\",\n        \"comments\" : [\n                {\n                        \"name\" : \"lina\",\n                        \"email\" : \"lina@email.com\",\n                        \"content\" : \"lina replay\"\n                }\n        ]\n}\n\n```\n\n\n\n#### addToSet修改器\n\n使用addToSet可以向一个数组添加元素,有一个限定条件,如果存在了就不添加.\n\n```json\n{\n        \"_id\" : ObjectId(\"5c0cc9f3cdb93a655448eec5\"),\n        \"name\" : \"foo\",\n        \"age\" : 12,\n        \"email\" : [\n                \"foo@example.com\",\n                \"foo@163.com\"\n        ]\n}\n## 添加集合\n> db.user.update({name:\"foo\"},{$addToSet:{email:\"foo@qq.com\"}})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n查询结果\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cc9f3cdb93a655448eec5\"),\n        \"name\" : \"foo\",\n        \"age\" : 12,\n        \"email\" : [\n                \"foo@example.com\",\n                \"foo@163.com\",\n                \"foo@qq.com\"\n        ]\n}\n>\n## 插入一个存在的数据\n> db.user.update({name:\"foo\"},{$addToSet:{email:\"foo@qq.com\"}})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 0 })\n> db.user.find().pretty()\n{\n        \"_id\" : ObjectId(\"5c0cc9f3cdb93a655448eec5\"),\n        \"name\" : \"foo\",\n        \"age\" : 12,\n        \"email\" : [\n                \"foo@example.com\",\n                \"foo@163.com\",\n                \"foo@qq.com\"\n        ]\n}\n```\n\nnModified键的值为 0 ,因为已经添加了,所以执行添加语句的时候不会重复添加的,这种机制减少了数据库的冗余数据.\n\n### 更新多个文档\n\n```json\n> db.clllections.update({x:1},{x:99})\nWriteResult({ \"nMatched\" : 1, \"nUpserted\" : 0, \"nModified\" : 1 })\n> db.clllections.find().pretty()\n{ \"_id\" : ObjectId(\"5c0ccc58cdb93a655448eec6\"), \"x\" : 99 }\n{ \"_id\" : ObjectId(\"5c0ccc61cdb93a655448eec7\"), \"x\" : 1 }\n{ \"_id\" : ObjectId(\"5c0ccc65cdb93a655448eec8\"), \"x\" : 1 }\n{ \"_id\" : ObjectId(\"5c0ccc6acdb93a655448eec9\"), \"x\" : 2 }\n>\n只有第一条匹配了 采用如下命令\n> db.clllections.update({x:1},{$set:{x:99}},false,true)\nWriteResult({ \"nMatched\" : 2, \"nUpserted\" : 0, \"nModified\" : 2 })\n> db.clllections.find().pretty()\n{ \"_id\" : ObjectId(\"5c0ccc58cdb93a655448eec6\"), \"x\" : 99 }\n{ \"_id\" : ObjectId(\"5c0ccc61cdb93a655448eec7\"), \"x\" : 99 }\n{ \"_id\" : ObjectId(\"5c0ccc65cdb93a655448eec8\"), \"x\" : 99 }\n{ \"_id\" : ObjectId(\"5c0ccc6acdb93a655448eec9\"), \"x\" : 2 }\n\n首先我们要将修改的数据赋值给$set,$set是一个修改器,我们将在上文详细讲解过,然后后面多了两个参数,第一个flase表示如果不存在update记录,是否将我们要更新的新文档插入,true 表示插入 false 表示不插入,第二个true表示是否更新全部属性的文章,false表示值更新第一条记录,true表示更新所有查到的文档.\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","tags":["MongoDB"],"categories":["数据库"]},{"title":"MongoDB入门","url":"/2018/12/08/MongoDB入门/","content":"\n {{ \"MongoD基础概念和安装\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n\n\n## MongoDB简介\n\nMongoDB是一种强大、灵活、可扩展的开源数据库。由C++编写旨在为WEB应用提供可拓展,高性能数据存储解决方案,MongoDB是一个介于关系数据库和非关系数据库之间的产品.它扩展了关系型数据库的众多有用功能，如辅助索引、范围査询（rangequery)和排序。MongoDB的功能非常丰富，比如内置的对MapReduce式聚合的支持，以及对地理空间索引的支持.\n\n### 丰富的数据模型\n\nMongoDB是面向文档的数据库，不是关系型数据库。放弃关系模型的主要原因就是为了获得更加方便的扩展性，当然还有其他好处。\n\n### 容易扩展\n\nMongoDB从最初设计的时候就考虑到了扩展的问题。它所采用的面向文档的数据模型使其可以自动在多台服务器之间分割数据。它还可以平衡集群的数据和负载，自动重排文档。这样开发者就可以专注于编写应用，而不是考虑如何扩展。要是需要更大的容量，只需在集群中添加新机器，然后让数据库来处理剩下的事。\n\n### 功能丰富\n\n索引       \t\t\t MongoDB支持通用辅助索引，能进行多种快速査询，也提供唯一的、复合的和地理空间索引能力。\n存储      \t  \t\t JavaScript开发人员不必使用存储过程了，可以直接在服务端存取JavaScript的函数和值。\n聚合         \t\t MongoDB支持MapReduce和其他聚合工具。\n固定集合   \t\t 集合的大小是有上限的，这对某些类型的数据（比如日志）特別有用。\n文件存储MongoDB支持用一种容易使用的协议存储大型文件和文件的元数据。有些关系型数据库的常见功能MongoDB并不具备，比如联接（join)和复杂的多行事务。这个架构上的考虑是为了提髙扩展性，因为这两个功能实在很难在一个分布式系统上实现。\n\n###  不牺牲速度\n\n卓越的性能是MongoDB的主要目标，也极大地影响了设计上的很多决定。MongoDB使用MongoDB传输协议作为与服务器交互的主要方式（与之对应的协议需要更多的开销，如HTTP/REST)。它对文档进行动态填充，预分配数据文件，用空间换取性能的稳定。默认的存储引擎中使用了内存映射文件，将内存管理工作交给操作系统去处理。动态查询优化器会“记住”执行査询最高效的方式。总之，MongoDB在各个方面都充分考虑了性能。虽然MongoDB功能强大，尽量保持关系型数据库的众多特性，但是它并不是要具备所有的关系型数据库的功能。它尽可能地将服务器端处理逻辑交给客户端（由驱动程序或者用户的应用程序处理)。这样精简的设计使得MongoDB获得了非常好的性能。\n\n### 简单管理\n\nMongoDB尽量让服务器自治来简化数据库的管理。除了启动数据库服务器之外，几乎没有什么必要的管理操作。如果主服务器挂掉了，MongoDB会自动切换到备份服务器上，并且将备份服务器提升为活跃服务器。在分布式环境下，集群只需要知道有新增加的节点，就会自动集成和配置新节点。MongoDB的管理理念就是尽可能地让服务器自动配置，让用户能在需要的时候调整设置（但不强制）。\n\n\n\n## 文档数据库概念\n\n### 文档\n\n文档(document)文档是MongoDB中数据的基本单元,非常类似于关系型数据库管理系统中的行,但是比传统行表示的信息更加复杂\n\n```json\n{\"name\":\"json\",\"age\":18,\"sex\":\"male\"}\n```\n\n#### 概要\n\n文档是一个键值对(key-value)(即BSON) MongoDB的文档不需要设置相同的字段,相同的字段不需要相同的数据类型\n\n文档只有一个键 \"name\" 其对应的(value)为\"json\"一般会包含多个键值对\n\n```json\n{\"name\":\"json\",\"age\":18,\"sex\":\"male\"}\n```\n\n#### 键值对有序\n\n```json\n{\"spoort\":\"football\",\"adress\":北京,\"phone\":\"XXXXXXXXXXX\"}\n{\"spoort\":\"football\",\"phone\":\"XXXXXXXXXXX\",\"adress\":北京}\n##这是两个数据\n```\n\n#### 区分大小写\n\n```json\n{\"name\":\"json\",\"age\":18,\"sex\":\"male\"}\n{\"Name\":\"json\",\"age\":18,\"sex\":\"male\"}\n## 这两个文档也是不同的\n```\n\n#### 区别字符数字\n\n```json\n{\"name\":\"json\",\"age\":18,\"sex\":\"male\"}\n{\"name\":\"json\",\"age\":\"18\",\"sex\":\"male\"}\n## 这两个文档也是不同的\n```\n\n#### 命名\n\n1. _id是系统保留的关键字,默认主键,该值集合必须唯一,且不可以更改.\n\n2. 键不能包含\\0或者空字符,\\0表示字符串结尾\n\n3. 不能以$开头\n\n4. 不能包含.(点号)\n\n5. 键是区别大小写的且不能重复 否则视为非法文档\n\n    ```json\n    {\"address\":\"china\",\"address\":\"USA\"}\n    ```\n\n\n### 集合\n\n集合(collection):这个在MongoDB中表示一组文档,类似于关系型数据库中的表,但是在MongoDB的表(就是集合)是没有模式的,你可以 将完全不同的文档放入到一个集合里,但是在实际使用中,为特定隐形规定一种模式.\n\n#### 无模式\n\n集合在数据库中没有固定结构,可以插入不同的格式和类型的数据,通常情况下会有一定的关联性\n\n```json\n{\"spoort\":\"football\",\"adress\":北京,\"phone\":\"XXXXXXXXXXX\"}\n{\"name\":\"json\",\"age\":18,\"sex\":\"male\"}\n##文档的键值完全不同, MongoDB不做强制要求,让开发更灵活.但一般都是一个相关类型的文档.\n```\n\n#### 命名\n\nUTF字符串:\n\n1. 不能是空字符串(\"\")\n\n2. 集合名不能包含\\0或者空字符,\\0表示字符串结尾\n\n3. 集合不能以“system.”开头.此前缀是系统本身保留的.\n\n4. 集合名不能包换$字符.有些驱动程序支持集合名里有这是系统生成 的\n\n5. 命名空间\n\n      把数据库添加到集合名字前面.中间用点号链接,得到集合的完全限定名,就是命名空间,如buyinplay.communtity点可以出现在集合名字buyinplay.communtity.reviews看作是community集合的子集可以帮我们更好的组织数据,使数据的结构更加清晰明了.\n\n### 数据库\n\n数据库(databases):在MongoDB中,一组集合可以组成一个数据库,一个MongoDB实例可以承载多个数据库,每个数据库都有独立的权限控制.\n\n#### 命名\n\n1. 数据库名不能是空字符(\"\")\n2. 数据库名不能包含\" 空格\"、$、/、和\\0(空字符)\n3. 数据库名带字母的全小写\n4. 数据库名最多包含64个字节\n\n### 数据类型\n\n数据类型(data tpye):类似于JSON,只支持:null,布尔,数字 ,字符串,数组,和对象.\n\n#### null\n\nnull:表示空值或者不存在的字段\n\n```json\n{\"x\":\"null\"}\n```\n\n#### 布尔\n\n布尔值包括两个 true false\n\n```\n{\"x\":\"true\"}\n```\n\n#### 数字\n\n支持:\n\n1. 32位整数\n\n2. 64位整数\n\n3. 64位浮点数\n\n    ```json\n    {\"x\":\"3.145\"}\n    ```\n\n\n#### 字符串\n\n```json\n{\"x\":\"heloworld\"}\n```\n\n#### 数组\n\n```json\n{\"x\":[\"a\",\"b\",\"c\"]}\n```\n\n###  _id和ObjectId\n\n​\tMongoDB中存储的文件必须有一个\"_id\"键,这个键的值可以是任何数据类型的,默认是一个objectId对象,在一个集合里面,每个文档都有唯一的\"_id\"值,来确保集合里面每个文档都能够唯一标识.\n\n#### ObjectId\n\nObjectId是\"_id\" 是默认类型,不同的机器全局可以用同种方法生成他.这也是MongoDB采用ObjectId而不是其他比较常规的做法.(比如自动增长的主键)的原因\n\n#### 自动生成的_id\n\n如果插入文档的时候没有指定\"_id\" 的值,系统会自动帮你创建一个.可以用MongoDB服务器来做这件事,冗长会在客户端由驱动程序完成.\n\n#### 日期\n\n在JavaScropt中,Date对象用作MongoDB的日期类型,创建一个新的Date对象是用new Date 而不是Date() 调用构造函数实际上会返回对日期的字符纯表示不是真正的Date对象,MongoDB支持Date作为键的一个值\n\n```json\n{\"name\":\"jack\",\"birthday\":new Date()}\n```\n\n#### 数组\n\n数组是一组值,既可以作为有序对象(链表、栈或者队列)来操作,也可以作为无需对象(集合)来操作\n\n```json\n{\"circle\":[\"pi\":3.1415926]}\n```\n\n数组中包含不同的元素,MongDB可以理解这种数据结构,并且对数组内容进行修改。\n\n#### 内嵌文档\n\n把整个文档作另外一个键的一个值 \n\n```json\n{\n    \"name\":\"jack\",\n    \"address\":{\n        \"stree\":\"XXXXXXXX\",\n        \"city\":\"XXXXX\"\n\t\t    \n\t}\n}\n```\n\n\n\n## MongoDB安装\n\n### 下载安装包\n\n```shell\nwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-4.0.4.tgz\n```\n\n### 解压安装包\n\n```shell\ntar -zxvfmongodb-linux-x86_64-4.0.4.tgz\n```\n\n### 重新命名\n\n```\nmv  mongodb-linux-x86_64-4.0.4mongodb\n```\n\n### 目录解析\n\n \n\n![F8GYuR.png](https://s1.ax1x.com/2018/12/08/F8GYuR.png)\n\n \n\n### 启动服务\n\n```shell\n./bin/mongod  --dbpath /path/to/database --logpath  /path/to/log  --fork  --port27017\n参数解释:\n--dbpath数据存储目录\n--logpath日志存储目录\n--port运行端口(默认27017)\n--fork后台进程运行\n```\n\n\n\n\n\n","tags":["MongoDB"],"categories":["数据库"]},{"title":"Redis的集群模式","url":"/2018/12/07/Redis的集群模式/","content":"\n {{ \"Redis的集群搭建 \"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### 集群\n\n即使使用哨兵，此时的Redis集群的每个数据库依然存有集群中的所有数据，从而导致集群的总数据存储量受限于可用存储内存最小的数据库节点，形成木桶效应。由于Redis中的所有数据都是基于内存存储，这一问题就尤为突出了尤其是当使用Redis做持久化存储服务使用时。\n对Redis进行水平扩容，在旧版Redis中通常使用客户端分片来解决这个问题，即启动多个Redis数据库节点，由客户端决定每个键交由哪个数据库节点存储，下次客户端读取该键时直接到该节点读取。这样可以实现将整个数据分布存储在N个数据库节点中，每个节点只存放总数据量的1/N。但对于需要扩容的场景来说，在客户端分片后，如果想增加更多的节点，就需要对数据进行手工迁移，同时在迁移的过程中为了保证数据的一致性，还需要将集群暂时下线，相对比较复杂。考虑到Redis实例非常轻量的特点，可以采用预分片技术（presharding)来在一定程度上避免此问题，具体来说是在节点部署初期，就提前考虑后的存储规模，建立足够多的实例（如128个节点），初期时数据很少，所以每个节点存储的数据也非常少，但由于节点轻量的特性，数据之外的内存幵销并不大，这使得只需要很少的服务器即可运行这些实例。曰后存储规模扩大后，所要做的不过是将某些实例迁移到其他服务器上，而不需要对所有数据进行重新分片并进行集群下线和数据迁移了。\n无论如何，客户端分片终归是有非常多的缺点，比如维护成本高，增加、移除节点较繁琐等。Redis3.0版的一大特性就是支持集群（Cluster,广义的“集群”相区别）功能。集群的特点在于拥有和单机实例同样的性能，同时在网络分区后能够提供一定的可访问性以及对主数据库故障恢复的支持。另外集群支持几乎所有的单机实例支持的命令，对于涉及多键的命令（如MGET),如果每个键都位于同一个节点中，则可以正常支持，否则会提示错误。除此之外集群还有一个限制是只能使用默认的0号数据库，如果执行SELECT切换数据库则会提示错误。\n哨兵与集群是两个独立的功能，但从特性来看哨兵可以视为集群的子集，当不需要数据分片或者已经在客户端进行分片的场景下哨兵就足够使用了，但如果需要进行水平扩容，则集群是一个非常好的选择\n\n###  安装步骤\n\n#### 安装gcc\n\n直接用yum install 安装\n\n![F3cN8A.md.png](https://s1.ax1x.com/2018/12/08/F3cN8A.md.png)\n\n#### 解压redis 包\n\n![F324NF.md.png](https://s1.ax1x.com/2018/12/08/F324NF.md.png)\n\n#### make\n\n![F3R0Dx.md.png](https://s1.ax1x.com/2018/12/08/F3R0Dx.md.png)\n\n如果出现   Jemalloc/jemalloc.h：没有那个文件\n执行make  distclean之后再make\n注意 *Redis Test(可以不用执行)*\n\n#### make install \n\n执行完make后，跳过Redis test 继续执行make install\n\n![F3Rg8H.png](https://s1.ax1x.com/2018/12/08/F3Rg8H.png)\n\n\n\n#### 目录解析\n\n查看默认安装目录：usr/local/bin\nRedis-benchmark:性能测试工具，可以在自己本子运行，看看自己本子性能如何(服务启动起来后执行)\nRedis-check-aof：修复有问题的AOF文件\nRedis-check-dump：修复有问题的dump.rdb文件\nRedis-sentinel：Redis集群使用\nredis-server：Redis服务器启动命令\nredis-cli：客户端，操作入口\n\n**为了演示方便下面我用配好的集群虚拟机**\n\n首先要解决一个环境问题 点击链接 即可获取 这里就不写了\n\n[链接](https://blog.csdn.net/qq_2439107956/article/details/81089711)\n\n####  创建redis-cluster\n\n 在 /opt 下执行 mkdir redis-cluster/\n![F3WpIU.png](https://s1.ax1x.com/2018/12/08/F3WpIU.png)\n\n#### 修改节点配置\n\n注意要修改六个哦 当然也可以偷懒 \n\n开启daemonize yes\ncluster-enabled yes\nPid文件名字\n指定端口\nLog文件名字\nDump.rdb名字\nAppendonly 关掉或者换名字\n\n#### star-all脚本\n\n```shell\n#!/bin/bash\n\ncd redis01\nredis-server redis.conf\ncd ..\ncd redis02\nredis-server redis.conf\ncd ..\ncd redis03\nredis-server redis.conf\ncd ..\ncd redis04\nredis-server redis.conf\ncd ..\ncd redis05\nredis-server redis.conf\ncd ..\ncd redis06\nredis-server redis.conf\ncd ..\n```\n\n\n\n#### shutdown-all 脚本\n\n```shell\n#!/bin/bash\n\nredis-cli -p 7001 shutdown\nredis-cli -p 7002 shutdown\nredis-cli -p 7003 shutdown\nredis-cli -p 7004 shutdown\nredis-cli -p 7005 shutdown\nredis-cli -p 7006 shutdown\n```\n\n### 启动节点\n\n![F3W5l9.md.png](https://s1.ax1x.com/2018/12/08/F3W5l9.md.png)\n\n#### 拼接集群\n\n```\n在redis 安装目录下  src下执行命令\n./redis-trib.rb  create  --replicas  1    127.0.0.1:7001   127.0.0.1:7002    127.0.0.1:7003  127.0.0.1:7004   127.0.0.1:7005 127.0.0.1:7006\n```\n\n#### 查看节点情况\n\n![F3fK00.png](https://s1.ax1x.com/2018/12/08/F3fK00.png)\n\n#### 分析\n\n一个集群至少要有三个主节点。选项--replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。分配原则尽量保证每个主数据库运行在不同的IP地址，每个从库和主库不在一个IP地址上。\n\n### 增加节点\n\nredis-trib.rb是使用CLUSTERMEET命令来使每个节点认识集群中的其他节点的，可想而知如果想要向集群中加入新的节点，也需要使用CLUSTERMEET命令实现。加入新节点非常简单，只需要向新节点（以下记作A)发送如下命令即可：\nCLUSTERMEETipport\nip和port是集群中任意一个节点的地址和端口号，A接收到客户端发来的命令后，会与该地址和端口号的节点B进行握手，使B将A认作当前集群中的一员。当B与A握手成功后，B会使用Gossip协议将节点A的信息通知给集群中的每一个节点。通过这一方式,即使集群中有多个节点，也只需要选择MEET其中任意一个节点，即可使新节点最终加入整个集群中。\n\n### 插槽分配\n\n````\n[OK] All nodes agree about slots configuration.\n>>> Check for open slots...\n>>> Check slots coverage...\n[OK] All 16384 slots covered.\n````\n\n•一个Redis集群包含16384个插槽（hashslot），数据库中的每个键都属于这16384个插槽的其中一个，集群使用公式CRC16(key)%16384来计算键key属于哪个槽，其中CRC16(key)语句用于计算键key的CRC16校验和。\n\n#### 算法\n\n代码如下\n\n````c\n*Copyright 2001-2010 Georges Menie (www.menie.org)\n*Copyright 2010 Salvatore SanfHippo (adapted to Redis coding style)\n*All rights reserved.\n*Redistribution and use in source and binary forms, with or without\n*modification, are permitted provided that the following conditions are met:\n\n\tRedistributions of source code must retain the above copyright\n\tnoticethis list of conditions and the following disclaimer.\n\tRedistributions in binary form must reproduce the above copyright\n\tnotice, this list of conditions and the following disclaimer in the\n\tdocumentation and/or other materials provided with the distribution.\n\tNeither the name of the University of California, Berkeley nor the\n\tnames of its contributors may be used to endorse or promote products\n\tderived from this software without specific prior written permission.\n        \n* THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ''AS IS, ， AND ANY\n* EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\n* WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n* DISCLAIMED. IN NO EVENT SHALL THE REGENTS AND CONTRIBUTORS BE LIABLE FOR ANY\n* DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n* (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\n* LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\n* ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n* (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n* SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n/* CRC16 implementation according to CCITT standards.\n* Note by Qantirez: this is actually the XMODEM CRC 16 algorithm, using the\n* following parameters:\n*Name \t\t\t \t\t:\"XMODEM\", also known as \"ZMODEM\", \"CRC-16/ACORN”\n* Width\t\t\t\t\t:16 bit\n* Poly\t\t\t\t\t:1021 (That is actually k^16 + x八12 + x^5 + 1)\t\t\t\t\t\n* Initialization\t\t :0000\t\n* Reflect Input byte\t\t:False\t\n* Reflect Output CRC\t\t:False\n* Xor constant to output CRC \t: 0000\n* Output for n123456789\"\t\t: 31C3\n*/\nstaticc0nstuintl6_tcrcl6tab[256]={\n0x0000,0x1021,0x2042,0x3063,0x4084,0x50a5,0x60c6,0x70e7,\n0x8108,0x9129,0xal4a,0xbl6b,0xcl8c,0xdlad,0xelce,0xflef,\n0x1231,0x0210,0x3273,0x2252,0x52b5,0x4294,0x72f7,0x62d6,\n0x9339,0x8318,0xb37b,0xa35a,0xd3bd,0xc39c,0xf3ff,0xe3de,\n0x2462,0x3443,0x0420,0x1401,0x64e6,0x74c7,0x44a4,0x5485,\n0xa56a,0xb54b,0x8528,0x9509,0xe5ee,0xf5cf,0xc5ac,0xd58d,\n0x3653,0x2672,0x1611,0x0630,0x76d7,0x66f6,0x5695,0x46b4,\n0xb75b,0xa77a,0x9719,0x8738,0xf7df,0xe7fe,0xd79d,0xc7bc,\n0x48c4,0x58e5,0x6886,0x78a7,0x0840,0x1861,0x2802,0x3823,\n0xc9cc,0xd9ed,0xe98e,0xf9af,0x8948,0x9969,0xa90a,0xb92b,\n0x5af5,0x4ad4,0x7ab7,0x6a96,0xla71,0x0a50,0x3a33,0x2al2,\n0xdbfd,0xcbdc,0xfbbf,0xeb9e,0x9b79,0x8b58,0xbb3b,0xabla,\n0x6ca6,0x7c87,0x4ce4,0x5cc5,0x2c22,0x3c03,0x0c60,0xlc41,\n0xedae,0xfd8f,0xcdec,0xddcd,0xad2a,0xbd0b,0x8d68,0x9d49,\n0x7e97,0x6eb6,0x5ed5,0x4ef4,0x3el3,0x2e32,0xle51,0x0e70,\n0xff9f,0xefbe,0xdfdd,0xcffc,0xbflb,0xaf3a,0x9f59,0x8f78,\n0x9188,0x81a9,0xblca,0xaleb,0xdl0c,0xcl2d,0xf14e,0xel6f,\n0x1080,0x00al,0x30c2,0x20e3,0x5004,0x4025,0x7046,0x6067,\n0x83b9,0x9398,0xa3fb,0xb3da,0xc33d,0xd31c,0xe37f,0xf35e,\n0x02bl,0x1290,0x22f3,0x32d2,0x4235,0x5214,0x6277,0x7256,\n0xb5ea,0xa5cb,0x95a8,0x8589,0xf56e,0xe54f,0xd52c,0xc50d,\n0x34e2,0x24c3,0xl4a0,0x0481,0x7466,0x6447,0x5424,0x4405,\n0xa7db,0xb7fa,0x8799,0x97b8,0xe75f,0xf77e,0xc71d,0xd73c,\n0x26d3,0x36f2,0x0691,0xl6b0,0x6657,0x7676,0x4615,0x5634,\n0xd94c,0xc96d,0xf90e,0xe92f,0x99c8,0x89e9,0xb98a,0xa9ab,\n0x5844,0x4865,0x7806,0x6827,0xl8c0,0x08el,0x3882,0x28a3,\n0xcb7d,0xdb5c,0xeb3f,0xfble,0x8bf9,0x9bd8,0xabbb,0xbb9a,\n0x4a75,0x5a54,0x6a37,0x7al6,0x0afl,0xlad0,0x2ab3,0x3a92,\n0xfd2e,0xed0f,0xdd6c,0xcd4d,0xbdaa,0xad8b,0x9de8,0x8dc9,\n0x7c26,0x6c07,0x5c64,0x4c45,0x3ca2,0x2c83,0xlce0,0x0ccl,\n0xeflf,0xff3e,0xcf5d,0xdf7c,0xaf9b,0xbfba,0x8fd9,0x9ff8,\n0x6el7,0x7e36,0x4e55,0x5e74,0x2e93,0x3eb2,0x0edl,0xlef0\n};\nuintl6_t crcl6(const char *buf, int len) {\n\tint counter;\n\tuintl6_t crc = 0;\n\tfor (counter = 0; counter < len; counter++)\n\t\tcrc = (crc«8) A crcl6tab[ ( (crc»8) A *buf++) &0x00FF];\n\treturn crc;\n}\n    \n````\n\n","tags":["Redis"],"categories":["数据库"]},{"title":"Redis主从复制","url":"/2018/12/07/Redis主从复制/","content":"\n {{ \"Redis主从复制\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 主从复制是什么\n\n主从复制，就是主机数据更新后根据配置和策略，自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主\n\n### 作用\n\n读写分离，性能扩展  容灾快速恢复\n\n![F3Cj00.png](https://s1.ax1x.com/2018/12/07/F3Cj00.png)\n\n当一个从数据库启动后，会向主数据库发送SYNC命令。同时主数据库接收到SYNC命令后会开始在后台保存快照（即RDB持久化的过程)，并将保存快照期间接收到的命令缓存 起來当快照完成后，Redis会将快照文件和所有缓存的命令发送给从数据库。从数据库收到后， 会载入快照文件并执行收到的缓存的命令。以上过程称为复制初始化。复制初始化结束后， 主数据库每当收到写命令时就会将命令同步给从数据库，从而保证主从数据库数据一致。 \n当主从数据库之间的连接断开重连后，Redis 2.6以及之前的版本会重新进行复制初始化（即主数据库重新保存快照并传送给从数据库)，即使从数据库可以仅有几条命令没有收到，主数据库也必须要将数据库里的所有数据重新传送给从数据库。这使得主从数据库断 线重连后的数据恢复过程效率很低下，在网络环境不好的时候这一问题尤其明显。Redis2.8 版的一个重要改进就是断线重连能够支持有条件的增量数据传输，当从数据库重新连接上主数据库后，主数据库只需要将断线期间执行的命令传送给从数据库从而大大提高Redis 复制的实用性。\n\n复制同步阶段会贯穿整个主从同步过程的始终，直到主从关系终止为止。在复制的过程中，快照无论在主数据库还是从数据库中都起了很大的作用，只要执行 复制就会进行快服即使我们关闭了 RDB方式的持久化（通过删除所有save参数)。 \n\n乐观复制Redis采用了乐观复制（optimistic replication)的复制策略，容忍在一定时 间内主从数据库的内容是不同的，但是两者的数据会最终同步。具体来说，Redis在主 从数据库之间复制数据的过程本身是异步的，这意味着，主数据库执行完客户端请求的命令后会立即将命令在主数据库的执行结果返回给客户端，并异步地将命令同步给 从数据库，而不会等待从数据库接收到该命令后再返回给客户端。这一特性保证了启 用复制后主数据库的性能不会受到影响，但另一方面也会产生一个主从数据库数据不 一致的时间窗口，当主数据库执行了一条写命令后，主数据库的数据已经发生的变动，然而在主数据库将该命令传送给从数据库之前，如果两个数据库之间的网络连接断开 了，此时二者之间的数据就会是不一致的。从这个角度来看，主数据库是无法得知某 个命令最终同步给了多少个从数据库的，不过Redis提供了两个配置选项来限制只有 当数据至少同步给指定数量的从数据库时，主数据库才是可写的.\n\n![F3kDud.md.png](https://s1.ax1x.com/2018/12/07/F3kDud.md.png)\n\n## 一主二仆\n\n![F3AZKH.png](https://s1.ax1x.com/2018/12/07/F3AZKH.png)\n\n数据库不仅可以接收主数据库的同步数据，自己也可以同时作为主数据库存在，形成类似图的结构，\n\n通过复制可以实现读写分离，以提高服务器的负载能力。在常见的场景中（如电子商务网站)，读的频率大于写，当单机的Redis无法应付大量的读请求时（尤其是较耗资源的 请求，如SORT命令等）可以通过复制功能建立多个从数据库节点，主数据库只进行写操作，而从数据库负责读操作。这种一主多从的结构很适合读多写少的场景，而当单个的主数据库不能够满足需求时，就需要使用Redis 3.0推出的集群功能，\n\n另一个相对耗时的操作是持久化，为了提高性能，可以通过复制功能建立一个（或若干个）从数据库，并在从数据库中启用持久化，同时在主数据库禁用持久化。当从数据库崩溃重启后主数据库会自动将数据同步过来，所以无需担心数据丢失。 \n然而当主数据库崩溃时，情况就稍显复杂了。手工通过从数据库数据恢复主数据库数据时，需要严格按照以下两步进行。 \n在从数据库中使用SLAVEOF NO ONE命令将从数据库提升成主数据库继续服务。启动之前崩溃的主数据库，然后使用SLAVEOF命令将其设置成新的主数据库的从 数据库，即可将数据同步回来。 \n注意当开启复制且主数据库关闭持久化功能时，一定不要使用Supervisor以及类似 的进程管理工具令主数据库崩溃后自动重启。同样当主数据库所在的服务器因故关闭 时，也要避免直接重新启动。这是因为当主数据库重新启动后，因为没有开启持久化功能，所以数据库中所有数据都被清空，这时从数据库依然会从主数据库中接收数据， 使得所有从数据库也被清空，导致从数据库的持久化失去意义。 \n无论哪种情况，手工维护从数据库或主数据库的重启以及数据恢复都相对麻烦，好在 Redis提供了一种自动化方案哨兵来实现这一过程，避免了手工维护的麻烦和容易出错的问题，\n\n###  实现配置\n\n#### 主机配置\n\n![F3VfgO.png](https://s1.ax1x.com/2018/12/07/F3VfgO.png)\n\n\n#### 从机配置\n\n![F3V7VA.png](https://s1.ax1x.com/2018/12/07/F3V7VA.png)\n\n![F3Vb5t.png](https://s1.ax1x.com/2018/12/07/F3Vb5t.png)\n\n#### 启动主机\n\n![F3VX28.png](https://s1.ax1x.com/2018/12/07/F3VX28.png)\n\n#### 启动从机\n\n![F3ZkGV.png](https://s1.ax1x.com/2018/12/07/F3ZkGV.png)\n\n![F3ZZMF.png](https://s1.ax1x.com/2018/12/07/F3ZZMF.png)\n\n### 缺点\n\n这种情况下一旦主机宕机,整个写服务都会瘫痪,因此为了避免群龙无首的局面,我们可以对配置进行优化 这里我们先采用薪火相传的模式.\n\n## 一脉相传\n\n![F3eIAA.md.png](https://s1.ax1x.com/2018/12/07/F3eIAA.md.png)\n\n上一个slave可以是下一个slave的Master，slave同样可以接收其他slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力,去中心化降低风险。中途变更转向:会清除之前的数据，重新建立拷贝最新的\n风险:一旦某个slave宕机，后面的slave都没法备份.\n\n### 主机配置\n\n![F3mmNR.png](https://s1.ax1x.com/2018/12/07/F3mmNR.png)\n\n\n\n### 从机配置\n\n![F3KQg0.png](https://s1.ax1x.com/2018/12/07/F3KQg0.png)\n\n![F3KYE4.png](https://s1.ax1x.com/2018/12/07/F3KYE4.png)\n\n### 主机启动\n\n![F3Kwgx.png](https://s1.ax1x.com/2018/12/07/F3Kwgx.png)\n\n### 从机启动\n\n![F3KOGn.png](https://s1.ax1x.com/2018/12/07/F3KOGn.png)\n\n![F3MkGR.png](https://s1.ax1x.com/2018/12/07/F3MkGR.png)\n\n## 反客为主\n\n当一个master宕机后，后面的slave可以立刻升为master，其后面的slave不用做任何修改。\n主机宕机\n\n![F3MmqO.png](https://s1.ax1x.com/2018/12/07/F3MmqO.png)\n\n\n\n#### 新主登基\n\n![F3MKde.png](https://s1.ax1x.com/2018/12/07/F3MKde.png)\n用 slaveof  no one  将从机变为主机。\n![F3MYsf.md.png](https://s1.ax1x.com/2018/12/07/F3MYsf.md.png)\n当原来的master又复活之后，后面的slave和原来的mamaster就没关系了\n\n![F3M6yV.png](https://s1.ax1x.com/2018/12/07/F3M6yV.png)\n\n当原来的master又复活之后，后面的slave和原来的master就没关系了\n\n## 哨兵模式\n\nRedis中复制的原理和使用方式，在一个典型的一主多从的Redis系统中， 从数据库在整个系统中起到了数据冗余备份和读写分离的作用。当主数据库遇到异常中断服务后，开发者可以通过手动的方式选择一个从数据库来升格为主数据库，以使得系统能够继续提供服务。然而整个过程相对麻烦且需要人工介入，难以实现自动化。为此Redis 2.8中提供了哨兵工具来实现自动化的系统监控和故障恢复功能。 \n\n``` \n注意Redis  2.6版也提供了哨兵工具，但此时的哨兵是1.0版，存在非常多的问题， \n在任何情况下都不应该使用这个版本的哨兵。这里说的哨兵都是Redis2.8之后的\n```\n\n\n\n### 功能\n\n顾名思义，哨兵的作用就是监控Redis系统的运行状况。它的功能包括以下两个。 \n(1)    监控主数据库和从数据库是否正常运行。 \n(2)    主数据库出现故障时自动将从数据库转换为主数据库。\n\n```\n配置哨兵监控一个系统时，只需要配置其监控主数据库即可，哨兵会自动发现所有复制该主数据库的从数据库，\n```\n\n### 主机配置\n\n![F3VfgO.png](https://s1.ax1x.com/2018/12/07/F3VfgO.png)\n\n### 从机配置\n\n![F3V7VA.png](https://s1.ax1x.com/2018/12/07/F3V7VA.png)\n\n![F3Vb5t.png](https://s1.ax1x.com/2018/12/07/F3Vb5t.png)\n\n###  哨兵配置\n\n自定义的/myredis目录下新建sentinel.conf文件\n\n![F38p4g.png](https://s1.ax1x.com/2018/12/07/F38p4g.png)\n\n### 启动主机\n\n![F3VX28.png](https://s1.ax1x.com/2018/12/07/F3VX28.png)\n\n### 启动从机\n\n![F3ZkGV.png](https://s1.ax1x.com/2018/12/07/F3ZkGV.png)\n\n![F3ZZMF.png](https://s1.ax1x.com/2018/12/07/F3ZZMF.png)\n\n### 启动哨兵\n\n[![F38P3j.md.png](https://s1.ax1x.com/2018/12/07/F38P3j.md.png)](https://imgchr.com/i/F38P3j)\n\n### 主机宕机\n\n![F38n5F.png](https://s1.ax1x.com/2018/12/07/F38n5F.png)\n\n### 新主登基\n\n![F388Dx.png](https://s1.ax1x.com/2018/12/07/F388Dx.png)\n\n其中— sdown表示哨兵主观认为主数据库停止服务了，而+odown则表示哨兵客观认为主数据库停止服务了，关于主观和客观的区别后文会详细介绍。此时哨兵开始执行故障恢复， 即挑选一个从数据库，将其升格为主数据库。+try -faiover 表示哨兵开始进行故障恢复，+failover -end 表示哨兵完成故障恢复，期间涉及的内容比较复杂，包括领头哨兵的选举、备选从数据库的选择等，放到后面介绍，此处只需要关注最后3条输出。+switch-master表示主数据库从6379端口迁移 到6381端口，即6381端口的从数据库被升格为主数据库，同时两个+slave则列出了新的主数据库的两个从数据库，端口分别为6380和6379。其中6379就是之前停止服务的主数据库，可见哨兵并没有彻底清除停止服务的实例的信息，这是因为停止服务的实例有 能会在之后的某个时间恢复服务，这时哨兵会让其重新加入进来，所以当实例停止服务后， 哨兵会更新该实例的信息，使得当其重新加入后可以按照当前信息继续对外提供服务。此例中6379端口的主数据库实例停止服务了，而6381端口的从数据库已经升格为主数据库， 当6379端口的实例恢复服务后，会转变为6380端口实例的从数据库来运行，所以哨兵将6379端口实例的信息修改成了6381端口实例的从数据库。 故障恢复完成后，可以使用Redis命令行客户端重新检查6379和6381两个端口上的 实例的复制信息：\n\n![F3Jk7V.png](https://s1.ax1x.com/2018/12/07/F3Jk7V.png)\n\n![F3JGtO.md.png](https://s1.ax1x.com/2018/12/07/F3JGtO.md.png)\n\n![F3JBHP.png](https://s1.ax1x.com/2018/12/07/F3JBHP.png)\n\n\n\n-sdown表示实例6379已经恢复服务了（与+sdown相反i同时+convert-to-slave 表示将6379端口的实例设置为6381端口实例的从数据库。\n\n### 实现原理\n\n一个哨兵进程启动时会读取配置文件的内容，通过如下的配置找出需要监控的主数据库: \n\n```\nsentinel monitor master~name ip redis-port quorum \n```\n其中master-name是一个由大小写字母、数字和组成的主数据库的名字，因为考虑到故障恢复后当前监控的系统的主数据库的地址和端口会产生变化，所以哨兵提供了命令可以通过主数据库的名字获取当前系统的主数据库的地址和端口号.ip表示当前系统中主数据库的地址，而redis-port则表示端口号。 \nquorum用来表示执行故障恢复操作前至少需要几个哨兵节点同意，后文会详细介绍。 一个哨兵节点可以同时监控多个Redis主从系统，只需要提供多个sentinel monitor \n配置即可，例如：\n```\nsentinel monitor mymaster 127.0.0.1 6379 2 \nsentinel monitor othermaster 192.168.1.3 6380 4\n```\n\n同时多个哨兵节点也可以同时监控同一个Redis主从系统，从而形成网状结构。\n\n配置文件中还可以定义其他监控相关的参数，每个配置选项都包含主数据库的名字使 得监控不同主数据库时可以使用不同的配置参数。例如： \n\n```\nsentinel down-after-milliseconds mymaster 60000 \nsentinel down-after-milliseconds othermaster 10000\n```\n\n上面的两行配置分别配置了 mymaster 和othermaster 的 down-after-milliseconds 选项分别为60000和10000。\n\n哨兵启动后，会与要监控的主数据库建立两条连接，这两个连接的建立方式与普通的Redis客户端无异。其中一条连接用来订阅该主数据的_Sentinel_:hello频道以获取 \n其他同样监控该数据库的哨兵节点的信息，另外哨兵也需要定期向主数据库发送INFO等 命令来获取主数据库本身的信息，客户端的连接进入订阅模式时就不 能再执行其他命令了，所以这时哨兵会使用另外一条连接来发送这些命令和主数据库的连接建立完成后，哨兵会定时执行下面3个操作。 \n(1) 每10秒哨兵会向主数据库和从数据库发送INFO命令。 \n(2) 每2秒哨兵会向主数据库和从数据库的_sentinel_:hello频道发送自己的 信息。 \n(3) 每1秒哨兵会向主数据库、从数据库和其他哨兵节点发送PING命令。 \n这3个操作贯穿哨兵进程的整个生命周期中，非常重要，可以说了解了这3个操作的 意义就能够了解哨兵工作原理的一半内容了。下面分别详细介绍。 \n首先，发送INFO命令使得哨兵可以获得当前数据库的相关信息包括运行ID、复制 信息等）从而实现新节点的自动发现。前面说配置哨兵监控Redis主从系统时只需要指定主数据库的信息即可，因为哨兵正是借助INFO命令来获取所有复制该主数据库的从数据库信息的。启动后，哨兵向主数据库发送INFO命令，通过解析返回结果来得知从数据库列表，而后对每个从数据库同样建立两个连接，两个连接的作用和前文介绍的与主数据库 建立的两个连接完全一致。在此之后，哨兵会每10秒定时向己知的所有主从数据库发送INFO命令来获取信息更新并进行相应操作，比如对新增的从数据库建立连接并加入监控列表，对主从数据库的角色变化（由故障恢复操作引起）进行信息更新等。接下来哨兵向主从数据库的_Sentinel_:hello 频道发送信息来与同样监控该数 据库的哨兵分享自己的信息。发送的消息内容为： \n<哨兵的地址 >,<哨兵的端口 >, <哨兵的运行ID>,<哨兵的配置版本>,<主数据库的名字>,<主数据库的地址>, <主数据库的端口>, <主数据库的配置版本> \n可以看到消息包括的哨兵的基本信息，以及其监控的主数据库的信息。\n\n哨兵会订阅每个其监控的数据库的_Sentinel_:hell0频道，所以当其他哨兵收到消息 后，会判断发消息的哨兵是不是新发现的哨兵。如果是则将其加入已发现的哨兵列表中并 创建一个到其的连接（与数据库不同，哨兵与哨兵之间只会创建一条连接用来发送PING 命令，而不需要创建另外一条连接来订阅频道，因为哨兵只需要订阅数据库的频道即可实 现自动发现其他哨兵）。同时哨兵会判断信息中主数据库的配置版本，如果该版本比当前记 录的主数据库的版本高，则更新主数据库的数据。配置版本的作用会在后面详细介绍。 实现了自动发现从数据库和其他哨兵节点后，哨兵要做的就是定时监控这些数据库 和节点有没有停止服务。这是通过每隔一定时间向这些节点发送PING命令实现的。时间 间隔与 down-after-milliseconds 选项有关当down-after-milliseconds 的值 小于1秒时哨兵会每隔down-after-milliseconds指定的时间发送一次PING命令，当down-after-milliseconds的值大于1秒时，哨兵会每隔1秒发送一次PING命令。 例如： \n\n```\n//每隔1秒发送一次PING命令 \nsentinel down-after-milliseconds mymaster 60000 \n//每隔600毫秒发送一次PING命令 \nsentinel down-after-milliseconds othermaster 600\n```\n\n当超过down-after-mill seconds选项指定时间后，如果被PING的数据库或节点仍然未进行回复，则哨兵认为其主观下线（ subjectively down)。主观下线表示从当前的哨兵进程看来，该节点已经下线。如果该节点是主数据库，则哨兵会进一步判断是否需要对其进行故障恢复：哨兵发送SENTINEL is-master-down-by-addr命令询问其他哨兵节点以了解他们是否也认为该主数据库主观下线，如果达到指定数量时，哨兵会认为其客观 下线(objectively down),并选举领头的哨兵节点对主从系统发起故障恢复。这个指定数量 即为前文介绍的quorum参数。例如，下面的配置： \nsentinel monitor mymaster 127.0.0.1 6379 2 \n该配置表示只有当至少两个Sentinel节点（包括当前节点）认为该主数据库主观下线时，当前哨兵节点才会认为该主数据库客观下线。进行接下来的选举领头哨兵步骤。 \n\n虽然当前哨兵节点发现了主数据库客观下线，需要故障恢复，但是故障恢复需要由领头的哨兵来完成，这样可以保证同一时间只有一个哨兵节点来执行故障恢复。选举领头哨兵的过程使用了Raft算法，具体过程如下。\n(1)发现主数据库客观下线的哨兵节点（下面称作A)向每个哨兵节点发送命令，要 求对方选自己成为领头哨兵。 \n(2)如果目标哨兵节点没有选过其他人，则会同意将A设置成领头哨兵。 \n(3)如果A发现有超过半数且超过quorum参数值的哨兵节点同意选自己成为领头哨兵，则A成功成为领头哨兵。\n(4)当有多个哨兵节点同时参选领头哨兵，则会出现没有任何节点当选的可能。此时 每个参选节点将等待一个随机时间重新发起参选请求，进行下一轮选举，直到选举成功。 具体过程可以参考Raft算法的过程http://raftconsensus.github.io/。因为要成为领头哨兵 必须有超过半数的哨兵节点支持，所以每次选举最多只会选出一个领头哨兵。 \n\n选出领头哨兵后，领头哨兵将会开始对主数据库进行故障恢复。故障恢复的过程相对 简单，具体如下。 首先领头哨兵将从停止服务的主数据库的从数据库中挑选一个来充当新的主数据库。 挑选的依据如下。 \n(1) 所有在线的从数据库中，选择优先级最高的从数据库。优先级可以通过 slave-priority选项来设置。 \n(2) 如果有多个最高优先级的从数据库，则复制的命令偏移量（见8.1.7节）越大（即 复制越完整）越优先。 \n(3)    如果以上条件都一样，则选择运行ID较小的从数据库。 选出一个从数据库后，领头哨兵将向从数据库发送SLAVEOF NO ONE命令使其升格为主数据库。而后领头哨兵向其他从数据库发送SLAVEOF命令来使其成为新主数据库的从数据库。最后一步则是更新内部的记录，将已经停止服务的旧的主数据库更新为新的主数据库的从数据库，使得当其恢复服务时自动以从数据库的身份继续服务。\n\n### 部署原则\n\n哨兵以独立进程的方式对一个主从系统进行监控，监控的效果好坏与否取决于哨兵的 视角是否有代表性。如果一个主从系统中配置的哨兵较少，哨兵对整个系统的判断的可靠 性就会降低。极端情况下，当只有一个哨兵时，哨兵本身就可能会发生单点故障。整体来 讲，相对稳妥的哨兵部署方案是使得哨兵的视角尽可能地与每个节点的视角一致，即： \n(1)为每个节点（无论是主数据库还是从数据库）部署一个哨兵； \n(2)使每个哨兵与其对应的节点的网络环境相同或相近。 \n这样的部署方案可以保证哨兵的视角拥有较高的代表性和可靠性。举例一个例子. 当网络分区后，如果哨兵认为某个分区是主要分区即意味着从每个节点观察，该分区均为主分区。 \n同时设置quorum的值为N/2+1 (其中N为哨兵节点数量），这样使得只有当大部分哨兵节点同意后才会进行故障恢复。 \n当系统中的节点较多时，考虑到每个哨兵都会和系统中的所有节点建立连接，为每个 节点分配一个哨兵会产生较多连接，尤其是当进行客户端分片时使用多个哨兵节点监控多 个主数据库会因为Redis不支持连接复用而产生大量冗余连接，具体可以见此issue： https://github.com/antirez/redis/issues/2257  ；  同时如果 Redis 节点负载较高，会在一定程度上影响其对哨兵的回复以及与其同机的哨兵与其他节点的通信。所以配置哨兵时还需要根据 实际的生产环境情况进行选择。","tags":["Redis"],"categories":["数据库"]},{"title":"Redis持久化","url":"/2018/12/07/Redis持久化/","content":" {{ \"RDB 和 AOF\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### Redi持久化方式\n\nRedis provides a different range of persistence options:\n\nThe RDB persistence performs point-in-time snapshots of your dataset at specified intervals.\n\nthe AOF persistence logs every write operation received by the server, that will be played again at server startup, reconstructing the original dataset. Commands are logged using the same format as the Redis protocol itself, in an append-only fashion. Redis is able to rewrite the log on background when it gets too big.\n\nIf you wish, you can disable persistence at all, if you want your data to just exist as long as the server is running.\n\nIt is possible to combine both AOF and RDB in the same instance. Notice that, in this case, when Redis restarts the AOF file will be used to reconstruct the original dataset since it is guaranteed to be the most complete.\nRedis 提供了2个不同形式的持久化方式。\nRDB （Redis DataBase）\nAOF （Append Of File）\n\n#### RDB \n​\t在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里。\n\n##### 如何执行\n​\tRedis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。\n\n##### fork\n在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制技术”，一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。这也是为什么推荐使用Linux而不是在windos平台上运行的原因之一.\n\n写时复制策略也保证了在fork的时刻虽然看上去生成了两份内存副本，但实际 上内存的占用量并不会增加一倍。这就意味着当系统内存只有2 GB，而Redis数据库 的内存有1.5GB时，执行fork后内存使用量并不会增加到3 GB (超出物理内存）。 为此需要确保Linux系统允许应用程序申请超过可用内存（物理内存和交换分区）的 空间，方法是在/etc/sysctl • conf 文件加入 vm. overcommit_memory = 1，然后 重启系统或者执行sysctl vm.overcommit_memory=l确保设置生效。 \n另外需要注意的是，当进行快照的过程中，如果写入操作较多，造成fork前后 数据差异较大，是会使得内存使用量显著超过实际数据大小的，因为内存中不仅保存 了当前的数据库数据，而且还保存着fork时刻的内存数据。进行内存用量估算时很容易忽略这一问题，造成内存用量超限。\n\n##### 配置文件\n\n###### 文件名称\n\n在redis.conf中配置文件名称，默认为dump.rdb\n\n![F1qxuF.png](https://s1.ax1x.com/2018/12/07/F1qxuF.png)\n\n###### 保存路径\n\nrdb文件的保存路径，也可以修改。默认为Redis启动时命令行所在的目录下\n![F1qzB4.png](https://s1.ax1x.com/2018/12/07/F1qzB4.png)\n\n###### 保存策略\n\n![F1LP41.png](https://s1.ax1x.com/2018/12/07/F1LP41.png)\n\n\n\n######  手动保存\n命令save: 只管保存，其它不管，全部阻塞\nsave vs bgsave\n当Redis无法写入磁盘的话，直接关掉Redis的写操作\n![F1LYDg.png](https://s1.ax1x.com/2018/12/07/F1LYDg.png)\n\n###### 压缩\nrdbcompression yes\n进行rdb保存时，将文件压缩\n![F1L526.png](https://s1.ax1x.com/2018/12/07/F1L526.png)\n\n###### 数据校验\n\n![F1LxRP.png](https://s1.ax1x.com/2018/12/07/F1LxRP.png)\n\n在存储快照后，还可以让Redis使用CRC64算法来进行数据校验，但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能\n\n###### 备份\n​\t先通过config\n​\tget dir  查询rdb文件的目录 \n\n###### 恢复\n​\t关闭Redis\n​\t先把备份的文件拷贝到工作目录下\n​\t启动Redis备份数据会直接加载\t \n\n###### 优点\n\n​\t节省磁盘空间,恢复速度快.\n\n###### 缺点\n\n•虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。\n•在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。\n\n### AOF\n\n以日志的形式来记录每个写操作，将Redis执行过的所有写指令记录下来(读操作不记录)，只许追加文件但不可以改写文件，Redis启动之初会读取该文件重新构建数据，换言之，Redis重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。\nAOF默认不开启，需要手动在配置文件中配置.\n可以在redis.conf中配置文件名称，默认为 appendonly.aof \n![F1vUyV.png](https://s1.ax1x.com/2018/12/07/F1vUyV.png)\nAOF文件的保存路径，同RDB的路径一致。\n\n#### 文件故障备份\n\nAOF的备份机制和性能虽然和RDB不同,但是备份和恢复的操作同RDB一样，都是拷贝备份文件，需要恢复时再拷贝到Redis工作目录下，启动系统即加载。\n\n#### AOF文件故障恢复\nAOF文件的保存路径，同RDB的路径一致。\n如遇到AOF文件损坏，可通过   redis-check-aof  --fix  appendonly.aof   进行恢复\n\n#### 同步频率\n\n始终同步，每次Redis的写入都会立刻记入日志\n每秒同步，每秒记入日志一次，如果宕机，本秒的数据可能丢失。\n把不主动进行同步，把同步时机交给操作系统。\n![F1xttH.png](https://s1.ax1x.com/2018/12/07/F1xttH.png)\n\n#### Rewrite\n\nAOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制,当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩，只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof\n\n#### 重写实现\n\nAOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，遍历新进程的内存中数据，每条记录有一条的Set语句。重写aof文件的操作，并没有读取旧的aof文件，而是将整个内存中的数据库内容用命令的方式重写了一个新的aof文件，这点和快照有点类似。\n重写虽然可以节约大量磁盘空间，减少恢复时间。但是每次重写还是有一定的负担的，因此设定Redis要满足一定条件才会进行重写。\n\n![F1x7EF.png](https://s1.ax1x.com/2018/12/07/F1x7EF.png)\n\n系统载入时或者上次重写完毕时，Redis会记录此时AOF大小，设为base_size,如果Redis的AOF当前大小>=\nbase_size+base_size*100%\n(默认)且当前大小>=64mb(默认)的情况下，Redis会对AOF进行重写。\n\n#### 优点\n\n备份机制更稳健，丢失数据概率更低。\n可读的日志文本，通过操作AOF稳健，可以处理误操作。\n\n![F1zP4H.png](https://s1.ax1x.com/2018/12/07/F1zP4H.png)\n\n#### 缺点\n\n比起RDB占用更多的磁盘空间。\n恢复备份速度要慢。\n每次读写都同步的话，有一定的性能压力。\n存在个别Bug，造成恢复不能。\n\n### 选择\n\n对数据不敏感，可以选单独用RDB\n\n•不建议单独用 AOF，因为可能会出现Bug。\n\n•如果只是做纯内存缓存，可以都不用。\n\n### 优先级\n\nAOF和RDB同时开启，系统默认取AOF的数据\n\n\n","tags":["Redis"],"categories":["数据库"]},{"title":"Redis事务","url":"/2018/12/07/Redis事务/","content":"\n {{ \"Redis的事务和秒杀场景设计\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### Redis事务\n\n![F1oW5V.md.png](https://s1.ax1x.com/2018/12/07/F1oW5V.md.png)\n\nRedis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断,Redis事务的主要作用就是串联多个命令防止别的命令插队\n\n#### Multi\n\n用于标记事务块的开始。Redis会将后续的命令逐个放入队列中，然后才能使用EXEC命令原子化地执行这个命令序列。\n\n这个命令的运行格式如下所示：\n\n#### MULTI\n\n这个命令的返回值是一个简单的字符串，总是OK。\n\n####  EXEC\n\n在一个事务中执行所有先前放入队列的命令，然后恢复正常的连接状态。\n\n当使用WATCH命令时，只有当受监控的键没有被修改时，EXEC命令才会执行事务中的命令，这种方式利用了检查再设置（CAS）的机制。\n\n这个命令的运行格式如下所示：\n\nEXEC\n\n这个命令的返回值是一个数组，其中的每个元素分别是原子化事务中的每个命令的返回值。 当使用WATCH命令时，如果事务执行中止，那么EXEC命令就会返回一个Null值。\n\n####  DISCARD\n\n清除所有先前在一个事务中放入队列的命令，然后恢复正常的连接状态。\n\n如果使用了WATCH命令，那么DISCARD命令就会将当前连接监控的所有键取消监控。\n\n这个命令的运行格式如下所示：\n\n```\nDISCARD\n```\n\n这个命令的返回值是一个简单的字符串，总是OK。\n\n#### WATCH\n\n当某个事务需要按条件执行时，就要使用这个命令将给定的键设置为受监控的。\n\n这个命令的运行格式如下所示：\n\n```\nWATCH key [key ...]\n```\n\n这个命令的返回值是一个简单的字符串，总是OK。\n\n对于每个键来说，时间复杂度总是O(1)。\n\n####  UNWATCH\n\n清除所有先前为一个事务监控的键。\n\n如果你调用了EXEC或DISCARD命令，那么就不需要手动调用UNWATCH命令。\n\n这个命令的运行格式如下所示：\n\n```\nUNWATCH\n```\n\n这个命令的返回值是一个简单的字符串，总是OK。\n\n时间复杂度总是O(1)。\n\n#### 总结\n\n\n从输入Multi命令开始，输入的命令都会依次进入命令队列中，但不会执行，至到输入Exec后，Redis会将之前的命令队列中的命令依次执行。\t\n\n![F1oTKJ.md.png](https://s1.ax1x.com/2018/12/07/F1oTKJ.md.png)\n\n组队的过程中可以通过discard来放弃组队。 \n\n### 事务的错误处理\n\n如果执行阶段某个命令报出了错误，则只有报错的命令不会被执行，而其他的命令都会执行，不会回滚。\n\n![F1T9rd.md.png](https://s1.ax1x.com/2018/12/07/F1T9rd.md.png)\n\n### Redis 事务的三特性\n\n#### 单独的隔离操作\n\n事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断\n\n#### 没有隔离级别的概念\n\n队列中的命令没有提交之前都不会实际的被执行，因为事务提交前任何指令都不会被实际执行，也就不存在“事务内的查询要看到事务里的更新，在事务外查询不能看到”这个让人万分头痛的问题 \n\n#### 不保证原子性\n\nRedis同一个事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 \n\n\n\n### 为什么要做成事务\n\n应用场景:抢购\n\n两个请求\n\n一个请求想给金额减8000\n\n一个请求想给金额减5000\n\n一个请求想给金额减1000\n\n![F1TKqs.md.png](https://s1.ax1x.com/2018/12/07/F1TKqs.md.png)\n\n这样的话即使在用户卡里没有金额的情况下仍旧可以进行支付。为了解决这个问题我们在Redis中使用到了锁的机制\n\n#### 悲观锁\n\n悲观锁(Pessimistic Lock), 顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。\n\n![F1HmHs.md.png](https://s1.ax1x.com/2018/12/07/F1HmHs.md.png)\n\n#### 乐观锁\n\n乐观锁(Optimistic Lock), 顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis就是利用这种check-and-set机制实现事务的。\n\n![F1HUER.md.png](https://s1.ax1x.com/2018/12/07/F1HUER.md.png)\n\n#### Rediss事务 -------秒杀\n\n 解决计数器和人员记录的事务操作\n\n![F1HyKe.md.png](https://s1.ax1x.com/2018/12/07/F1HyKe.md.png)\n\n#### 超卖问题\n\n![F1HRUI.md.png](https://s1.ax1x.com/2018/12/07/F1HRUI.md.png)\n\n#### 乐观锁\n\n![F1HvGV.md.png](https://s1.ax1x.com/2018/12/07/F1HvGV.md.png)\n\n#### lua脚本\n\n![F1HzxU.png](https://s1.ax1x.com/2018/12/07/F1HzxU.png)","tags":["Redis"],"categories":["数据库"]},{"title":"Memcached","url":"/2018/12/04/memcached/","content":"\n {{ \"memcached是一个高性能的分布式内存对象缓存系统\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n#### 简介\n\nMemcached 是一个高性能的分布式内存对象缓存系统，用于动态Web应用以减轻数据库负载。它通过在内存中缓存数据和对象来减少读取数据库的次数，从而提高动态、数据库驱动网站的速度。Memcached基于一个存储键/值对的hashmap。其守护进程（daemon ）是用C写的，但是客户端可以用任何语言来编写，并通过memcached协议与守护进程通信。\n\n#### 为什么用Memcached\n\n网站的高并发读写需求,传统的数据库开始出现瓶颈\n\n##### 单机Mysql时代  \n\n在90年代，一个网站的访问量一般都不大，用单个数据库完全可以轻松应付。在那个时候，更多的都是静态网页，动态交互类型的网站不多。\n\n![FQkOFU.md.png](https://s1.ax1x.com/2018/12/04/FQkOFU.md.png)\n\n90年代的架构遇到的问题\n\n1.数据量的总大小 一个机器放不下时\n2.数据的索引（B+ Tree）一个机器的内存放不下时\n3.访问量(读写混合)一个实例不能承受\n\n##### Memcached(缓存)+MySQL+垂直拆分\n\n随着访问量的上升，几乎大部分使用MySQL架构的网站在数据库上都开始出现了性能问题，web程序不再仅仅专注在功能上，同时也在追求性能。程序员们开始大量的使用缓存技术来缓解数据库的压力，优化数据库的结构和索引。开始比较流行的是通过文件缓存来缓解数据库压力，但是当访问量继续增大的时候，多台web机器通过文件缓存不能共享，大量的小文件缓存也带了了比较高的IO压力。在这个时候，Memcached出现了。\n\n![FQAQTf.md.png](https://s1.ax1x.com/2018/12/04/FQAQTf.md.png)\n\nMemcached作为一个独立的分布式的缓存服务器，为多个web服务器提供了一个共享的高性能缓存服务，在Memcached服务器上，又发展了根据hash算法来进行多台Memcached缓存服务的扩展，然后又出现了一致性hash来解决增加或减少缓存服务器导致重新hash带来的大量缓存失效的弊端\n\n##### Mysql主从读写分离\n\n由于数据库的写入压力增加，Memcached只能缓解数据库的读取压力。读写集中在一个数据库上让数据库不堪重负，大部分网站开始使用主从复制技术来达到读写分离，以提高读写性能和读库的可扩展性。Mysql的master-slave模式成为这个时候的网站标配了。\n\n![FQAgXR.md.png](https://s1.ax1x.com/2018/12/04/FQAgXR.md.png)\n\n#####  分表分库+水平拆分+mysql集群\n\n在Memcached的高速缓存，MySQL的主从复制，读写分离的基础之上，这时MySQL主库的写压力开始出现瓶颈，而数据量的持续猛增，由于MyISAM使用表锁，在高并发下会出现严重的锁问题，大量的高并发MySQL应用开始使用InnoDB引擎代替MyISAM。\n\n 同时，开始流行使用分表分库来缓解写压力和数据增长的扩展问题。这个时候，分表分库成了一个热门技术，是面试的热门问题也是业界讨论的热门技术问题。也就在这个时候，MySQL推出了还不太稳定的表分区，这也给技术实力一般的公司带来了希望。虽然MySQL推出了MySQL Cluster集群，但性能也不能很好满足互联网的要求，只是在高可靠性上提供了非常大的保证。\n\n![FQAf76.md.png](https://s1.ax1x.com/2018/12/04/FQAf76.md.png)\n\n##### Mysql的拓展性能\n\nMySQL数据库也经常存储一些大文本字段，导致数据库表非常的大，在做数据库恢复的时候就导致非常的慢，不容易快速恢复数据库。比如1000万4KB大小的文本就接近40GB的大小，如果能把这些数据从MySQL省去，MySQL将变得非常的小。关系数据库很强大，但是它并不能很好的应付所有的应用场景。MySQL的扩展性差（需要复杂的技术来实现），大数据下IO压力大，表结构更改困难，正是当前使用MySQL的开发人员面临的问题。\n\n##### 今天的架构\n\n![FQAxN8.md.png](https://s1.ax1x.com/2018/12/04/FQAxN8.md.png)\n\n\n\n##### 解答\n\n多数的Web数据应用都保存到了关系型数据库中,如Mysq,Web服务器从中读取数据并在浏览器中显示.但是随着数据量的增大,访问的集中,关系型数据库性能出现瓶颈,响应速度慢导致网站打开延迟等问题,所以Memcached的主要的目的是**通过自身内存中缓存关系型数据库查询的查询结果,减少数据库自身被访问的次数,以提高动态Web应用速度,提高网站架构的并发能力和拓展属性**\n\n通过事先规划好的系统内存空间中临时缓存数据库中的各种数据,已达到减少前端业务服务对关系型数据库的直接高并发访问,从而达到提升大规模数据集群中动态服务的并发访问能力.\n\nWeb服务器读取数据时先读Memcached服务器,如果Memcached没有.则向数据库请求数据.然后Web再报请求到的数据发送到Memcached.\n\n![FQmUot.png](https://s1.ax1x.com/2018/12/04/FQmUot.png)\n\n\n\n#### Memcached 特征\n\n##### 协议简单\n\n因此，通过telnet也能在memcached上保存数据、取得数据。下面是例子。\n\n```\n$ telnet localhost 11211\nTrying 127.0.0.1\nConnected to localhost.localdomain (127.0.0.1).\nEscape character is '^]'.\nset foo 0 0 3 （保存命令）\nbar （数据）\nSTORED （结果）\nget foo （取得命令）\nVALUE foo 0 3 （数据）\nbar （数据）\n```\n\n##### 事件处理\n\n```\nlibevent是个程序库，它将Linux的epoll、BSD类操作系统的kqueue等事件处理功能封装成统一的接口。即使对服务器的连接数增加，也能发挥O(1)的性能。memcached使用这个libevent库，因此能在Linux、BSD、Solaris等操作系统上发挥其高性能。关于事件处理，可以参考Dan Kegel的The C10K Problem。\n```\n\n##### 内存存储方式\n\n```\n为了提高性能，memcached中保存的数据都存储在memcached内置的内存存储空间中。由于数据仅存在于内存中，因此重启memcached、重启操作系统会导致全部数据消失。另外，内容容量达到指定值之后，就基于LRU(Least Recently Used)算法自动删除不使用的缓存。memcached本身是为缓存而设计的服务器，因此并没有过多考虑数据的永久性问题。\n```\n\n##### Memcached 不互通信的分布式\n\n```\nmemcached尽管是“分布式”缓存服务器，但服务器端并没有分布式功能。各个memcached不会互相通信以共享信息。那么，怎样进行分布式呢？这完全取决于客户端的实现。本文也将介绍memcached的分布式。\n```\n\n![FQl3SP.png](https://s1.ax1x.com/2018/12/04/FQl3SP.png)\n\n​\t\t\tmemcached的分布式\n\n#### memcached的内存存储\n\n最近的memcached默认的情况下采用了名为Slab Allocation的机制分配,在该机制出现之前,内存的分配通过对所有记录进行简单的malloc和free来进行的.但是这种方式会导致内存碎片,家中操作系统内存管理的负担,在最坏的情况下会导致操作系统memcached进程,Slab Allocation就是为了解决这个问题的\n\n##### 原理\n\n将分配的内存分割成各种尺寸的块（chunk）， 并把尺寸相同的块分成组（chunk的集合），每个chunk集合被称为slab。\n\n![FQlgw4.png](https://s1.ax1x.com/2018/12/04/FQlgw4.png)\n\nSlab Allocation可以重复使用已分配的内存的目的.分配到的内存不会被释放,而是被重复利用.\n\n##### 主要术语\n\n###### Page \n\n分配给Slab的内存空间,默认是1MB.会分配给Slab之后跟库slab的大小切分成chunk.\n\n###### Chunk\n\n用于缓存记录的内存空间\n\n###### Slab Class\n\n特定大小的chunk的组\n\n##### 选择存储记录的组的方法\n\n![FQ1y9I.png](https://s1.ax1x.com/2018/12/04/FQ1y9I.png)\n\n\n\n######  缺点\n\n由于分配的时特定长度的内存,因此无法又想利用分配的内存.比如将100字节的数据缓存到128字节的chunk中,剩余的28字节就浪费掉了\n\n对于该问题目前还没有完美的解决方案,但在文档中记载了比较有效的解决方案。\n\nThe most efficient way to reduce the waste is to use a list of size classes that closely matches (if that's at all possible) common sizes of objects that the clients of this particular installation of memcached are likely to store.\n\n就是说,如果预先知道客户端发送的数据的公用大小,或者仅缓存大小相同的数据的情况下, 只要使用适合数据大小的组的列表,就可以减少浪费。\n\n但是很遗憾,现在还不能进行任何调优,只能期待以后的版本了。 但是,我们可以调节slab class的大小的差别。 接下来说明growth factor选项。\n\n使用Growth Factor进行调优\n\nmemcached在启动时指定 Growth Factor因子(通过-f选项), 就可以在某种程度上控制slab之间的差异。默认值为1.25。 但是,在该选项出现之前,这个因子曾经固定为2,称为“powers of 2”策略。\n\n![FQ5HRH.png](https://s1.ax1x.com/2018/12/04/FQ5HRH.png)\n\n可以看到从从1字节开始组的大小增大为原来的2倍,这样设置的问题是slab之间的差别比较大,有些情况下就比较浪费内存,因此,为了减少内存浪费,两年前追加了growth facthor这个选项\n\n![FQIuYF.png](https://s1.ax1x.com/2018/12/04/FQIuYF.png)\n\n从图中可见,组间的差距比2时小的多,更合适缓存几百字节的记录,从上面的输出结果来看,可能会有些计算误差.这些误差是为了保持字节数的对齐故意设置的.\n\n将memcached引入产品,或是直接使用默认值进行部署时,最好是重新计算一下数据的预期平均长度,调整growth factor以获得恰当的长度,避免内存的浪费.\n\n##### 查看memcached的内部状态\n\n首先开启memcached服务 \n\n[![FlpDjP.md.png](https://s1.ax1x.com/2018/12/05/FlpDjP.md.png)](https://imgchr.com/i/FlpDjP)\n\n其次连接服务\n\n![FlpyB8.png](https://s1.ax1x.com/2018/12/05/FlpyB8.png)\n\n![Flp6HS.png](https://s1.ax1x.com/2018/12/05/Flp6HS.png)\n\n详细的信息可以参考memcached软件包内的protocol.txt\n\n##### 查看slabs的使用状况\n\n![Flp2NQ.png](https://s1.ax1x.com/2018/12/05/Flp2NQ.png)\n\n参数的含义\n\n| 列         | 含义                                      |\n| ---------- | ----------------------------------------- |\n| #          | slab class编号                            |\n| Item_Size  | Chunk大小                                 |\n| Max_age    | LRU内最旧的记录生存时间                   |\n| Pages      | 分配给Slab的页数                          |\n| Count      | Slab内的记录数                            |\n| Full?      | Slab内是否含有空闲chunk                   |\n| Evicted    | 从LRU中移除未过期item的次数               |\n| Evict_Time | 最后被移除缓存的时间，0表示当前就有被移除 |\n| OOM        | M参数                                     |\n\n##### memcached的分布式\n\n下面假设memcached有三台服务器node1-node3,保存的键名是\"tokyo\",“kanagawa”,“chiba”,“saitma”,“gunma”\n\n[![FlpRhj.md.png](https://s1.ax1x.com/2018/12/05/FlpRhj.md.png)](https://imgchr.com/i/FlpRhj)\n\n首先向memcached中添加“tokyo”,将“tokyo”传给客户端程序库后,客户端实现的算法就会根据\"键\"来决定保存数据的memcached服务器,服务器选定之后,命令他保存\"totyo\"及其值.同样的\"kanagawa\",“chiba”,“saitma”,“gunma”都是先选择服务器再保存的.\n\n![Flka4S.png](https://s1.ax1x.com/2018/12/05/Flka4S.png)\n\n接下来获取保存的数据,获取时也要获取的键\"tokyo\"传递给函数库.函数库通过与数据库保存相同的算法,根据\"键\"选择服务器.使用的算法相同,就能选中与保存相同的服务器,然后发送get命令.只要数据没有因为某些原因被删除,就能获得保存的值.\n\n![Flk4gJ.png](https://s1.ax1x.com/2018/12/05/Flk4gJ.png)\n这样不同的键保存到不同的不同的不同的服务器上,就实现memcached的分布式,memcached服务器增多后,键就会分散,即使一台memcached服务器发生故障无法链接,也不会影响其他的缓存,系统依旧能够继续运行.\n\n###### 算法实现\n\n​\t首先求的字符串的[CRC](https://metacpan.org/release/String-CRC32)根据该值处于服务器节点数目得到的余数决定服务器,当选择的服务器无法连接的时候,Cache:Memcached将会链接次数添加到键之后,再次计算哈希值并尝试链接.这个动作叫做rehash.\n\n```perl\nuse strict;\nuse warnings;\nuse String::CRC32;\nmy @nodes = @ARGV;\nmy @keys = (’a’..’z');\nmy %nodes;\nforeach my $key ( @keys ) {\nmy $hash = crc32($key);\nmy $mod = $hash % ( $#nodes + 1 );\nmy $server = $nodes[ $mod ];\npush @{ $nodes{ $server } }, $key;\n}\nforeach my $node ( sort keys %nodes ) {\nprintf “%s: %s\\n”, $node, join “,”, @{ $nodes{$node} };\n```\n\n```\n执行结果\ntokyo       => node2\nkanagawa => node3\nchiba       => node2\nsaitama   =>node1\ngunma     =>node1\n```\n\n###### 求余算法缺点\n\n添加或移除服务器时，缓存重组的代价相当巨大。 添加服务器后，余数就会产生巨变，这样就无法获取与保存时相同的服务器， 从而影响缓存的命中率。\n\n###### Consistent Hashing算法\n\n```\n 1)  首先求出memcached服务器（节点）的哈希值，并将其配置到0～232的圆（continuum）上。\n\n 2)  然后用同样的方法求出存储数据的键的哈希值，并映射到圆上。 \n\n 3)  然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过232仍然找不到服务器，就会保存到第一台memcached服务器上。\n```\n\n\n\n![FlErT0.png](https://s1.ax1x.com/2018/12/05/FlErT0.png)\n\n\n\nConsistent Hashing：添加服务器\n\n![FlE6YT.png](https://s1.ax1x.com/2018/12/05/FlE6YT.png)\n\n参考博客:https://blog.csdn.net/hguisu/article/details/7353551","tags":["Memcached"],"categories":["数据库"]},{"title":"Redis","url":"/2018/12/03/Redis/","content":"\n {{ \"Redis内存数据库的介绍\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Nosql简介\n\nNoSQL(NoSQL = Not Only SQL )，意即“不仅仅是SQL”，泛指非关系型的数据库。随着互联网web2.0网站的兴起，传统的关系数据库在应付web2.0网站，特别是超大规模和高并发的SNS类型的web2.0纯动态网站已经显得力不从心，暴露了很多难以克服的问题，而非关系型的数据库则由于其本身的特点得到了非常迅速的发展。NoSQL数据库的产生就是为了解决大规模数据集合多重数据种类带来的挑战，尤其是大数据应用难题，包括超大规模数据的存储。\n\n例如谷歌或Facebook每天为他们的用户收集万亿比特的数据。这些类型的数据存储不需要固定的模式，无需多余操作就可以横向扩展。\n\n## Redis简介\n\n### 历史与发展\n\n2008年，意大利的一家创业公司Merzia推出了一款基于MySQL的网站实时统计系统LLOOGG，然而没过多久，公司的创始人开始对MySQL的性能感到失望，决定自己为该系统量身定做一个数据库；量身定做的数据库系统于2009年开发完成，名叫Redis（Remote Dictionary Server，远程字典服务）; Redis创始者还是很有开源精神的，它不仅仅想让Redis应用在他们公司的网站实时统计系统中，还希望更多的人使用它;在2009年将Redis开源发布，代码托管在Github上; Redis在短短几年就拥有了庞大的用户群体，Hacker News在2012年发布了一份数据库的使用情况调查，结果显示12%的公司在使用Redis； 国内的新浪微博，知乎,街旁; 国外的Github、Stack Overflow 、Flickr、暴雪和Instagram,都是Redis的用户\n\nredis是一个key-value存储系统。和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set -- 有序集合)和hash（哈希类型）。这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。在此基础上，redis支持各种不同方式的排序。与memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。\nRedis 是一个高性能的key-value数据库。 redis的出现，很大程度补偿了memcached这类key/value存储的不足，在部分场合可以对关系数据库起到很好的补充作用。它提供了Java C/C++ C# PHP JavaScript Perl Object-C，Python，Ruby，Erlang等客户端，使用很方便。 \nRedis支持主从同步。数据可以从主服务器向任意数量的从服务器上同步，从服务器可以是关联其他从服务器的主服务器。这使得Redis可执行单层树复制。存盘可以有意无意的对数据进行写操作。由于完全实现了发布/订阅机制，使得从数据库在任何地方同步树时，可订阅一个频道并接收主服务器完整的消息发布记录。同步对读取操作的可扩展性和数据冗余很有帮助。\n[官网](https://redis.io/)\n\n### 特性\n\n#### 存储结构\n\n1. 字符串类型(sting)\tRedis基本类型，一个key对应一个value，value最多可以是512M；String是二进制安全的（即redis的String可以包含任何数据）\n2. 散列类型 (hash)    一个键值对集合，String类型的field和value映射表，类似Java里面的Map<String,Object>。\n3. 列表类型(list)   字符串列表，按照插入顺序排序，底层实现是个链表。\n4. 集合类型(set)  String类型的无序集合，且不允许重复的成员，通过Hashtable实现\n5. 有序集合类型(zset)   ：在Set的基础上，不同的是每个元素都会关联一个double类型的分数，通过分数来为集合中的成员进行从小到大的排序。Zset的成员是唯一的，但分数（score）却可以重复\n\n#### 常用命令\n\n[基础命令链接](http://www.runoob.com/redis/redis-commands.html)\n\n#### key的其他命令\n\n```\n keys *\n exists key的名字，判断某个key是否存在\n move key db   --->当前库就没有了，被移除了\n expire key 秒钟：为给定的key设置过期时间\n ttl key 查看还有多少秒过期，-1表示永不过期，-2表示已过期\n type key 查看你的key是什么类型\n```\n\n###### String其他命令\n\n```\n单值单value\nset/get/del/append/strlen\nIncr/decr/incrby/decrby,一定要是数字才能进行加减\ngetrange/setrange\ngetrange:获取指定区间范围内的值，类似between......and的关系   从零到负一表示全部\nsetrange设置指定区间范围内的值，格式是setrange key值 具体值\nsetex(set with expire)键秒值/setnx(set if not exist)  \n\t\t\t\tsetex:设置带过期时间的key，动态设置。\n\t\t\t\tsetnx:只有在 key 不存在时设置 key 的值。\n mset/mget/msetnx\t\t\n \t\t\t\tmset:同时设置一个或多个 key-value 对。  \n \t\t\t\tmget:获取所有(一个或多个)给定 key 的值。\n\t\t\t\tmsetnx:同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。\n getset(先get再set)\n \t\t\t\tgetset:将给定 key 的值设为 value ，并返回 key 的旧值(old value)。\n\t\t\t\t简单一句话，先get然后立即set\n \t\t\t\t\t\t\n```\n\n###### List其他命令\n\n```\n单值多value\nlpush/rpush/lrange\nlpop/rpop  左删除 右删除\nlindex，按照索引下标获得元素(从上到下)  通过索引获取列表中的元素 lindex key index\nllen\nlrem key 数字N 给定值v1    解释(删除N个值等于v1的元素)  \nltrim key 开始index 结束index，截取指定范围的值后再赋值给key\n\t\t\t\tltrim：截取指定索引区间的元素，格式是ltrim list的key 起始索引 结束索引\nrpoplpush 源列表 目的列表\n\t\t\t\t移除列表的最后一个元素，并将该元素添加到另一个列表并返回\nlset key index value\t\t\t\n linsert key  before/after 已有值 插入的新值\n \t\t\t\t在list某个已有值的前后再添加具体值\n \t\t\t\t\n它是一个字符串链表，left、right都可以插入添加；\n如果键不存在，创建新的链表；\n如果键已存在，新增内容；\n如果值全移除，对应的键也就消失了。\n它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。\n```\n\n[![FMDtrd.md.png](https://s1.ax1x.com/2018/12/03/FMDtrd.md.png)](https://imgchr.com/i/FMDtrd)\n\n###### Set其他命令\n\n```\n单值多value\nsadd/smembers/sismember\nscard，获取集合里面的元素个数\nsrem key value 删除集合中元素\nsrandmember key 某个整数(随机出几个数)\nspop key 随机出栈\nsmove key1 key2 在key1里某个值      作用是将key1里已存在的某个值赋给key2\n\n\t\t\t\t差集：sdiff 在第一个set里面而不在后面任何一个set里面的项\n数学集合类\t\t 交集：sinter\n\t\t\t\t并集：sunion\n\n```\n\n\n\n###### Hash其他命令\n\n```\nKV模式不变，但V是一个键值对\n\nhset/hget/hmset/hmget/hgetall/hdel\nhlen\nhexists key 在key里面的某个值的key\nhkeys/hvals\nhincrby/hincrbyfloat\nhsetnx  \t\t不存在赋值，存在了无效。\n```\n\n######  Zset其他命令\n\n```\n在set基础上，加一个score值。\n\t\t之前set是k1    v1 v2 v3，\n\t\t现在zset是k1 score1 v1 score2 v2\nzrangebyscore key 开始score 结束score\nzrem key 某score下对应的value值，作用是删除元素\n\t\t\t\t删除元素，格式是zrem zset的key 项的值，项的值可以是多个\n\t\t\t\tzrem key score某个对应值，可以是多个值\nzcard key获得几个元素\n\t\t\t\tzcard ：获取集合中元素个数\n\t\t\t\tzrank： 获取value在zset中的下标位置\nZcount key score区间\n\t\t\t\tzscore：按照值获得对应的分数\n\t\t\t\t正序、逆序获得下标索引值\nZrank key values值，作用是获得下标值\nZscore key 对应值,获得分数\nzrevrank key values值，作用是逆序获得下标值    正序、逆序获得下标索引值\nzrevrange\nzrevrangebyscore  key 结束score 开始score\n```\n\n#### 内存存储与持久化\n\nRedis数据库中所有数据都存储在内存中.由于内存的读写速度快于硬盘,因此Redis在性能上对比其他基于硬盘存储的数据库有非常明显的优势,在一台普通的笔记本电脑上,可以在一秒内读取超过十万个键值.\n\n```shell\nredis-benchmark -q -n 100000 #在1核1Gcentos6.10上测试的Redis的吞吐量\n```\n\n[![Flca3q.png](https://s1.ax1x.com/2018/12/05/Flca3q.png)](https://imgchr.com/i/Flca3q)\n\n####  功能丰富\n\nRedis虽然做为数据库开发的,但是由于为其提供了丰富的功能,越来越多的人将其作用缓存.队列系统,Redis可谓是名副其实的多面手.\n\nRedis可以为每个键设置生存时间(Time to Live,TTL)生存时间到期后键会自动被删除,这一功能配合出色的性能让Redis可以作为缓存系统来使用,而且Reids支持持久化和丰富的数据类型.\n\nRedis是单线程模型,Memcached是多线程模型.多和服务器上Memcached的性能更高一些,但是Redis的性能已经足够优越,在绝大部分的场景下,其性能都不会成为瓶颈.所以在使用是应该个更加关心两者在功能上的区别如果是如到了*高级的数据类型*或者是持久化等功能,Redis将会是Memcached很好的替代品.\n\n作为缓存系统,Redis可以限定数据占用的最大的内存空间,在数据达到空间先之后,可以按照一定的规则自动淘汰不需要的键,\n\nRedis的列表类型键可以用来实现队列,并且支持阻塞式读取,可以很容易的实现一个高性能的优先级队列,同时在更高舱面上,Redis还可支持\"发布/订阅\"的消息模式,可以基于次构建聊天室系统.\n\n除基本的会话token之外，Redis还提供很简便的FPC平台。回到一致性问题，即使重启了Redis实例，因为有磁盘的持久化，用户也不会看到页面加载速度的下降，这是一个极大改进，类似PHP本地FPC。\n\n再次以Magento为例，Magento提供一个插件来使用Redis作为[全页缓存后端](https://github.com/colinmollenhour/Cm_Cache_Backend_Redis)。\n\n此外，对WordPress的用户来说，Pantheon有一个非常好的插件  [wp-redis](https://wordpress.org/plugins/wp-redis/)，这个插件能帮助你以最快速度加载你曾浏览过的页面。\n\nRedis在内存中对数字进行递增或递减的操作实现的非常好。集合（Set）和有序集合（Sorted Set）也使得我们在执行这些操作的时候变的非常简单，Redis只是正好提供了这两种数据结构。所以，我们要从排序集合中获取到排名最靠前的10个用户–我们称之为“user_scores”，我们只需要像下面一样执行即可：\n\n当然，这是假定你是根据你用户的分数做递增的排序。如果你想返回用户及用户的分数，你需要这样执行：\n\nZRANGE user_scores 0 10 WITHSCORES\n\n#### 简单稳定\n\nRedis使用命令来读写数据,命令语句之于Redis就相当于SQL语言之对于关系数据库,列入在关系数据库中想要获取posts表内的id为1的记录的title字段的值可以使用如下SQL语句来实现。\n\n```sql\nSELECT title FROM posts WHERE id=1 LIMIT=1  #SQL语句\n```\n\n```redis\nHGET post:1 title\n```\n\nRedis 提供了几十种不同编程语言的客户端库，这些库都很好的封装了Redis的命令，是的在程序中与Redis进行交互变得更加容易。有些库还提供了可以将编程语言的数据类型直接以对应的形式存储到Redis中（如将数组直接以列表类型存到Redis）的简单方法，使用起来非常方便。\n\n同时Redis使用C语言开发，代码量只有3万多行。这降低了用户通过Redis源代码来使之更加适合自己项目需要的门槛。对于希望“榨干”数据库性能开发者而，这无疑是一个很大的吸引力。因此Redis的开发不仅仅是Salvatore Sanfilippo 和*Pieter* *Noordhuis* 有近100名开发者为Redis贡献了代码。良好的开发分为和严谨的发布机制使得Redis的版本非常可靠，如次多的公司在项目中使用后Redis也可以印证这以一点。\n\n#### memcached与Redis比较\n\n| 数据库    | CPU      | 内存使用率             | 持久性                   | 数据结构 | 工作环境      |\n| --------- | -------- | ---------------------- | ------------------------ | -------- | ------------- |\n| memcached | 支持多核 | 高                     | 无                       | 简单     | Linux/Windos  |\n| Redis     | 单核     | 低（压缩比memcache高） | 有（硬盘存储，主从同步） | 复杂     | 推荐使用Linux |\n\n\n\n","tags":["Redis"],"categories":["数据库"]},{"title":"Hive","url":"/2018/12/03/Hive/","content":"\n {{ \"Hive的基本概念\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Hive 简介\n\nhive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。 Hive是建立在 Hadoop 上的数据仓库基础构架。它提供了一系列的工具，可以用来进行数据提取转化加载（ETL），这是一种可以存储、查询和分析存储在 Hadoop 中的大规模数据的机制。Hive 定义了简单的类 SQL 查询语言，称为 HQL，它允许熟悉 SQL 的用户查询数据。同时，这个语言也允许熟悉 MapReduce 开发者的开发自定义的 mapper 和 reducer 来处理内建的 mapper 和 reducer 无法完成的复杂的分析工作。Hive 没有专门的数据格式。 Hive 可以很好的工作在 Thrift 之上，控制分隔符，也允许用户指定数据格式。\n\n本质是：将HQL转化成MapReduce程序\n\n![kSZmjS.png](https://s2.ax1x.com/2019/01/16/kSZmjS.png)\n\n## Hive的基本架构\n\n![img](https://s1.ax1x.com/2018/11/18/FSUaqS.md.jpg)\n\n| 单元名称           | 功能                                                        |\n| ------------------ | ----------------------------------------------------------- |\n| CLI                | Hive的命令行借口（shell环境）                               |\n| hiveserver2        | 让Hive以提供Thrift服务，允许用不同语言编写的客户端访问。    |\n| Hive Web Interface | Hive的web借口，这个简单的Web可以替代CLI。但新版本已经被废弃 |\n| Metastore          | 用于保存Hive中的元数据的服务模式                            |\n| RDBMS              | 可以使Mysql或Derby（嵌入的数据库）                          |\n| Hive Driver        | 包含Hive编辑器、优化器、执行器。                            |\n\n### Hive的执行引擎\n\nHive的原始设计是以MapReduce作为执行引擎（目前仍然还是）。Hive的执行引擎还包括Apache Tez,对Spark的支持也在开发中，Tez和Spark都是通用有向无环图（DAG）他们比MapReduce更加灵活，性能更加优越。在Mapreduce过程中，中间的作业输出会被“物化”存储到HDFS上，Tez和Spark则不同，他们根据Hive的规划器请求，把中间结果写到本地的磁盘上，甚至在内存中缓存，可以避免额外的复制开销。\n\n**执行引擎由属性hive.execution.engine来控制，默认值是mr（MapReduce）**可以通过下面的语句设置Tez为Hive的执行引擎。\n\n```\n  hive> SET hive.execution.engine=tez;\n```\n\n## Hive服务\n\ncli Hive命令行接口（shell环境）默认\n\nhiveserver2 让Hive以提供Thrift服务的服务器的形式运行，允许不同语言编写的客户端进行访问，hiverserver2支持认证多用户并发，使用Thrift、JDBC和ODBC链接客户端需要运行Hive服务器和Hive进行通信。通过设置hive.server2.thrift.port 配置属性来知名服务器所监听的端口号默认是(10000)\n\nbeeline 以嵌入式的方式工作的Hive命令行接口（类似于常规的CLI），或者使用JDBC连接到HiveServer2进程。\n\nhwi Hive 的Web借口。在没有安装任何客户端软件的情况下，这个简单的Web借口可以代替CLI。另外，Hue是一个更加全面的Web借口，其中包括运行Hive的查询和浏览Hive meatastore的应用程序。\n\n- jar 和 hadoop jar类似  是运行类路径中 包含Hadoop 和Hive类Java应用程序的简便方法。\n- metastore 默认情况下metastore和Hive服务运行在同一个进程里。使用这个服务，可以让meatstore作为一个单独的（远程）进程运行，通过设置**METASTORE_PORT**环境变量(或者使用 -p 的命令来制定服务器监听的端口号默认是**9083**)\n\n```\n### Hive客户端\n\n如果以服务器的方式运行Hive（hive - - server hiveserver2 ）,可以在应用程序中以不同的机制连接到服务器，Hive客户端和服务之间的联系如图所示\n```\n\n## Hive的工作原理\n\n![kSZtjU.png](https://s2.ax1x.com/2019/01/16/kSZtjU.png)\n\n### 查询解析和编译\n\n1. Hive接口如命令行或者Web UI 发送查询驱动程序（任何数据库驱动程序，如JDBC、ODBC等）来执行\n\n2. 在驱动的帮助下查询编译器分析查询检查的语法和查询计划或者查询的要求。\n\n3. 编译器发送元数据请求到MetaStore。\n\n4. MetaStore发送元数据，以编译器响应。\n\n5. 编译器检查要求，重新发送计划给驱动程序。到此为止，查询解析和编译完成。\n\n6. 驱动器发送的执行计划到执行引擎\n\n    #### 内部执行\n\n    1. 在内部执行作业的过程是一个MapReduce工作，Hadoop 2.0采用的就是MapReduce2.0，它是在YARN中运行。\n    2. 与此同时，在执行引擎可以通过Metastore执行元数据操作。\n    3. 执行应请接受来自数据节点的结果。\n    4. 执行引擎发送这些结果值给驱动程序。\n    5. 驱动程序将结果发给Hive接口。\n\n## 架构原理\n\n1．用户接口：Client\n\nCLI（hive shell）、JDBC/ODBC(java访问hive)、WEBUI（浏览器访问hive）\n\n2．元数据：Metastore\n\n元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；\n\n默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore\n\n3．Hadoop\n\n使用HDFS进行存储，使用MapReduce进行计算。\n\n4．驱动器：Driver\n\n（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。\n\n（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。\n\n（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。\n\n（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。\n\n![kSeVa9.png](https://s2.ax1x.com/2019/01/16/kSeVa9.png)\n\nHive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。\n\n## Hive 和传统数据库的比较\n\n| 比较项   | 传统数据库           | HiveQL                                      |\n| -------- | -------------------- | ------------------------------------------- |\n| ANSI SQL | 支持                 | 不完全支持                                  |\n| 更新     | UPDATE\\INSERT\\DELETE | insert OVERWRITE\\INTO TABLE（不建议update） |\n| 事务     | 支持                 | 不支持                                      |\n| 模式     | 写模式               | 读模式                                      |\n| 数据保存 | 块设备、本地文件系统 | HDFS                                        |\n| 延时     | 低                   | 高                                          |\n| 多表插入 | 不支持               | 支持                                        |\n| 子查询   | 完全支持             | 只能用在From子句中                          |\n| 视图     | Updatable            | Read-only                                   |\n| 可扩展性 | 低                   | 高                                          |\n| 数据规模 | 小                   | 大                                          |\n\n## Hive安装\n\n1.把apache-hive-1.2.1-bin.tar.gz上传到linux的/opt/software目录下\n\n```shell\ntar -zxvf apache-hive-1.2.1-bin.tar.gz -C /opt/module/\n```\n\n2.修改apache-hive-1.2.1-bin.tar.gz的名称为hive\n\n```\nmv apache-hive-1.2.1-bin/ hive\n```\n\n3.修改/opt/module/hive/conf目录下的hive-env.sh.template名称为hive-env.sh\n\n```shell\n mv hive-env.sh.template hive-env.sh\n```\n\n4.配置hive-env.sh文件 \n\n```properties\n##配置HADOOP_HOME路径\nexport HADOOP_HOME=/opt/module/hadoop\n### 配置HIVE_CONF_DIR路径\nexport HIVE_CONF_DIR=/opt/module/hive/conf\n```\n\nHadoop集群配置\n\n启动hdfs和yarn\n\n```shell\nstart-dfs.sh\nstart-yarn.sh\n```\n\n创建工作目录\n\n```shell\nhadoop fs -mkdir /tmp\nhadoop fs -mkdir -p /user/hive/warehouse\n```\n\n或者在配置文件中关闭权限检查  在hadoop 的hdfs-site.xml 中\n\n```xml\n<property>\n\t\t<name>dfs.permissions.enable</name>\n\t\t<value>false</value>\n</property>\n```\n\n### MySql安装:\n\n软件链接：https://pan.baidu.com/s/1MMfHnjd8HY41ShbYtvqGZg 提取码：ycwm \n\nMetastore默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore;\n\n1．查看mysql是否安装，如果安装了，卸载mysql\n\n```shell\nrpm -qa|grep mysql\nmysql-libs-5.1.73-7.el6.x86_64\nrpm -e --nodeps mysql-libs-5.1.73-7.el6.x86_64\n```\n\n2.安装Mysql\n\n```shell\nrpm -ivh MySQL-server-5.6.24-1.el6.x86_64.rpm\n```\n\n3.查看随机密码\n\n```shell\ncat /root/.mysql_secret\n```\n\n4.查看Mysql状态\n\n```shell\nservice mysql status\nservice mysql start\n```\n\n5.安装客户端\n\n```shell\nrpm -ivh MySQL-client-5.6.24-1.el6.x86_64.rpm\n```\n\n6.登录mysql\n\n```shell\n mysql -uroot -p你的密码\n```\n\n7.修改密码\n\n```\nmysql>SET PASSWORD=PASSWORD('123456');\n```\n\n### user表配置\n\n```shell\n## 登录mysql\nmysql -uroot -p123456\n\n## 使用mysql数据库\nmysql>use mysql;\n\n## 展示mysql数据库中的所有表\nmysql>show tables;\n\n## 展示user表的结构\nmysql>desc user;\n\n## mysql>select User, Host, Password from user;\nmysql>update user set host='%' where host='localhost';\n\n## 删除root用户的其他host\nmysql>delete from user where Host='daanode1';\nmysql>delete from user where Host='127.0.0.1';\nmysql>delete from user where Host='::1';\n\n## 刷新权限\nmysql>flush privileges;\n\n## 退出\nmysql>quit;\n```\n\n### Hive元数据配置到MySql\n\n```shell\n cp mysql-connector-java-5.1.27-bin.jar  /opt/module/hive/lib/\n```\n\n```xml\n[hadoop@datanode1 conf]$ pwd\n/opt/module/hive/conf\n[hadoop@datanode1 conf]$ vim hive-site.xml\n\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n        <property>\n          <name>javax.jdo.option.ConnectionURL</name>\n          <value>jdbc:mysql://datanode1:3306/metastore?createDatabaseIfNotExist=true</value>\n          <description>JDBC connect string for a JDBC metastore</description>\n        </property>\n\n        <property>\n          <name>javax.jdo.option.ConnectionDriverName</name>\n          <value>com.mysql.jdbc.Driver</value>\n          <description>Driver class name for a JDBC metastore</description>\n        </property>\n\n        <property>\n          <name>javax.jdo.option.ConnectionUserName</name>\n          <value>root</value>\n          <description>username to use against metastore database</description>\n        </property>\n\n        <property>\n          <name>javax.jdo.option.ConnectionPassword</name>\n          <value>123456</value>\n          <description>password to use against metastore database</description>\n        </property>\n</configuration>\n```\n\n配置完毕后，如果启动hive异常，可以重新启动虚拟。\n\n1.连接mysql\n\n```shell\n[hadoop@datanode1 conf]$ mysql -uroot -p\nEnter password:\n```\n\n\n\n```mysql\nmysql> show databases;\n+--------------------+\n| Database           |\n+--------------------+\n| information_schema |\n| metastore          |\n| mysql              |\n| mysqlsource        |\n| oozie              |\n| performance_schema |\n| student            |\n+--------------------+\n7 rows in set (0.00 sec)\n```\n\n## 常见属性配置\n\n###  Hive数据仓库位置配置\n\n​    (1）Default数据仓库的最原始位置是在hdfs上的：/user/hive/warehouse路径下。\n\n​     (2）在仓库目录下，没有对默认的数据库default创建文件夹。如果某张表属于default数据库，直接在数据仓库目录下创建一个文件夹。\n\n​      (3）修改default数据仓库原始位置（将hive-default.xml.template如下配置信息拷贝到hive-site.xml文件中）。\n\n```xml\n<property>\n\t<name>hive.metastore.warehouse.dir</name>\n\t<value>/user/hive/warehouse</value>\n\t<description>location of default database for the warehouse</description>\n</property>\n```\n\n配置同组用户有执行权限\n\n\n```shell\nbin/hdfs dfs -chmod g+w /user/hive/warehouse\n```\n\n### 查询后信息显示配置\n\n(1）在hive-site.xml文件中添加如下配置信息，就可以实现显示当前数据库，以及查询表的头信息配置。\n\n```xml\n<property>\n\t<name>hive.cli.print.header</name>\n\t<value>true</value>\n</property>\n\n<property>\n\t<name>hive.cli.print.current.db</name>\n\t<value>true</value>\n</property>\n```\n\n###  Hive运行日志信息配置\n\n1．Hive的log默认存放在/tmp/hadoop/hive.log目录下（当前用户名下）\n\n2．修改hive的log存放日志到/opt/module/hive/logs\n\n​\t修改 hive-log4j.properties.template  文件名称为  hive-log4j.properties\n\n```shell\n[hadoop@datanode1 conf]$ pwd\n/opt/module/hive/conf\n[hadoop@datanode1 conf]$ mv hive-log4j.properties.template hive-log4j.properties\n[hadoop@datanode1 conf]$ vim hive-log4j.properties\n\n## 设置存储位置为\nhive.log.dir=/opt/module/hive/logs\n```\n\n### 参数配置方式\n\n1．查看当前所有的配置信息\n\n```shell\nhive>set;\n```\n\n2．参数的配置三种方式\n\n​       （1）配置文件方式\n\n默认配置文件：hive-default.xml                      用户自定义配置文件：hive-site.xml\n\n​       注意：用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效。\n\n​\t（2）命令行参数方式\n\n启动Hive时，可以在命令行添加-hiveconf param=value来设定参数仅对本次hive启动有效。\n\n例如：\n\n```shell\n[hadoop@datanode1 hive]$ bin/hive -hiveconf mapred.reduce.tasks=10;\nhive > set mapred.reduce.tasks;\nmapred.reduce.tasks=10\n```\n\n（3）参数声明方式\n\n```properties\nhive> set mapred.reduce.tasks=100;\nhive>  set mapred.reduce.tasks;\nmapred.reduce.tasks=100\n```\n\n上述三种设定方式的优先级依次递增。即配置文件<命令行参数<参数声明。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在会话建立以前已经完成了。\n\n## Hive 命令操作\n\n1.在hive cli命令窗口中如何查看hdfs文件系统\n\n```shell\nhive> dfs -ls /;\nFound 11 items\n```\n\n2.在hive cli命令窗口中如何查看本地文件系统\n\n```shell\nhive> ! ls /opt/module/;\n```\n\n3.查看在hive中输入的所有历史命令\n\n```shell\n[hadoop@datanode1 ~]$ pwd\n/home/hadoop\n[hadoop@datanode1 ~]$  cat .hivehistory\n```\n\n### 交互命令\n\n```shell\nbin/hive -help\nusage: hive\n -d,--define <key=value>          Variable subsitution to apply to hive\n                                  commands. e.g. -d A=B or --define A=B\n    --database <databasename>     Specify the database to use\n -e <quoted-query-string>         SQL from command line\n -f <filename>                    SQL from files\n -H,--help                        Print help information\n    --hiveconf <property=value>   Use value for given property\n    --hivevar <key=value>         Variable subsitution to apply to hive\n                                  commands. e.g. --hivevar A=B\n -i <filename>                    Initialization SQL file\n -S,--silent                      Silent mode in interactive shell\n -v,--verbose                     Verbose mode (echo executed SQL to the console)\n```\n\n1.“-e”不进入hive的交互窗口执行sql语句\n\n```\n[hadoop@datanode1 hive]$ bin/hive -e \"select id from student;\"\n```\n\n2.“-f”执行脚本中sql语句\n\n```shell\n[hadoop@datanode1 datas]$ vim hivef.sql\nselect *from student;\n## 执行\n[hadoop@datanode1 hive]$ bin/hive -f /opt/module/datas/hivef.sql\n## 将结果写入文件中\n[hadoop@datanode1 hive]$ bin/hive -f /opt/module/datas/hivef.sql  > /opt/module/datas/hive_result.txt\n```\n\n\n\n\n\n","tags":["Hive"],"categories":["大数据"]},{"title":"Flume架构","url":"/2018/11/16/Flume/","content":"\n {{ \"Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Flume 介绍\n\n Flume是由cloudera软件公司产出的高可用、高可靠、分布式的海量日志收集系统、聚合和传输的系统、于2009年被捐赠了apache软件基金会，为Hadoop相关组件之一。Flume初始发行版本目前统称为Flume OG，2011年10月在完成了里程碑的改动：重构核心组件、核心配置以及代码架构之后、Flume NG 推出，它是Flume1.X版本的统称。\n\nApache Flume 是一个系统，用于从大量数据产生商那里移动海量数据到存储、索引或者分析数据的系统，它可以将数据的消费者和是生产者解耦，是的在一方不知情的情况下改变另外一方变得很容易，除了解耦还可而已提供故障隔离，并在生产商和存储系统之间添加一个额外的缓冲区，Flume支持在日志系统中定制各类数据的发送方，用于收集数据，如控制台、文件、Thrift—RPC、syslog日志系统等，也支持对数据进行简单的处理，并发送到各种数据接收方，如HDFS、Hbase等。\n\n　备注：Flume参考资料\n\n　　　　官方网站： http://flume.apache.org/\n　　　　用户文档： http://flume.apache.org/FlumeUserGuide.html\n　　　　开发文档： http://flume.apache.org/FlumeDeveloperGuide.html\n\n## Flume 的特点\n\n### 可靠性 \n\n当节点出现故障时，日志能够被传送到其他节点上而不会丢失。Flume提供了三种级别的可靠性保障，从强到弱依次分别为：end-to-end（收到数据agent首先将event写到磁盘上，当数据传送成功后，再删除；如果数据发送失败，可以重新发送。），Store on failure（这也是scribe采用的策略，当数据接收方crash时，将数据写到本地，待恢复后，继续发送），Besteffort（数据发送到接收方后，不会进行确认）。\n\n### flume的可恢复性\n\n靠Channel。推荐使用FileChannel，事件持久化在本地文件系统里(性能较差)。\n\n## Flume 架构\n\n### Client\n\n生产数据，运行在一个独立线程\n\n### Event\n\nFlume将数据表示成Event，数据结构很简单，具有一个主题和一个报头的集合、事件的主题是一个字节数组,通常通过Flume传送的负载。报头被称为一个map，其中有字符串key和字符串value，报头不是用来传输数据的，而是为了路由和跟踪发送事件的优先级和严重性。报头也可以用给事件增加事件ID或者UUUID。每个事件本质上必须是一个独立的记录，而不是记录的一部分。这也要求每个事件要适应Flume Agent JVM的内存，如果使用File Channel 应该有足够的硬盘空间来支持。如果数据不能表示为多个独立记录，Flume可能不适用于这样的案例。\n\n###  Agent\n\nFlume运行的核心是Agent，Agent本身是一个Java进程，也是最小独立运行的单位，运行在日志收集节点Agent是一个完整的数据收集工具，含有三个核心组件，分别是Source、Channel、Sink，Flume基于数据流将进行设计，数据流由事件（Event）贯穿始终。事件作为Flume的基本数据单位，携带日志数据（以字节数组形式）并且携带有头信息，由Agent 外部的数据源如图中的Web Server 生成。\n\n![](https://s1.ax1x.com/2018/11/18/FSpI9f.jpg)\n\n\n\n### Source\n\nSource是从一些生产数据的应用中接收数据的活跃组件。也有自己产生数据的Source，不过通常用于测试目的。Source可以监听一个或者多个网络端口，用于接收数据或者可以从本地文件系统读取数据，每一个Source必须至少链接一个Channel。基于一些标准，一个Source可以写入多个Channel，复制事件到所有或者部分的Channel。flume提供了很多内置的Source， 支持 Avro， log4j， syslog 和 http post(body为json格式)\n\n| Source类型          | 描述                                              |\n| ------------------- | :------------------------------------------------ |\n| Avro                | 支持Avro协议，即Avro RPC,内置支持                 |\n| Thrift              | 支持Thrift协议，内置支持                          |\n| Exec                | 基于Unix的命令在标准输出上产生数据                |\n| JMS                 | 从 JMS系统中读取数据                              |\n| Spooling Directory  | 监控指定目录内数据变化                            |\n| Netcat              | 监控某个端口，将流经端口的文本行数据作为Event输入 |\n| Sequence Gennerator | 序列生产器的数据源，生产序列数据                  |\n| Syslog              | 读取syslog数据，产生Event，支持UDP和TCP协议       |\n| HTTP                | 基于HTTP POST 或者GT 方式的数据源，支持JSON等格式 |\n| Legacy              | 兼容Flume OG 中的Source(0.9 x 版本)               |\n\n#### Avro源\n\n| 属性名            | 默认值 | 描述                                                         |\n| ----------------- | ------ | ------------------------------------------------------------ |\n| **channels**      | -      |                                                              |\n| **type**          | -      | 类型名称Avro                                                 |\n| **bind**          | -      | 需要监听的主机名或者ip                                       |\n| **port**          | -      | 需要监听端口                                                 |\n| threads           | -      | 工作线程最大线程数                                           |\n| selector.type     |        |                                                              |\n| selector.*        |        |                                                              |\n| interceptors      | -      | 空格分隔的拦截器地址                                         |\n| interceptors.*    |        |                                                              |\n| compression-type  | none   | 压缩类型必须和AvroSource值相同                               |\n| ssl               | false  | 是否启用ssl加密同时还要配置keystroe和keystore-password       |\n| keystore          | -      | 为SSL提供java密钥文件所在路径                                |\n| keystore-password | –      | 为SLL提供的java密钥文件 密码                                 |\n| keystore-type     | JKS    | 密钥库类型可以是“JKS\"或者\"PKCS12\"                            |\n| exclude-protocols | SSLv3  | 空格指定分开的列表，用来指定在SSL/TLS协议中排除。SSLv3将总是被排除除了所指定的协议 |\n| ipFilter          | false  | 如果需要为netty开启ip过滤，将此选项设置为true                |\n| ipFilterRules     | –      | 配netty的ip过滤设置表达式规则                                |\n\n示例：\n\n```shell\na1.sources = r1\na1.channels = c1\na1.sources.r1.type = avro\na1.sources.r1.channels = c1\na1.sources.r1.bind = 0.0.0.0\na1.sources.r1.port = 4141\n```\n\n#### exec 源\n\n\n\n| 属性名          | 默认值 | 描述                                       |\n| --------------- | ------ | ------------------------------------------ |\n| **channels**    | -      |                                            |\n| **type**        | -      | 类型名称 exec                              |\n| **command**     | -      | 要执行的命令                               |\n| shell           | -      | 用于执行命令的shell                        |\n| restartThrottle | 1000   | 毫秒为单位，用于声明等待多久后尝试重试命令 |\n| restart         | false  | 如果cmd挂了是否重启cmd                     |\n| logStdErr       | false  | 无论是否是标准错误都应该被记录             |\n| batchSize       | 20     | 同时发送到通道中的最大行数                 |\n| batchTimeout    | 3000   | 如果缓冲区没有满，经过多长时间发送数据     |\n| selector.type   |        | 复制还是多路复制                           |\n| selector.*      |        | 依赖于selector.type的值                    |\n| interceptors    | -      | 空格分隔的拦截器列表                       |\n| interceptors.*  |        |                                            |\n\n示例：\n\n```shell\na1.sources = r1\na1.channels = c1\na1.sources.r1.type = exec\na1.sources.r1.command = tail -F /var/log/secure\na1.sources.r1.channels = c1\n```\n\n### Channel\n\nChannel一般地被认为是被动的组件，负责缓存Agent已经接收但尚未写出到另外一个Agent或者存储系统的数据，虽然他们可以为了清理或者垃圾回收运行自己的线程。Channel的行为很像是队列，Source把数据写入到他们，Sink从他们中读取数据，多个Source可以安全地写入到相同的Channel，并且多个Sink可以从相同的Channel进行读取，一个Sink只能从一个Channel，多个Sink可以从相同的Channel读取，它可以保证只有一个Sink 会从Channel读取一个指定的事件。\n\n| Channel类型        | 描述                                                         |\n| ------------------ | ------------------------------------------------------------ |\n| Memory             | Event数据存储在内存中                                        |\n| JDBC               | Event数据存储在持久化存储中                                  |\n| File               | Event数据存储在磁盘文件中                                    |\n| Spilable Memory    | Event数据存储在内存和硬盘上，当内存队列已满，将持久化到磁盘文件(不建议生产环境使用) |\n| Pseudo Transaction | 测试用途                                                     |\n| Customm            | 自定义                                                       |\n\nmemory   Channel\n\n| 属性名称                     | 默认值          | 描述                                                   |\n| ---------------------------- | --------------- | ------------------------------------------------------ |\n| **type**                     | –               | 名称memory                                             |\n| capacity                     | 100             | 存储在channel中的最大容量                              |\n| transactionCapacity          | 100             | 从一个source中去或者给一个sink，每个事务中最大的事件数 |\n| keep-alive                   | 3               | 对于添加或者删除一个事件超时的秒钟                     |\n| byteCapacityBufferPercentage | 20              | 对于添加或者删除一个事件的超时的秒钟                   |\n| byteCapacity                 | see description | 最大内存所有事件允许的总字节数                         |\n\n```\na1.channels = c1\na1.channels.c1.type = memory\na1.channels.c1.capacity = 10000\na1.channels.c1.transactionCapacity = 10000\na1.channels.c1.byteCapacityBufferPercentage = 20\na1.channels.c1.byteCapacity = 800000\nJDBC Channel\n```\n\n### Sink\n\n连续轮询各自的Channel来读取和删除事件,Sink将事件提送到下一阶段,(RPC Sink 的情况下,)活到最终目的地.一旦在下一阶段或其目的地中的数据是安全的,Sink通过的事务提交通知Channel,可以从Channel中删除这些事件。\n\n| Sink类型       | 描述                                                |\n| -------------- | --------------------------------------------------- |\n| HDFS           | 数据写入HDFS                                        |\n| Hbase          | 数据写入HBase                                       |\n| Logger         | 数据被转换成Avro Event，然后发送到配置的RPC端口上   |\n| Avro           | 数据被转换成Thrift Event，然后发送到配置的RPC端口上 |\n| IRC            | 数据在IRC上进行回收                                 |\n| File Roll      | 数据存储到本地文件系统                              |\n| Null           | 丢弃所有数据                                        |\n| Morphine Solor | 数据发送到Solr搜索服务器（集群）                    |\n| ElasticSearch  | 数据发送到ElasticSearch搜索服务器（集群）           |\n| Custome sink   | 自定义                                              |\n\n#### HDFS sink\n\n| **channel**            | -            | 描述                                                         |\n| ---------------------- | ------------ | ------------------------------------------------------------ |\n| **type**               | -            | HDFS                                                         |\n| **hdfs.path**          | -            | 写入hdfs的路径，需要包含文件系统表示，比如：hdfs://namenode/flume/webdata/ 可以使用flume提供日期及%{host}表达式 |\n| hdfs.filePrefix        | FlumeData    | 写入hdfs的文件名前缀，可以使用flume提供的日期及%{host}表达式 |\n| hdfs.fileSuffix        | -            | 写入hdfs的文件名后缀，比如：： .lzo .log                     |\n| hdfs.inUsePrefix       | -            | 临时文件的文件名前缀，hdfs sink 会先往 目标目录中写临时文件再根据相关规则重命名成最终文件 |\n| hdfs.inUseSuffix       | .tmp         | 临时文件的后缀名                                             |\n| hdfs.rollInterval      | 30           | hdfs sink 间隔多长时间将临时文件滚动成最终文明考吗，再根据相关的明明规则重命名为最终目标文件 |\n| hdfs.rollSize          | 1024         | 当临时文件达到该大小（单位：bytes）时，滚动成为目标文件，如果设置成0，则表示不根据临时文件大小来滚动文件 |\n| hdfs.rollCount         | 10           | 当events数据达到该数量时候，将临时文件滚动成为目标文件；如果设置为0，则表示部根据临时文件大小来滚动该文件 |\n| hdfs.idleTimeout       | 0            | 当目前被打开的临时文件在该参数指定的事件秒内没有任何数据写入，则将该临时文件关闭并重新命名。 |\n| hdfs.batchSize         | 100          | 每个批次刷新到hdfs上的events 数量                            |\n| hdfs.codeC             | -            | 文件的压缩格式，包括：gzip,bzip lzo lzop snappy              |\n| hdfs.fileType          | SequenceFile | 文件格式 包括：SequenceFile, DataStream ，CompressedStream，当使用DataStream的时候，文本不会被压缩，不需要设置hdfs.codeC，当使用CompressedStream的时候必须设置一个正确的hdfs.codeC的值 |\n| hdfs.maxOpenFiles      | 5000         | 最大允许打开的HDFS文件数，当打开的文件数到达该值，最早打开的文件将会被关闭 |\n| hdfs.minBlockReplicas  | -            | 写入HDFS文件块的最小副本数，该参数会影响文件的滚动配置，一般将该参数设置成1，才可以按照配置正确滚动文件。 |\n| hdfs.writeFormat       | Writable     | 写入sequence文件的格式。包括：Text Writable（默认）          |\n| hdfs.callTimeout       | 10000        | 执行HDFS操作的超时时间（单位：毫秒）                         |\n| hdfs.threadsPoolSize   | 10           | hhdfs sink 启动的操作 HDFS的线程数                           |\n| hdfs.rollTimerPoolSize | 1            | HDFS sink 启动的根据事件滚动文件的线程数                     |\n| hdfs.kerberosPrincipal | -            | HDFS 安全认证kerberos配置                                    |\n| hdfs.kerberosKeytab    | -            | HDFS 安全认证kerberos配置                                    |\n| hdfs.proxyUser         |              | 代理用户                                                     |\n| hdfs.round             | false        | 是否启用时间上的“舍弃”，这里的“舍弃”类似于“四舍五入”后面在介绍。如果启用，则会很影响除了除了%t的其他所有时间的表达式。 |\n| hdfs.roundValue        | 1            | 时间上进行舍弃的值                                           |\n| hdfs.roundUnit         | second       | 单位值 -秒 分 时                                             |\n| hdfs.timeZone          | Local Time   | 解析文件路径的时区名称，例如：美国/洛杉矶                    |\n| hdfs.closeTries        | 0            | hdfs sink 关闭文件的尝试次数；如果设置为1，当一次关闭文件失败后，hdfs sink 将不会在尝试关闭文件，这个为未关闭的文件，将会一直留在那，并且时打开状态，设置为0，当一次关闭失败后，hdfs  sink会继续尝试下一次关闭，直到成功 |\n| hdfs.retryInterval     | 180          | hdfs sink 尝试关闭文件的时间间隔，如果设置为0，表示不尝试，相当于将hdfs.closeTries设置为1 |\n| serializer             | TEXT         | 序列化类型。其他的还有：avro_event或者是实现了EventSerializer.Builder的类名 |\n| serializer.*           |              |                                                              |\n\n```\na1.channels = c1\na1.sinks = k1\na1.sinks.k1.type = hdfs\na1.sinks.k1.channel = c1\na1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%S\na1.sinks.k1.hdfs.filePrefix = events-\na1.sinks.k1.hdfs.round = true\na1.sinks.k1.hdfs.roundValue = 10\na1.sinks.k1.hdfs.roundUnit = minute\n```\n\n\n\n## 分区和拦截器\n\n大型的数据集常常被组织为分区(Partition),这样做的好处是,如果查询仅涉及到数据集的某个子集,查询过程就可以被限制在特定的分区范围内。Flume 事件的数据通常按时间件来分区，因此，我们可以定期运行某个进程来对已经完成分区进行各种数据转换处理（例如，删除重复的数据）\n\nagent.sinks.sink1.hdfs.path=/tmp/flume/year=%Y/month=%m/day=%d\n\n此处，我们选择天作为分区粒度，不过也可是使用其他级别的分区粒度，结果将导致不同的文件目录布局方案，在Flume与HDFS sink相关的文件提供了格式转义序列的完整，一个Flume时间将被写入那个分区是由时间的header中的timestamp（时间戳）决定的，在默认的情况下 事件header 并没有timestamp，但是他可以通过Flume 拦截器（interceptor）来添加，拦截器是一种能够对事件流中的事件进行修改或者删除的组件，他们连接source并在事件被传递到channel之前对时间进行处理。下面的两行卑职用于source1增加一个时间拦截器，它将为source产生的每个时事件添加一个timestamp header：\n\nagent1.sources.source1.interceptors = interceptor1\n\nagent1.sources.source1.interceptors = interceptor1.type = timestamp\n\n事件拦截器可以确保这些是时间戳能够如实反映事件穿件的事件，对于某些数据由时间戳就已经足够了，如果存在多层Flume多层代理，那么事件事件的创建事件和吸入的事件之间可能会存在有明显的差异，尤其是当代理出现停机的情况，针对这种情况我们可以对HDFS sink 中的hdfs。useLocalTimeStamp属性进行设置，以便使用用HDFS Sink的Flume 代理所产生的时间戳。\n\n## 文件格式\n\n一般而言，使用二进制来存储数据比使用文本格式的文件更小 HDFS sink使用的文件格式由hdfs.fileAType属性及其他一些属性的组合控制。\n\n默认的Hdfs.fileType文件格式为SequenceFile，把事件写入到一个顺序文件，在这个文件中，键的数据类型为LongWritable，它包括的是事件的时间戳(如果事件的header中没有时间戳，就使用当前事件)。值得数据类型为BytesWritable，其中包含的是事件的body。如果hdfs.writeFormat被设置为Text，那么顺序文件中的值就变成Text数据类型\n\n针对Avro文件所使用的配置；収不同它的Hdfs。filetype的属性被设置为DataStream，就像纯文本一样，此外，serializer（之一前面没有前缀hdfs.）必须设置为avro_event。如果想要启动压缩则需要设置serializer.compressionCodes属性。\n\n## 扇出\n\n指的是从一个source向多个channel，亦即向多个sink传递事物。\n\n![](https://s1.ax1x.com/2018/11/18/FSYnKS.png)\n\n```shell\n案例演示：将事件同时传递到HDFS sink和logger sink。\n\n注意：如上图channel1a，sink1a是到达HDFS,channel1b,sink1b是到达logger\n\nagent1.sources = source1\nagent1.sinks= sink1a sink1b  #多个sink，中间用空格\nagent1.channels=channel1a  channel1b  #多个channel中间同样空格分割\nagent1.sources.source1.channels = channnel1a  channel1b\nagent1.sinks.sink1a.channnel = channel1a\nagent1.sinks.sink1b.channnel =channel1b\nagent1.sources.source1.type = spoodir\nagnet1.sources.source1.spooDir = /tmp/spooldir\nagent1.sinks.sink1a.type =hdfs\nagent1.sinks.sink1a.hdfs.path=/tmp/flume\nagent1.sinks.sink1a.hdfs.fileType = DataStream  #默认类型是SequenceFile，顺序文件，dataStream类似纯文本。\nagent1.channels.channel1a.type =file  \nagent1.channels.channel1b.type = memory \n\n #注意：上面两个channel配置类型不一样，一个是file,一个是memory。如果同一个机器上，配置两个channel都是file channel。那么则必须要通过配置使得它们分别指向不同的数据存储目录和检查点目录（因为默认情况下，filechannel 具有持久性，数据会被写入到本地系统，而默认情况下file channel写入的目录都是用户主目录中的同一个默认文件夹），否则会出现数据重叠覆盖的情况。\n```\n\n## 交付保证 \n\nFlume 使用独立的事物来负责从Spooling Directory source到每一个channel的每批事件的传递，在本例中有一个事物负责从source到链接HDFS sink 的channel的事件交付，另外一个负责从source到链接logger sink的channel 的同一批事件的交付，若其中有任何一个事物失败（例如 channel满溢），这些事件都不会从source中删除，而是等待稍后重试。\n\n## 复用和复用选择器\n\n正常扇出流是向所有的channel复制事件，但有些时候，人们有更多的行为方式选择，从而使某些事件被发送到这一个channel，而另外一些事件被发送到另外一个channel。\n\n\n\n##  Flume的拓扑结构\n\n### \t\t\t\t\tlume Agent连接\n\n\n\n![FSNYBF.png](https://s1.ax1x.com/2018/11/18/FSNYBF.png)\n\n### 单source，多channel、sink\n\n![FSNdhR.png](https://s1.ax1x.com/2018/11/18/FSNdhR.png)\n\n\n\n### Flume负载均衡\n\n![FSN110.png](https://s1.ax1x.com/2018/11/18/FSN110.png)\n\n### Flume Agent聚合\n\n![FSN110.png](https://s1.ax1x.com/2018/11/18/FSN110.png)\n\n","tags":["Flume"],"categories":["大数据"]},{"title":"Kafka深度解析","url":"/2018/11/16/kafka工作流分析/","content":"\n {{ \"Kafka技术内幕笔记\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n### 分区模型\n\nKafka集群向多个消息代理服务器（brokerserver）组成，发布至Kafka集群的每条消息都有一个类别，用主题（topic）来表示。不同类型的数据，可以设置不同的主题。一个主题一般会有多个消息的订阅者，当生产者发布消息到某个主题时，订阅了这个主题的消费者都可以接收到生产者写人的新消息。Kafka集群为每个主题维护了分布式的分区（partition）日志文件，物理意义上可以把主题看作分区的日志文件（partitionedLog）。每个分区都是一个有序的、不可变的记录序列，新的消息会不断追加到提交日志（commitlog）。分区中的每条消息都会按照时间顺序分配到一个单调递增的顺序编号叫作偏移盘（offset），这个偏移量能够唯一地定位当前分区中的每一条消息。每个分区的偏移量都从0开始，不同分区之间的偏移量都是独立的不会互相影响发布到Kafka主题的每条消息包括键值和时间戳。消息到达服务端的指定分区后，都会分配到一个向增的偏移量原始的消息内容和分配的偏移量以及其他一些元数据信息最后都会存储到分区日志文件中。消息的键也可以不用设置，这种情况下消息会均衡地分布到不同的分区。\n\n### 分区原则\n\n（1）指定了patition，则直接使用；\n\n（2）未指定patition但指定key，通过对key的value进行hash出一个patition；\n\n（3）patition和key都未指定，使用轮询选出一个patition。\n\n```java\nDefaultPartitioner类\npublic int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {\n        List<PartitionInfo> partitions = cluster.partitionsForTopic(topic);\n        int numPartitions = partitions.size();\n        if (keyBytes == null) {\n            int nextValue = nextValue(topic);\n            List<PartitionInfo> availablePartitions = cluster.availablePartitionsForTopic(topic);\n            if (availablePartitions.size() > 0) {\n                int part = Utils.toPositive(nextValue) % availablePartitions.size();\n                return availablePartitions.get(part).partition();\n            } else {\n                // no partitions are available, give a non-available partition\n                return Utils.toPositive(nextValue) % numPartitions;\n            }\n        } else {\n            // hash the keyBytes to choose a partition\n            return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;\n        }\n    }\n```\n\n\n\n## 消费模型\n\n消息由生产者发布到Kafka集群后会被消费者消费消息的消费模型有两种：推送模型（push)和拉取模型（pull）Kafka采用拉取模型，由消费者自己记录消费状态，每个消费者相独立地顺序读取每个分区的消息。消费者拉取的最大上限通过最高水位（watermark）控制，生产者最新写入的消息如果还没有达到备份数量，对消费者是不可见的。这种由消费者控制偏移韭量的优点是：消费者可以按照任意的顺序消费消息。比如，消费者可以重置到旧的偏移量，重新处理之前已经消费过的消息或者直接跳到最近的位置从当前时刻开始消费。在这里我们可以把分区成是一个目录,TOPIC是由PartitionLogs(分区日志)组成的\n\n为什么要分区:为了保证读写的高效性。\n\n\n\n## 分布式模型\n\nKafka每个主题的多个分区日志分布式地存储在Kafka集群上，同时为了故障容错，每个分区都会副本的方式复制到多个消息代理节点上。其中一个节点会作为主副本（Leader），其他节点作为备份副本（Follower，也叫作从副本.主副本会负责所有的客户端读写操作，备份副本仅仅从主副本同步数据。当主副本现故障时，备份副本中的一个副本会被选择为新的主副本。只有主副本接受读写,每个服务端都会作为某些分区的主副本,这样Kafka集群的所有服务端整体上对客户端是负载均衡的.生产者发布消息时根据消息是否有键，采用不同的分区策略。消息没有键时，通过轮询方式进行客户端负载均衡·\n\n分区是消费者最小并行单位增加服务器节点会提升集群的性能，增加消费者数量会提升处理性能。同一个消费组下多个消费者互相协调消费工作，Kafka会将所有的分区平均地分配给所有的消费者实例，每个消费者都可以分配到数量均等的分区，Kafka的消费组管理协议会动态地维护消费组的成员列表，当一个新消费者加入消费组，或者有消费者离开消费组，都会触发再平衡操作。\n\n\n\n## Kafka的设计与实现\n\n预读（read-ahead）会提前将一个比较大的磁盘块读人内存。\n\n后写（write-behind）会将很多小的逻辑写操作合并起来组合成一个大的物理写操作。并且，操作系统还会将主内存剩余的所有空闲内存空间都用作磁盘缓存（diskcache/pagecache),所有的磁盘读写操作都会经过统一的磁盘缓存（除了直接I/O会绕过磁盘缓存）。综合这几点优化特点，如果是针对磁盘的顺序访问，某些情况下它可能比随机的内存访问都要快，甚至可以和网络的速度相差无几。\n\n消息系统内的消息从生产者保存到服务端，消费者再从服务端读取出来，数据的传输效率决定了生产者和消费者的性能。生产者如果每发送一条消息都直接通过网络发送到服务端，势必会造成过多的网络请求。如果我们能够将多条消息按照分区进行分组，并采用批量的方式一次发送一个消息集，并且对消息集进行压缩，就可以减少网络传输的带宽，进一步提高数据的传输效率。\n\nKafka的消息有多个订阅者的使用场景，生产者发布的消息一般会被不同的消费者消费多次。使用“零拷贝技术”（zero-copy）只需将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中（发送给不同的使用者时，都可以重复使用同一个页面缓存），避免了重复的复制操作。这样，消息使用的速度基本上等同于网络连接的速度了。\n\n## Kafka生产者与消费者\n\nKafka的生产者将消息直接发送给分区主副本所在的消息代理节点，并不需要经过任何的中间路由层。为了做到这一点，所有消息代理节点都会保存一份相同的元数据，这份元数据记录了每个主题分区对应的主副本节点。生严者客户端在发送消息之前，会向任意一个代理节点请求元数据，井确定每条消息对应的目标节点然后把消息直接发送给对应的目标节点。\n\n生产者采用批量发送消息集的方式解决了网络请求过多的问题。生产者会尝试在内存中收集足够数据，并在一个请求中一次性发送一批数据。另外，我们还可以为生产者客户端设置“在指定的时间内收集不超过指定数量的消息”。比如，设置消息大小上限等于64字节，延迟时间等于100毫秒，表示在100毫秒内消息大小达到64字节要立即发送；如果在100毫秒时还没达到64字节，也要把已经收集的消息发送出去客户端采用这种缓冲机制，在发送消息前会收集尽可能多的数据，通过每次牺牲一点点额外的延迟来换取更高的吞吐量。相应地服务端的l/O消耗也会大大降低。\n\n消费者读取消息、有两种方式。第一种是消息代理主动地“推送”消息、给下游的消费者，由消息代理控制数据传输的速率，但是消息代理对下游消费者是否能及时处理不得而知。如果数据的消费速率低于产生速率，消费者会处于超负荷状态，那么发送给消费者的消息就会堆积得越来越多。而且，推送方式也难以应付不同类型的消费者，因为不同消费者的消费速率不一定都相同，消息代理需要调整不同的传输速率。并让消费者充分利用系统的资源，这种方式实现实现起来比较困难。\n\n第二种读取方式是消费者从消息代理主动地“拉取”数据，消息代理是无状态的，它不需要标记哪些消息被消费者处理过，也不需要保证一条消息只会被一个消费者处理。而且，不同的消费者可以按照向己最大的处理能力来拉取数据，即使有时候某个消费者的处理速度稍微落后，它也不会影响其他的消费者，并且在这个消费者恢复处理速度后，仍然可以追赶之前落后的数据。\n\n因为消息系统不能作为严格意义上的数据库，所以保存在消息系统中的数据，在不用之后应该及时地删除掉并释放磁盘空间。消息需要删除，其原因一般是消息被消费之后不会再使用了，大多数消息系统会在消息代理记录关于消息是否已经被消费过的状态：当消息从消息代理发送给消费者时（基于推送模型），消息代理会在本地记录这条消息“已经被消费过了”。但如果消费者没能处理这条消息（比如由于网络原因、请求超时或消费者挂掉），就会导致“消息丢失”。解决消息丢失的一种办法是添加应答机制，消息代理在发送完消息后只把消息标记为“已发送”，只有收到消费者返回的应答信息才表示“己消费”。但还是会存在一个问题：消费者处理完消息就失败了，导致应答没有返回给消息代理，这样消息代理又会重新发送消息，导致消息被重复处理。这种方案还有一个缺点：消息代理需要保存每条消息的多种状态（比如，消息状态为“已发送”时，消息代理需要锁住这条消息，保证－消息不会发送两次），这种方式需要在客户端和服务端做一些复杂的状态一致性保证。\n\nKafka采用了基于拉取模型的消费状态处理，它将主题分成多个有序的分区，任何时刻每个分区都只被一个消费者使用。并且，消费者会记录每个分区的消费进度（即偏移量）。每个消费者只需要为每个分区记录一个整数值，而不需要像其他消息系统那样记录每条消息的状态。假设有10000条消息，传统方式需要记录10000条消息的状态；如果用Kafka的分区机制，假设有10个分区，每个分区1000条消息，总共只需要记录10个分区的消费状态（需要保存的状态数据少了很多，而且也没有了锁）。\n\n和传统方式需要跟踪每条消息的应答不同，Kafka的消费者会定时地将分区的消费进度保存成检查点文件，表示“这个位置之前的消息都已经被消费过了”。传统方式需要消费者发送每条消息的应答，服务端再对应答做出不同的处理；而Kafka只需要让消费者记录消费进度，服务端不需要记录消息的任何状态。除此之外，让消费者记录分区的消费进度还有一个好处：消费者可以“故意”回退到某个旧的偏移量位置，然后重新处理数据。虽然这种处理方式看起来违反了队列模型的规定（一条消息发送给队列的一个消费者之后，就不会被其他消费者再次处理），但在实际运用中，很多消费者都需要这种功能。比如，消费者的处理逻辑代码出现了问题，在部署并启动消费者后，需要处理之前的消息并重新计算。\n\n和生产者采用批量发送消息类似，消费者拉取消息也可以一次拉取一批消息。消费者客户端拉取消息，然后处理这一批消息，这个过程一般套在一个死循环里，表示消费者永远处于消费消息的状态（因为消息系统的消息总是一直产生数据，所以消费者也要一直消费消息）。消费者采用拉取方式消费消息有一个缺点：如果消息代理没有数据或者数据量很少，消费者可能需要不断地**轮询**，并等待新数据的到来（拉取模式主动权在消费者手里，但是消费者并不知道消息代理有没有新的数据；如果是推送模式，只有新数据产生时，消息代理才会发送数据给消费者，就不存在这种问题）。解决这个问题的方案是：允许消费者的拉取请求以阻塞式、长轮询的方式等待，直到有新的数据到来。我们可以为消费者客户端设置“指定的字节数量”，表示消息代理在还没有收集足够的数据时，客户端的拉取请求就不会立即返回。\n\n\n\n## 副本机制和容错处理\n\nKafka的副本机制会在多个服务端节点（简称节点即消息代理节点）上对每个主题分区的日志进行复制。当集群中的某个节点出现故障时，访问故障节点的请求会被转移到其他正常节点的副本上。副本的单位是主题的分区，Kafka每个主题的每个分区都有一个主副本以及0个或多个备份副本。备份副本会保持和主副本的数据同步，用来在主副本失效时替换为主副本。\n\n分布式系统的容错需要确定分区是否处于存活。存活定义有两个条件：\n\n1.节点必须和ZK保持会话；\n2.如果这个节点是某个分区的备份副本，它必须对分区主副本的写操作进行复制，并且复制的进度不能落后太多。\n\n满足这两个条件，叫作“正在同步中”（in-sync）。如果一个备份副本挂掉、没有响应或者落后太多，主副本就会将其从同步副本集合中移除。反之，如果备份副本重新赶上主副本，它就会加入到主副本的同步集合中。在Kafka中，一条消息只有被ISR集合的所有副本都运用到本地的日志文件，才会认为消息被成功提交了。任何时刻，只要ISR至少有一个副本是存活的，Kafka就可以保证一条消息一旦被提交，就不会丢失”。只有已经提交的消息才能被消费者消费，因此消费者不用担心会看到因为主副本失败而丢失的消息。\n\n## Kafka生产过程分析\n\n### 写入方式\n\nproducer采用推（push）模式将消息发布到broker，每条消息都被追加（append）到分区（patition）中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障kafka吞吐率）。\n\n![FvxUde.png](https://s2.ax1x.com/2019/01/13/FvxUde.png)\n\n(1）producer先从zookeeper的 \"/brokers/.../state\"节点找到该partition的leader\n\n(2) producer将消息发送给该leader\n\n(3）leader将消息写入本地log\n\n(4）followers从leader pull消息，写入本地log后向leader发送ACK\n\n(5) leader收到所有ISR中的replication的ACK后，增加HW（high watermark，最后commit 的offset）并向producer发送ACK\n\n### 存储策略\n\n无论消息是否被消费，kafka都会保留所有消息。有两种策略可以删除旧数据：\n\n(1) 基于时间：log.retention.hours=168\n\n(2) 基于大小：log.retention.bytes=1073741824\n\n需要注意的是，因为Kafka读取特定消息的时间复杂度为O(1)，即与文件大小无关，所以这里删除过期文件与提高 Kafka 性能无关。\n\n### Zookeeper存储结构\n\n![Fx9ep9.png](https://s2.ax1x.com/2019/01/13/Fx9ep9.png)\n\n\n\n\n\n\n\n","tags":["Kafka"],"categories":["大数据"]},{"title":"Kafka命令操作","url":"/2018/11/14/Kafka命令操作/","content":"\n {{ \"本文主要介绍Kafka的shell命令\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 查看当前服务器所有的topic\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --list\n```\n\n## 创建topic\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --create --replication-factor 3 --partitions 1 --topic first\n选项说明：\n\t--topic 定义topic名\n\t--replication-factor  定义副本数\n\t--partitions  定义分区数\t\n```\n\n##  删除topic\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --delete --topic first\n```\n\n## 创建生产者发送消息\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-console-producer.sh --broker-list datanode1:9092 --topic test\n```\n\n## 创建消费者接受消息\n\n```shell\n[hadoop@datanode2 kafka]$ bin/kafka-console-consumer.sh --zookeeper datanode1:2181 --from-beginning --topic test\n--from-beginning：会把first主题中以往所有的数据都读取出来。根据业务场景选择是否\n```\n\n\n\n## 查看某一个topic的详情\n\n```shell\n[hadoop@datanode1 kafka]$ bin/kafka-topics.sh --zookeeper datanode1:2181 --describe --topic test\nTopic:test      PartitionCount:1        ReplicationFactor:3     Configs:\nTopic: test     Partition: 0    Leader: 2       Replicas: 2,3,1 Isr: 2,3,1\n\n```\n\n","tags":["Kafka"],"categories":["大数据"]},{"title":"Kafka与消息队列","url":"/2018/11/14/Kafka介绍与消息队列/","content":"\n {{ \"消息队列的好处\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 消息队列（Message Queue）\n\n消息: 网络中的两台计算机或者两个通讯设备之间传递的数据。例如说：文本、音乐、视频等内容。\n\n队列：一种特殊的线性表（数据元素首尾相接），特殊之处在于只允许在首部删除元素和在尾部追加元素。入队、出队。\n\n消息队列：顾名思义，消息+队列，保存消息的队列。消息的传输过程中的容器；主要提供生产、消费接口供外部调用做数据的存储和获取。\n\n## 消息队列分类\n\nMQ分类：点对点（P2P）、发布订阅（Pub/Sub）\n\n共同点：消息生产者生产消息发送到queue中，然后消息消费者从queue中读取并且消费消息。\n\n不同点：    P2P模型包含：消息队列(Queue)、发送者(Sender)、接收者(Receiver)一个生产者生产的消息只有一个消费者(Consumer)（即一旦被消费，消息就不在消息队列中）。打电话。\n\nPub/Sub包含：消息队列(Queue)、主题（Topic）、发布者（Publisher）、订阅者（Subscriber）\n\n每个消息可以有多个消费者，彼此互不影响。比如我发布一个微博：关注我的人都能够看到。\n\n## 消息队列模式\n\n1. 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）\n\n点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。\n\n1. 发布/订阅模式（一对多，数据生产后，推送给所有订阅者）\n\n发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。\n\n## 消息队列的实现原理\n\n![消息队列的实现原理](https://s1.ax1x.com/2018/11/18/izvOfS.jpg)\n\n## 消息队列的好处\n\n- 解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。\n- 冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的\"插入-获取-删除\"范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。\n- 扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。\n- 灵活性 & 峰值处理能力： 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。\n- 可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。\n- 顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）\n- 缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。\n- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。\n\n**Kafka**\n\nKafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。\n\n\n\n![Kafka架构图](https://s1.ax1x.com/2018/11/18/FSSEL9.png)\n\nProducer ：消息生产者，就是向kafka broker发消息的客户端；\n\n1. Consumer ：消息消费者，向kafka broker取消息的客户端；\n2. Topic ：可以理解为一个队列\n3. Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic；\n4. Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic；\n5. Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序；\n6. Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka。\n\n## Kafka 和其他消息队列的比较\n\n![](https://s1.ax1x.com/2018/11/18/izv4QH.jpg)","tags":["Kafka"],"categories":["大数据"]},{"title":"Kafka和的安装与配置","url":"/2018/11/14/Kafka的安装与配置/","content":"\n {{ \"本文主要介绍Kafka的安装与配置\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## 集群规划\n\n| datanode1 | datanode2 | datanode3 |\n| --------- | --------- | --------- |\n| zk        | zk        | zk        |\n| kafka     | kafka     | kafka     |\n\n\n\n## kafka jar包下载地址\n\n<http://kafka.apache.org/downloads.html>\n\n## kafka集群安装部署 \n\n1.  解压安装包  \n\n```shell\n[hadoop@datanode1 software]$ tar -zxvf kafka_2.11-0.8.2.2.tgz -C /opt/module/\n```\n\n2. 修改解压后的名称\n\n```shell\n[hadoop@datanode1 module]$ mv kafka_2.11-0.8.2.2/ kafka\n```\n\n3. /opt/module/kafka目录下创建logs文件夹\n\n```shell\n[hadoop@datanode1 kafka]$ mkdir logs/\n```\n\n4. 修改配置文件\n\n```shell\n[hadoop@datanode1 kafka]$ cd config/\n[hadoop@datanode1 config]$ vim server.properties\n#broker的全局唯一编号，不能重复\nbroker.id=0\n#删除topic功能使能\ndelete.topic.enable=true\n#处理网络请求的线程数量\nnum.network.threads=3\n#用来处理磁盘IO的现成数量\nnum.io.threads=8\n#发送套接字的缓冲区大小\nsocket.send.buffer.bytes=102400\n#接收套接字的缓冲区大小\nsocket.receive.buffer.bytes=102400\n#请求套接字的缓冲区大小\nsocket.request.max.bytes=104857600\n#kafka运行日志存放的路径\t\nlog.dirs=/opt/module/kafka/logs\n#topic在当前broker上的分区个数\nnum.partitions=1\n#用来恢复和清理data下数据的线程数量\nnum.recovery.threads.per.data.dir=1\n#segment文件保留的最长时间，超时将被删除 7天\nlog.retention.hours=168\n#配置连接Zookeeper集群地址\nzookeeper.connect=datanode1:2181,datanode2:2181,datanode2:2181\n\n```\n\n 5.配置环境变量\n\n```shell\n[hadoop@datanode1 config]$ sudo vim /etc/profile\n#KAFKA_HOME\nexport KAFKA_HOME=/opt/module/kafka\nexport PATH=$PATH:$KAFKA_HOME/bin\n[hadoop@datanode1 config]$ source /etc/profile\n```\n\n6.分发安装包\n\n```shell\n[hadoop@datanode1 module]$ xsync kafka\n##分发完毕后要在其他节点上配置/opt/module/kafka/config/server.properties broker.id  值  笔者在这里也是 坑了半天发现分发之后 忘记了改broker.id 值\n```\n\n7.启动集群\n\n````shell\n[hadoop@datanode1]$ bin/kafka-server-start.sh config/server.properties &\n[hadoop@datanode2]$ bin/kafka-server-start.sh config/server.properties &\n[hadoop@datanode3]$ bin/kafka-server-start.sh config/server.properties &\n````\n\n\n\n## Kafaka  Manager\n\n为了简化开发者和服务工程师维护Kafka集群的工作，yahoo构建了一个叫做Kafka管理器的基于Web工具，叫做 Kafka Manager。这个管理工具可以很容易地发现分布在集群中的哪些topic分布不均匀，或者是分区在整个集群分布不均匀的的情况。它支持管理多个集群、选择副本、副本重新分配以及创建Topic。同时，这个管理工具也是一个非常好的可以快速浏览这个集群的工具，有如下功能：\n\n> 1.管理多个kafka集群\n> 2.便捷的检查kafka集群状态(topics,brokers,备份分布情况,分区分布情况)\n> 3.选择你要运行的副本\n> 4.基于当前分区状况进行\n> 5.可以选择topic配置并创建topic(0.8.1.1和0.8.2的配置不同)\n> 6.删除topic(只支持0.8.2以上的版本并且要在broker配置中设置delete.topic.enable=true)\n> 7.Topic list会指明哪些topic被删除（在0.8.2以上版本适用）\n> 8.为已存在的topic增加分区\n> 9.为已存在的topic更新配置\n> 10.在多个topic上批量重分区\n> 11.在多个topic上批量重分区(可选partition broker位置)\n\n这里提供编译好了的包，下载后可以直接使用，可以不用去sbt编译。\n链接：http://pan.baidu.com/s/1bQ3lkM 密码：c251\n\n将下载完之后的上传到Linux上解压\n\n```shell\n[hadoop@datanode1 software]$ unzip kafka-manager-1.3.0.7.zip\n[hadoop@datanode1 software]$ mv kafka-manager-1.3.0.7 kafka-manager \n```\n\n修改application.conf中zk配置\n\n```shell\nhttp.port=9001 （默认9000）  #会与hadoop端口发生冲突\nkafka-manager.zkhosts=\"192.168.1.101:2181,192.168.1.102:2181,192.168.1.103:2181\" #写ip地址不写主机名\n```\n\n用kafkamanage是在jdk8基础上的，所以先安装jdk8,只需下载解压即可。\n\n```shell\n#想要看到读取,写入速度需要开启JMX，修改kafka-server-start.sh 添加一行即可：添加JMX端口8999\nif [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then \nexport KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" \nexport JMX_PORT=\"8999\" \nfi\n##如果初始化内存不够 KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\" 可以设置小一些 JVM系列涉及到到过,之所以设置相同为了防止内存抖动\n```\n\n 想要看到读取,写入速度需要开启JMX，修改kafka-server-start.sh 添加一行即可：添加JMX端口8999\n\n```shell\nif [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then\nexport KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"  ## 和你自己的设置内存带下保持相同\nexport JMX_PORT=\"8999\" \nfi\n##\"每个kafka broker都需要修改，修改后进行重启kafka\"\n```\n\n启动kafka manager\n\n```shell\n[hadoop@datanode1 module]$ cd kafka\n[hadoop@datanode1 module]$ bin/kafka-manager -java-home /opt/module/jdk1.8.0_162/\n在后台运行 \n[hadoop@datanode1 bin]$ nohup ./kafka-manager  -java-home /opt/module/jdk1.8.0_162/  -Dconfig.file=../conf/application.conf >/dev/null 2>&1 &\n```\n\n在localhost:9001查看web页面\n\n创建cluster: 点击顶部Cluster、Add Cluster\n\n\n\n\n\n配置cluster\n\n\n\n\n\n​\t\t\t名字；集群zkhost格式：host1:2181,host2:2181,host3:2181\n​\t\t\t  kafka版本，勾选上JMX和下面的勾选框，可显示更多指标\n\n创建完毕后，可查看\n\n\n\ntopics相关：\n\n\n\n","tags":["Kafka"],"categories":["大数据"]},{"title":"Java虚拟机------JVM分析工具","url":"/2018/11/12/Java虚拟机------JVM分析工具/","content":"\n {{ \"主要介绍JVM的分析工具\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n# jps\n\n````\njps:Java Virtual Machine Process Status Tool\n\nhttp://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jps.html\n\njps [ options ][ hostid ]\n\n\t\t -q 只显示pid，不显示class名称,jar文件名和传递给main 方法的参数\n\n\t\t-m -m 输出传递给main 方法的参数，在嵌入式jvm上可能是null\n\n\t\t-l  输出应用程序main class的完整package名 或者 应用程序的jar文件完整路径名\n\n\t\t-v 输出传递给JVM的参数\n\njps host\n\n查看host的jps情况（前提：host提供jstatd服务）\n````\n\n\n\n\n\n\n\n## JVM分析工具-jmap1\n\n```\nMemory Map 观察运行中的jvm物理内存的占用情况。\n\n官方地址：http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jmap.html\n\n \tjmap [ option ] pid\n\n\tpid   进程号（常用）\n\n\t参数如下：\n\n\t\t-heap：打印jvm heap的情况(垃圾收集器类型)\n\n\t\t-histo：打印jvm heap的直方图。其输出信息包括类名，对象数量，对象占用大小。\n\n\t\t-histo：live ：同上，但是只打印存活对象的情况\n\t\n\t\t-permstat：打印permanent generation heap（方法区）情况\n\n\t\t-finalizerinfo：打印正等候回收的对象信息\nMemory Map 观察运行中的jvm物理内存的占用情况。\n\n官方地址：http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jmap.html\n\n \tjmap [ option ] pid\n\n\tpid   进程号（常用）\n\n\t参数如下：\n\n\t-heap：打印jvm heap的情况(垃圾收集器类型)\n\n\t-histo：打印jvm heap的直方图。其输出信息包括类名，对象数量，对象占用大小。\n\n\t-histo：live ：同上，但是只打印存活对象的情况\n\n\t-permstat：打印permanent generation heap（方法区）情况\n\n\t-finalizerinfo：打印正等候回收的对象信息\n\n用jmap把进程内存使用情况dump到文件中，再用jhat分析查看。jmap进行dump命令格式如下：\n\njmap -dump:format=b,file=dumpFileName pid\n\njmap -dump:format=b,file=4574.heap20151215  4574\n\nDumping heap to 4574.heap20151215\n\nHeap dump file created\n\ndump出来的文件可以用MAT、VisualVM等工具查看，这里用jhat查看：\n\njhat -port 9998 /tmp/dump.dat\n\n注意如果Dump文件太大，可能需要加上-J-Xmx512m这种参数指定最大堆内存，即jhat -J-Xmx512m -port 9998 /tmp/dump.dat。然后就可以在浏览器中输入主机地址:9998查看了。 \t\n```\n\n\n\n\n\n\n\n## jinfo\n\n```\nConfiguration Info\n\n官方地址：http://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jinfo.html\n\njinfo [ option ] pid\n\npid   进程号\n\n参数如下：\n\n\t\tno option  打印命令行参数和系统属性\n\n\t\t-flags  打印命令行参数\n\n\t\t-sysprops  打印系统属性\n\t\n\t\t-h  帮助\n```\n\n\n\n##  jstack1\n\n\n\n```\njstack:Stack Trace\n\nhttp://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstack.html\n\njstack能得到运行java程序的java stack和native stack的信息。可以轻松得知当前线程的运行情况\n\njstack [ option ] pid\n\n[ option ] 参数如下\n\n\t\t-l长列表. 打印关于锁的附加信息,例如属于java.util.concurrent的ownable synchronizers列表.\n\n\t\t-m打印java和native c/c++框架的所有栈信息.\n\ntid指Java Thread id。nid指native线程的id。prio是线程优先级。[0x00007fd4f8684000]是线程栈起始地址\n\ndump 文件里，值得关注的线程状态有：\n\n死锁，Deadlock（重点关注）\n\n等待资源，Waiting on condition（重点关注）\n\n等待获取监视器，Waiting on monitor entry（重点关注）\n\n阻塞，Blocked（重点关注） \n\n执行中，Runnable  \n\n暂停，Suspended\n\n对象等待中，Object.wait() 或 TIMED_WAITING\n\n 停止，Parked\n```\n\n\n\n## jstat\n\n```\njstat: Java Virtual Machine Statistics Monitoring Tool\n\nhttp://docs.oracle.com/javase/1.5.0/docs/tooldocs/share/jstat.html\n\n Usage: jstat -help|-options\n\n      jstat -<option> [-t][-h] <pid> [<interva[s|ms]> [<count>]]\n\n参数解释：\n\nOptions — 选项，我们一般使用 -gcutil /-gc 查看gc情况\n\npid      — VM的进程号，即当前运行的java进程号\n\ninterval[s|ms]  ——  间隔时间，单位为秒或者毫秒，默认为ms。必须是正整型。\n\ncount     — 打印次数，如果缺省则打印无数次\n\n例如：jstat -gc 4645 500 10  表示查看进程号为4645的gc 每500ms打印一次  共打印10次\n```\n\n\n\n\n\n```\nS0C：年轻代中第一个survivor（幸存区）的容量 (字节)\nS1C：年轻代中第二个survivor（幸存区）的容量 (字节)\nS0U：年轻代中第一个survivor（幸存区）目前已使用空间 (字节)\nS1U：年轻代中第二个survivor（幸存区）目前已使用空间 (字节)\nEC：年轻代中Eden（伊甸园）的容量 (字节)\nEU：年轻代中Eden（伊甸园）目前已使用空间 (字节)\nOC：Old代的容量 (字节)\nOU：Old代目前已使用空间 (字节)\nPC：Perm(持久代)的容量 (字节)\nPU：Perm(持久代)目前已使用空间 (字节)\nYGC：从应用程序启动到采样时年轻代中gc次数\nYGCT：从应用程序启动到采样时年轻代中gc所用时间(s)\nFGC：从应用程序启动到采样时old代(全gc)gc次数\nFGCT：从应用程序启动到采样时old代(全gc)gc所用时间(s)\nGCT：从应用程序启动到采样时gc用的总时间(s)\n\n```\n\njstat -gccapacity pid\n\n```\n\nNGCMN：年轻代(young)中初始化(最小)的大小 (字节)\nNGCMX：年轻代(young)的最大容量 (字节)\nNGC：年轻代(young)中当前的容量 (字节)\nOGCMN：old代中初始化(最小)的大小 (字节) \nOGCMX：old代的最大容量 (字节)\nOGC：old代当前新生成的容量 (字节)\nPGCMN：perm代中初始化(最小)的大小 (字节) \nPGCMX：perm代的最大容量 (字节)   \nPGC：perm代当前新生成的容量 (字节)\nS0：年轻代中第一个survivor（幸存区）已使用的占当前容量百分比\nS1：年轻代中第二个survivor（幸存区）已使用的占当前容量百分比\nE：年轻代中Eden（伊甸园）已使用的占当前容量百分比\nO：old代已使用的占当前容量百分比\nS0CMX：年轻代中第一个survivor（幸存区）的最大容量 (字节)\nS1CMX ：年轻代中第二个survivor（幸存区）的最大容量 (字节)\nECMX：年轻代中Eden（伊甸园）的最大容量 (字节)\nDSS：当前需要survivor（幸存区）的容量 (字节)（Eden区已满）\nTT： 持有次数限制\nMTT ： 最大持有次数限制\n```\n\n\n\n## jvm分析工具-jconsole\n\n```\n可视化的jvm监控软件。\n\n可以监控本地或者远程进程。\n\n主要包括：概览、内存、线程、类、VM概要、MBean选项卡。\n\n概览选项卡：呈现四幅图表：主要包括堆内存使用量、类、线程、CPU占有率。\n\n内存选项卡：包含堆内、非堆、内存池的使用量图表和详细信息。相当于jstat命令。\n\n线程选项卡：显示所有的线程的信息和图表。相当于jstack\n\n类选项卡：加载的类的信息。\n\nVm概要：VM的概要信息包括堆大小，垃圾收集信息、vm参数等。\n\nMbean选项：managed beans ，被管理的beans\n\n```\n\n## jvm分析工具-jmx\n\n```\nJMX（Java Management Extensions，即Java管理扩展）是一个为应用程序、设备、系统等植入管理功能的框架。\n开启jmx：\n无需验证的配置：\n-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.authenticate=false \n-Dcom.sun.management.jmxremote.port=1234 \n-Dcom.sun.management.jmxremote.ssl=false\n将-Dcom.sun.management.jmxremote.authenticate=false 去掉就是需要验证信息即：登录名和密码：验证用户的配置文件在$java_home/jre/lib/management/jmxremote.password\n默认有一个$java_home/jre/lib/management/jmxremote.password.template\n修改下即可。\njmxremote.password.template默认是只读权限。\n并更改为可写的权限 chmod a+w jmxremote.password\n修改后把jmxremote.password的读权限取消 a-r 。否则会提示：Error: Password file read access must be restricted: \nmanagement/jmxremote.access配置下权限。\n连接时指定ip：端口和jmx连接即可。\n远程的线程dump可以dump查看，堆的dump是需要从远程拉到本地来查看。\n\n```\n\n\n\njconsole和jvisualvm远程\n\n```\njconsole:\n在连接界面有显示本地连接或者远程连接,输入相应的主机名和jmx的端口号即可。\njvisualvm：\n在远程的选项添加计算机，然后右键添加jmx连接或者jstatd连接。\n```\n\n","tags":["JVM"],"categories":["Java"]},{"title":"Java虚拟机--------JVM常见参数","url":"/2018/11/12/Java虚拟机-JVM常见参数/","content":"\n{{ \"JVM系列常见参数及学习思路\"}}： <Excerpt in index | 首页摘要><!-- more -->\n\n## JVM 调优常见参数\n\n```\nJava1.7的jvm参数查看一下官方网站。\nhttp://docs.oracle.com/javase/7/docs/technotes/tools/windows/java.html\nJava1.8\nhttp://docs.oracle.com/javase/8/docs/technotes/tools/windows/java.html\nHotspotvm知识查看一下官方网站。\nhttp://www.oracle.com/technetwork/java/javase/tech/index-jsp-136373.html\n主要的参数是：堆的大小、栈的大小、新生代和老年代的比值、新生代中eden和s0、s1的比值。\n-Xms:初始堆大小，默认是物理内存的1/64。默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到--Xmx的最大限制。例如：-Xms 20m。\n-Xmx:最大堆大小。默认是物理内存的1/4  默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。\n-XX:NewSize=n：设置年轻代大小（初始值）。 \n-XX:MaxNewSize：设置年轻代最大值。\n-XX:NewRatio=n:设置年轻代和年老代的比值。\n-XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。\n-XX:PermSize（1.8之后改为MetaspaceSize）  设置持久代(perm gen)初始值，默认是物理内存的1/64。\n-XX:MaxPermSize=n:（1.8之后改为MaxMetaspaceSize）设置最大持久代大小。\n-Xss：每个线程的堆栈大小。\n```\n\n## JVM 学习思路\n\n```\njdk --> java develop kit ,jre ---> java runtime environment，java 有个特点：一次编译运行，就是有这个jvm，jvm，java viturl machine\njvm ---> 一、程序隔离区，一个程序共享区\n\njvm -->堆---> 为了内存溢出---> gc(垃圾)--->判断（引用、可达性分析）--->最终判断生死需要两次判断（如果其中调用了finalize方法，就会复活，不会被回收）---> 回收算法（标记+清除+整理，复制）--->选择合适的回收器（paranew + cms）--> 对象进入老年代得三种可能，---> jvm分析工具（jps，jmap，jconsole，jstat）\n```\n\n","tags":["JVM"],"categories":["Java"]},{"title":"Java虚拟机------垃圾收集器","url":"/2018/11/11/Java虚拟机------垃圾回收器/","content":"\n{{ \"JVM系列最核心的文章没有之一\"}}： <Excerpt in index | 首页摘要><!-- more -->\n\n## 引用\n### 强引用 \n\n只要引用存在,垃圾回收器就永远不会回收。当内存空足，Java虚拟机宁愿抛出**OutOfMemoryError**错误，使程序异常终止，也不会靠随意回收具有强引用的对象来解决内存不足的问题。\n\n```java\nObject obj = new Object();\n```\n\n//可直接通过obj取得对应的对象 如obj.equels(new Object());\n\n而这样 obj对象对后面new Object的一个强引用，只有当obj这个引用被释放之后，对象才会被释放掉，这也是我们经常所用到的编码形式。\n\n### 软引用\n\n如果一个对象只具有软引用，则**内存空间足够，垃圾回收器就不会回收它；如果内存空间不足了，就会回收这些对象的内存。只要垃圾回收器没有回收它，该对象就可以被程序使用。软引用可用来实现**内存敏感的高速缓存**\n\n软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收器回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中。非必须引用，内存溢出之前进行回收，可以通过以下代码实现\n\n```java\nObject obj = new Object();\nSoftReference<Object> sf = new SoftReference<Object>(obj);\nobj = null;\nsf.get();//有时候会返回null\n```\n\n\n\n### 弱引用\n\n弱引用与软引用的区别在于：只具有弱引用的对象拥有**更短暂**的生命周期。在垃圾回收器线程扫描它所管辖的内存区域的过程中，**一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存**。不过，由于垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象。\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n\n弱引用是在第二次垃圾回收时回收，**短时间内通过弱引用取对应的数据，可以取到**，当执行过第二次垃圾回收时，将返回null。\n弱引用主要用于监控对象是否已经被垃圾回收器标记为即将回收的垃圾，可以通过弱引用的isEnQueued方法返回对象是否被垃圾回收器标记。\n\n|              | 生命周期                             | 数据                                                         |\n| ------------ | ------------------------------------ | ------------------------------------------------------------ |\n| **软引用:**  | 内存空间足够，垃圾回收器就不会回收它 | 内存空间足够是都可以取到.(**存敏感的高速缓存**)              |\n| **弱引用**() | 不管当前内存空间足够与否，都会回收它 | 回收短时间内通过弱引用取对应的数据，可以取到,执行过第**二次垃圾回收**时，将返回null。 |\n\n\n\n### 虚引用\n\n“虚引用”顾名思义，就是形同虚设，与其他几种引用都不同，虚引用并不会决定对象的生命周期。如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。\n虚引用主要用来跟踪对象被垃圾回收器回收的活动。虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之 关联的引用队列中。\n\n对被引用对象的生存时间不影响；无法通过虚引用来取得一个对象实例；为一个对象设置虚引用关联的**唯一目的**就是能在这个对象被收集器回收时收到一个系统通知；jdk提PhantomReference类来实现虚引用。\n\n## 对象的可达性分析\t\n\n判断对象存活，常用的方式是**引用计数算法、可达性分析**：主流的商用程序语言的主流实现中是通过可达性分析；\n\n引用计数算法基本思想：给对象中添加一个引用计数器，每当对象被一个地方引用，计数器便+1；当引用失效时，计数器-1。当对象的计数器为0时，该对象便是一个不被使用的对象，即“死亡”。\n\n### 引用计数法\n\n[![计数法](https://s1.ax1x.com/2018/11/18/izjqxJ.md.png)\n\n\n引用计数器实现简单，效率高。然而难以解决对象之间相互循环引用的问题（两个失效对象相互保存了对方的指针）。故JVM判定对象是否存活，并没有使用引用计数器，而是使用可达性分析算法。\n\n可达性分析基本思想：通过一系列的称为“GC    Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain），当一个对象到GC   Roots没有任何引用链相连（即不可达）时，则证明此对象是不可用的。\n\nGC Roots对象：\n\n1. 虚拟机栈（栈帧中的本地变量表）中引用的对象。\n2. 方法区中类静态属性引用的对象。\n3. 方法区中常量引用的对象。\n4. 本地方法栈中JNI（Java Native Interface即一般说的Native方法）引用的对象。\n\n[![izjHGF.md.png](https://s1.ax1x.com/2018/11/18/izjHGF.md.png)\n\n　　          目前可达性分析算法已是主流算法，原因是，引用计数器算法无法解决**对象互相引用**的问题\n\n   \n\n### 对象的生死\n\n不可达的对象真正死亡**需要两次标记**\n\n可达性分析判断过程:\n\n对对象进行可达性分析之后，发现他没有**任何引用链相连**，则对他进行**第一次标记*，并进行一次筛选，筛选条件是是否需要执行finalize()方法，当对象没有覆盖finalize方法或已经finalize方法已经被调用过，则认为没有必要执行，如果判断为有必要执行，则会将他放入一个叫做F-Queue的队列中，稍后JVM会**自动建立一个低优先级**的finalizer线程去执行这些对象的finalize方法，Finalize()方法是对象逃脱死亡命运的最后一次机会，稍后GC将对F-Queue中的对象进行第二次小规模的标记，如果对象要在finalize()中重新与引用链上的任何一个对象建立关联那么他被移除出“即将回收”的集合，否则就被回收了。\n\n  ```java\n@Override\n\n  protected void finalize() throws Throwable {\n\n  System.out.println(\"finalized\");\n\n  }\n  ```\n\n## 垃圾收集器-GC算法\n\n\n\n- **最基础**的收集算法是“标记-清除”（Mark-Sweep）算法，此方法分为两个阶段：标记、清除。\n\n   标记所有要回收对象,标记完成后,统一回收所有被标记的对象\n\n   不足:一个是效率问题，标记和清除两个过程的效率都不高；另一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致以后在程序运行过程中需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。\n\n\n\n![izj7PU.png](https://s1.ax1x.com/2018/11/18/izj7PU.png)\n\n\n\n\n* **复制算法**:它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 \n\n\n\n  **无内存碎片**，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。缺点：实际可用内存缩小为了原来的一半。\n\n  大多数的对象(98%)存活时间很短,不需要1:1划分两个区域\n\n  1块较大的Eden和2快较小的Survivor空间,每次使用eden和一块survivor之后,将保存存活的对象复制到另外一块survivor区域里\n\n  survivor空间不够需要老年代进行分配担保​      \n\n  **HotSpot虚拟机默认Eden和Survivor的大小比例是8:1:1；浪费10%。**\n\n![复制算法](https://s1.ax1x.com/2018/11/18/izjb24.jpg)\n\n\n\n\n- **标记-整理算法**\n\n   标记需要回收的对象,将存活对象移动到一段,然后将端边界以外的内存回收\n\n   ![izjo5T.md.png](https://s1.ax1x.com/2018/11/18/izjo5T.md.png)\n\n\n\n\n* **分代收集算法(当前商业虚拟机采用的垃圾收集算法)   **\n\n                将堆分为新生代和老年代\n                新生代每次都有少量对象存活,用复制算法,只需付出付出少量存活对象的成本\n                老年代对象存活率高,用标记-清除-或者标记整理算法(没有额外空间进行分配担保)\n\n## 垃圾收集器-GC方式:\n\n\n\nMinor GC：从年轻代空间（包括 Eden 和 Survivor 区域）回收内存被称为 Minor GC。每次 Minor GC 会清理年轻代的内存。\n\nMajor GC 是清理老年代或者永久代。\n\nFull GC 是清理整个堆空间—包括年轻代和老年代或者永久代。\n\n|           |       清理       |           线程           |\n| :-------: | :--------------: | :----------------------: |\n| Major GC  | 老年代或者永久代 |   并发的处理而不用停掉   |\n| Full   GC |  清理整个堆空间  | 停掉所有(stop-the-world) |\n\n## 内存分配策略 \n\n对象优先在Eden分配：大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够大的连续空间进行分配时，虚拟机将发起一次Minor GC，此时对象会进如survivor区，当对象满足一些条件后会进入老年代。\n\n三种方式进入老年代：\n\n长期存活的对象将进入老年代：虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC，年龄就增加1岁，当它的**年龄增加到一定程度**（默认为15岁），就将会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold设置。\n\n如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。\n\n大对象:需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组\n\n**大对象直接进入老年代：**虚拟机提供了一个-XX:PretenureSizeThreshold参数，令大于这个设置值的对象直接在老年代分配。这样做的目的是避免在Eden区及两个Survivor区之间发生大量的内存复制。\n\n**空间分配担保：**在发生Minor GC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那么Minor GC可以确保是安全的。\n\n如果不成立，则虚拟机会查看XX:HandlePromotionFailure设置值是否允许担保失败。\n\n如果允许，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试着进行一次Minor GC，尽管这次Minor GC是有风险的；如果小于，或者HandlePromotionFailure设置不允许冒险，那这时也要改为进行一次Full GC。\n\n\n\n## 垃圾收集器------收集器\n\n收集器就是内存回收的具体实现。\n\njava虚拟机规范没有对收集器应该如何实现有任何规定，因为不同版本、不同厂商的虚拟机提供的垃圾收集器都可能会有很大的差异。\n\n目前讨论jdk1.7之后的hotspot虚拟机（这个版本正式提供了商用的G1收集器，之前都是实验状态）。\n\n\n\n### 垃圾收集器------并行和并发\n\n并行（Parallel）：指多条垃圾收集线程并行工作，但是此时：**用户线程**仍然处于**线程等待**状态。\n\n并发（Concurrent）：指用户线程和垃圾收集线程同时执行（但不一定是并行的，可能会交替执行），用户程序在**继续运行**，而垃圾收集程序运行于另一个cpu上。\n\n\n\n### 垃圾收集器-Serial\n\nSerial收集器是最基础、历史最悠久的新生代的收集器。\n\n特点：单线程、stop-the-world 、复制算法\n\n缺点：影响用户响应时间\n\n优点：回收时简单高效、对于限定单个cpu环境下，serial收集器由于没有线程交互的开销，专心做垃圾收集，可以获得最高的单线程收集效率。\n\n所以：serial 收集器 对于运行在client模式下的虚拟机来说，是一个很好的选择。\n\nSerialOld收集器是Serial的**老年代收集器**，采用“标记-整理”\n\n![izjIaV.md.png](https://s1.ax1x.com/2018/11/18/izjIaV.md.png)\n\n\n\n### 垃圾收集器-ParNew\n\nParNew收集器其实是Serial的多线程版本，除了他是使用多条线程来进行垃圾回收之外和Serial是完全一样的。新生代收集器\n\n特点：多线程、stop-the-world\n\n缺点：单个cpu下，运行效果甚至没Serial好。\n\n优点点：回收时简单高效、对于限定多个cpu环境下，效果比serial好，相比其它多线程收集器它可以和CMS收集器配合使用。\n\n所以：parnew收集器是运行在server模式下的首选收集器。\n\n\n\n![izjg2Q.md.jpg](https://s1.ax1x.com/2018/11/18/izjg2Q.md.jpg)\n\n\n\n\n\n### 垃圾收集器-Parallel Scanvenge\n\nParallel Scanvenge收集器是一个新生代收集器，采用复制算法。\n\n特点：收集新生代，复制算法，多线程，高吞吐、自适应\n\n1、与其它的收集器侧重垃圾回收时用户的停顿时间不同，它主要侧重与吞吐量，吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)。\n\n停顿时间越短就越适合需要与用户交互的程序，高吞吐量则是可以高效率地利用cpu时间尽快完成任务。\n\n2、他有一个自适应开关(-XX:+UseAdaptiveSizePolicy)：打开后，用户只需要把基本的内存数据（堆最大，初始量）设置好，然后设置更关注最大停顿时间或者更关注吞吐量，收集器会把细节参数自动调节。\n\nParallel Old 老年代收集器，采用标记-整理算法。\n\n![izjWKs.jpg](https://s1.ax1x.com/2018/11/18/izjWKs.jpg)\n\n\n\n### 垃圾收集器-CMS\n\nCMS（concurrent mark sweep）收集器是一个以获取最短回收停顿时间为目标的老年代收集器。\n\n特点：并发收集、低停顿。\n\n基于 标记-清除算法实现，但是整个过程比较复杂一些。过程分为4步：\n\n1. 初始标记：仅仅标记GCRoot能直接关联到的对象。速度很快，“stop the world”\n\n2. 并发标记：GCRoot Tracing。耗时长和用户线程同步。\n\n3. 重新标记：修正并发标记时，由于用户程序运行导致的标记变动。“stop the world”停顿稍长一些。\n\n4. 并发清除：耗时长，和用户线程同步。\n\n缺点：吞吐量会变低、浮动垃圾无法处理、标记-清除的碎片（设置参数是 fullgc前开启碎片整理功能，gc停顿时间延长）。\n\n\n\n![izjhbq.jpg](https://s1.ax1x.com/2018/11/18/izjhbq.jpg)\n\n\n\n### 垃圾收集器-G1\n\nG1（Garbage-First）收集器是当今收集器领域最前沿成果之一。2004年sun发表第一篇G1论文，10年后才开发出G1的商用版本。\n\nhotspot开发团队赋予它的使命：未来替调CMS收集器。\n\n特点：\n\n1. 并行与并发：利用多cpu缩短stop-the-world的时间，使用并发方式解决其它收集器需要停顿的gc动作。\n\n2. 分代收集：新老代收集区分对待。\n\n3. 空间整合：G1从整理看是基于标记-整理，但是局部看是基于复制算法实现的，不会产生碎片。\n\n4. 可预测的停顿：能够让使用者指定在M毫秒的时间片段上，消耗在垃圾回收的时间不得超过N毫秒。\n\n过程：初始标记、并发标记、最终标记、筛选回放。前三个和CMS一致，筛选回放是根据用户设置的停顿目标来选择回收价值最高的进行回收。\n\n![izj5V0.md.jpg](https://s1.ax1x.com/2018/11/18/izj5V0.md.jpg)\n\n\n\n\n\n\n\n## 垃圾回收器比较\n\n| GC器                  | 特点                                         | 优点                                                         | 缺点                                                         | 应用场景                                                     |\n| --------------------- | -------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n| Serial                | 单线程、stop-the-world 、复制算法            | 回收时简单高效、对于限定单个cpu环境下，serial收集器由于没有线程交互的开销，专心做垃圾收集，可以获得最高的单线程收集效率。 | stop the world给用户带来不良体验，如计算机每运行一段时间就暂停下来响应几分钟处理垃圾收收集。 | client模式下的默认新生代收集器。                             |\n| **ParNew****收集器*   | 多线程、stop-the-world                       | 回收时简单高效、对于限定多个cpu环境下，效果比serial好，相比其它多线程收集器它可以和CMS收集器配合使用。 |                                                              | 运行在Server模式下的VM首选新生代收集器。                     |\n| **Parallel Scavenge** | 收集新生代，复制算法，多线程，高吞吐、自适应 | 更关注吞吐量，即吞吐量                                       | 牺牲停顿时间，交互性差；                                     | 适用于后台运算而不需要太多的交互的任务。                     |\n| CMS                   | 并发收集、低停顿。                           | 并发收集；低停顿。                                           | 吞吐量会变低、浮动垃圾无法处理、标记-清除的碎片（设置参数是 fullgc前开启碎片整理功能，gc停顿时间延长）。 | 在互联网站或者B/S系统的服务端上，**重视服务的响应速度**，希望系统停顿时间最短，给用户带来较好的体验。 |\n| G1                    |                                              | 1.并发与并行；2.分代收集；3.空间整合；4.可预测的停顿。       |                                                              | 面向服务端应用。                                             |","tags":["JVM"],"categories":["Java"]},{"title":"Java虚拟机------JVM内存区域","url":"/2018/11/10/Java虚拟机------JVM内存区域/","content":" {{ \"JVM内存区域运行时数据区域分为两种\"}}： <Excerpt in index | 首页摘要><!-- more -->\n\n## JVM内存区域\n\n运行时数据区域分为两种:\n\n1. 线程隔离的数据区:\n\n   - 程序计数器\n   - Java虚拟机栈\n   - 本地方法栈\n\n2. 所有线程程共享的数据区:\n\n   - Java堆\n\n   - 方法区\n\n\n![izxikV.png](https://s1.ax1x.com/2018/11/18/izxikV.png)   \n\n## JVM 内存区域 ------栈介绍\n\n所谓 \"栈\"包括:Java虚拟机栈,本地方法栈,他们的作用相似,区别只是:\n\n**虚拟机栈:虚拟机栈为虚拟机执行Java方法(也就是字节码)服务**\n\n**本地方法栈: 虚拟机使用到的Native方法服务**\n\n**程序员人为的分为“堆栈”中的“栈”。**\n\n栈里存放了编译期可知的各种**基本数据类型**（boolean、byte、char、short、int、float、long、double）、**对象引用**和指向了一条字节码指令的**地址**。\n\n每个**方法**在执行的同时都会创建一个**栈帧（StackFrame）**用于存储**局部变量表、操作数栈、动态链接、方法出口等信息。**每一个方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。\n\n局部变量表所需的内存空间在编译期间完成分配，其中**64位的long和double类型的数据会占2个局部变量空间，其余的数据类型只占用1个。**当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。\n\n操作数栈也要操作栈，主要是在方法计算时存放的栈。\n\n## JVM 内存区域 ------堆介绍\n\nJava堆（Java Heap）是Java虚拟机所管理的内存中最大的一块，此内存区域就是存放**对象实例，**几乎所有的对象实例都在这里分配内存。\n\nJava堆是垃圾收集器管理的主要区域；内存回收的角度来看Java堆中还可以细分为：**新生代和老年代**；新生代细致一点的有Eden空间、From Survivor空间、To Survivor空间。\n\n在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx设置最大内存和-Xms设置初始内存）\n\n## JVM内存区域-方法区 \n\n方法区又叫**静态区：**用于存储已被虚拟机加载的**类信息、常量池、静态变量**、即时编译器**编译后的代码**等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆）；\n\n对于HotSpot虚拟机是使用**永久代**来实现方法区；\n\nJava虚拟机规范对方法区的限制非常宽松，除了不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，这区域的内存回收目标主要是针对**常量池**的回收和对类型的卸载，条件相当苛刻。\n\n```java\npublic class Test{\n\npublic static void main(String[] args){\n\n//s1,s2分别位于堆中不同空间\n\nString s1=new String(\"hello\");\n\nString s2=new String(\"hello\");\n\nSystem.out.println(s1==s2);//输出false\n\n//s3,s4位于池中同一空间\n\nString s3=\"hello\";\n\nString s4=\"hello\";\n\nSystem.out.println(s3==s4);//输出true\n\n\t}\n}\n```\n\n## JVM 内存区域-----异常\n\n程序计数器\n**没有指定**任何OutOfMemoryError情况\njava虚拟机栈\\本地方法栈区域\n如果线程请求的栈深度**大**于虚拟机所允许的深度，将抛出StackOverflowError异常；如果扩展时无法申请到足够的内存，就会抛出OutOfMemoryError异常\n堆\n如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常报错后dump出信息： -XX:+HeapDumpOnOutOfMemoryError\n方法区\n当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常\n\n\n\n\n","tags":["JVM"],"categories":["Java"]},{"title":"Java虚拟机------JVM介绍","url":"/2018/11/10/Java虚拟机------JVM介绍/","content":" {{ \"Java平台和语言最开始只是SUN公司在1990年12月开始研究的一个内部项目\"}}：<Excerpt in index | 首页摘要><!-- more -->\n\n## Java的平台无关性\nJava平台和语言最开始只是SUN公司在1990年12月开始研究的一个内部项目***[stealth 秘密行动]***,主要研究：交互式电视、烤面包箱等微型系统.Sun内部人员把这个项目称为***Green  JamesGosling[詹姆斯·高斯林]***\n\n消费类电子产品要求可靠性高、费用低、标准化、使用简单，用户并不关心CPU的型号，也不欣赏专用昂贵的RISC处理器，他们需要这些产品**是建立在一个标准的基础之上，有一系列可选的方案。**为了使整个系统与平台无关，Gosling首先从改写C编译器着手。但是Gosling在改写过程中感到仅C是无法满足需要的，于是在1991年6月份开始准备**开发一个新的语言。**\n\nGosling在设计中采用了虚机器码（VirtualMachineCode）方式，即Java语言编译后产生的是虚拟机码，虚拟机码运行在一个解释器上，每一个操作系统均有一个**解释器**。这样一来，Java就成了平台无关语言。\n\n## JVM简介\n\nJvm英文全称：Java Virtual Machine（Java虚拟机）。\n\n虚拟机：**通过软件来模拟出来的具有完整的硬件系统功能的、运行在完全隔离的环境中的完整的计算机系统。**例如：在人工的草原养殖场模拟真实的草原环境，你们家里的小池塘模拟真实的海洋环境。jvm世界观：java对象在jvm里的生老病死。Jvm一样也是通过在实际的计算机中软件虚构出来的，用来模拟一套完整的运行平台。**Java的一次编写处处运行：是指在一段代码可以在不同的平台运行，这就归功于jvm。作为java编译器和os之间的虚拟解释器，jvm根据不同的os，将java编译后的目标代码（字节码）解释成不同os可以运行的机器指令**，所以说：有了jvm之后呢，java语言在不同平台上运行时不需要重新编译。一次编写，处处运行！\n\n## JVM发展史\n\n从1996年初Sun公司发布的JDK 1.0中所包含的Sun Classic VM至今有很多jvm；\n\n本次详细介绍下跟我们有关的虚拟机\n\n1. Sun Classic\n2. Exact VM\n3. Sun HotSpot VM \n\n其余的比如：\n\n1. Sun Mobile-Embedded VM / Meta-Circular VM\n\n2. BEA JRockit / IBM J9 VM\n\n3. Azul VM / BEA Liquid VM\n\n## JVM发展史-Exact VM\n\n**JDK 1.2**时，曾在Solaris平台上发布过一款名为Exact VM的虚拟机，它的执行系统已经具备现代高性能虚拟机的雏形：如支持编译器与解释器混合工作模式。\n\nExact VM因它使用准确式内存管理（Exact Memory Management，也可以叫Non-Conservative/Accurate Memory Management）而得名。\n\n虚拟机可以知道内存中某个位置的数据具体是什么类型。譬如内存中有一个32位的整数123456，它到底是一个reference类型指向123456的内存地址还是一个数值为123456的整数，虚拟机将有能力分辨出来，这样才能在GC（垃圾收集）的时候准确判断堆上的数据是否还可能被使用。\n\n由于使用了准确式内存管理，Exact VM可以抛弃以前Classic VM基于handle的对象查找方式每次定位对象都少了一次间接查找的开销，提升执行性能。\n\n基于handle(句柄)的对象查找：当123456指向的对象经过一次垃圾回收后，内存地址需要重新规整。内存地址发生了变化为654321，不能将内存中所有的值为123456数据都改为654321。**使用句柄来管理对象内存地址的变动，所以定位对象时先去句柄查下实际的地址再去查找对象本身的属性。类似于对象的户口变更登记册。**\n\n句柄可以理解为：引用的引用。指针的指针。\n\n## 发展史- SC/EV\n\nSun Classic / Exact VM的生命周期\n\n虽然Exact VM的技术相对Classic VM来说先进了许多，但是在商业应用上只存在了很短暂的时间就被更为优秀的**HotSpot VM**所取代，甚至还没有来得及发布Windows和Linux平台下的商用版本。\n\n而Classic VM的生命周期则相对长了许多，它在JDK 1.2之前是Sun JDK中唯一的虚拟机，在JDK 1.2时，它与HotSpot VM并存，但默认使用的是Classic VM（用户可用java -hotspot参数切换至HotSpot VM），而在**JDK 1.3时，HotSpot VM成为默认虚拟机**，但Classic VM仍作为虚拟机的“备用选择”发布（使用java -classic参数切换），**直到JDK 1.4的时候，Classic VM才完全退出商用虚拟机的历史舞台**\n\n## JVM发展史-热点代码探测技术\n\nHotSpot VM的热点代码探测技术\n\n1. **可以通过执行计数器找出最具有编译价值的代码，**根据执行计数器判断是否达到阈值，没到就解释执行，否则提交编译请求，通知JIT编译器以方法为单位进行编译。\n\n所以：如果一个方法被频繁调用，或方法中有效循环次数很多，将会分别触发标准编译和OSR（栈上替换）编译动作。OSR：由于代码块可能是在代码块在解释执行过程中直接切换到本地代码执行，所以也叫做栈上替换（OSROnStackReplacement）\n\n2. **通过编译器与解释器恰当地协同工作**，可以在最优化的程序响应时间与最佳执行性能中取得平衡，即时编译的时间压力也相对减小，这样有助于引入更多的代码优化技术，**输出质量更高的本地代码[机器执行码]。**","tags":["JVM"],"categories":["Java"]}]