<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="L6Lm9d5Crl"/>
  
  
  
  
  <title>Spark内核解析1 | 菜鸟清风</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spark通讯架构 脚本探究：">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark内核解析1">
<meta property="og:url" content="https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="菜鸟清风">
<meta property="og:description" content="Spark通讯架构 脚本探究：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190609103825.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609114709.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115516.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115616.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120140.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120949.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121127.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609122558.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123350.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121509.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123756.png">
<meta property="article:published_time" content="2019-06-09T02:32:57.000Z">
<meta property="article:modified_time" content="2020-01-12T13:08:27.790Z">
<meta property="article:author" content="清风笑丶">
<meta property="article:tag" content="Spark">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190609103825.png">
  
    <link rel="alternative" href="/https://blog.csdn.net/weixin_39084521/rss/list" title="菜鸟清风" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  
  
<link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

        <a href="/" class="profilepic">
            
            <img lazy-src="/img/avatar.jpg" class="js-avatar">
            
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">清风笑丶</a></h1>
        </hgroup>
        
        
            <form>
                <input type="text" class="st-default-search-input search" id="local-search-input" placeholder="搜索一下" autocomplete="off">
            </form>
            <div id="local-search-result"></div>
        
        
            <script type="text/javascript">
                (function() {
                    'use strict';
                    function getMatchData(keyword, data) {
                        var matchData = [];
                        for(var i =0;i<data.length;i++){
                            if(data[i].title.toLowerCase().indexOf(keyword)>=0) 
                                matchData.push(data[i])
                        }
                        return matchData;
                    }
                    var $input = $('#local-search-input');
                    var $resultContent = $('#local-search-result');
                    $input.keyup(function(){
                        $.ajax({
                            url: '/search.json',
                            dataType: "json",
                            success: function( json ) {
                                var str='<ul class=\"search-result-list\">';                
                                var keyword = $input.val().trim().toLowerCase();
                                $resultContent.innerHTML = "";
                                if ($input.val().trim().length <= 0) {
                                    $resultContent.empty();
                                    $('#switch-area').show();
                                    return;
                                }
                                var results = getMatchData(keyword, json);
                                if(results.length === 0){
                                    $resultContent.empty();
                                    $('#switch-area').show();
                                    return;
                                } 
                                for(var i =0; i<results.length; i++){
                                    str += "<li><a href='"+ results[i].url +"' class='search-result-title'>"+ results[i].title +"</a></li>";
                                }
                                str += "</ul>";
                                $resultContent.empty();
                                $resultContent.append(str);
                                $('#switch-area').hide();
                            }
                        });
                    });
                })();
            </script>
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        
        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a  href="/archives/">所有文章</a></li>
                        
                            <li><a  href="/categories/Java/">Java</a></li>
                        
                            <li><a  href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE">大数据</a></li>
                        
                            <li><a  href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93">数据库</a></li>
                        
                            <li><a  href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">数据结构</a></li>
                        
                            <li><a  href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl github"  target="_blank" href="https://github.com/bigdataxiaohan" title="github">github</a>
                            
                                <a class="fl zhihu"  target="_blank" href="https://www.zhihu.com/people/qing-feng-xiao-zhu-15/activities" title="zhihu">zhihu</a>
                            
                                <a class="fl mail"  target="_blank" href="mailto:467008580@qq.com" title="mail">mail</a>
                            
                        </ul>
                    </nav>
                </section>
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/AVL%E6%A0%91/" style="font-size: 10px;">AVL树</a> <a href="/tags/Docker/" style="font-size: 14.44px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 10px;">Dubbo</a> <a href="/tags/Elasticsearch/" style="font-size: 17.78px;">Elasticsearch</a> <a href="/tags/Eureka/" style="font-size: 10px;">Eureka</a> <a href="/tags/Feign/" style="font-size: 10px;">Feign</a> <a href="/tags/Flink/" style="font-size: 10px;">Flink</a> <a href="/tags/Flume/" style="font-size: 11.11px;">Flume</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/GraphX/" style="font-size: 10px;">GraphX</a> <a href="/tags/HBase/" style="font-size: 11.11px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 11.11px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 13.33px;">Hadoop</a> <a href="/tags/Hbase/" style="font-size: 11.11px;">Hbase</a> <a href="/tags/Hive/" style="font-size: 13.33px;">Hive</a> <a href="/tags/Hystrix/" style="font-size: 10px;">Hystrix</a> <a href="/tags/JPA/" style="font-size: 10px;">JPA</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JSR107/" style="font-size: 10px;">JSR107</a> <a href="/tags/JVM/" style="font-size: 14.44px;">JVM</a> <a href="/tags/JavaWeb/" style="font-size: 10px;">JavaWeb</a> <a href="/tags/Kafka/" style="font-size: 14.44px;">Kafka</a> <a href="/tags/MapReduce/" style="font-size: 14.44px;">MapReduce</a> <a href="/tags/Memcached/" style="font-size: 10px;">Memcached</a> <a href="/tags/MongoDB/" style="font-size: 15.56px;">MongoDB</a> <a href="/tags/Mybatis/" style="font-size: 15.56px;">Mybatis</a> <a href="/tags/Oozie/" style="font-size: 10px;">Oozie</a> <a href="/tags/RDD/" style="font-size: 14.44px;">RDD</a> <a href="/tags/REST/" style="font-size: 10px;">REST</a> <a href="/tags/RPC/" style="font-size: 10px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 11.11px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15.56px;">Redis</a> <a href="/tags/Ribbon/" style="font-size: 10px;">Ribbon</a> <a href="/tags/SSM/" style="font-size: 11.11px;">SSM</a> <a href="/tags/SparKSQL/" style="font-size: 12.22px;">SparKSQL</a> <a href="/tags/Spark/" style="font-size: 20px;">Spark</a> <a href="/tags/SparkStreaming/" style="font-size: 12.22px;">SparkStreaming</a> <a href="/tags/Spring/" style="font-size: 14.44px;">Spring</a> <a href="/tags/Spring-Security/" style="font-size: 10px;">Spring Security</a> <a href="/tags/SpringBoot/" style="font-size: 16.67px;">SpringBoot</a> <a href="/tags/SpringBoot-Admin/" style="font-size: 10px;">SpringBoot Admin</a> <a href="/tags/SpringCloud/" style="font-size: 18.89px;">SpringCloud</a> <a href="/tags/SpringConfig/" style="font-size: 10px;">SpringConfig</a> <a href="/tags/SpringMVC/" style="font-size: 15.56px;">SpringMVC</a> <a href="/tags/Sqoop/" style="font-size: 10px;">Sqoop</a> <a href="/tags/Structured-Streaming/" style="font-size: 10px;">Structured Streaming</a> <a href="/tags/Thymeleaf/" style="font-size: 10px;">Thymeleaf</a> <a href="/tags/Zookeeper/" style="font-size: 11.11px;">Zookeeper</a> <a href="/tags/zuul/" style="font-size: 10px;">zuul</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 11.11px;">二叉树</a> <a href="/tags/%E4%BB%BB%E5%8A%A1/" style="font-size: 10px;">任务</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 10px;">优先队列</a> <a href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" style="font-size: 10px;">哈希表</a> <a href="/tags/%E5%A0%86/" style="font-size: 10px;">堆</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 10px;">字典树</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 10px;">并查集</a> <a href="/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/" style="font-size: 10px;">微服务</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B/" style="font-size: 10px;">技术选型</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 10px;">数组</a> <a href="/tags/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">日志框架</a> <a href="/tags/%E6%A0%88/" style="font-size: 10px;">栈</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" style="font-size: 10px;">红黑树</a> <a href="/tags/%E7%BB%AA%E8%AE%BA/" style="font-size: 10px;">绪论</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 10px;">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size: 10px;">队列</a>
                    </div>
                </section>
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://blog.csdn.net/weixin_39084521?t=1">csdn</a>
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://segmentfault.com/u/qingfengxiao">segmentfault</a>
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://www.jianshu.com/u/67dbb2933255">简书</a>
                    
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">文科男,理工芯。有借必有贷,有问必有答。</div>
                </section>
                
            </div>
        </div>
    </header>
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">清风笑丶</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/avatar.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">清风笑丶</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/categories/Java/">Java</a></li>
                
                    <li><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE">大数据</a></li>
                
                    <li><a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93">数据库</a></li>
                
                    <li><a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">数据结构</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="github" target="_blank" href="https://github.com/bigdataxiaohan" title="github">github</a>
                    
                        <a class="zhihu" target="_blank" href="https://www.zhihu.com/people/qing-feng-xiao-zhu-15/activities" title="zhihu">zhihu</a>
                    
                        <a class="mail" target="_blank" href="mailto:467008580@qq.com" title="mail">mail</a>
                    
                </div>
            </nav>
        </header>
    </div>
</nav>
      <div class="body-wrap"><article id="post-Spark内核解析" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a  href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/" class="article-date">
      <time datetime="2019-06-09T02:32:57.000Z" itemprop="datePublished">2019-06-09</time>
</a>

    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark内核解析1
    </h1>
  


      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
         Spark通讯架构 脚本探究：<Excerpt in index | 首页摘要><a id="more"></a> 

<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark 内核泛指 Spark 的核心运行机制，包括 Spark 核心组件的运行机制、Spark 任务调度机制、Spark 内存管理机制、Spark 核心功能的运行原理。</p>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作。Driver 在 Spark 作业执行时主要负责：</p>
<ol>
<li><p>将用户程序转化为任务（job）；</p>
</li>
<li><p>在 Executor 之间调度任务（task）；</p>
</li>
<li><p>跟踪 Executor 的执行情况；</p>
</li>
<li><p>通过 UI 展示查询运行情况；</p>
</li>
</ol>
<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Spark Executor 节点是一个 JVM 进程，负责在 Spark 作业中运行具体任务，任务彼此之间相互独立。Spark 应用启动时， Executor  节点被同时启动， 并且始终伴随着整个 Spark  应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行， 会将出错节点上的任务调度到其他 Executor 节点上继续运行。</p>
<p>Executor 有两个核心功能：</p>
<ol>
<li><p>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程； </p>
</li>
<li><p>它们通过自身的块管理器（ Block Manager）为用户程序中要求缓存的 RDD提供内存式存储。RDD  是直接缓存在 Executor  进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</p>
</li>
</ol>
<h3 id="运行流程"><a href="#运行流程" class="headerlink" title="运行流程"></a>运行流程</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/Graphx/20190609103825.png" alt=""></p>
<p>Spark 通用运行流程， 不论 Spark 以何种模式进行部署， 任务提交后， 都会先启动 Driver 进程，随后 Driver 进程向集群管理器注册应用程序，之后集群管理器根据此任务的配置文件分配 Executor 并启动，当 Driver 所需的资源全部满足后， </p>
<p>Driver 开始执行 main 函数， Spark 查询为懒执行， 当执行到 action 算子时开始<code>反向推算</code>，根据宽依赖进行 stage 的划分，随后每一个 stage 对应一个 taskset，taskset 中有多个 task，根据本地化原则， task 会被分发到指定的 Executor 去执行，在任务执行的过程中， Executor 也会不断与 Driver 进行通信，报告任务运行情况。</p>
<h2 id="部署模式"><a href="#部署模式" class="headerlink" title="部署模式"></a>部署模式</h2><h3 id="Standalone"><a href="#Standalone" class="headerlink" title="Standalone"></a>Standalone</h3><p>Standalone 集群有四个重要组成部分， 分别是：</p>
<p>Driver： 是一个进程，我们编写的 Spark  应用程序就运行在 Driver  上， 由Driver 进程执行； </p>
<p>Master：是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责； </p>
<p>Worker：是一个进程，一个 Worker 运行在集群中的一台服务器上，主要负责两个职责，一个是用自己的内存存储 RDD 的某个或某些 partition；另一个是启动其他进程和线程（Executor） ，对 RDD 上的 partition 进行并行的处理和计算。</p>
<p>Executor：是一个进程， 一个 Worker 上可以运行多个 Executor， Executor 通过启动多个线程（ task）来执行对 RDD 的 partition 进行并行计算，也就是执行我们对 RDD 定义的例如 map、flatMap、reduce 等算子操作。</p>
<h4 id="Standalone-Client"><a href="#Standalone-Client" class="headerlink" title="Standalone Client"></a>Standalone Client</h4><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609114709.png" alt=""></p>
<p>Standalone Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后向 Master 注册应用程序，Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<h4 id="Standalone-Cluster"><a href="#Standalone-Cluster" class="headerlink" title="Standalone Cluster"></a>Standalone Cluster</h4><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115516.png" alt=""></p>
<p> Standalone Cluster 模式下，任务提交后，Master 会找到一个 Worker 启动 Driver进程， Driver 启动后向 Master 注册应用程序， Master 根据 submit 脚本的资源需求找到内部资源至少可以启动一个 Executor 的所有 Worker，然后在这些 Worker 之间分配 Executor，Worker 上的 Executor 启动后会向 Driver 反向注册，所有的 Executor 注册完成后，Driver 开始执行 main 函数，之后执行到 Action 算子时，开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<p>注意， Standalone  的两种模式下（ client/Cluster） ， Master  在接到 Driver  注册</p>
<p>Spark 应用程序的请求后，会获取其所管理的剩余资源能够启动一个 Executor 的所有 Worker， 然后在这些 Worker 之间分发 Executor， 此时的分发只考虑 Worker 上的资源是否足够使用，直到当前应用程序所需的所有 Executor 都分配完毕， Executor 反向注册完毕后，Driver 开始执行 main 程序。</p>
<h3 id="YARN-Client"><a href="#YARN-Client" class="headerlink" title="YARN Client"></a>YARN Client</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609115616.png" alt=""></p>
<p>在 YARN Client 模式下，Driver 在任务提交的本地机器上运行，Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster， 随后 ResourceManager 分配 container ， 在 合 适 的 NodeManager   上启动 ApplicationMaster ，此时的 </p>
<p>ApplicationMaster  的功能相当于一个 ExecutorLaucher， 只负责向 ResourceManager 申请 Executor 内存。</p>
<p>ResourceManager  接到 ApplicationMaster  的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程， Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 taskSet，之后将 task 分发到各个 Executor 上执行。</p>
<h3 id="YARN-Cluster"><a href="#YARN-Cluster" class="headerlink" title="YARN Cluster"></a>YARN Cluster</h3><p>在 YARN  Cluster  模式下， 任务提交后会和 ResourceManager  通讯申请启动ApplicationMaster， 随后 ResourceManager  分配 container，在合适的 NodeManager上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。</p>
<p>Driver 启动后向 ResourceManager 申请 Executor 内存， ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动 Executor 进程，Executor 进程启动后会向 Driver 反向注册， Executor 全部注册完成后 Driver 开始执行 main 函数，之后执行到 Action 算子时，触发一个 job，并根据宽依赖开始划分 stage，每个 stage  生成对应的 taskSet，之后将 task  分发到各个Executor 上执行。</p>
<h2 id="通讯架构"><a href="#通讯架构" class="headerlink" title="通讯架构"></a>通讯架构</h2><p>Spark2.x 版本使用 Netty 通讯框架作为内部通讯组件。spark  基于 netty 新的 rpc框架借鉴了 Akka 的中的设计， 它是基于Actor 模型。</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120140.png" alt=""></p>
<p>Scala里面处理通信采用Actor架构，Actor架构其实就是一个邮局模型， AKKA为给予Actor模型的工程实现。Akka不同版本之间无法通信，存在兼容性问题。用户使用Akka与Spark中的Akka存在冲突。Spark对Akka没有自身维护，需要新功能时只能等待新版本，比较牵制Spark发展。因此在Spark2中已经抛弃了Akka。</p>
<p>Spark早期版本中采用Akka作为内部通信部件。<br>Spark1.3中引入Netty通信框架，为了解决Shuffle的大数据传输问题使用<br>Spark1.6中Akka和Netty可以配置使用。Netty完全实现了Akka在Spark中的功能。<br>Spark2系列中，Spark抛弃Akka，使用Netty。</p>
<p>Spark 通讯框架中各个组件（ Client/Master/Worker）可以认为是一个个独立的实体，各个实体之间通过消息来进行通信。具体各个组件之间的关系图如下： </p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609120949.png" alt=""></p>
<p>Endpoint（ Client/Master/Worker）有 1 个 InBox 和 N 个 OutBox（ N&gt;=1，N 取决于当前 Endpoint 与多少其他的 Endpoint 进行通信， 一个与其通讯的其他 Endpoint 对应一个 OutBox）， Endpoint  接收到的消息被写入 InBox， 发送出去的消息写入OutBox 并被发送到其他 Endpoint 的 InBox 中。</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121127.png" alt=""></p>
<p>1) RpcEndpoint：RPC 端点，Spark 针对每个节点（ Client/Master/Worker）都称之为一个 Rpc 端点，且都实现 RpcEndpoint 接口，内部根据不同端点的需求，设计不同的消息和不同的业务处理，如果需要发送（询问）则调用 Dispatcher；</p>
<p>2) RpcEnv： RPC  上下文环境， 每个 RPC  端点运行时依赖的上下文环境称为RpcEnv；</p>
<p>3) Dispatcher：消息分发器，针对于 RPC 端点需要发送消息或者从远程 RPC 接收到的消息，分发至对应的指令收件箱/发件箱。如果指令接收方是自己则存入收件箱，如果指令接收方不是自己，则放入发件箱； </p>
<p>4)  Inbox：指令消息收件箱，一个本地 RpcEndpoint 对应一个收件箱，Dispatcher 在每次向 Inbox 存入消息时， 都将对应 EndpointData 加入内部 ReceiverQueue 中， 另外 Dispatcher 创建时会启动一个单独线程进行轮询 ReceiverQueue，进行收件箱消息消费； </p>
<p>5)  RpcEndpointRef：RpcEndpointRef 是对远程 RpcEndpoint 的一个引用。当我们需要向一个具体的 RpcEndpoint 发送消息时，一般我们需要获取到该 RpcEndpoint 的引用，然后通过该应用发送消息。</p>
<p>6)  OutBox ： 指令消息发件箱 ， 对于当前 RpcEndpoint 来说 ， 一个目标RpcEndpoint  对应一个发件箱， 如果向多个目标 RpcEndpoint 发送信息， 则有多个OutBox。当消息放入 Outbox 后，紧接着通过 TransportClient 将消息发送出去。消息放入发件箱以及发送过程是在同一个线程中进行； </p>
<p>7)  RpcAddress： 表示远程的 RpcEndpointRef 的地址， Host + Port。</p>
<p>8)  TransportClient：Netty 通信客户端，一个 OutBox 对应一个 TransportClient，TransportClient 不断轮询 OutBox，根据 OutBox 消息的 receiver 信息，请求对应的远程 TransportServer；</p>
<p>9)  TransportServer ： Netty通信服务端 ， 一 个 RpcEndpoint 对应一个TransportServer，接受远程消息后调用Dispatcher 分发消息至对应收发件箱； </p>
<p>RpcEndPoint就代表一个通信端点， 一个端点就有一个inbox，  一个 transportServer  一个 Dispatcher，  根据你通信的其他端点的数目，就有多个Outbox， 一个outbox有一个 transportClient，   transportClient主要负责和 transportServer来通信。</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609122558.png" alt=""></p>
<p>在我们的传统认知中，多个端点要通信，中间要有一个节点类似于总的路由，节点之间的通信靠中间的“路由”，而 Spark没有中间的这个“路由”，如果中间的“路由”存在一定会存在瓶颈问题。Spark很巧妙的把中间的“路由”拆分到各个节点上。</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123350.png" alt=""></p>
<h3 id="高层视图"><a href="#高层视图" class="headerlink" title="高层视图"></a>高层视图</h3><p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609121509.png" alt=""></p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/%E5%86%85%E6%A0%B8/20190609123756.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">trait</span> <span class="title">RpcEndpoint</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * The [[RpcEnv]] that this [[RpcEndpoint]] is registered to.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">val</span> rpcEnv: <span class="type">RpcEnv</span></span><br><span class="line">   ....</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.send]] or [[RpcCallContext.reply)]]. If receiving a</span></span><br><span class="line"><span class="comment">   * unmatched message, [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">" does not implement 'receive'"</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"> <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Process messages from [[RpcEndpointRef.ask]]. If receiving a unmatched message,</span></span><br><span class="line"><span class="comment">   * [[SparkException]] will be thrown and sent to `onError`.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">receiveAndReply</span></span>(context: <span class="type">RpcCallContext</span>): <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; context.sendFailure(<span class="keyword">new</span> <span class="type">SparkException</span>(self + <span class="string">" won't reply anything"</span>))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Invoked before [[RpcEndpoint]] starts to handle any message.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">onStart</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// By default, do nothing.</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>   RpcEndpoint  注意三个方法，</p>
<p>1、receive   改方法被子类实现，用于接收其他节点发送的消息。<br>2、receiveAndReply    该方法被子类实现，用于接收并回复其他节点发送的消息。<br>3、onStart    该方法被子类实现，该方法在端口启动的时候自动调用。</p>
<p>我们查看以下RpcEnv的实现发现实现是NettyRpcEnv</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[netty] <span class="class"><span class="keyword">class</span> <span class="title">NettyRpcEnv</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val conf: <span class="type">SparkConf</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    javaSerializerInstance: <span class="type">JavaSerializerInstance</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    host: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    securityManager: <span class="type">SecurityManager</span></span>) <span class="keyword">extends</span> <span class="title">RpcEnv</span>(<span class="params">conf</span>) <span class="keyword">with</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="keyword">val</span> transportConf = <span class="type">SparkTransportConf</span>.fromSparkConf(</span><br><span class="line">    conf.clone.set(<span class="string">"spark.rpc.io.numConnectionsPerPeer"</span>, <span class="string">"1"</span>),</span><br><span class="line">    <span class="string">"rpc"</span>,</span><br><span class="line">    conf.getInt(<span class="string">"spark.rpc.io.threads"</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置一个消息分发器</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> dispatcher: <span class="type">Dispatcher</span> = <span class="keyword">new</span> <span class="type">Dispatcher</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> streamManager = <span class="keyword">new</span> <span class="type">NettyStreamManager</span>(<span class="keyword">this</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> transportContext = <span class="keyword">new</span> <span class="type">TransportContext</span>(transportConf,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">NettyRpcHandler</span>(dispatcher, <span class="keyword">this</span>, streamManager))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createClientBootstraps</span></span>(): java.util.<span class="type">List</span>[<span class="type">TransportClientBootstrap</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (securityManager.isAuthenticationEnabled()) &#123;</span><br><span class="line">      java.util.<span class="type">Arrays</span>.asList(<span class="keyword">new</span> <span class="type">SaslClientBootstrap</span>(transportConf, <span class="string">""</span>, securityManager,</span><br><span class="line">        securityManager.isSaslEncryptionEnabled()))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      java.util.<span class="type">Collections</span>.emptyList[<span class="type">TransportClientBootstrap</span>]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> clientFactory = transportContext.createClientFactory(createClientBootstraps())</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A separate client factory for file downloads. This avoids using the same RPC handler as</span></span><br><span class="line"><span class="comment">   * the main RPC context, so that events caused by these clients are kept isolated from the</span></span><br><span class="line"><span class="comment">   * main RPC traffic.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * It also allows for different configuration of certain properties, such as the number of</span></span><br><span class="line"><span class="comment">   * connections per peer.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> fileDownloadFactory: <span class="type">TransportClientFactory</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> timeoutScheduler = <span class="type">ThreadUtils</span>.newDaemonSingleThreadScheduledExecutor(<span class="string">"netty-rpc-env-timeout"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Because TransportClientFactory.createClient is blocking, we need to run it in this thread pool</span></span><br><span class="line">  <span class="comment">// to implement non-blocking send/ask.</span></span><br><span class="line">  <span class="comment">// <span class="doctag">TODO:</span> a non-blocking TransportClientFactory.createClient in future</span></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="keyword">val</span> clientConnectionExecutor = <span class="type">ThreadUtils</span>.newDaemonCachedThreadPool(</span><br><span class="line">    <span class="string">"netty-rpc-connection"</span>,</span><br><span class="line">    conf.getInt(<span class="string">"spark.rpc.connect.threads"</span>, <span class="number">64</span>))</span><br><span class="line"></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> server: <span class="type">TransportServer</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> stopped = <span class="keyword">new</span> <span class="type">AtomicBoolean</span>(<span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * A map for [[RpcAddress]] and [[Outbox]]. When we are connecting to a remote [[RpcAddress]],</span></span><br><span class="line"><span class="comment">   * we just put messages to its [[Outbox]] to implement a non-blocking `send` method.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">    <span class="comment">// 多个地址对应的发件箱</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> outboxes = <span class="keyword">new</span> <span class="type">ConcurrentHashMap</span>[<span class="type">RpcAddress</span>, <span class="type">Outbox</span>]()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove the address's Outbox and stop it.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[netty] <span class="function"><span class="keyword">def</span> <span class="title">removeOutbox</span></span>(address: <span class="type">RpcAddress</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> outbox = outboxes.remove(address)</span><br><span class="line">    <span class="keyword">if</span> (outbox != <span class="literal">null</span>) &#123;</span><br><span class="line">      outbox.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 启动TransportServer来接收远程消息</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">startServer</span></span>(bindAddress: <span class="type">String</span>, port: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> bootstraps: java.util.<span class="type">List</span>[<span class="type">TransportServerBootstrap</span>] =</span><br><span class="line">      <span class="keyword">if</span> (securityManager.isAuthenticationEnabled()) &#123;</span><br><span class="line">        java.util.<span class="type">Arrays</span>.asList(<span class="keyword">new</span> <span class="type">SaslServerBootstrap</span>(transportConf, securityManager))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        java.util.<span class="type">Collections</span>.emptyList()</span><br><span class="line">      &#125;</span><br><span class="line">    server = transportContext.createServer(bindAddress, port, bootstraps)</span><br><span class="line">    dispatcher.registerRpcEndpoint(</span><br><span class="line">      <span class="type">RpcEndpointVerifier</span>.<span class="type">NAME</span>, <span class="keyword">new</span> <span class="type">RpcEndpointVerifier</span>(<span class="keyword">this</span>, dispatcher))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="meta">@Nullable</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">lazy</span> <span class="keyword">val</span> address: <span class="type">RpcAddress</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (server != <span class="literal">null</span>) <span class="type">RpcAddress</span>(host, server.getPort()) <span class="keyword">else</span> <span class="literal">null</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 注册当前端点</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">setupEndpoint</span></span>(name: <span class="type">String</span>, endpoint: <span class="type">RpcEndpoint</span>): <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">    dispatcher.registerRpcEndpoint(name, endpoint)</span><br><span class="line">  &#125;</span><br><span class="line">....</span><br></pre></td></tr></table></figure>

<p>我们似乎没有看到Inbox在哪里点击Dispatcher</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">EndpointData</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    val name: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val endpoint: <span class="type">RpcEndpoint</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val ref: <span class="type">NettyRpcEndpointRef</span></span>) </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> inbox = <span class="keyword">new</span> <span class="type">Inbox</span>(ref, endpoint)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="启动脚本"><a href="#启动脚本" class="headerlink" title="启动脚本"></a>启动脚本</h2><h3 id="start-all-sh"><a href="#start-all-sh" class="headerlink" title="start-all.sh"></a>start-all.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Start all spark daemons.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts the master on this node.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts a worker on each node specified <span class="keyword">in</span> conf/slaves</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then  #如果没有发现Spark环境变量</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)" # 获得当前的目录把当前目录设置为SPARK_HOME</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Load the Spark configuration</span></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh" #加载 spark-config.sh配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start Master</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-master.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start Workers</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/start-slaves.sh</span><br></pre></td></tr></table></figure>

<h3 id="spark-config-sh"><a href="#spark-config-sh" class="headerlink" title="spark-config.sh"></a>spark-config.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> included <span class="keyword">in</span> all the spark scripts with <span class="built_in">source</span> <span class="built_in">command</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> should not be executable directly</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> also should not be passed any arguments, since we need original $*</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> symlink and absolute path should rely on SPARK_HOME to resolve</span></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export SPARK_CONF_DIR="$&#123;SPARK_CONF_DIR:-"$&#123;SPARK_HOME&#125;/conf"&#125;" #设置 SPARK_CONF_DIR 目录</span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the PySpark classes to the PYTHONPATH:</span></span><br><span class="line">if [ -z "$&#123;PYSPARK_PYTHONPATH_SET&#125;" ]; then</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYTHONPATH="$&#123;SPARK_HOME&#125;/python/lib/py4j-0.10.4-src.zip:$&#123;PYTHONPATH&#125;"</span><br><span class="line">  export PYSPARK_PYTHONPATH_SET=1</span><br><span class="line">fi</span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.0_162</span><br></pre></td></tr></table></figure>

<h3 id="start-master-sh"><a href="#start-master-sh" class="headerlink" title="start-master.sh"></a>start-master.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Unless required by applicable law or agreed to <span class="keyword">in</span> writing, software</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> distributed under the License is distributed on an <span class="string">"AS IS"</span> BASIS,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> See the License <span class="keyword">for</span> the specific language governing permissions and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> limitations under the License.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Starts the master on the machine this script is executed on.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line">CLASS="org.apache.spark.deploy.master.Master"  #调用Master</span><br><span class="line"></span><br><span class="line">if [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-master.sh [options]"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">ORIGINAL_ARGS="$@"</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"  </span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh" #加载环境变量</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then # 如果没有端口 默认7077</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)           # 如果没有设置HOST 则把/usr/sbin/check-hostname作为主机名</span><br><span class="line">          SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">          ;;</span><br><span class="line">      (*)</span><br><span class="line">          SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">          ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS 1 \</span><br><span class="line">  --host $SPARK_MASTER_HOST --port $SPARK_MASTER_PORT --webui-port $SPARK_MASTER_WEBUI_PORT \</span><br><span class="line"><span class="meta">  $</span><span class="bash">ORIGINAL_ARGS</span></span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">  <span class="comment">// 1、初始化log对象</span></span><br><span class="line">  <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">  <span class="comment">// 2、加载SparkConf</span></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></span><br><span class="line">  <span class="comment">// 3、解析Master启动参数</span></span><br><span class="line">  <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">MasterArguments</span>(argStrings, conf)</span><br><span class="line">  <span class="comment">// 4、启动RPC框架端点</span></span><br><span class="line">  <span class="keyword">val</span> (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf)</span><br><span class="line">  rpcEnv.awaitTermination()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="start-slaves-sh"><a href="#start-slaves-sh" class="headerlink" title="start-slaves.sh"></a>start-slaves.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Starts a slave instance on each machine specified <span class="keyword">in</span> the conf/slaves file.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)" #获取当前的目录</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh" #加载配置</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the port number <span class="keyword">for</span> the master</span></span><br><span class="line">if [ "$SPARK_MASTER_PORT" = "" ]; then</span><br><span class="line">  SPARK_MASTER_PORT=7077</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_MASTER_HOST" = "" ]; then</span><br><span class="line">  case `uname` in</span><br><span class="line">      (SunOS)</span><br><span class="line">          SPARK_MASTER_HOST="`/usr/sbin/check-hostname | awk '&#123;print $NF&#125;'`"</span><br><span class="line">          ;;</span><br><span class="line">      (*)</span><br><span class="line">          SPARK_MASTER_HOST="`hostname -f`"</span><br><span class="line">          ;;</span><br><span class="line">  esac</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Launch the slaves 调用了start-slave.sh</span></span><br><span class="line">"$&#123;SPARK_HOME&#125;/sbin/slaves.sh" cd "$&#123;SPARK_HOME&#125;" \; "$&#123;SPARK_HOME&#125;/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"</span><br></pre></td></tr></table></figure>

<h3 id="start-slave-sh"><a href="#start-slave-sh" class="headerlink" title="start-slave.sh"></a>start-slave.sh</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Starts a slave on the machine this script is executed on.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Environment Variables</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_INSTANCES  The number of worker instances to run on this</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           slave.  Default is 1.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_PORT       The base port number <span class="keyword">for</span> the first worker. If <span class="built_in">set</span>,</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           subsequent workers will increment this number.  If</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           <span class="built_in">unset</span>, Spark will find a valid port number, but</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           with no guarantee of a predictable pattern.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   SPARK_WORKER_WEBUI_PORT The base port <span class="keyword">for</span> the web interface of the first</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           worker.  Subsequent workers will increment this</span></span><br><span class="line"><span class="meta">#</span><span class="bash">                           number.  Default is 8081.</span></span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> NOTE: This exact class name is matched downstream by SparkSubmit.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Any changes need to be reflected there.</span></span><br><span class="line">CLASS="org.apache.spark.deploy.worker.Worker"</span><br><span class="line"></span><br><span class="line">if [[ $# -lt 1 ]] || [[ "$@" = *--help ]] || [[ "$@" = *-h ]]; then</span><br><span class="line">  echo "Usage: ./sbin/start-slave.sh [options] &lt;master&gt;"</span><br><span class="line">  pattern="Usage:"</span><br><span class="line">  pattern+="\|Using Spark's default log4j profile:"</span><br><span class="line">  pattern+="\|Registered signal handlers for"</span><br><span class="line"></span><br><span class="line">  "$&#123;SPARK_HOME&#125;"/bin/spark-class $CLASS --help 2&gt;&amp;1 | grep -v "$pattern" 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/sbin/spark-config.sh"</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;/bin/load-spark-env.sh"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> First argument should be the master; we need to store it aside because we may</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> need to insert arguments between it and the other arguments</span></span><br><span class="line">MASTER=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Determine desired worker port</span></span><br><span class="line">if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then</span><br><span class="line">  SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Start up the appropriate number of workers on this machine.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> quick <span class="built_in">local</span> <span class="keyword">function</span> to start a worker</span></span><br><span class="line">function start_instance &#123;</span><br><span class="line">  WORKER_NUM=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ "$SPARK_WORKER_PORT" = "" ]; then</span><br><span class="line">    PORT_FLAG=</span><br><span class="line">    PORT_NUM=</span><br><span class="line">  else</span><br><span class="line">    PORT_FLAG="--port"</span><br><span class="line">    PORT_NUM=$(( $SPARK_WORKER_PORT + $WORKER_NUM - 1 ))</span><br><span class="line">  fi</span><br><span class="line">  WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))</span><br><span class="line"><span class="meta">   #</span><span class="bash">调用org.apache.spark.deploy.worker.Worker</span></span><br><span class="line">  "$&#123;SPARK_HOME&#125;/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \</span><br><span class="line">     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@" </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [ "$SPARK_WORKER_INSTANCES" = "" ]; then</span><br><span class="line">  start_instance 1 "$@"</span><br><span class="line">else</span><br><span class="line">  for ((i=0; i&lt;$SPARK_WORKER_INSTANCES; i++)); do</span><br><span class="line">    start_instance $(( 1 + $i )) "$@"</span><br><span class="line">  done</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>workerMain方法</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(argStrings: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="type">Utils</span>.initDaemon(log)</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span></span><br><span class="line">    <span class="keyword">val</span> args = <span class="keyword">new</span> <span class="type">WorkerArguments</span>(argStrings, conf)</span><br><span class="line">    <span class="keyword">val</span> rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores,</span><br><span class="line">      args.memory, args.masters, args.workDir, conf = conf)</span><br><span class="line">    rpcEnv.awaitTermination()</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h2 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h2><h3 id="spark-submit"><a href="#spark-submit" class="headerlink" title="spark-submit"></a>spark-submit</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">disable</span> randomized <span class="built_in">hash</span> <span class="keyword">for</span> string <span class="keyword">in</span> Python 3.3+</span></span><br><span class="line">export PYTHONHASHSEED=0</span><br><span class="line"></span><br><span class="line">exec "$&#123;SPARK_HOME&#125;"/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"</span><br></pre></td></tr></table></figure>

<h3 id="spark-class"><a href="#spark-class" class="headerlink" title="spark-class"></a>spark-class</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">. "$&#123;SPARK_HOME&#125;"/bin/load-spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find the java binary</span></span><br><span class="line">if [ -n "$&#123;JAVA_HOME&#125;" ]; then</span><br><span class="line">  RUNNER="$&#123;JAVA_HOME&#125;/bin/java"</span><br><span class="line">else</span><br><span class="line">  if [ "$(command -v java)" ]; then</span><br><span class="line">    RUNNER="java"</span><br><span class="line">  else</span><br><span class="line">    echo "JAVA_HOME is not set" &gt;&amp;2</span><br><span class="line">    exit 1</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Find Spark jars.</span></span><br><span class="line">if [ -d "$&#123;SPARK_HOME&#125;/jars" ]; then</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/jars"</span><br><span class="line">else</span><br><span class="line">  SPARK_JARS_DIR="$&#123;SPARK_HOME&#125;/assembly/target/scala-$SPARK_SCALA_VERSION/jars"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ ! -d "$SPARK_JARS_DIR" ] &amp;&amp; [ -z "$SPARK_TESTING$SPARK_SQL_TESTING" ]; then</span><br><span class="line">  echo "Failed to find Spark jars directory ($SPARK_JARS_DIR)." 1&gt;&amp;2</span><br><span class="line">  echo "You need to build Spark with the target \"package\" before running this program." 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">else</span><br><span class="line">  LAUNCH_CLASSPATH="$SPARK_JARS_DIR/*"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Add the launcher build dir to the classpath <span class="keyword">if</span> requested.</span></span><br><span class="line">if [ -n "$SPARK_PREPEND_CLASSES" ]; then</span><br><span class="line">  LAUNCH_CLASSPATH="$&#123;SPARK_HOME&#125;/launcher/target/scala-$SPARK_SCALA_VERSION/classes:$LAUNCH_CLASSPATH"</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> For tests</span></span><br><span class="line">if [[ -n "$SPARK_TESTING" ]]; then</span><br><span class="line">  unset YARN_CONF_DIR</span><br><span class="line">  unset HADOOP_CONF_DIR</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The launcher library will <span class="built_in">print</span> arguments separated by a NULL character, to allow arguments with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> characters that would be otherwise interpreted by the shell. Read that <span class="keyword">in</span> a <span class="keyword">while</span> loop, populating</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> an array that will be used to <span class="built_in">exec</span> the final <span class="built_in">command</span>.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The <span class="built_in">exit</span> code of the launcher is appended to the output, so the parent shell removes it from the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> array and checks the value to see <span class="keyword">if</span> the launcher succeeded.</span></span><br><span class="line">build_command() &#123;</span><br><span class="line">  "$RUNNER" -Xmx128m -cp "$LAUNCH_CLASSPATH" org.apache.spark.launcher.Main "$@"</span><br><span class="line">  printf "%d\0" $?</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">CMD=()</span><br><span class="line">while IFS= read -d '' -r ARG; do</span><br><span class="line">  CMD+=("$ARG")</span><br><span class="line">done &lt; &lt;(build_command "$@")</span><br><span class="line"></span><br><span class="line">COUNT=$&#123;#CMD[@]&#125;</span><br><span class="line">LAST=$((COUNT - 1))</span><br><span class="line">LAUNCHER_EXIT_CODE=$&#123;CMD[$LAST]&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Certain JVM failures result <span class="keyword">in</span> errors being printed to stdout (instead of stderr), <span class="built_in">which</span> causes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the code that parses the output of the launcher to get confused. In those cases, check <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">exit</span> code is an <span class="built_in">integer</span>, and <span class="keyword">if</span> it<span class="string">'s not, handle it as a special error case.</span></span></span><br><span class="line">if ! [[ $LAUNCHER_EXIT_CODE =~ ^[0-9]+$ ]]; then</span><br><span class="line">  echo "$&#123;CMD[@]&#125;" | head -n-1 1&gt;&amp;2</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">if [ $LAUNCHER_EXIT_CODE != 0 ]; then</span><br><span class="line">  exit $LAUNCHER_EXIT_CODE</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">CMD=("$&#123;CMD[@]:0:$LAST&#125;")</span><br><span class="line">exec "$&#123;CMD[@]&#125;"</span><br></pre></td></tr></table></figure>

<p>查看SparkSubmit</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkSubmit</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Cluster managers</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">YARN</span> = <span class="number">1</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">STANDALONE</span> = <span class="number">2</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">MESOS</span> = <span class="number">4</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">LOCAL</span> = <span class="number">8</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">ALL_CLUSTER_MGRS</span> = <span class="type">YARN</span> | <span class="type">STANDALONE</span> | <span class="type">MESOS</span> | <span class="type">LOCAL</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Deploy modes</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLIENT</span> = <span class="number">1</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLUSTER</span> = <span class="number">2</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">ALL_DEPLOY_MODES</span> = <span class="type">CLIENT</span> | <span class="type">CLUSTER</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// Special primary resource names that represent shells rather than application jars.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARK_SHELL</span> = <span class="string">"spark-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">PYSPARK_SHELL</span> = <span class="string">"pyspark-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARKR_SHELL</span> = <span class="string">"sparkr-shell"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">SPARKR_PACKAGE_ARCHIVE</span> = <span class="string">"sparkr.zip"</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">R_PACKAGE_ARCHIVE</span> = <span class="string">"rpkg.zip"</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> <span class="type">CLASS_NOT_FOUND_EXIT_STATUS</span> = <span class="number">101</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// scalastyle:off println</span></span><br><span class="line">  <span class="comment">// Exposed for testing</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">var</span> exitFn: <span class="type">Int</span> =&gt; <span class="type">Unit</span> = (exitCode: <span class="type">Int</span>) =&gt; <span class="type">System</span>.exit(exitCode)</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="keyword">var</span> printStream: <span class="type">PrintStream</span> = <span class="type">System</span>.err</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printWarning</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = printStream.println(<span class="string">"Warning: "</span> + str)</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printErrorAndExit</span></span>(str: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printStream.println(<span class="string">"Error: "</span> + str)</span><br><span class="line">    printStream.println(<span class="string">"Run with --help for usage help or --verbose for debug output"</span>)</span><br><span class="line">    exitFn(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">printVersionAndExit</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    printStream.println(<span class="string">""</span><span class="string">"Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version %s</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string">                        "</span><span class="string">""</span>.format(<span class="type">SPARK_VERSION</span>))  </span><br><span class="line">    printStream.println(<span class="string">"Using Scala %s, %s, %s"</span>.format(</span><br><span class="line">      <span class="type">Properties</span>.versionString, <span class="type">Properties</span>.javaVmName, <span class="type">Properties</span>.javaVersion))</span><br><span class="line">    printStream.println(<span class="string">"Branch %s"</span>.format(<span class="type">SPARK_BRANCH</span>))</span><br><span class="line">    printStream.println(<span class="string">"Compiled by user %s on %s"</span>.format(<span class="type">SPARK_BUILD_USER</span>, <span class="type">SPARK_BUILD_DATE</span>))</span><br><span class="line">    printStream.println(<span class="string">"Revision %s"</span>.format(<span class="type">SPARK_REVISION</span>))</span><br><span class="line">    printStream.println(<span class="string">"Url %s"</span>.format(<span class="type">SPARK_REPO_URL</span>))</span><br><span class="line">    printStream.println(<span class="string">"Type --help for more information."</span>)</span><br><span class="line">    exitFn(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// scalastyle:on println</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> appArgs = <span class="keyword">new</span> <span class="type">SparkSubmitArguments</span>(args)</span><br><span class="line">    <span class="keyword">if</span> (appArgs.verbose) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      printStream.println(appArgs)</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">    &#125;</span><br><span class="line">    appArgs.action <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">SUBMIT</span> =&gt; submit(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">KILL</span> =&gt; kill(appArgs)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">SparkSubmitAction</span>.<span class="type">REQUEST_STATUS</span> =&gt; requestStatus(appArgs)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<h3 id="spark-shell"><a href="#spark-shell" class="headerlink" title="spark-shell"></a>spark-shell</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Shell script <span class="keyword">for</span> starting the Spark Shell REPL</span></span><br><span class="line"></span><br><span class="line">cygwin=false</span><br><span class="line">case "$(uname)" in</span><br><span class="line">  CYGWIN*) cygwin=true;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Enter posix mode <span class="keyword">for</span> bash</span></span><br><span class="line">set -o posix</span><br><span class="line"></span><br><span class="line">if [ -z "$&#123;SPARK_HOME&#125;" ]; then</span><br><span class="line">  source "$(dirname "$0")"/find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">export _SPARK_CMD_USAGE="Usage: ./bin/spark-shell [options]"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> SPARK-4161: scala does not assume use of the java classpath,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> so we need to add the <span class="string">"-Dscala.usejavacp=true"</span> flag manually. We</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">do</span> this specifically <span class="keyword">for</span> the Spark shell because the scala REPL</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> has its own class loader, and any additional classpath specified</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> through spark.driver.extraClassPath is not automatically propagated.</span></span><br><span class="line">SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Dscala.usejavacp=true"</span><br><span class="line"></span><br><span class="line">function main() &#123;</span><br><span class="line">  if $cygwin; then</span><br><span class="line">    # Workaround for issue involving JLine and Cygwin</span><br><span class="line">    # (see http://sourceforge.net/p/jline/bugs/40/).</span><br><span class="line">    # If you're using the Mintty terminal emulator in Cygwin, may need to set the</span><br><span class="line">    # "Backspace sends ^H" setting in "Keys" section of the Mintty options</span><br><span class="line">    # (see https://github.com/sbt/sbt/issues/562).</span><br><span class="line">    stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">    export SPARK_SUBMIT_OPTS="$SPARK_SUBMIT_OPTS -Djline.terminal=unix"</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">    stty icanon echo &gt; /dev/null 2&gt;&amp;1</span><br><span class="line">  else</span><br><span class="line">    export SPARK_SUBMIT_OPTS</span><br><span class="line">    "$&#123;SPARK_HOME&#125;"/bin/spark-submit --class org.apache.spark.repl.Main --name "Spark shell" "$@"</span><br><span class="line">  fi  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Copy restore-TTY-on-exit <span class="built_in">functions</span> from Scala script so spark-shell exits properly even <span class="keyword">in</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> binary distribution of Spark <span class="built_in">where</span> Scala is not installed</span></span><br><span class="line">exit_status=127</span><br><span class="line">saved_stty=""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> restore stty settings (<span class="built_in">echo</span> <span class="keyword">in</span> particular)</span></span><br><span class="line">function restoreSttySettings() &#123;</span><br><span class="line">  stty $saved_stty</span><br><span class="line">  saved_stty=""</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function onExit() &#123;</span><br><span class="line">  if [[ "$saved_stty" != "" ]]; then</span><br><span class="line">    restoreSttySettings</span><br><span class="line">  fi</span><br><span class="line">  exit $exit_status</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> to reenable <span class="built_in">echo</span> <span class="keyword">if</span> we are interrupted before completing.</span></span><br><span class="line">trap onExit INT</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> save terminal settings</span></span><br><span class="line">saved_stty=$(stty -g 2&gt;/dev/null)</span><br><span class="line"><span class="meta">#</span><span class="bash"> clear on error so we don<span class="string">'t later try to restore them</span></span></span><br><span class="line">if [[ ! $? ]]; then</span><br><span class="line">  saved_stty=""</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">main "$@" #调用的main函数 最终执行的依旧是spark-submi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> record the <span class="built_in">exit</span> status lest it be overwritten:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">then</span> reenable <span class="built_in">echo</span> and propagate the code.</span></span><br><span class="line">exit_status=$?</span><br><span class="line">onExit</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> org.apache.spark.repl</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Main</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  initializeLogIfNecessary(<span class="literal">true</span>)</span><br><span class="line">  <span class="type">Signaling</span>.cancelOnInterrupt()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">  <span class="keyword">val</span> rootDir = conf.getOption(<span class="string">"spark.repl.classdir"</span>).getOrElse(<span class="type">Utils</span>.getLocalDir(conf))</span><br><span class="line">  <span class="keyword">val</span> outputDir = <span class="type">Utils</span>.createTempDir(root = rootDir, namePrefix = <span class="string">"repl"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> sparkContext: <span class="type">SparkContext</span> = _</span><br><span class="line">  <span class="keyword">var</span> sparkSession: <span class="type">SparkSession</span> = _</span><br><span class="line">  <span class="comment">// this is a public var because tests reset it.</span></span><br><span class="line">  <span class="keyword">var</span> interp: <span class="type">SparkILoop</span> = _</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> hasErrors = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">scalaOptionError</span></span>(msg: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    hasErrors = <span class="literal">true</span></span><br><span class="line">    <span class="type">Console</span>.err.println(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    doMain(args, <span class="keyword">new</span> <span class="type">SparkILoop</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Visible for testing</span></span><br><span class="line">  <span class="keyword">private</span>[repl] <span class="function"><span class="keyword">def</span> <span class="title">doMain</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>], _interp: <span class="type">SparkILoop</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    interp = _interp</span><br><span class="line">    <span class="keyword">val</span> jars = <span class="type">Utils</span>.getUserJars(conf, isShell = <span class="literal">true</span>).mkString(<span class="type">File</span>.pathSeparator)</span><br><span class="line">    <span class="keyword">val</span> interpArguments = <span class="type">List</span>(</span><br><span class="line">      <span class="string">"-Yrepl-class-based"</span>,</span><br><span class="line">      <span class="string">"-Yrepl-outdir"</span>, <span class="string">s"<span class="subst">$&#123;outputDir.getAbsolutePath&#125;</span>"</span>,</span><br><span class="line">      <span class="string">"-classpath"</span>, jars</span><br><span class="line">    ) ++ args.toList</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> settings = <span class="keyword">new</span> <span class="type">GenericRunnerSettings</span>(scalaOptionError)</span><br><span class="line">    settings.processArguments(interpArguments, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!hasErrors) &#123;</span><br><span class="line">      interp.process(settings) <span class="comment">// Repl starts and goes in loop of R.E.P.L</span></span><br><span class="line">      <span class="type">Option</span>(sparkContext).map(_.stop)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">createSparkSession</span></span>(): <span class="type">SparkSession</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> execUri = <span class="type">System</span>.getenv(<span class="string">"SPARK_EXECUTOR_URI"</span>)</span><br><span class="line">    conf.setIfMissing(<span class="string">"spark.app.name"</span>, <span class="string">"Spark shell"</span>)</span><br><span class="line">    <span class="comment">// SparkContext will detect this configuration and register it with the RpcEnv's</span></span><br><span class="line">    <span class="comment">// file server, setting spark.repl.class.uri to the actual URI for executors to</span></span><br><span class="line">    <span class="comment">// use. This is sort of ugly but since executors are started as part of SparkContext</span></span><br><span class="line">    <span class="comment">// initialization in certain cases, there's an initialization order issue that prevents</span></span><br><span class="line">    <span class="comment">// this from being set after SparkContext is instantiated.</span></span><br><span class="line">    conf.set(<span class="string">"spark.repl.class.outputDir"</span>, outputDir.getAbsolutePath())</span><br><span class="line">    <span class="keyword">if</span> (execUri != <span class="literal">null</span>) &#123;</span><br><span class="line">      conf.set(<span class="string">"spark.executor.uri"</span>, execUri)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (<span class="type">System</span>.getenv(<span class="string">"SPARK_HOME"</span>) != <span class="literal">null</span>) &#123;</span><br><span class="line">      conf.setSparkHome(<span class="type">System</span>.getenv(<span class="string">"SPARK_HOME"</span>))</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> builder = <span class="type">SparkSession</span>.builder.config(conf)</span><br><span class="line">    <span class="keyword">if</span> (conf.get(<span class="type">CATALOG_IMPLEMENTATION</span>.key, <span class="string">"hive"</span>).toLowerCase == <span class="string">"hive"</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (<span class="type">SparkSession</span>.hiveClassesArePresent) &#123;</span><br><span class="line">        <span class="comment">// In the case that the property is not set at all, builder's config</span></span><br><span class="line">        <span class="comment">// does not have this value set to 'hive' yet. The original default</span></span><br><span class="line">        <span class="comment">// behavior is that when there are hive classes, we use hive catalog.</span></span><br><span class="line">        sparkSession = builder.enableHiveSupport().getOrCreate()</span><br><span class="line">        logInfo(<span class="string">"Created Spark session with Hive support"</span>)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// Need to change it back to 'in-memory' if no hive classes are found</span></span><br><span class="line">        <span class="comment">// in the case that the property is set to hive in spark-defaults.conf</span></span><br><span class="line">        builder.config(<span class="type">CATALOG_IMPLEMENTATION</span>.key, <span class="string">"in-memory"</span>)</span><br><span class="line">        sparkSession = builder.getOrCreate()</span><br><span class="line">        logInfo(<span class="string">"Created Spark session"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// In the case that the property is set but not to 'hive', the internal</span></span><br><span class="line">      <span class="comment">// default is 'in-memory'. So the sparkSession will use in-memory catalog.</span></span><br><span class="line">      sparkSession = builder.getOrCreate()</span><br><span class="line">      logInfo(<span class="string">"Created Spark session"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    sparkContext = sparkSession.sparkContext</span><br><span class="line">    sparkSession</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[repl] <span class="class"><span class="keyword">trait</span> <span class="title">SparkILoopInit</span> </span>&#123;</span><br><span class="line">  self: <span class="type">SparkILoop</span> =&gt;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Print a welcome message */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">printWelcome</span></span>() &#123;</span><br><span class="line">    echo(<span class="string">""</span><span class="string">"Welcome to</span></span><br><span class="line"><span class="string">      ____              __</span></span><br><span class="line"><span class="string">     / __/__  ___ _____/ /__</span></span><br><span class="line"><span class="string">    _\ \/ _ \/ _ `/ __/  '_/</span></span><br><span class="line"><span class="string">   /___/ .__/\_,_/_/ /_/\_\   version %s</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string">"</span><span class="string">""</span>.format(<span class="type">SPARK_VERSION</span>))</span><br><span class="line">    <span class="keyword">import</span> <span class="type">Properties</span>._</span><br><span class="line">    <span class="keyword">val</span> welcomeMsg = <span class="string">"Using Scala %s (%s, Java %s)"</span>.format(</span><br><span class="line">      versionString, javaVmName, javaVersion)</span><br><span class="line">    echo(welcomeMsg)</span><br><span class="line">    echo(<span class="string">"Type in expressions to have them evaluated."</span>)</span><br><span class="line">    echo(<span class="string">"Type :help for more information."</span>)</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">asyncMessage</span></span>(msg: <span class="type">String</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isReplInfo || isReplPower)</span><br><span class="line">      echoAndRefresh(msg)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initLock = <span class="keyword">new</span> java.util.concurrent.locks.<span class="type">ReentrantLock</span>()</span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initCompilerCondition = initLock.newCondition() <span class="comment">// signal the compiler is initialized</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initLoopCondition = initLock.newCondition()     <span class="comment">// signal the whole repl is initialized</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> initStart = <span class="type">System</span>.nanoTime</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">withLock</span></span>[<span class="type">T</span>](body: =&gt; <span class="type">T</span>): <span class="type">T</span> = &#123;</span><br><span class="line">    initLock.lock()</span><br><span class="line">    <span class="keyword">try</span> body</span><br><span class="line">    <span class="keyword">finally</span> initLock.unlock()</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// a condition used to ensure serial access to the compiler.</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initIsComplete = <span class="literal">false</span></span><br><span class="line">  <span class="meta">@volatile</span> <span class="keyword">private</span> <span class="keyword">var</span> initError: <span class="type">String</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">elapsed</span></span>() = <span class="string">"%.3f"</span>.format((<span class="type">System</span>.nanoTime - initStart).toDouble / <span class="number">1000000000</span>L)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// the method to be called when the interpreter is initialized.</span></span><br><span class="line">  <span class="comment">// Very important this method does nothing synchronous (i.e. do</span></span><br><span class="line">  <span class="comment">// not try to use the interpreter) because until it returns, the</span></span><br><span class="line">  <span class="comment">// repl's lazy val `global` is still locked.</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">initializedCallback</span></span>() = withLock(initCompilerCondition.signal())</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Spins off a thread which awaits a single message once the interpreter</span></span><br><span class="line">  <span class="comment">// has been initialized.</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createAsyncListener</span></span>() = &#123;</span><br><span class="line">    io.spawn &#123;</span><br><span class="line">      withLock(initCompilerCondition.await())</span><br><span class="line">      asyncMessage(<span class="string">"[info] compiler init time: "</span> + elapsed() + <span class="string">" s."</span>)</span><br><span class="line">      postInitialization()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// called from main repl loop</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">awaitInitialized</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">    <span class="keyword">if</span> (!initIsComplete)</span><br><span class="line">      withLock &#123; <span class="keyword">while</span> (!initIsComplete) initLoopCondition.await() &#125;</span><br><span class="line">    <span class="keyword">if</span> (initError != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="comment">// scalastyle:off println</span></span><br><span class="line">      println(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        |Failed to initialize the REPL due to an unexpected error.</span></span><br><span class="line"><span class="string">        |This is a bug, please, report it along with the error diagnostics printed below.</span></span><br><span class="line"><span class="string">        |%s."</span><span class="string">""</span>.stripMargin.format(initError)</span><br><span class="line">      )</span><br><span class="line">      <span class="comment">// scalastyle:on println</span></span><br><span class="line">      <span class="literal">false</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// private def warningsThunks = List(</span></span><br><span class="line">  <span class="comment">//   () =&gt; intp.bind("lastWarnings", "" + typeTag[List[(Position, String)]], intp.lastWarnings _),</span></span><br><span class="line">  <span class="comment">// )</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">postInitThunks</span> </span>= <span class="type">List</span>[<span class="type">Option</span>[() =&gt; <span class="type">Unit</span>]](</span><br><span class="line">    <span class="type">Some</span>(intp.setContextClassLoader _),</span><br><span class="line">    <span class="keyword">if</span> (isReplPower) <span class="type">Some</span>(() =&gt; enablePowerMode(<span class="literal">true</span>)) <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line">  ).flatten</span><br><span class="line">  <span class="comment">// ++ (</span></span><br><span class="line">  <span class="comment">//   warningsThunks</span></span><br><span class="line">  <span class="comment">// )</span></span><br><span class="line">  <span class="comment">// called once after init condition is signalled</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">postInitialization</span></span>() &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      postInitThunks foreach (f =&gt; addThunk(f()))</span><br><span class="line">      runThunks()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ex: <span class="type">Throwable</span> =&gt;</span><br><span class="line">        initError = stackTraceString(ex)</span><br><span class="line">        <span class="keyword">throw</span> ex</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      initIsComplete = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (isAsync) &#123;</span><br><span class="line">        asyncMessage(<span class="string">"[info] total init time: "</span> + elapsed() + <span class="string">" s."</span>)</span><br><span class="line">        withLock(initLoopCondition.signal())</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">initializeSpark</span></span>() &#123;</span><br><span class="line">    intp.beQuietDuring &#123;</span><br><span class="line">      command(<span class="string">""</span><span class="string">"</span></span><br><span class="line"><span class="string">        @transient val spark = org.apache.spark.repl.Main.interp.createSparkSession()</span></span><br><span class="line"><span class="string">        @transient val sc = &#123;</span></span><br><span class="line"><span class="string">          val _sc = spark.sparkContext</span></span><br><span class="line"><span class="string">          if (_sc.getConf.getBoolean("</span>spark.ui.reverseP<span class="string">roxy", false)) &#123;</span></span><br><span class="line"><span class="string">            val proxyUrl = _sc.getConf.get("</span>spark.ui.reverseProxyU<span class="string">rl", null)</span></span><br><span class="line"><span class="string">            if (proxyUrl != null) &#123;</span></span><br><span class="line"><span class="string">              println(s"</span><span class="type">Spark</span> <span class="type">Context</span> <span class="type">Web</span> <span class="type">UI</span> is available at $&#123;proxyUrl&#125;/proxy/$&#123;_sc.applicationId&#125;<span class="string">")</span></span><br><span class="line"><span class="string">            &#125; else &#123;</span></span><br><span class="line"><span class="string">              println(s"</span><span class="type">Spark</span> <span class="type">Context</span> <span class="type">Web</span> <span class="type">UI</span> is available at <span class="type">Spark</span> <span class="type">Master</span> <span class="type">Public</span> <span class="type">URL</span><span class="string">")</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          &#125; else &#123;</span></span><br><span class="line"><span class="string">            _sc.uiWebUrl.foreach &#123;</span></span><br><span class="line"><span class="string">              webUrl =&gt; println(s"</span><span class="type">Spark</span> context <span class="type">Web</span> <span class="type">UI</span> available at $&#123;webUrl&#125;<span class="string">")</span></span><br><span class="line"><span class="string">            &#125;</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">          println("</span><span class="type">Spark</span> context available as <span class="symbol">'s</span>c' <span class="string">" +</span></span><br><span class="line"><span class="string">            s"</span>(master = $&#123;_sc.master&#125;, app id = $&#123;_sc.applicationId&#125;).<span class="string">")</span></span><br><span class="line"><span class="string">          println("</span><span class="type">Spark</span> session available as <span class="symbol">'spar</span>k'.<span class="string">")</span></span><br><span class="line"><span class="string">          _sc</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        "</span><span class="string">""</span>)</span><br><span class="line">      command(<span class="string">"import org.apache.spark.SparkContext._"</span>)</span><br><span class="line">      command(<span class="string">"import spark.implicits._"</span>)</span><br><span class="line">      command(<span class="string">"import spark.sql"</span>)</span><br><span class="line">      command(<span class="string">"import org.apache.spark.sql.functions._"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// code to be executed only after the interpreter is initialized</span></span><br><span class="line">  <span class="comment">// and the lazy val `global` can be accessed without risk of deadlock.</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">var</span> pendingThunks: <span class="type">List</span>[() =&gt; <span class="type">Unit</span>] = <span class="type">Nil</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">addThunk</span></span>(body: =&gt; <span class="type">Unit</span>) = synchronized &#123;</span><br><span class="line">    pendingThunks :+= (() =&gt; body)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">runThunks</span></span>(): <span class="type">Unit</span> = synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (pendingThunks.nonEmpty)</span><br><span class="line">      logDebug(<span class="string">"Clearing "</span> + pendingThunks.size + <span class="string">" thunks."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (pendingThunks.nonEmpty) &#123;</span><br><span class="line">      <span class="keyword">val</span> thunk = pendingThunks.head</span><br><span class="line">      pendingThunks = pendingThunks.tail</span><br><span class="line">      thunk()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>







      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a  href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/">Spark内核解析1</a></p>
        <p><span>文章作者:</span><a  href="/" title="访问 清风笑丶 的个人博客">清风笑丶</a></p>
        <p><span>发布时间:</span>2019年06月09日 - 10时32分</p>
        <p><span>最后更新:</span>2020年01月12日 - 21时08分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/" title="Spark内核解析1">https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/</a>
            <span class="copy-path" data-clipboard-text="原文: https://www.hphblog.cn/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/　　作者: 清风笑丶" title=""></span>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



<nav id="article-nav">
  
    <a  href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Spark内核解析2
        
      </div>
    </a>
  
  
    <a  href="/2019/06/08/Spark%E4%B9%8BGraphX/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Spark之GraphX</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>


  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#概述"><span class="toc-number">1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#核心组件"><span class="toc-number">2.</span> <span class="toc-text">核心组件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Driver"><span class="toc-number">2.1.</span> <span class="toc-text">Driver</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Executor"><span class="toc-number">2.2.</span> <span class="toc-text">Executor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行流程"><span class="toc-number">2.3.</span> <span class="toc-text">运行流程</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#部署模式"><span class="toc-number">3.</span> <span class="toc-text">部署模式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Standalone"><span class="toc-number">3.1.</span> <span class="toc-text">Standalone</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Standalone-Client"><span class="toc-number">3.1.1.</span> <span class="toc-text">Standalone Client</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Standalone-Cluster"><span class="toc-number">3.1.2.</span> <span class="toc-text">Standalone Cluster</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN-Client"><span class="toc-number">3.2.</span> <span class="toc-text">YARN Client</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN-Cluster"><span class="toc-number">3.3.</span> <span class="toc-text">YARN Cluster</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#通讯架构"><span class="toc-number">4.</span> <span class="toc-text">通讯架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#高层视图"><span class="toc-number">4.1.</span> <span class="toc-text">高层视图</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#启动脚本"><span class="toc-number">5.</span> <span class="toc-text">启动脚本</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#start-all-sh"><span class="toc-number">5.1.</span> <span class="toc-text">start-all.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-config-sh"><span class="toc-number">5.2.</span> <span class="toc-text">spark-config.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#start-master-sh"><span class="toc-number">5.3.</span> <span class="toc-text">start-master.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#start-slaves-sh"><span class="toc-number">5.4.</span> <span class="toc-text">start-slaves.sh</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#start-slave-sh"><span class="toc-number">5.5.</span> <span class="toc-text">start-slave.sh</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#任务提交"><span class="toc-number">6.</span> <span class="toc-text">任务提交</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-submit"><span class="toc-number">6.1.</span> <span class="toc-text">spark-submit</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-class"><span class="toc-number">6.2.</span> <span class="toc-text">spark-class</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-shell"><span class="toc-number">6.3.</span> <span class="toc-text">spark-shell</span></a></li></ol></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>
<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";
    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }
    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })
    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>




<div class="bdsharebuttonbox">
	<a href="#" class="fx fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="fx fa-weixin bds_weixin" data-cmd="weixin" title="分享到微信"></a>
	<a href="#" class="fx fa-qq bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
	<a href="#" class="fx fa-facebook-official bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
	<a href="#" class="fx fa-twitter bds_twi" data-cmd="twi" title="分享到Twitter"></a>
	<a href="#" class="fx fa-linkedin bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
	<a href="#" class="fx fa-files-o bds_copy" data-cmd="copy" title="分享到复制网址"></a>
</div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>




    
        <section class="changyan" id="comments">
  <!--<div id="uyan_frame"></div>-->
  <div id="SOHUCS"></div>
  <script charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/changyan.js"></script>
  <script type="text/javascript">
    window.changyan.api.config({
      appid: 'cyu9wWYgq',
      conf: 'prod_cca6a7c58b43f725f8489bdcee045320'
    });
  </script>
  <style>#feedAv{ margin-top: -250px !important;transform: scale(0) !important;}</style>
  <style>#pop_ad{ margin-top: -250px !important;transform: scale(0) !important;}</style>
</section>

    



    <div class="scroll" id="post-nav-button">
        
            <a  href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/" title="上一篇: Spark内核解析2">
                <i class="fa fa-angle-left"></i>
            </a>
        
        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>
        
            <a  href="/2019/06/08/Spark%E4%B9%8BGraphX/" title="下一篇: Spark之GraphX">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>
    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/01/05/Flink%E5%88%9D%E8%AF%86/">Flink初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%903/">Spark内核解析3</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/">Spark内核解析2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/">Spark内核解析1</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/08/Spark%E4%B9%8BGraphX/">Spark之GraphX</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/07/Spark%E4%B9%8BStructuredStreaming/">Spark之StructuredStreaming</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/06/Spark%E4%B9%8BSparkStreaming%E7%9A%84DStream%E6%93%8D%E4%BD%9C/">Spark之SparkStreaming的DStream操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Spark%E4%B9%8BSparkStreaming%E6%95%B0%E6%8D%AE%E6%BA%90/">Spark之SparkStreaming数据源</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/03/Spark%E4%B9%8BSparkStreaming%E7%90%86%E8%AE%BA%E7%AF%87/">Spark之SparkStreaming理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Spark%E4%B9%8BSparkSQL%E6%95%B0%E6%8D%AE%E6%BA%90/">Spark之SparkSQL数据源</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/Spark%E4%B9%8BSparkSQL%E5%AE%9E%E6%88%98/">Spark之SparkSQL实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/Spark%E4%B9%8BSparkSQL/">Spark之SparkSQL理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%873/">Spark之RDD实战篇3</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/">Spark之RDD实战2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/">Spark之RDD实战篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD/">Spark之RDD理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/">Spark之RDD理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/Spark%E7%94%9F%E6%80%81%E5%9C%88%E5%8F%8A%E5%AE%89%E8%A3%85/">Spark生态圈及安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%93%88%E5%B8%8C%E8%A1%A8/">数据结构之哈希表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%BA%A2%E9%BB%91%E6%A0%91/">数据结构之红黑树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8BAVL%E6%A0%91/">数据结构之AVL树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%B9%B6%E6%9F%A5%E9%9B%86/">数据结构之并查集</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E5%85%B8%E6%A0%91/">数据结构之字典树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%A0%86%E4%B8%8E%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/">数据结构之堆与优先队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91/">数据结构之二叉树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%80%92%E5%BD%92/">数据结构之递归</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/">数据结构之链表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/">数据结构之队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/">数据结构之栈</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%95%B0%E7%BB%84/">数据结构之数组</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%89%8D%E6%8F%90/">数据结构与算法前置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/28/Java%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E5%92%8CSpringCloud%E6%80%BB%E7%BB%93/">Java项目架构演进和SpringCloud总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8ESpringConfig%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/">SpringCloud与SpringConfig分布式配置中心</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8Ezuul/">SpringCloud与zuul</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8EHystrix%E6%96%AD%E8%B7%AF%E5%99%A8/">SpringCloud与Hystrix断路器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/26/SpringCloud%E4%B8%8EFeign%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/">SpringCloud与Feign</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/25/SpringCloud%E7%9A%84Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/">SpringCloud的Ribbon负载均衡</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/SpringCloud%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0Eureka/">SpringCloud注册与发现Eureka</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/21/SpringCloud%E4%B8%8EREST%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA/">微服务与SpringCloud</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/18/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%8ESpringCloud/">微服务与SpringCloud</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/14/SpringBoot%E5%92%8C%E7%9B%91%E6%8E%A7%E7%AE%A1%E7%90%86/">SpringBoot和监控管理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/14/SpringBoot%E4%B8%8ESpringCloud%E9%9B%86%E6%88%90/">SpringBoot与SpringCloud集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E4%B8%8EDubbo%E9%9B%86%E6%88%90/">SpringBoot与Dubbo集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E5%AE%89%E5%85%A8/">SpringBoot与安全</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E4%B8%8E%E4%BB%BB%E5%8A%A1/">SpringBoot与任务</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/12/SpringBoot%E5%92%8CElasticSearch%E9%9B%86%E6%88%90/">SpringBoot和Elasticsearch集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/12/Elasticsearch%E7%AE%80%E4%BB%8B/">Elasticsearch简介</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/11/SpringBoot%E5%92%8CRabbitMQ%E9%9B%86%E6%88%90/">SpringBoot和RabbitMQ集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/11/SpringBoot%E5%92%8C%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列RabbitMQ</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/10/SpringBoot%E4%B8%8ERedis%E7%BC%93%E5%AD%98/">SpringBoot与Redis缓存</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/09/SpringBoot%E5%92%8C%E7%BC%93%E5%AD%98/">SpringBoot和缓存</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/09/SpringBoot%E4%B8%8EJPA/">SpringBoot与JPA</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/08/SpringBoot%E4%B8%8EMybatis%E7%9A%84%E9%9B%86%E6%88%90/">SpringBoot与Mybatis的集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/08/SpringBoot%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/">SpringBoot数据访问</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/07/DockerFile/">DockerFile</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/06/Docker%E5%AD%98%E5%82%A8%E5%8D%B7/">Docker存储卷</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/06/Dokcer%E7%BD%91%E7%BB%9C/">Dokcer网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/05/Docker%E7%9A%84%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/">Docker的基础命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/04/Docker%E5%88%9D%E8%AF%86%E4%B8%8E%E5%AE%89%E8%A3%85/">Docker初识与安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/03/SpringBoot%E4%BD%BF%E7%94%A8%E5%A4%96%E7%BD%AE%E7%9A%84Servlet%E5%AE%B9%E5%99%A8/">SpringBoot使用外置的Servlet容器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/02/SpringBoot%E9%85%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E5%BC%8FServlet%E5%AE%B9%E5%99%A8/">SpringBoot配置嵌入式Servlet容器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E4%B9%8BSpringMVC%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/">SpringBoot之SpringMVC自动配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E4%B9%8BThymeleaf/">SpringBoot之Thymeleaf</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E7%9A%84Web%E5%BC%80%E5%8F%91/">SpringBoot的Web开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/28/SpringBoot%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/">SpringBoot的日志框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/28/SpringBoot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E6%8E%A2%E7%A9%B6/">SpringBoot自动装配探究</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/25/SpringBoot%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/">SpringBoot的配置文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/25/SpringBoot%E5%88%9D%E8%AF%86/">SpringBoot初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/SSM%E9%9B%86%E6%88%90/">SSM集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/SSM%E6%95%B4%E5%90%88/">SSM整合</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/Mybatis%E4%B9%8B%E5%8A%A8%E6%80%81SQL/">Mybatis之动态SQL</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/Mybatis%E7%9A%84resultMap%E8%87%AA%E5%AE%9A%E4%B9%89%E6%98%A0%E5%B0%84/">Mybatis的resultMap自定义映射</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/18/MyBatis%E7%9A%84CURD/">MyBatis的CURD</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/18/MyBatis%E5%85%A8%E5%B1%80%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%92%8C%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6/">MyBatis全局配置文件和映射文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/16/Mybatis%E5%85%A5%E9%97%A8/">Mybatis入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/13/Spring%E5%92%8CSpringMVC%E6%95%B4%E5%90%88/">Spring和SpringMVC整合</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/12/SpringMV%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/">SpringMV工作流程分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/11/SpringMVC%E8%BF%9B%E9%98%B6/">SpringMVC处理Json、文件上传、拦截器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/Spring%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%E6%88%96%E5%93%8D%E5%BA%94%E6%95%B0%E6%8D%AE/">SpringMVC处理请求或响应数据</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/06/SpringMVC%E6%A6%82%E8%BF%B0/">SpringMVC概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/05/Spring%E4%BA%8B%E5%8A%A1%E6%A6%82%E8%BF%B0/">Spring声明式事务</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/05/JdbcTemplate/">JdbcTemplate</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/03/AOP%E6%A6%82%E8%BF%B0/">AOP概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/02/SpringIOC%E5%AE%B9%E5%99%A8%E5%92%8CBean%E7%9A%84%E9%85%8D%E7%BD%AE/">Spring IOC容器和Bean的配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/02/Spring%E6%A6%82%E8%BF%B0/">Spring概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/18/Hive%E8%B0%83%E4%BC%98/">Hive调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/17/Hive%E6%9F%A5%E8%AF%A2/">Hive查询</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/16/Hive%E6%95%B0%E6%8D%AE%E6%8D%AE%E7%B1%BB%E5%9E%8B/">Hive数据据类型 DDL DML</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/15/Kafka-API%E5%AE%9E%E6%88%98/">KafkaAPI实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/11/Git%E4%BD%BF%E7%94%A8/">Git使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/Oozie/">Oozie</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/08/Sqoop/">Sqoop</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/07/Flume%E6%A1%88%E4%BE%8BGanglia%E7%9B%91%E6%8E%A7/">Flume案例Ganglia监控</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/06/ZooKeeper%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8CAPI/">ZooKeeper的安装和API</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/05/Zookeeper%E5%85%A5%E9%97%A8/">Zookeeper入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/HBase%E4%BC%98%E5%8C%96/">HBase优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/HBase%E7%9A%84Shell%E5%91%BD%E4%BB%A4%E5%92%8CJavaAPI/">HBase的Shell命令和JavaAPI</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/30/HBase%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86/">HBase数据模型和读写原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/30/Hbase%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%89%E8%A3%85/">HBase原理和安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/MapReduce%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B2/">MapReduce高级编程2</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/MapReduce%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/">MapReduce高级编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/MapReduce%E7%BC%96%E7%A8%8B%E5%88%A8%E6%9E%90/">MapReduce源码刨析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/MapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/">MapReduce的工作机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/Mapreduce/">MapReduce入门和优化方案</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Hadoop%E7%9A%84RPC/">Hadoop的RPC工作原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Hadoop%E7%9A%84IO%E6%93%8D%E4%BD%9C/">Hadoop的I/O操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/19/Yarn/">Yarn</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/19/HDFS%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/">HDFS高级功能</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/18/HDFS%E7%9A%84%E6%93%8D%E4%BD%9CSHELL%E5%92%8CAPI/">HDFS的操作SHELL和API</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/17/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS/">Hadoop分布式文件系统HDFS</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/17/Hadoop%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/">Hadoop简介与分布式安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E7%9A%84JavaAPI/">Elasticsearch的JavaAPI</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%88%B6%E6%8E%A2%E7%A9%B6/">Elasticsearch分布式机制探究</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90/">Elasticsearch聚合分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/">Elasticsearch增删改查</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/15/ElasticSearch%E7%B4%A2%E5%BC%95/">ElasticSearch索引</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/15/Elasticsearch%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/">Elasticsearch简介与安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/12/MongoDB%E8%BF%9B%E9%98%B6/">MongoDB进阶</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/11/MongoDB%E8%81%9A%E5%90%88/">MongoDB聚合</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/10/MongoDB%E7%B4%A2%E5%BC%95/">MongoDB索引</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/09/MongoDB%E6%9F%A5%E8%AF%A2/">MongoDB查询</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/09/MongoDB%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/">MongoDB基础命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/08/MongoDB%E5%85%A5%E9%97%A8/">MongoDB入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E7%9A%84%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/">Redis的集群模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">Redis主从复制</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E6%8C%81%E4%B9%85%E5%8C%96/">Redis持久化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E4%BA%8B%E5%8A%A1/">Redis事务</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/04/memcached/">Memcached</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/03/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/03/Hive/">Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/Flume/">Flume架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%88%86%E6%9E%90/">Kafka深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/">Kafka命令操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">Kafka与消息队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">Kafka和的安装与配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/12/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/">Java虚拟机------JVM分析工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/12/Java%E8%99%9A%E6%8B%9F%E6%9C%BA-JVM%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0/">Java虚拟机--------JVM常见参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/11/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/">Java虚拟机------垃圾收集器</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/">Java虚拟机------JVM内存区域</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E4%BB%8B%E7%BB%8D/">Java虚拟机------JVM介绍</a></li></ul>
    
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

    <script>
        $(".post-list").addClass("toc-article");
        // $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#toc, .switch-btn, .switch-area").toggle();
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
                $(".switch-btn, .switch-area").fadeToggle(300);
            }
        })
    </script>




    <script>
        
    </script>

</div>
      <footer id="footer">

    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2020 清风笑丶
            </div>
            <div class="footer-right">
                <a href="http://beian.miit.gov.cn" target="_blank"> 豫ICP备18042969号-1	</a><a href="" target="_blank"></a>
                <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >极客到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>


<script src="/js/main.js"></script>


    <script>
        $(document).ready(function() {
            var backgroundnum = 2;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129731340-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?a138f5cac94c7795df86f17cea34efc4";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(
            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>