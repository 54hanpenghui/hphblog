<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="baidu-site-verification" content="L6Lm9d5Crl"/>
  
  
  
  
  <title>Spark之RDD实战篇 | 菜鸟清风</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Spark RDD创建、转换、行动算子、RDD的持久化：">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark之RDD实战篇">
<meta property="og:url" content="https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/index.html">
<meta property="og:site_name" content="菜鸟清风">
<meta property="og:description" content="Spark RDD创建、转换、行动算子、RDD的持久化：">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527214645.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215048.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215246.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527222920.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527230912.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232248.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232843.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083705.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084629.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084943.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528091220.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093127.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093534.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528094743.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528100745.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102152.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102902.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559010918532.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528103820.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104041.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104554.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104857.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110138.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110533.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111123.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111935.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528114417.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115239.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559015691057.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115636.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120250.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120451.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120547.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120819.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120952.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121504.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121702.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528183859.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528185402.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528192803.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201005.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201529.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201656.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201735.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202419.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202856.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528204821.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224007.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224100.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224651.png">
<meta property="og:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529115207.png">
<meta property="article:published_time" content="2019-05-27T13:38:53.000Z">
<meta property="article:modified_time" content="2020-01-12T13:08:25.363Z">
<meta property="article:author" content="清风笑丶">
<meta property="article:tag" content="Spark">
<meta property="article:tag" content="RDD">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527214645.png">
  
    <link rel="alternative" href="/https://blog.csdn.net/weixin_39084521/rss/list" title="菜鸟清风" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/favicon.png">
  
  
  
<link rel="stylesheet" href="//cdn.bootcss.com/animate.css/3.5.0/animate.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  <link rel="stylesheet" href="/font-awesome/css/font-awesome.min.css">
  <link rel="apple-touch-icon" href="/apple-touch-icon.png">
  
  
      <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  
  <!-- 加载特效 -->
    <script src="/js/pace.js"></script>
    <link href="/css/pace/pace-theme-flash.css" rel="stylesheet" />
  <script>
      var yiliaConfig = {
          fancybox: true,
          animate: true,
          isHome: false,
          isPost: true,
          isArchive: false,
          isTag: false,
          isCategory: false,
          open_in_new: false
      }
  </script>
<meta name="generator" content="Hexo 4.2.0"></head>
<body>
  <div id="container">
    <div class="left-col">
    <div class="overlay"></div>
<div class="intrude-less">
    <header id="header" class="inner">
        
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

        <a href="/" class="profilepic">
            
            <img lazy-src="/img/avatar.jpg" class="js-avatar">
            
        </a>
        <hgroup>
          <h1 class="header-author"><a href="/">清风笑丶</a></h1>
        </hgroup>
        
        
            <form>
                <input type="text" class="st-default-search-input search" id="local-search-input" placeholder="搜索一下" autocomplete="off">
            </form>
            <div id="local-search-result"></div>
        
        
            <script type="text/javascript">
                (function() {
                    'use strict';
                    function getMatchData(keyword, data) {
                        var matchData = [];
                        for(var i =0;i<data.length;i++){
                            if(data[i].title.toLowerCase().indexOf(keyword)>=0) 
                                matchData.push(data[i])
                        }
                        return matchData;
                    }
                    var $input = $('#local-search-input');
                    var $resultContent = $('#local-search-result');
                    $input.keyup(function(){
                        $.ajax({
                            url: '/search.json',
                            dataType: "json",
                            success: function( json ) {
                                var str='<ul class=\"search-result-list\">';                
                                var keyword = $input.val().trim().toLowerCase();
                                $resultContent.innerHTML = "";
                                if ($input.val().trim().length <= 0) {
                                    $resultContent.empty();
                                    $('#switch-area').show();
                                    return;
                                }
                                var results = getMatchData(keyword, json);
                                if(results.length === 0){
                                    $resultContent.empty();
                                    $('#switch-area').show();
                                    return;
                                } 
                                for(var i =0; i<results.length; i++){
                                    str += "<li><a href='"+ results[i].url +"' class='search-result-title'>"+ results[i].title +"</a></li>";
                                }
                                str += "</ul>";
                                $resultContent.empty();
                                $resultContent.append(str);
                                $('#switch-area').hide();
                            }
                        });
                    });
                })();
            </script>
        
        
            <div id="switch-btn" class="switch-btn">
                <div class="icon">
                    <div class="icon-ctn">
                        <div class="icon-wrap icon-house" data-idx="0">
                            <div class="birdhouse"></div>
                            <div class="birdhouse_holes"></div>
                        </div>
                        <div class="icon-wrap icon-ribbon hide" data-idx="1">
                            <div class="ribbon"></div>
                        </div>
                        
                        <div class="icon-wrap icon-link hide" data-idx="2">
                            <div class="loopback_l"></div>
                            <div class="loopback_r"></div>
                        </div>
                        
                        
                        <div class="icon-wrap icon-me hide" data-idx="3">
                            <div class="user"></div>
                            <div class="shoulder"></div>
                        </div>
                        
                    </div>
                </div>
                <div class="tips-box hide">
                    <div class="tips-arrow"></div>
                    <ul class="tips-inner">
                        <li>菜单</li>
                        <li>标签</li>
                        
                        <li>友情链接</li>
                        
                        
                        <li>关于我</li>
                        
                    </ul>
                </div>
            </div>
        
        <div id="switch-area" class="switch-area">
            <div class="switch-wrap">
                <section class="switch-part switch-part1">
                    <nav class="header-menu">
                        <ul>
                        
                            <li><a  href="/archives/">所有文章</a></li>
                        
                            <li><a  href="/categories/Java/">Java</a></li>
                        
                            <li><a  href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE">大数据</a></li>
                        
                            <li><a  href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93">数据库</a></li>
                        
                            <li><a  href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">数据结构</a></li>
                        
                            <li><a  href="/about/">关于我</a></li>
                        
                        </ul>
                    </nav>
                    <nav class="header-nav">
                        <ul class="social">
                            
                                <a class="fl github"  target="_blank" href="https://github.com/bigdataxiaohan" title="github">github</a>
                            
                                <a class="fl zhihu"  target="_blank" href="https://www.zhihu.com/people/qing-feng-xiao-zhu-15/activities" title="zhihu">zhihu</a>
                            
                                <a class="fl mail"  target="_blank" href="mailto:467008580@qq.com" title="mail">mail</a>
                            
                        </ul>
                    </nav>
                </section>
                
                <section class="switch-part switch-part2">
                    <div class="widget tagcloud" id="js-tagcloud">
                        <a href="/tags/AVL%E6%A0%91/" style="font-size: 10px;">AVL树</a> <a href="/tags/Docker/" style="font-size: 14.44px;">Docker</a> <a href="/tags/Dubbo/" style="font-size: 10px;">Dubbo</a> <a href="/tags/Elasticsearch/" style="font-size: 17.78px;">Elasticsearch</a> <a href="/tags/Eureka/" style="font-size: 10px;">Eureka</a> <a href="/tags/Feign/" style="font-size: 10px;">Feign</a> <a href="/tags/Flink/" style="font-size: 11.11px;">Flink</a> <a href="/tags/Flume/" style="font-size: 11.11px;">Flume</a> <a href="/tags/Git/" style="font-size: 10px;">Git</a> <a href="/tags/GraphX/" style="font-size: 10px;">GraphX</a> <a href="/tags/HBase/" style="font-size: 11.11px;">HBase</a> <a href="/tags/HDFS/" style="font-size: 11.11px;">HDFS</a> <a href="/tags/Hadoop/" style="font-size: 13.33px;">Hadoop</a> <a href="/tags/Hbase/" style="font-size: 11.11px;">Hbase</a> <a href="/tags/Hive/" style="font-size: 13.33px;">Hive</a> <a href="/tags/Hystrix/" style="font-size: 10px;">Hystrix</a> <a href="/tags/JPA/" style="font-size: 10px;">JPA</a> <a href="/tags/JSP/" style="font-size: 10px;">JSP</a> <a href="/tags/JSR107/" style="font-size: 10px;">JSR107</a> <a href="/tags/JVM/" style="font-size: 14.44px;">JVM</a> <a href="/tags/JavaWeb/" style="font-size: 10px;">JavaWeb</a> <a href="/tags/Kafka/" style="font-size: 14.44px;">Kafka</a> <a href="/tags/MapReduce/" style="font-size: 14.44px;">MapReduce</a> <a href="/tags/Memcached/" style="font-size: 10px;">Memcached</a> <a href="/tags/MongoDB/" style="font-size: 15.56px;">MongoDB</a> <a href="/tags/Mybatis/" style="font-size: 15.56px;">Mybatis</a> <a href="/tags/Oozie/" style="font-size: 10px;">Oozie</a> <a href="/tags/RDD/" style="font-size: 14.44px;">RDD</a> <a href="/tags/REST/" style="font-size: 10px;">REST</a> <a href="/tags/RPC/" style="font-size: 10px;">RPC</a> <a href="/tags/RabbitMQ/" style="font-size: 11.11px;">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 15.56px;">Redis</a> <a href="/tags/Ribbon/" style="font-size: 10px;">Ribbon</a> <a href="/tags/SSM/" style="font-size: 11.11px;">SSM</a> <a href="/tags/SparKSQL/" style="font-size: 12.22px;">SparKSQL</a> <a href="/tags/Spark/" style="font-size: 20px;">Spark</a> <a href="/tags/SparkStreaming/" style="font-size: 12.22px;">SparkStreaming</a> <a href="/tags/Spring/" style="font-size: 14.44px;">Spring</a> <a href="/tags/Spring-Security/" style="font-size: 10px;">Spring Security</a> <a href="/tags/SpringBoot/" style="font-size: 16.67px;">SpringBoot</a> <a href="/tags/SpringBoot-Admin/" style="font-size: 10px;">SpringBoot Admin</a> <a href="/tags/SpringCloud/" style="font-size: 18.89px;">SpringCloud</a> <a href="/tags/SpringConfig/" style="font-size: 10px;">SpringConfig</a> <a href="/tags/SpringMVC/" style="font-size: 15.56px;">SpringMVC</a> <a href="/tags/Sqoop/" style="font-size: 10px;">Sqoop</a> <a href="/tags/Structured-Streaming/" style="font-size: 10px;">Structured Streaming</a> <a href="/tags/Thymeleaf/" style="font-size: 10px;">Thymeleaf</a> <a href="/tags/Zookeeper/" style="font-size: 11.11px;">Zookeeper</a> <a href="/tags/zuul/" style="font-size: 10px;">zuul</a> <a href="/tags/%E4%BA%8C%E5%8F%89%E6%A0%91/" style="font-size: 11.11px;">二叉树</a> <a href="/tags/%E4%BB%BB%E5%8A%A1/" style="font-size: 10px;">任务</a> <a href="/tags/%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/" style="font-size: 10px;">优先队列</a> <a href="/tags/%E5%93%88%E5%B8%8C%E8%A1%A8/" style="font-size: 10px;">哈希表</a> <a href="/tags/%E5%A0%86/" style="font-size: 10px;">堆</a> <a href="/tags/%E5%AD%97%E5%85%B8%E6%A0%91/" style="font-size: 10px;">字典树</a> <a href="/tags/%E5%B9%B6%E6%9F%A5%E9%9B%86/" style="font-size: 10px;">并查集</a> <a href="/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/" style="font-size: 10px;">微服务</a> <a href="/tags/%E6%8A%80%E6%9C%AF%E9%80%89%E5%9E%8B/" style="font-size: 10px;">技术选型</a> <a href="/tags/%E6%95%B0%E7%BB%84/" style="font-size: 10px;">数组</a> <a href="/tags/%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/" style="font-size: 10px;">日志框架</a> <a href="/tags/%E6%A0%88/" style="font-size: 10px;">栈</a> <a href="/tags/%E7%BA%A2%E9%BB%91%E6%A0%91/" style="font-size: 10px;">红黑树</a> <a href="/tags/%E7%BB%AA%E8%AE%BA/" style="font-size: 10px;">绪论</a> <a href="/tags/%E9%80%92%E5%BD%92/" style="font-size: 10px;">递归</a> <a href="/tags/%E9%93%BE%E8%A1%A8/" style="font-size: 10px;">链表</a> <a href="/tags/%E9%98%9F%E5%88%97/" style="font-size: 10px;">队列</a>
                    </div>
                </section>
                
                
                <section class="switch-part switch-part3">
                    <div id="js-friends">
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://blog.csdn.net/weixin_39084521?t=1">csdn</a>
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://segmentfault.com/u/qingfengxiao">segmentfault</a>
                    
                      <a target="_blank"  class="main-nav-link switch-friends-link" href="https://www.jianshu.com/u/67dbb2933255">简书</a>
                    
                    </div>
                </section>
                
                
                
                <section class="switch-part switch-part4">
                
                    <div id="js-aboutme">文科男,理工芯。有借必有贷,有问必有答。</div>
                </section>
                
            </div>
        </div>
    </header>
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
      <div class="overlay">
          <div class="slider-trigger"></div>
          <h1 class="header-author js-mobile-header hide"><a href="/" title="回到主页">清风笑丶</a></h1>
      </div>
    <div class="intrude-less">
        <header id="header" class="inner">
            <a href="/" class="profilepic">
                
                    <img lazy-src="/img/avatar.jpg" class="js-avatar">
                
            </a>
            <hgroup>
              <h1 class="header-author"><a href="/" title="回到主页">清风笑丶</a></h1>
            </hgroup>
            
            <nav class="header-menu">
                <ul>
                
                    <li><a href="/archives/">所有文章</a></li>
                
                    <li><a href="/categories/Java/">Java</a></li>
                
                    <li><a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE">大数据</a></li>
                
                    <li><a href="/categories/%E6%95%B0%E6%8D%AE%E5%BA%93">数据库</a></li>
                
                    <li><a href="/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84">数据结构</a></li>
                
                    <li><a href="/about/">关于我</a></li>
                
                <div class="clearfix"></div>
                </ul>
            </nav>
            <nav class="header-nav">
                <div class="social">
                    
                        <a class="github" target="_blank" href="https://github.com/bigdataxiaohan" title="github">github</a>
                    
                        <a class="zhihu" target="_blank" href="https://www.zhihu.com/people/qing-feng-xiao-zhu-15/activities" title="zhihu">zhihu</a>
                    
                        <a class="mail" target="_blank" href="mailto:467008580@qq.com" title="mail">mail</a>
                    
                </div>
            </nav>
        </header>
    </div>
</nav>
      <div class="body-wrap"><article id="post-Spark之RDD实战篇" class="article article-type-post" itemscope itemprop="blogPost">
  
    <div class="article-meta">
      <a  href="/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/" class="article-date">
      <time datetime="2019-05-27T13:38:53.000Z" itemprop="datePublished">2019-05-27</time>
</a>

    </div>
  
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark之RDD实战篇
    </h1>
  


      </header>
      
      <div class="article-info article-info-post">
        
    <div class="article-category tagcloud">
    <a class="article-category-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/">大数据</a>
    </div>


        
    <div class="article-tag tagcloud">
        <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/RDD/" rel="tag">RDD</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Spark/" rel="tag">Spark</a></li></ul>
    </div>

        <div class="clearfix"></div>
      </div>
      
    
    <div class="article-entry" itemprop="articleBody">
      
          
         Spark RDD创建、转换、行动算子、RDD的持久化：<Excerpt in index | 首页摘要><a id="more"></a> 

<h2 id="RDD编程"><a href="#RDD编程" class="headerlink" title="RDD编程"></a>RDD编程</h2><p>在Spark中，RDD被表示为对象，通过对象上的方法调用来对RDD进行转换。经过一系列的transformations定义RDD之后，就可以调用action触发RDD的计算，action可以是向应用程序返回结果(count, collect等)，或者是向存储系统保存数据(saveAsTextFile等)。在Spark中，只有遇到action，才会执行RDD的计算(即延迟计算)，这样在运行时可以通过管道的方式传输多个转换。</p>
<p>要使用Spark，开发者需要编写一个Driver程序，它被提交到集群以调度运行Worker，Driver中定义了一个或多个RDD，并调用RDD上的action，Worker则执行RDD分区计算任务。</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527214645.png" alt=""></p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215048.png" alt=""></p>
<h2 id="解析器集成"><a href="#解析器集成" class="headerlink" title="解析器集成"></a>解析器集成</h2><p>与Ruby和Python类似，Scala提供了一个交互式Shell (解析器)，借助内存数据带来的低延迟特性，可以让用户通过解析器对大数据进行交互式查询。Spark解析器将用户输入的多行命令解析为相应Java对象的示例如图所示</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527215246.png" alt=""></p>
<p>Scala解析器处理过程一般为：</p>
<p>①将用户输入的每一行编译成一个类；</p>
<p>②将该类载入到JVM 中；</p>
<p>③调用该类的某个函数。在该类中包含一个单利对象，对象中包含当前行的变量或函数，在初始化方法中包含处理该行的代码。例如，如果用户输入“varx=5”，在换行输入primln(x),那解析器会定义一个叫Linel的类，该类包含X，第二行编译成println (Linel.getlnstance().x)。</p>
<p>Spark中做了以下两个改变。<br>(1)类传输：为了让工作节点能够从各行生成的类中获取到字节码，通过HTTP传输。<br>(2)代码生成器的改动：通常各种代码生成的单例对象是由类的静态方法来提供的。也就是说，当序列化一个引用上一行定义变量的闭包（例如上面例子的Linel.x), Java不会通过检索对象树的方式去传输包含x的Linel实例。因此工作节点不能够得到x,在Spark中修改了代码生成器的逻辑，让各行对象的实例可以被字节应用。在图中显示了 Spark修改之后解析器是如何把用户输入的每一行变成Java对象的。</p>
<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>Spark提供了 3种持久化RDD的存储策略：</p>
<p>1.未序列化Java对象存在内存中、</p>
<p>2.序列化的数据存于内存中</p>
<p>3.存储在磁盘中</p>
<p>第一个选项的性能是最优的，因为可以直接访问在Java虚拟机内存里的RDD对象；在空间有限的情况下，第二种方式可以让用户釆用比Java对象更有效的内存组织方式，但代价是降低了性能；第三种策略使用于RDD太大的情形，每次重新计算该RDD会带来额外的资源开销（如I/O等)。对于内存使用LRU回收算法来进行管理，当计算得到一个新的RDD分区，但没有足够空间来存储时，系统会从最近最少使用的RDD回收其一个分区的空间。除非该RDD是新分区对应的RDD，这种情况下Spark会将旧的分区继续保留在内存中，防止同一个RDD的分区被循环调入/调出。这点很关键，因为大部分的操作会在一个RDD的所有分区上进行，那么很有可能己经存在内存中的分区将再次被使用。</p>
<h2 id="多用户管理"><a href="#多用户管理" class="headerlink" title="多用户管理"></a>多用户管理</h2><p>RDD模型将计算分解为多个相互独立的细粒度任务，这使得它在多用户集群能够支持多种资源共享算法。特别地，每个RDD应用可以在执行过程中动态调整访问资源。<br>在每个应用程序中，Spark运行多线程同时提交作业，并通过一种等级公平调度器来实现多个作业对集群资源的共享，这种调度器和Hadoop Fair Scheduler类似。该算法主 要用于创建基于针对相同内存数据的多用户应用，例如：Spark SQL引擎有一个服务 模式支持多用户并行查询。公平调度算法确保短的作业能够在即使长作业占满集群资源的情况下尽早完成。</p>
<p>Spark的公平调度也使用延迟调度，通过轮询每台机器的数据，在保持公平的情况下给予作业高的本地性。Spark支持多级本地化访问策略（本地化)，包括内存、磁盘和机 架。</p>
<p>由于任务相互独立，调度器还支持取消作业来为高优先级的作业腾出资源。Spark中可以使用Mesos来实现细粒度的资源共享，这使得Spark应用能相互之间或在不同的计算框架之间实现资源的动态共享。Spark使用Sparrow系统扩展支持分布式调度，该调度允许多个Spark应用以去中心化的方式在同一集群上排队工作，同时提供数据本地性、低延迟和公平性。</p>
<h2 id="RDD创建"><a href="#RDD创建" class="headerlink" title="RDD创建"></a>RDD创建</h2><h3 id="集合中创建RDD"><a href="#集合中创建RDD" class="headerlink" title="集合中创建RDD"></a>集合中创建RDD</h3><p>从已有的集合中创建RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527222920.png" alt=""></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//并行化操作  </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parallelize</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123; <span class="comment">//默认是多少呢</span></span><br><span class="line">    assertNotStopped()</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq, numSlices, <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">String</span>]]())</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//本地模式下  </span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> =</span><br><span class="line">    scheduler.conf.getInt(<span class="string">"spark.default.parallelism"</span>, totalCores)  </span><br><span class="line"><span class="comment">//CoarseGrainedSchedulerBackend</span></span><br><span class="line"> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">defaultParallelism</span></span>(): <span class="type">Int</span> = &#123;</span><br><span class="line">    conf.getInt(<span class="string">"spark.default.parallelism"</span>, math.max(totalCoreCount.get(), <span class="number">2</span>))</span><br><span class="line">  &#125;</span><br><span class="line"><span class="comment">//stanlone继承了CoarseGrainedSchedulerBackend 因此绝大部分的情况下并行化处理数据的并行度为CPU的核数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//makeRDD本质上还是调用了parallelize</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      seq: <span class="type">Seq</span>[<span class="type">T</span>],</span><br><span class="line">      numSlices: <span class="type">Int</span> = defaultParallelism): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">    parallelize(seq, numSlices)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Distribute a local Scala collection to form an RDD, with one or more</span></span><br><span class="line"><span class="comment"> * location preferences (hostnames of Spark nodes) for each object.</span></span><br><span class="line"><span class="comment"> * Create a new partition for each collection item.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">makeRDD</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](seq: <span class="type">Seq</span>[(<span class="type">T</span>, <span class="type">Seq</span>[<span class="type">String</span>])]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  <span class="keyword">val</span> indexToPrefs = seq.zipWithIndex.map(t =&gt; (t._2, t._1._2)).toMap</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ParallelCollectionRDD</span>[<span class="type">T</span>](<span class="keyword">this</span>, seq.map(_._1), math.max(seq.size, <span class="number">1</span>), indexToPrefs)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> test1 = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> seq = <span class="type">List</span>((<span class="number">1</span>,<span class="type">List</span>(<span class="string">"datanode1"</span>)),(<span class="number">2</span>,<span class="type">List</span>(<span class="string">"datanode2"</span>)))  <span class="comment">//可以提供位置信息</span></span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527230912.png" alt=""></p>
<p><code>def parallelize[T: ClassTag]</code> 和<code>def makeRDD[T: ClassTag]</code>返回的都是ParallelCollectionRDD,而且这个makeRDD的实现不可以自己指定分区的数量，而是固定为seq参数的size大小。</p>
<h3 id="外部存储系统的数据集创建"><a href="#外部存储系统的数据集创建" class="headerlink" title="外部存储系统的数据集创建"></a>外部存储系统的数据集创建</h3><p>包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> datasets =sc.textFile(<span class="string">"hdfs://datanode1:9000/input/test.txt"</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232248.png" alt=""></p>
<h2 id="RDD转换"><a href="#RDD转换" class="headerlink" title="RDD转换"></a>RDD转换</h2><h3 id="map"><a href="#map" class="headerlink" title="map()"></a>map()</h3><p>map操作时对RDD中的每一个数都执行一个指定的函数来产生一个新的RDD,任何元RDD中的元素在新RDD中都有且只有一个元素与之对应。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>).collect()</span><br><span class="line"><span class="keyword">val</span> map = data.map(_ * <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190527232843.png" alt=""></p>
<h3 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions()"></a>mapPartitions()</h3><p>类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是<code>Iterator[T] =&gt; Iterator[U]</code>。假设有N个元素，有M个分区，那么map的函数的将被调用N次,而mapPartitions被调用M次,一个函数一次处理所有分区。其中<code>preservesPartitioning</code>表示是否保留父RDD的partitiones分区信息，如果在映射过程中需要频繁创建对象，使用mapPartitions操作要比map操作高 效得多。比如，将RDD中的所有数据通过JDBC连接写入数据库，如果使用map函数，可能要为每一个元素都创建一个connection,这样开销很大。如果使用mapPartitions，那么只需要针对每一个分区建立一个connectiono mapPartitionsWithlndex操作作用类似于mapPartitions,只是输入参数多了一个分区索引。W</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mapPartitions</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">     f: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">Iterator</span>[<span class="type">U</span>],</span><br><span class="line">     preservesPartitioning: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">   <span class="keyword">val</span> cleanedF = sc.clean(f)</span><br><span class="line">   <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>(</span><br><span class="line">     <span class="keyword">this</span>,</span><br><span class="line">     (context: <span class="type">TaskContext</span>, index: <span class="type">Int</span>, iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedF(iter),</span><br><span class="line">     preservesPartitioning)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD使RDD有两个分区</span></span><br><span class="line"> <span class="keyword">var</span> rdd1= sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">///使用mapPartitions对rddl进行重新分区</span></span><br><span class="line">  <span class="keyword">var</span> rdd2 = rdd1.mapPartitions&#123; x =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span>(x.hasNext)&#123;</span><br><span class="line">          i += x.next()</span><br><span class="line">    &#125;</span><br><span class="line">     result.::(i).iterator</span><br><span class="line">   &#125;&#125;</span><br><span class="line">  <span class="comment">//rdd2将rddl中每个分区中的数值累加</span></span><br><span class="line">  rdd2.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">//重新对rdd1分区</span></span><br><span class="line"><span class="keyword">var</span> rdd3 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     (x,iter) =&gt; &#123;</span><br><span class="line">       <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">       <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">       <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">          i += iter.next()</span><br><span class="line">         &#125;</span><br><span class="line">        result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png" alt=""></p>
<h3 id="glom"><a href="#glom" class="headerlink" title="glom()"></a>glom()</h3><p>RDD中每一个分区所有类型为T的数据转变成元素类型为T的数组[Array[T]].</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">16</span>,<span class="number">4</span>)</span><br><span class="line">rdd.glom().collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083705.png" alt=""></p>
<h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap()"></a>flatMap()</h3><p>flatMap操作原RDD中的每一个元素生成一个或多个元素来构建新的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line"><span class="keyword">val</span> flatMap = rdd1.flatMap(<span class="number">1</span> to _)</span><br><span class="line">flatMap.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084629.png" alt=""></p>
<h3 id="filter"><a href="#filter" class="headerlink" title="filter()"></a>filter()</h3><p>返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> sourceFilter = sc.parallelize(<span class="type">Array</span>(<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"flink"</span>,<span class="string">"hphblog"</span>))</span><br><span class="line"><span class="keyword">val</span> filter = sourceFilter.filter(_.contains(<span class="string">"h"</span>))</span><br><span class="line">filter.collect</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528084943.png" alt=""></p>
<h3 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex()"></a>mapPartitionsWithIndex()</h3><p>类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是<code>(Int, Interator[T]) =&gt;Iterator[U]</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//创建RDD使RDD有两个分区</span></span><br><span class="line"> <span class="keyword">var</span> rdd1= sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line"><span class="comment">///使用mapPartitions对rddl进行重新分区</span></span><br><span class="line">  <span class="keyword">var</span> rdd2 = rdd1.mapPartitions&#123; x =&gt; &#123;</span><br><span class="line">      <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">      <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">      <span class="keyword">while</span>(x.hasNext)&#123;</span><br><span class="line">          i += x.next()</span><br><span class="line">    &#125;</span><br><span class="line">     result.::(i).iterator</span><br><span class="line">   &#125;&#125;</span><br><span class="line">  <span class="comment">//rdd2将rddl中每个分区中的数值累加</span></span><br><span class="line">  rdd2.collect</span><br><span class="line"></span><br><span class="line"><span class="comment">//重新对rdd1分区</span></span><br><span class="line"><span class="keyword">var</span> rdd3 = rdd1.mapPartitionsWithIndex&#123;</span><br><span class="line">     (x,iter) =&gt; &#123;</span><br><span class="line">       <span class="keyword">var</span> result = <span class="type">List</span>[<span class="type">String</span>]()</span><br><span class="line">       <span class="keyword">var</span> i = <span class="number">0</span></span><br><span class="line">       <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">          i += iter.next()</span><br><span class="line">         &#125;</span><br><span class="line">        result.::(x + <span class="string">"|"</span> + i).iterator</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/20190528083325.png" alt=""></p>
<h3 id="sample-withReplacement-fraction-seed"><a href="#sample-withReplacement-fraction-seed" class="headerlink" title="sample(withReplacement, fraction, seed)"></a>sample(withReplacement, fraction, seed)</h3><p>以指定的随机种子随机抽样出数量为fraction的数据，withReplacement表示是抽出的数据是否放回，true为有放回的抽样，false为无放回的抽样，seed用于指定随机数生成器种子。例子从RDD中随机且有放回的抽出50%的数据，随机种子值为2（即可能以1 2 3的其中一个起始值）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line">rdd.collect</span><br><span class="line"><span class="keyword">var</span> sample1 = rdd.sample(<span class="literal">true</span>,<span class="number">0.5</span>,<span class="number">2</span>)</span><br><span class="line">sample1.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528091220.png" alt=""></p>
<h3 id="distinct-numTasks"><a href="#distinct-numTasks" class="headerlink" title="distinct([numTasks]))"></a>distinct([numTasks]))</h3><p>对原来RDD进行去重后返回一个新的RDD. 默认情况下，只有8个并行任务来操作，但是可以传入一个可选的numTasks参数改变它。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">7</span>,<span class="number">8</span>))</span><br><span class="line"><span class="keyword">val</span> rdd1 = rdd.distinct()</span><br><span class="line">rdd1.collect</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.distinct(<span class="number">10</span>)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093127.png" alt=""></p>
<h3 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h3><p>对RDD进行分区操作，如果原有的partionRDD和现有的partionRDD是一致的话就不进行分区， 否则会生成ShuffleRDD。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"hadoop"</span>),(<span class="number">2</span>,<span class="string">"spark"</span>),(<span class="number">3</span>,<span class="string">"flink"</span>),(<span class="number">4</span>,<span class="string">"hphblog"</span>)),<span class="number">4</span>)</span><br><span class="line">rdd.partitions.size</span><br><span class="line"><span class="keyword">var</span> rdd2 = rdd.partitionBy(<span class="keyword">new</span> org.apache.spark.<span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br><span class="line">rdd.collect</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528093534.png" alt=""></p>
<h3 id="coalesce-numPartitions-shuffle"><a href="#coalesce-numPartitions-shuffle" class="headerlink" title="coalesce((numPartitions, shuffle)"></a>coalesce((numPartitions, shuffle)</h3><p>缩减分区数，用于大数据集过滤后，提高小数据集的执行效率。shuffle默认关闭.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10000</span>,<span class="number">4</span>)</span><br><span class="line"><span class="keyword">val</span> coalesceRDD = rdd.coalesce(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> shuffleRDD = rdd.coalesce(<span class="number">2</span>,<span class="literal">true</span>)</span><br><span class="line">shuffleRDD.collect</span><br><span class="line">rdd.collect</span><br><span class="line">coalesceRDD.partitions.size</span><br><span class="line">shuffleRDD.partitions.size</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528094743.png" alt=""></p>
<h3 id="repartition-numPartitions"><a href="#repartition-numPartitions" class="headerlink" title="repartition(numPartitions)"></a>repartition(numPartitions)</h3><p>根据分区数，从新通过网络随机洗牌所有数据。底层调用的是<code>coalesce(numPartitions, shuffle = true)</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="number">1</span> to <span class="number">10000</span>,<span class="number">4</span>)</span><br><span class="line">rdd.partitions.size</span><br><span class="line"><span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">2</span>)</span><br><span class="line">rerdd.partitions.size</span><br><span class="line"><span class="keyword">val</span> rerdd = rdd.repartition(<span class="number">4</span>)</span><br><span class="line">rerdd.partitions.size</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528100745.png" alt=""></p>
<h3 id="repartitionAndSortWithinPartitions"><a href="#repartitionAndSortWithinPartitions" class="headerlink" title="repartitionAndSortWithinPartitions"></a>repartitionAndSortWithinPartitions</h3><p>repartitionAndSortWithinPartitions函数是repartition函数的变种，与repartition函数不同的是，repartitionAndSortWithinPartitions在给定的partitioner内部进行排序，性能比repartition要高。 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repartitionAndSortWithinPartitions</span></span>(partitioner: <span class="type">Partitioner</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, partitioner).setKeyOrdering(ordering)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="sortBy-ascending-numTasks"><a href="#sortBy-ascending-numTasks" class="headerlink" title="sortBy([ascending], [numTasks])"></a>sortBy([ascending], [numTasks])</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">    f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">    ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd =sc.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>))</span><br><span class="line">rdd.sortBy(x =&gt; x ,ascending=<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102152.png" alt=""></p>
<h3 id="union-otherDataset"><a href="#union-otherDataset" class="headerlink" title="union(otherDataset)"></a>union(otherDataset)</h3><p>对源RDD和参数RDD求并集后返回一个新的RDD <code>不去重</code></p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528102902.png" alt=""></p>
<h3 id="subtract-otherDataset"><a href="#subtract-otherDataset" class="headerlink" title="subtract (otherDataset)"></a>subtract (otherDataset)</h3><p>计算差的一种函数，去除两个RDD中相同的元素，不同的RDD将保留下来 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.subtract(rdd2)</span><br><span class="line">rdd3.collect</span><br><span class="line"><span class="keyword">val</span> rdd4 =rdd2.subtract(rdd1)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559010918532.png" alt=""></p>
<h3 id="intersection-otherDataset"><a href="#intersection-otherDataset" class="headerlink" title="intersection(otherDataset)"></a>intersection(otherDataset)</h3><p>对源RDD和参数RDD求交集后返回一个新的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">5</span> to <span class="number">15</span>)</span><br><span class="line"><span class="keyword">val</span> rdd3 = rdd1.intersection(rdd2)</span><br><span class="line">rdd3.collect</span><br><span class="line"><span class="keyword">val</span> rdd4 = rdd2.intersection(rdd1)</span><br><span class="line">rdd4.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528103820.png" alt=""></p>
<h3 id="cartesian-otherDataset"><a href="#cartesian-otherDataset" class="headerlink" title="cartesian(otherDataset)"></a>cartesian(otherDataset)</h3><p>笛卡尔积</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="number">1</span> to <span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="number">2</span> to <span class="number">5</span>)</span><br><span class="line">rdd1.cartesian(rdd2).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104041.png" alt=""></p>
<h3 id="pipe-command-envVars"><a href="#pipe-command-envVars" class="headerlink" title="pipe(command, [envVars])"></a>pipe(command, [envVars])</h3><p>管道，对于每个分区，都执行一个perl或者shell脚本，返回输出的RDD</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/sh</span></span><br><span class="line">echo "hello Spark This is Linux bash"</span><br><span class="line">while read LINE; do</span><br><span class="line">   echo "&gt;&gt;&gt;"$&#123;LINE&#125;</span><br><span class="line">done</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>(<span class="string">"hi"</span>,<span class="string">"Hello"</span>,<span class="string">"hadoop"</span>,<span class="string">"spark"</span>,<span class="string">"flink"</span>,<span class="string">"hphblog"</span>),<span class="number">1</span>)</span><br><span class="line">rdd.pipe(<span class="string">"/home/hadoop/pipe.sh"</span>).collect()</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104554.png" alt=""></p>
<h3 id="join-otherDataset-numTasks"><a href="#join-otherDataset-numTasks" class="headerlink" title="join(otherDataset, [numTasks])"></a>join(otherDataset, [numTasks])</h3><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528104857.png" alt=""></p>
<h3 id="cogroup-otherDataset-numTasks"><a href="#cogroup-otherDataset-numTasks" class="headerlink" title="cogroup(otherDataset,[numTasks])"></a>cogroup(otherDataset,[numTasks])</h3><p>在类型为(K,V)和(K,W)的RDD上调用，返回一个<code>(K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;))</code>类型的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.cogroup(rdd1).collect()</span><br><span class="line"><span class="keyword">val</span> rdd2 = sc.parallelize(<span class="type">Array</span>((<span class="number">4</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">5</span>),(<span class="number">3</span>,<span class="number">6</span>)))</span><br><span class="line">rdd.cogroup(rdd2).collect()</span><br><span class="line"><span class="keyword">val</span> rdd3 = sc.parallelize(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">"a"</span>),(<span class="number">1</span>,<span class="string">"d"</span>),(<span class="number">2</span>,<span class="string">"b"</span>),(<span class="number">3</span>,<span class="string">"c"</span>)))</span><br><span class="line">rdd3.cogroup(rdd2).collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110138.png" alt=""></p>
<h3 id="reduceByKey-func-numTasks"><a href="#reduceByKey-func-numTasks" class="headerlink" title="reduceByKey(func, [numTasks])"></a>reduceByKey(func, [numTasks])</h3><p>在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，reduce任务的个数可以通过第二个可选的参数来设置。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">1</span>),(<span class="string">"spark"</span>,<span class="number">5</span>),(<span class="string">"spark"</span>,<span class="number">5</span>),(<span class="string">"flink"</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> reduce = rdd.reduceByKey((x,y)=&gt;(x+y))</span><br><span class="line">reduce.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528110533.png" alt=""></p>
<h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>groupByKey也是对每个key进行操作，但只生成一个sequence</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> words = <span class="type">Array</span>(<span class="string">"hadoop"</span>, <span class="string">"spark"</span>, <span class="string">"spark"</span>, <span class="string">"flink"</span>, <span class="string">"flink"</span>, <span class="string">"flink"</span>)</span><br><span class="line"><span class="keyword">val</span> wordPairsRDD = sc.parallelize(words).map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line"><span class="keyword">val</span> group = wordPairsRDD.groupByKey()</span><br><span class="line">group.collect()</span><br><span class="line"><span class="keyword">val</span> result = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">result.collect</span><br><span class="line"><span class="keyword">val</span> map = group.map(t =&gt; (t._1, t._2.sum))</span><br><span class="line">map.collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111123.png" alt=""></p>
<h3 id="combineByKey-C"><a href="#combineByKey-C" class="headerlink" title="combineByKey[C]"></a>combineByKey[C]</h3><p>(  createCombiner: V =&gt; C,  mergeValue: (C, V) =&gt; C,  mergeCombiners: (C, C) =&gt; C) 对相同K，把V合并成一个集合。</p>
<p>createCombiner: combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就 和之前的某个元素的键相同。如果这是一个新的元素,combineByKey() 会使用一个叫作 createCombiner() 的函数来创建<br> 那个键对应的累加器的初始值</p>
<p>mergeValue: 如果这是一个在处理当前分区之前已经遇到的键， 它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并</p>
<p>mergeCombiners: 由于每个分区都是独立处理的， 因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器， 就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> scores = <span class="type">Array</span>((<span class="string">"Fred"</span>, <span class="number">88</span>), (<span class="string">"Fred"</span>, <span class="number">95</span>), (<span class="string">"Fred"</span>, <span class="number">91</span>), (<span class="string">"Wilma"</span>, <span class="number">93</span>), (<span class="string">"Wilma"</span>, <span class="number">95</span>), (<span class="string">"Wilma"</span>, <span class="number">98</span>))</span><br><span class="line"><span class="keyword">val</span> input = sc.parallelize(scores)</span><br><span class="line"><span class="keyword">val</span> combine = input.combineByKey(</span><br><span class="line">          (v)=&gt;(v,<span class="number">1</span>),</span><br><span class="line">          (acc:(<span class="type">Int</span>,<span class="type">Int</span>),v)=&gt;(acc._1+v,acc._2+<span class="number">1</span>),</span><br><span class="line">          (acc1:(<span class="type">Int</span>,<span class="type">Int</span>),acc2:(<span class="type">Int</span>,<span class="type">Int</span>))=&gt;(acc1._1+acc2._1,acc1._2+acc2._2)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = combine.map&#123;</span><br><span class="line">         <span class="keyword">case</span> (key,value) =&gt; (key,value._1/value._2.toDouble)</span><br><span class="line">&#125;</span><br><span class="line">result.collect()</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528111935.png" alt=""></p>
<h3 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h3><p><code>(zeroValue:U,[partitioner: Partitioner]) (seqOp:(U, V) =&gt; U,combOp: (U, U) =&gt; U)</code></p>
<p>在kv对的RDD中，，按key将value进行分组合并，合并时，将每个value和初始值作为seq函数的参数，进行计算，返回的结果作为一个新的kv对，然后再将结果按照key进行合并，最后将每个分组的value传递给combine函数进行计算（先将前两个value进行计算，将返回结果和下一个value传给combine函数，以此类推），将key与计算结果作为一个新的kv对输出。</p>
<p>seqOp函数用于在每一个分区中用初始值逐步迭代value，combOp函数用于合并每个分区中的结果。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_)</span><br><span class="line">agg.collect()</span><br><span class="line">agg.partitions.size</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">1</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.aggregateByKey(<span class="number">0</span>)(math.max(_,_),_+_).collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528114417.png" alt=""></p>
<h3 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h3><p><code>(zeroValue: V)(func: (V, V) =&gt; V): RDD[(K, V)]</code>aggregateByKey的简化操作，seqop和combop相同</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="number">1</span>,<span class="number">3</span>),(<span class="number">1</span>,<span class="number">2</span>),(<span class="number">1</span>,<span class="number">4</span>),(<span class="number">2</span>,<span class="number">3</span>),(<span class="number">3</span>,<span class="number">6</span>),(<span class="number">3</span>,<span class="number">8</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">val</span> agg = rdd.foldByKey(<span class="number">0</span>)(_+_)</span><br><span class="line">agg.collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115239.png" alt=""></p>
<h3 id="sortByKey-ascending-numTasks"><a href="#sortByKey-ascending-numTasks" class="headerlink" title="sortByKey([ascending], [numTasks])"></a>sortByKey([ascending], [numTasks])</h3><p>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"hadoop"</span>),(<span class="number">6</span>,<span class="string">"hohblog"</span>),(<span class="number">2</span>,<span class="string">"flink"</span>),(<span class="number">1</span>,<span class="string">"spark"</span>)))</span><br><span class="line">rdd.sortByKey(<span class="literal">true</span>).collect()</span><br><span class="line">rdd.sortByKey(<span class="literal">false</span>).collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/1559015691057.png" alt=""></p>
<h3 id="mapValues"><a href="#mapValues" class="headerlink" title="mapValues"></a>mapValues</h3><p>针对于(K,V)形式的类型只对V进行操作 </p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>((<span class="number">3</span>,<span class="string">"hadoop"</span>),(<span class="number">6</span>,<span class="string">"hohblog"</span>),(<span class="number">2</span>,<span class="string">"flink"</span>),(<span class="number">1</span>,<span class="string">"spark"</span>)))</span><br><span class="line">rdd.mapValues(_+<span class="string">"==&gt; www.hphblog.cn"</span>).collect()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528115636.png" alt=""></p>
<h2 id="RDD行动算子"><a href="#RDD行动算子" class="headerlink" title="RDD行动算子"></a>RDD行动算子</h2><h3 id="reduce-func"><a href="#reduce-func" class="headerlink" title="reduce(func)"></a>reduce(func)</h3><p>通过func函数聚集RDD中的所有元素，这个功能必须是可交换且可并联的</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.reduce(_+_)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">"a"</span>,<span class="number">1</span>),(<span class="string">"a"</span>,<span class="number">3</span>),(<span class="string">"c"</span>,<span class="number">3</span>),(<span class="string">"d"</span>,<span class="number">5</span>)))</span><br><span class="line">rdd1.reduce((x,y)=&gt;(x._1 + y._1,x._2 + y._2))</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120250.png" alt=""></p>
<h3 id="collect"><a href="#collect" class="headerlink" title="collect()"></a>collect()</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.collect()</span><br></pre></td></tr></table></figure>

<p>在驱动程序中，以数组的形式返回数据集的所有元素</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120451.png" alt=""></p>
<h3 id="count"><a href="#count" class="headerlink" title="count()"></a>count()</h3><p>返回RDD的元素个数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd. count()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120547.png" alt=""></p>
<h3 id="first"><a href="#first" class="headerlink" title="first()"></a>first()</h3><p>返回RDD的第一个元素（类似于take(1)）</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.first()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120819.png" alt=""></p>
<h3 id="take-n"><a href="#take-n" class="headerlink" title="take(n)"></a>take(n)</h3><p>返回一个由数据集的前n个元素组成的数组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528120952.png" alt=""></p>
<h3 id="takeSample-withReplacement-num-seed"><a href="#takeSample-withReplacement-num-seed" class="headerlink" title="takeSample(withReplacement,num, [seed])"></a>takeSample(withReplacement,num, [seed])</h3><p>返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.takeSample(<span class="literal">true</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.takeSample(<span class="literal">false</span>,<span class="number">10</span>,<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121504.png" alt=""></p>
<h3 id="takeOrdered-n"><a href="#takeOrdered-n" class="headerlink" title="takeOrdered(n)"></a>takeOrdered(n)</h3><p>返回前几个的排序</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">2</span>)</span><br><span class="line">rdd.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528121702.png" alt=""></p>
<h3 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h3><p><code>(zeroValue: U)(seqOp: (U, T) ⇒ U, combOp: (U, U) ⇒ U)</code>aggregate函数将每个分区里面的元素通过seqOp和初始值进行聚合，然后用combine函数将每个分区的结果和初始值(zeroValue)进行combine操作。这个函数最终返回的类型不需要和RDD中元素类型一致。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;, </span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528183859.png" alt=""></p>
<p>为什么是58呢:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rdd.mapPartitionsWithIndex&#123;</span><br><span class="line">    (partid,iter)=&gt;&#123;</span><br><span class="line">        <span class="keyword">var</span> part_map = scala.collection.mutable.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">List</span>[<span class="type">Int</span>]]()</span><br><span class="line">        <span class="keyword">var</span> part_name = <span class="string">"part_"</span> + partid</span><br><span class="line">        part_map(part_name) = <span class="type">List</span>[<span class="type">Int</span>]()</span><br><span class="line">        <span class="keyword">while</span>(iter.hasNext)&#123;</span><br><span class="line">            part_map(part_name) :+= iter.next()<span class="comment">//:+= 列表尾部追加元素</span></span><br><span class="line">        &#125;</span><br><span class="line">        part_map.iterator</span><br><span class="line">    &#125;</span><br><span class="line">&#125;.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528185402.png" alt=""></p>
<p>遍历第一个分区的数据我们知道第一个分区的数据是(1,2,3,4,5),第二个分区的数据是(6,7,8,9,10)首先在每一个分区执行<code>(x : Int,y : Int) =&gt; x + y</code>我们传入的zeroValue的值为1,即在<code>part_0中zeroValue+5+4+3+2+1=19</code>,在<code>part_1中zeroValue+6+7+8+9+10=41</code>,在将连个分局的结果合并<code>(a : Int,b : Int) =&gt; a + b</code>,并且使用zeroValue的值1即<code>zeroValue+part_0+part_1=1+16+41=58</code>因此结果为58.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x * y&#125;,</span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br></pre></td></tr></table></figure>

<p>相同的我们可以刻分析出来</p>
<p>首先在每一个分区执行<code>(x : Int,y : Int) =&gt; x * y</code>我们传入的zeroValue的值为1,即在<code>part_0中zeroValue*5*4*3*2*1=120</code>,在<code>part_1中zeroValue*6*7*8*9*10=30240</code>,在将连个分局的结果合并<code>(a : Int,b : Int) =&gt; a + b</code>,并且使用zeroValue的值1即<code>zeroValue+part_0+part_1=1+120+30240=30361</code>因此结果为30361.</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528192803.png" alt=""></p>
<h3 id="fold-num-func"><a href="#fold-num-func" class="headerlink" title="fold(num)(func)"></a>fold(num)(func)</h3><p>折叠操作，aggregate的简化操作，seqop和combop一样。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.aggregate(<span class="number">1</span>)(</span><br><span class="line">     &#123;(x : <span class="type">Int</span>,y : <span class="type">Int</span>) =&gt; x + y&#125;,</span><br><span class="line">      &#123;(a : <span class="type">Int</span>,b : <span class="type">Int</span>) =&gt; a + b&#125;</span><br><span class="line">      )</span><br><span class="line">rdd.fold(<span class="number">1</span>)(_+_)</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201005.png" alt=""></p>
<h3 id="saveAsTextFile-path"><a href="#saveAsTextFile-path" class="headerlink" title="saveAsTextFile(path)"></a>saveAsTextFile(path)</h3><p>将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>,<span class="number">2</span>)</span><br><span class="line">rdd.saveAsTextFile(<span class="string">"hdfs://datanode1:9000/spark/saveAsTextFile/"</span>)</span><br></pre></td></tr></table></figure>



<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201529.png" alt=""></p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201656.png" alt=""></p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528201735.png" alt=""></p>
<h3 id="saveAsSequenceFile-path"><a href="#saveAsSequenceFile-path" class="headerlink" title="saveAsSequenceFile(path)"></a>saveAsSequenceFile(path)</h3><p>将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。</p>
<h3 id="saveAsObjectFile-path"><a href="#saveAsObjectFile-path" class="headerlink" title="saveAsObjectFile(path)"></a>saveAsObjectFile(path)</h3><p>用于将RDD中的元素序列化成对象，存储到文件中。</p>
<h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey()"></a>countByKey()</h3><p>针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">List</span>((<span class="string">"hadoop"</span>,<span class="number">3</span>),(<span class="string">"spark"</span>,<span class="number">2</span>),(<span class="string">"hphblog"</span>,<span class="number">3</span>),(<span class="string">"flink"</span>,<span class="number">9</span>),(<span class="string">"flink"</span>,<span class="number">9</span>),(<span class="string">"spark"</span>,<span class="number">10</span>)),<span class="number">3</span>)</span><br><span class="line">rdd.countByKey()</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202419.png" alt=""></p>
<h3 id="foreach-func"><a href="#foreach-func" class="headerlink" title="foreach(func)"></a>foreach(func)</h3><p>在数据集的每一个元素上，运行函数func进行更新。注意foreach遍历RDD,将函数f应用于每一个元素.要注意如果对RDD执行foreach,只会在Executor端有效,而不是Driver.比如rdd.collect().foreach(println),只会在Executor端有效,Driver端是看不到的.</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528202856.png" alt=""></p>
<h3 id="sortBy-funct"><a href="#sortBy-funct" class="headerlink" title="sortBy(funct)"></a>sortBy(funct)</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">"A"</span>,<span class="number">2</span>),(<span class="string">"D"</span>,<span class="number">5</span>), (<span class="string">"A"</span>,<span class="number">1</span>), (<span class="string">"B"</span>,<span class="number">6</span>), (<span class="string">"B"</span>,<span class="number">3</span>), (<span class="string">"E"</span>, <span class="number">7</span>),(<span class="string">"C"</span>,<span class="number">4</span>)))</span><br><span class="line">rdd.sortBy(x =&gt; x).collect</span><br><span class="line">rdd.sortBy(x =&gt; x._2,<span class="literal">false</span>).collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528204821.png" alt=""></p>
<h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><p>Spark速度非常快的原因之一，就是在不同操作中可以在内存中持久化或缓存个数据集。当持久化某个RDD后，每一个节点都将把计算的分片结果保存在内存中，并在对此RDD或衍生出的RDD进行的其他动作中重用。这使得后续的动作变得更加迅速。RDD相关的持久化和缓存，是Spark最重要的特征之一。可以说，缓存是Spark构建迭代式算法和快速交互式查询的关键。如果一个有持久化数据的节点发生故障，Spark 会在需要用到缓存的数据时重算丢失的数据分区。如果 希望节点故障的情况不会拖累我们的执行速度，也可以把数据备份到多个节点上。</p>
<h3 id="缓存方式"><a href="#缓存方式" class="headerlink" title="缓存方式"></a>缓存方式</h3><p>RDD通过persist方法或cache方法可以将前面的计算结果缓存，默认情况下 persist() 会把数据以序列化的形式缓存在 JVM 的堆空 间中。 </p>
<p>但是并不是这两个方法被调用时立即缓存，而是触发后面的action时，该RDD将会被缓存在计算节点的内存中，并供后面重用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">persist</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)  <span class="comment">//默认的持久化是内存中</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cache</span></span>(): <span class="keyword">this</span><span class="class">.<span class="keyword">type</span> </span>= persist()   <span class="comment">//cache最终也是调用了persist方法</span></span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224007.png" alt=""></p>
<p>在存储级别的末尾加上“_2”来把持久化数据存为两份</p>
<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224100.png" alt=""></p>
<p>缓存有可能丢失，或者存储存储于内存的数据由于内存不足而被删除，RDD的缓存容错机制保证了即使缓存丢失也能保证计算的正确执行。通过基于RDD的一系列转换，丢失的数据会被重算，由于RDD的各个Partition是相对独立的，因此只需要计算丢失的部分即可，并不需要重算全部Partition。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">10</span>)</span><br><span class="line"><span class="keyword">val</span> nocache = rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line"><span class="keyword">val</span> cache =  rdd.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">cache.cache</span><br><span class="line">nocache.collect</span><br><span class="line">nocache.collect</span><br><span class="line">cache.collect</span><br><span class="line">cache.collect</span><br><span class="line">cache.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190528224651.png" alt=""></p>
<p>我们发现持久化的内存时间戳没有变化,未持久化的内存时间戳是有变化的</p>
<h2 id="RDD检查点机制"><a href="#RDD检查点机制" class="headerlink" title="RDD检查点机制"></a>RDD检查点机制</h2><p>Spark中对于数据的保存除了持久化操作之外，还提供了一种检查点的机制，检查点（本质是通过将RDD写入Disk做检查点）是为了通过lineage做容错的辅助，lineage过长会造成容错成本过高，这样就不如在中间阶段做检查点容错，如果之后有节点出现问题而丢失分区，从做检查点的RDD开始重做Lineage，就会减少开销。检查点通过将数据写入到HDFS文件系统实现了RDD的检查点功能。</p>
<p>cache 和 checkpoint 是有显著区别的，  缓存把 RDD 计算出来然后放在内存中，但是RDD 的依赖链（相当于数据库中的redo 日志）， 也不能丢掉， 当某个点某个 executor 宕了，上面cache 的RDD就会丢掉， 需要通过依赖链重放计算出来， 不同的是， checkpoint是把 RDD 保存在 HDFS中， 是多副本可靠存储，所以依赖链就可以丢掉了，就斩断了依赖链， 是通过复制实现的高容错。</p>
<p>如果存在以下场景，则比较适合使用检查点机制：</p>
<p>1）DAG中的Lineage过长，如果重算，则开销太大（如在PageRank中）。</p>
<p>2）在宽依赖上做Checkpoint获得的收益更大。</p>
<p>为当前RDD设置检查点。该函数将会创建一个二进制的文件，并存储到checkpoint目录中，该目录是用SparkContext.setCheckpointDir()设置的。在checkpoint的过程中，该RDD的所有依赖于父RDD中的信息将全部被移出。对RDD进行checkpoint操作并不会马上被执行，必须执行Action操作才能触发。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> data = sc.parallelize(<span class="number">1</span> to <span class="number">1000</span> , <span class="number">5</span>)</span><br><span class="line">sc.setCheckpointDir(<span class="string">"hdfs://datanode1:9000/checkpoint"</span>)</span><br><span class="line">data.checkpoint</span><br><span class="line">data.count</span><br><span class="line"><span class="keyword">val</span> ch1 = sc.parallelize(<span class="number">1</span> to <span class="number">20</span>)</span><br><span class="line"><span class="keyword">val</span> ch2 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line"><span class="keyword">val</span> ch3 = ch1.map(_.toString+<span class="string">"["</span>+<span class="type">System</span>.currentTimeMillis+<span class="string">"]"</span>)</span><br><span class="line">ch3.checkpoint</span><br><span class="line">ch2.collect</span><br><span class="line">ch2.collect</span><br><span class="line">ch2.collect</span><br><span class="line">ch3.collect</span><br><span class="line">ch3.collect</span><br><span class="line">ch3.collect</span><br></pre></td></tr></table></figure>

<p><img src="https://hphimages-1253879422.cos.ap-beijing.myqcloud.com/%E5%A4%A7%E6%95%B0%E6%8D%AE/Spark/RDD/20190529115207.png" alt=""></p>

      
    </div>
    
  </div>
  
    
    <div class="copyright">
        <p><span>本文标题:</span><a  href="/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/">Spark之RDD实战篇</a></p>
        <p><span>文章作者:</span><a  href="/" title="访问 清风笑丶 的个人博客">清风笑丶</a></p>
        <p><span>发布时间:</span>2019年05月27日 - 21时38分</p>
        <p><span>最后更新:</span>2020年01月12日 - 21时08分</p>
        <p>
            <span>原始链接:</span><a class="post-url" href="/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/" title="Spark之RDD实战篇">https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/</a>
            <span class="copy-path" data-clipboard-text="原文: https://www.hphblog.cn/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/　　作者: 清风笑丶" title=""></span>
        </p>
        <p>
            <span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license noopener" href="http://creativecommons.org/licenses/by-nc-sa/3.0/cn/" target="_blank" title="中国大陆 (CC BY-NC-SA 3.0 CN)" target = "_blank">"署名-非商用-相同方式共享 3.0"</a> 转载请保留原文链接及作者。
        </p>
    </div>



<nav id="article-nav">
  
    <a  href="/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption"><</strong>
      <div class="article-nav-title">
        
          Spark之RDD实战2
        
      </div>
    </a>
  
  
    <a  href="/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">Spark之RDD理论篇</div>
      <strong class="article-nav-caption">></strong>
    </a>
  
</nav>


  
</article>

    <div id="toc" class="toc-article">
    <strong class="toc-title">文章目录</strong>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD编程"><span class="toc-number">1.</span> <span class="toc-text">RDD编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#解析器集成"><span class="toc-number">2.</span> <span class="toc-text">解析器集成</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#内存管理"><span class="toc-number">3.</span> <span class="toc-text">内存管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#多用户管理"><span class="toc-number">4.</span> <span class="toc-text">多用户管理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD创建"><span class="toc-number">5.</span> <span class="toc-text">RDD创建</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#集合中创建RDD"><span class="toc-number">5.1.</span> <span class="toc-text">集合中创建RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#外部存储系统的数据集创建"><span class="toc-number">5.2.</span> <span class="toc-text">外部存储系统的数据集创建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD转换"><span class="toc-number">6.</span> <span class="toc-text">RDD转换</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#map"><span class="toc-number">6.1.</span> <span class="toc-text">map()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitions"><span class="toc-number">6.2.</span> <span class="toc-text">mapPartitions()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#glom"><span class="toc-number">6.3.</span> <span class="toc-text">glom()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#flatMap"><span class="toc-number">6.4.</span> <span class="toc-text">flatMap()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#filter"><span class="toc-number">6.5.</span> <span class="toc-text">filter()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapPartitionsWithIndex"><span class="toc-number">6.6.</span> <span class="toc-text">mapPartitionsWithIndex()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sample-withReplacement-fraction-seed"><span class="toc-number">6.7.</span> <span class="toc-text">sample(withReplacement, fraction, seed)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#distinct-numTasks"><span class="toc-number">6.8.</span> <span class="toc-text">distinct([numTasks]))</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#partitionBy"><span class="toc-number">6.9.</span> <span class="toc-text">partitionBy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#coalesce-numPartitions-shuffle"><span class="toc-number">6.10.</span> <span class="toc-text">coalesce((numPartitions, shuffle)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#repartition-numPartitions"><span class="toc-number">6.11.</span> <span class="toc-text">repartition(numPartitions)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#repartitionAndSortWithinPartitions"><span class="toc-number">6.12.</span> <span class="toc-text">repartitionAndSortWithinPartitions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sortBy-ascending-numTasks"><span class="toc-number">6.13.</span> <span class="toc-text">sortBy([ascending], [numTasks])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#union-otherDataset"><span class="toc-number">6.14.</span> <span class="toc-text">union(otherDataset)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#subtract-otherDataset"><span class="toc-number">6.15.</span> <span class="toc-text">subtract (otherDataset)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#intersection-otherDataset"><span class="toc-number">6.16.</span> <span class="toc-text">intersection(otherDataset)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cartesian-otherDataset"><span class="toc-number">6.17.</span> <span class="toc-text">cartesian(otherDataset)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pipe-command-envVars"><span class="toc-number">6.18.</span> <span class="toc-text">pipe(command, [envVars])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#join-otherDataset-numTasks"><span class="toc-number">6.19.</span> <span class="toc-text">join(otherDataset, [numTasks])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cogroup-otherDataset-numTasks"><span class="toc-number">6.20.</span> <span class="toc-text">cogroup(otherDataset,[numTasks])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reduceByKey-func-numTasks"><span class="toc-number">6.21.</span> <span class="toc-text">reduceByKey(func, [numTasks])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#groupByKey"><span class="toc-number">6.22.</span> <span class="toc-text">groupByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#combineByKey-C"><span class="toc-number">6.23.</span> <span class="toc-text">combineByKey[C]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregateByKey"><span class="toc-number">6.24.</span> <span class="toc-text">aggregateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foldByKey"><span class="toc-number">6.25.</span> <span class="toc-text">foldByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sortByKey-ascending-numTasks"><span class="toc-number">6.26.</span> <span class="toc-text">sortByKey([ascending], [numTasks])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mapValues"><span class="toc-number">6.27.</span> <span class="toc-text">mapValues</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD行动算子"><span class="toc-number">7.</span> <span class="toc-text">RDD行动算子</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#reduce-func"><span class="toc-number">7.1.</span> <span class="toc-text">reduce(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect"><span class="toc-number">7.2.</span> <span class="toc-text">collect()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#count"><span class="toc-number">7.3.</span> <span class="toc-text">count()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#first"><span class="toc-number">7.4.</span> <span class="toc-text">first()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#take-n"><span class="toc-number">7.5.</span> <span class="toc-text">take(n)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#takeSample-withReplacement-num-seed"><span class="toc-number">7.6.</span> <span class="toc-text">takeSample(withReplacement,num, [seed])</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#takeOrdered-n"><span class="toc-number">7.7.</span> <span class="toc-text">takeOrdered(n)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#aggregate"><span class="toc-number">7.8.</span> <span class="toc-text">aggregate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#fold-num-func"><span class="toc-number">7.9.</span> <span class="toc-text">fold(num)(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsTextFile-path"><span class="toc-number">7.10.</span> <span class="toc-text">saveAsTextFile(path)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsSequenceFile-path"><span class="toc-number">7.11.</span> <span class="toc-text">saveAsSequenceFile(path)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#saveAsObjectFile-path"><span class="toc-number">7.12.</span> <span class="toc-text">saveAsObjectFile(path)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#countByKey"><span class="toc-number">7.13.</span> <span class="toc-text">countByKey()</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#foreach-func"><span class="toc-number">7.14.</span> <span class="toc-text">foreach(func)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sortBy-funct"><span class="toc-number">7.15.</span> <span class="toc-text">sortBy(funct)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD持久化"><span class="toc-number">8.</span> <span class="toc-text">RDD持久化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#缓存方式"><span class="toc-number">8.1.</span> <span class="toc-text">缓存方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RDD检查点机制"><span class="toc-number">9.</span> <span class="toc-text">RDD检查点机制</span></a></li></ol>
</div>
<style>
    .left-col .switch-btn {
        display: none;
    }
    .left-col .switch-area {
        display: none;
    }
</style>
<input type="button" id="tocButton" value="隐藏目录"  title="点击按钮隐藏或者显示文章目录">

<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

<script>
    var valueHide = "隐藏目录";
    var valueShow = "显示目录";
    if ($(".left-col").is(":hidden")) {
        $("#tocButton").attr("value", valueShow);
    }
    $("#tocButton").click(function() {
        if ($("#toc").is(":hidden")) {
            $("#tocButton").attr("value", valueHide);
            $("#toc").slideDown(320);
            $(".switch-btn, .switch-area").fadeOut(300);
        }
        else {
            $("#tocButton").attr("value", valueShow);
            $("#toc").slideUp(350);
            $(".switch-btn, .switch-area").fadeIn(500);
        }
    })
    if ($(".toc").length < 1) {
        $("#toc, #tocButton").hide();
        $(".switch-btn, .switch-area").show();
    }
</script>




<div class="bdsharebuttonbox">
	<a href="#" class="fx fa-weibo bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
	<a href="#" class="fx fa-weixin bds_weixin" data-cmd="weixin" title="分享到微信"></a>
	<a href="#" class="fx fa-qq bds_sqq" data-cmd="sqq" title="分享到QQ好友"></a>
	<a href="#" class="fx fa-facebook-official bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
	<a href="#" class="fx fa-twitter bds_twi" data-cmd="twi" title="分享到Twitter"></a>
	<a href="#" class="fx fa-linkedin bds_linkedin" data-cmd="linkedin" title="分享到linkedin"></a>
	<a href="#" class="fx fa-files-o bds_copy" data-cmd="copy" title="分享到复制网址"></a>
</div>
<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"2","bdMiniList":false,"bdPic":"","bdStyle":"2","bdSize":"24"},"share":{}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>




    
        <section class="changyan" id="comments">
  <!--<div id="uyan_frame"></div>-->
  <div id="SOHUCS"></div>
  <script charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/changyan.js"></script>
  <script type="text/javascript">
    window.changyan.api.config({
      appid: 'cyu9wWYgq',
      conf: 'prod_cca6a7c58b43f725f8489bdcee045320'
    });
  </script>
  <style>#feedAv{ margin-top: -250px !important;transform: scale(0) !important;}</style>
  <style>#pop_ad{ margin-top: -250px !important;transform: scale(0) !important;}</style>
</section>

    



    <div class="scroll" id="post-nav-button">
        
            <a  href="/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/" title="上一篇: Spark之RDD实战2">
                <i class="fa fa-angle-left"></i>
            </a>
        
        <a title="文章列表"><i class="fa fa-bars"></i><i class="fa fa-times"></i></a>
        
            <a  href="/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/" title="下一篇: Spark之RDD理论篇">
                <i class="fa fa-angle-right"></i>
            </a>
        
    </div>
    <ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2020/01/20/Flink%E8%BF%90%E8%A1%8C%E6%9E%B6%E6%9E%84/">Flink运行架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2020/01/05/Flink%E5%88%9D%E8%AF%86/">Flink初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/10/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%903/">Spark内核解析3</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%902/">Spark内核解析2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/09/Spark%E5%86%85%E6%A0%B8%E8%A7%A3%E6%9E%90/">Spark内核解析1</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/08/Spark%E4%B9%8BGraphX/">Spark之GraphX</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/07/Spark%E4%B9%8BStructuredStreaming/">Spark之StructuredStreaming</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/06/Spark%E4%B9%8BSparkStreaming%E7%9A%84DStream%E6%93%8D%E4%BD%9C/">Spark之SparkStreaming的DStream操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/05/Spark%E4%B9%8BSparkStreaming%E6%95%B0%E6%8D%AE%E6%BA%90/">Spark之SparkStreaming数据源</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/03/Spark%E4%B9%8BSparkStreaming%E7%90%86%E8%AE%BA%E7%AF%87/">Spark之SparkStreaming理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/06/01/Spark%E4%B9%8BSparkSQL%E6%95%B0%E6%8D%AE%E6%BA%90/">Spark之SparkSQL数据源</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/Spark%E4%B9%8BSparkSQL%E5%AE%9E%E6%88%98/">Spark之SparkSQL实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/30/Spark%E4%B9%8BSparkSQL/">Spark之SparkSQL理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/29/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%873/">Spark之RDD实战篇3</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/28/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%872/">Spark之RDD实战2</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD%E5%AE%9E%E6%88%98%E7%AF%87/">Spark之RDD实战篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD%E7%90%86%E8%AE%BA%E7%AF%87/">Spark之RDD理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/27/Spark%E4%B9%8BRDD/">Spark之RDD理论篇</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/Spark%E7%94%9F%E6%80%81%E5%9C%88%E5%8F%8A%E5%AE%89%E8%A3%85/">Spark生态圈及安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/26/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%93%88%E5%B8%8C%E8%A1%A8/">数据结构之哈希表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/23/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E7%BA%A2%E9%BB%91%E6%A0%91/">数据结构之红黑树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/22/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8BAVL%E6%A0%91/">数据结构之AVL树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%B9%B6%E6%9F%A5%E9%9B%86/">数据结构之并查集</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/14/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%AD%97%E5%85%B8%E6%A0%91/">数据结构之字典树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/10/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E5%A0%86%E4%B8%8E%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97/">数据结构之堆与优先队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/07/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E4%BA%8C%E5%8F%89%E6%A0%91/">数据结构之二叉树</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/06/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%80%92%E5%BD%92/">数据结构之递归</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/04/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%93%BE%E8%A1%A8/">数据结构之链表</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E9%98%9F%E5%88%97/">数据结构之队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%A0%88/">数据结构之栈</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B%E6%95%B0%E7%BB%84/">数据结构之数组</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/05/02/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E5%89%8D%E6%8F%90/">数据结构与算法前置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/28/Java%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84%E6%BC%94%E8%BF%9B%E5%92%8CSpringCloud%E6%80%BB%E7%BB%93/">Java项目架构演进和SpringCloud总结</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8ESpringConfig%E5%88%86%E5%B8%83%E5%BC%8F%E9%85%8D%E7%BD%AE%E4%B8%AD%E5%BF%83/">SpringCloud与SpringConfig分布式配置中心</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8Ezuul/">SpringCloud与zuul</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/27/SpringCloud%E4%B8%8EHystrix%E6%96%AD%E8%B7%AF%E5%99%A8/">SpringCloud与Hystrix断路器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/26/SpringCloud%E4%B8%8EFeign%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/">SpringCloud与Feign</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/25/SpringCloud%E7%9A%84Ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/">SpringCloud的Ribbon负载均衡</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/23/SpringCloud%E6%B3%A8%E5%86%8C%E4%B8%8E%E5%8F%91%E7%8E%B0Eureka/">SpringCloud注册与发现Eureka</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/21/SpringCloud%E4%B8%8EREST%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%9E%84%E5%BB%BA/">微服务与SpringCloud</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/18/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%8ESpringCloud/">微服务与SpringCloud</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/14/SpringBoot%E5%92%8C%E7%9B%91%E6%8E%A7%E7%AE%A1%E7%90%86/">SpringBoot和监控管理</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/14/SpringBoot%E4%B8%8ESpringCloud%E9%9B%86%E6%88%90/">SpringBoot与SpringCloud集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E4%B8%8EDubbo%E9%9B%86%E6%88%90/">SpringBoot与Dubbo集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E5%AE%89%E5%85%A8/">SpringBoot与安全</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/13/SpringBoot%E4%B8%8E%E4%BB%BB%E5%8A%A1/">SpringBoot与任务</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/12/SpringBoot%E5%92%8CElasticSearch%E9%9B%86%E6%88%90/">SpringBoot和Elasticsearch集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/12/Elasticsearch%E7%AE%80%E4%BB%8B/">Elasticsearch简介</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/11/SpringBoot%E5%92%8CRabbitMQ%E9%9B%86%E6%88%90/">SpringBoot和RabbitMQ集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/11/SpringBoot%E5%92%8C%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列RabbitMQ</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/10/SpringBoot%E4%B8%8ERedis%E7%BC%93%E5%AD%98/">SpringBoot与Redis缓存</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/09/SpringBoot%E5%92%8C%E7%BC%93%E5%AD%98/">SpringBoot和缓存</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/09/SpringBoot%E4%B8%8EJPA/">SpringBoot与JPA</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/08/SpringBoot%E4%B8%8EMybatis%E7%9A%84%E9%9B%86%E6%88%90/">SpringBoot与Mybatis的集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/08/SpringBoot%E6%95%B0%E6%8D%AE%E8%AE%BF%E9%97%AE/">SpringBoot数据访问</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/07/DockerFile/">DockerFile</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/06/Docker%E5%AD%98%E5%82%A8%E5%8D%B7/">Docker存储卷</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/06/Dokcer%E7%BD%91%E7%BB%9C/">Dokcer网络</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/05/Docker%E7%9A%84%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/">Docker的基础命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/04/Docker%E5%88%9D%E8%AF%86%E4%B8%8E%E5%AE%89%E8%A3%85/">Docker初识与安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/03/SpringBoot%E4%BD%BF%E7%94%A8%E5%A4%96%E7%BD%AE%E7%9A%84Servlet%E5%AE%B9%E5%99%A8/">SpringBoot使用外置的Servlet容器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/04/02/SpringBoot%E9%85%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%E5%BC%8FServlet%E5%AE%B9%E5%99%A8/">SpringBoot配置嵌入式Servlet容器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E4%B9%8BSpringMVC%E8%87%AA%E5%8A%A8%E9%85%8D%E7%BD%AE/">SpringBoot之SpringMVC自动配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E4%B9%8BThymeleaf/">SpringBoot之Thymeleaf</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/29/SpringBoot%E7%9A%84Web%E5%BC%80%E5%8F%91/">SpringBoot的Web开发</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/28/SpringBoot%E7%9A%84%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/">SpringBoot的日志框架</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/28/SpringBoot%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%E6%8E%A2%E7%A9%B6/">SpringBoot自动装配探究</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/25/SpringBoot%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/">SpringBoot的配置文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/25/SpringBoot%E5%88%9D%E8%AF%86/">SpringBoot初识</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/24/SSM%E9%9B%86%E6%88%90/">SSM集成</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/SSM%E6%95%B4%E5%90%88/">SSM整合</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/Mybatis%E4%B9%8B%E5%8A%A8%E6%80%81SQL/">Mybatis之动态SQL</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/19/Mybatis%E7%9A%84resultMap%E8%87%AA%E5%AE%9A%E4%B9%89%E6%98%A0%E5%B0%84/">Mybatis的resultMap自定义映射</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/18/MyBatis%E7%9A%84CURD/">MyBatis的CURD</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/18/MyBatis%E5%85%A8%E5%B1%80%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%92%8C%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6/">MyBatis全局配置文件和映射文件</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/16/Mybatis%E5%85%A5%E9%97%A8/">Mybatis入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/13/Spring%E5%92%8CSpringMVC%E6%95%B4%E5%90%88/">Spring和SpringMVC整合</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/12/SpringMV%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E5%88%86%E6%9E%90/">SpringMV工作流程分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/11/SpringMVC%E8%BF%9B%E9%98%B6/">SpringMVC处理Json、文件上传、拦截器</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/07/Spring%E5%A4%84%E7%90%86%E8%AF%B7%E6%B1%82%E6%88%96%E5%93%8D%E5%BA%94%E6%95%B0%E6%8D%AE/">SpringMVC处理请求或响应数据</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/06/SpringMVC%E6%A6%82%E8%BF%B0/">SpringMVC概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/05/Spring%E4%BA%8B%E5%8A%A1%E6%A6%82%E8%BF%B0/">Spring声明式事务</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/05/JdbcTemplate/">JdbcTemplate</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/03/AOP%E6%A6%82%E8%BF%B0/">AOP概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/02/SpringIOC%E5%AE%B9%E5%99%A8%E5%92%8CBean%E7%9A%84%E9%85%8D%E7%BD%AE/">Spring IOC容器和Bean的配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/03/02/Spring%E6%A6%82%E8%BF%B0/">Spring概述</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/18/Hive%E8%B0%83%E4%BC%98/">Hive调优</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/17/Hive%E6%9F%A5%E8%AF%A2/">Hive查询</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/16/Hive%E6%95%B0%E6%8D%AE%E6%8D%AE%E7%B1%BB%E5%9E%8B/">Hive数据据类型 DDL DML</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/15/Kafka-API%E5%AE%9E%E6%88%98/">KafkaAPI实战</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/11/Git%E4%BD%BF%E7%94%A8/">Git使用</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/10/Oozie/">Oozie</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/08/Sqoop/">Sqoop</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/07/Flume%E6%A1%88%E4%BE%8BGanglia%E7%9B%91%E6%8E%A7/">Flume案例Ganglia监控</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/06/ZooKeeper%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8CAPI/">ZooKeeper的安装和API</a></li><li class="post-list-item"><a class="post-list-link" href="/2019/01/05/Zookeeper%E5%85%A5%E9%97%A8/">Zookeeper入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/HBase%E4%BC%98%E5%8C%96/">HBase优化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/31/HBase%E7%9A%84Shell%E5%91%BD%E4%BB%A4%E5%92%8CJavaAPI/">HBase的Shell命令和JavaAPI</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/30/HBase%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AF%BB%E5%86%99%E5%8E%9F%E7%90%86/">HBase数据模型和读写原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/30/Hbase%E5%8E%9F%E7%90%86%E5%92%8C%E5%AE%89%E8%A3%85/">HBase原理和安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/MapReduce%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B2/">MapReduce高级编程2</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/28/MapReduce%E9%AB%98%E7%BA%A7%E7%BC%96%E7%A8%8B/">MapReduce高级编程</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/25/MapReduce%E7%BC%96%E7%A8%8B%E5%88%A8%E6%9E%90/">MapReduce源码刨析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/24/MapReduce%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6/">MapReduce的工作机制</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/22/Mapreduce/">MapReduce入门和优化方案</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Hadoop%E7%9A%84RPC/">Hadoop的RPC工作原理</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/20/Hadoop%E7%9A%84IO%E6%93%8D%E4%BD%9C/">Hadoop的I/O操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/19/Yarn/">Yarn</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/19/HDFS%E9%AB%98%E7%BA%A7%E5%8A%9F%E8%83%BD/">HDFS高级功能</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/18/HDFS%E7%9A%84%E6%93%8D%E4%BD%9CSHELL%E5%92%8CAPI/">HDFS的操作SHELL和API</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/17/Hadoop%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS/">Hadoop分布式文件系统HDFS</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/17/Hadoop%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%89%E8%A3%85/">Hadoop简介与分布式安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E7%9A%84JavaAPI/">Elasticsearch的JavaAPI</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%BA%E5%88%B6%E6%8E%A2%E7%A9%B6/">Elasticsearch分布式机制探究</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E8%81%9A%E5%90%88%E5%88%86%E6%9E%90/">Elasticsearch聚合分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/16/Elasticsearch%E5%A2%9E%E5%88%A0%E6%94%B9%E6%9F%A5/">Elasticsearch增删改查</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/15/ElasticSearch%E7%B4%A2%E5%BC%95/">ElasticSearch索引</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/15/Elasticsearch%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85/">Elasticsearch简介与安装</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/12/MongoDB%E8%BF%9B%E9%98%B6/">MongoDB进阶</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/11/MongoDB%E8%81%9A%E5%90%88/">MongoDB聚合</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/10/MongoDB%E7%B4%A2%E5%BC%95/">MongoDB索引</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/09/MongoDB%E6%9F%A5%E8%AF%A2/">MongoDB查询</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/09/MongoDB%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/">MongoDB基础命令</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/08/MongoDB%E5%85%A5%E9%97%A8/">MongoDB入门</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E7%9A%84%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F/">Redis的集群模式</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6/">Redis主从复制</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E6%8C%81%E4%B9%85%E5%8C%96/">Redis持久化</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/07/Redis%E4%BA%8B%E5%8A%A1/">Redis事务</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/04/memcached/">Memcached</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/03/Redis/">Redis</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/12/03/Hive/">Hive</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/Flume/">Flume架构</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/16/kafka%E5%B7%A5%E4%BD%9C%E6%B5%81%E5%88%86%E6%9E%90/">Kafka深度解析</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E5%91%BD%E4%BB%A4%E6%93%8D%E4%BD%9C/">Kafka命令操作</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E4%BB%8B%E7%BB%8D%E4%B8%8E%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">Kafka与消息队列</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/14/Kafka%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE/">Kafka和的安装与配置</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/12/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/">Java虚拟机------JVM分析工具</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/12/Java%E8%99%9A%E6%8B%9F%E6%9C%BA-JVM%E5%B8%B8%E8%A7%81%E5%8F%82%E6%95%B0/">Java虚拟机--------JVM常见参数</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/11/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E5%99%A8/">Java虚拟机------垃圾收集器</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F/">Java虚拟机------JVM内存区域</a></li><li class="post-list-item"><a class="post-list-link" href="/2018/11/10/Java%E8%99%9A%E6%8B%9F%E6%9C%BA------JVM%E4%BB%8B%E7%BB%8D/">Java虚拟机------JVM介绍</a></li></ul>
    
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>

    <script>
        $(".post-list").addClass("toc-article");
        // $(".post-list-item a").attr("target","_blank");
        $("#post-nav-button > a:nth-child(2)").click(function() {
            $(".fa-bars, .fa-times").toggle();
            $(".post-list").toggle(300);
            if ($(".toc").length > 0) {
                $("#toc, #tocButton").toggle(200, function() {
                    if ($(".switch-area").is(":visible")) {
                        $("#toc, .switch-btn, .switch-area").toggle();
                        $("#tocButton").attr("value", valueHide);
                        }
                    })
            }
            else {
                $(".switch-btn, .switch-area").fadeToggle(300);
            }
        })
    </script>




    <script>
        
    </script>

</div>
      <footer id="footer">

    <div class="outer">
        <div id="footer-info">
            <div class="footer-left">
                &copy; 2020 清风笑丶
            </div>
            <div class="footer-right">
                <a href="http://beian.miit.gov.cn" target="_blank"> 豫ICP备18042969号-1	</a><a href="" target="_blank"></a>
                <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            </div>
        </div>
        
            <div class="visit">
                
                    <span id="busuanzi_container_site_pv" style='display:none'>
                        <span id="site-visit" >极客到访数: 
                            <span id="busuanzi_value_site_uv"></span>
                        </span>
                    </span>
                
                
                    <span>, </span>
                
                
                    <span id="busuanzi_container_page_pv" style='display:none'>
                        <span id="page-visit">本页阅读量: 
                            <span id="busuanzi_value_page_pv"></span>
                        </span>
                    </span>
                
            </div>
        
    </div>
</footer>

    </div>
    
<script src="https://7.url.cn/edu/jslib/comb/require-2.1.6,jquery-1.9.1.min.js"></script>


<script src="/js/main.js"></script>


    <script>
        $(document).ready(function() {
            var backgroundnum = 2;
            var backgroundimg = "url(/background/bg-x.jpg)".replace(/x/gi, Math.ceil(Math.random() * backgroundnum));
            $("#mobile-nav").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
            $(".left-col").css({"background-image": backgroundimg,"background-size": "cover","background-position": "center"});
        })
    </script>


<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-129731340-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->



	<script>
	var _hmt = _hmt || [];
	(function() {
	  var hm = document.createElement("script");
	  hm.src = "//hm.baidu.com/hm.js?a138f5cac94c7795df86f17cea34efc4";
	  var s = document.getElementsByTagName("script")[0]; 
	  s.parentNode.insertBefore(hm, s);
	})();
	</script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';                 
    }       
});
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


<div class="scroll" id="scroll">
    <a href="#"><i class="fa fa-arrow-up"></i></a>
    <a href="#comments"><i class="fa fa-comments-o"></i></a>
    <a href="#footer"><i class="fa fa-arrow-down"></i></a>
</div>
<script>
    $(document).ready(function() {
        if ($("#comments").length < 1) {
            $("#scroll > a:nth-child(2)").hide();
        };
    })
</script>

<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

  <script language="javascript">
    $(function() {
        $("a[title]").each(function() {
            var a = $(this);
            var title = a.attr('title');
            if (title == undefined || title == "") return;
            a.data('title', title).removeAttr('title').hover(
            function() {
                var offset = a.offset();
                $("<div id=\"anchortitlecontainer\"></div>").appendTo($("body")).html(title).css({
                    top: offset.top - a.outerHeight() - 15,
                    left: offset.left + a.outerWidth()/2 + 1
                }).fadeIn(function() {
                    var pop = $(this);
                    setTimeout(function() {
                        pop.remove();
                    }, pop.text().length * 800);
                });
            }, function() {
                $("#anchortitlecontainer").remove();
            });
        });
    });
</script>


  </div>
</body>
</html>